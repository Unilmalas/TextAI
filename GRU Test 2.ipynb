{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 576 characters, 7 unique.\n",
      "----\n",
      "a e cbdfd cccda bfafccfcebfddbedaaefcdfcfdefeadb bae fbb abf ffcbad  c aaa   dffbadcf ebadadcbcbdbbef abefffbfa fad efabbeaacbeacfdffedffc bab dbffcadfbcacbacbbcebff ac bedddabdfca d bcf b bdaa dddf dddf decbe ccd abdfd ffcfcfeff abfbafcfadc da   fefdcbfbcbfabd dcaacbcbf  bddbfbedeffedba accef  deeffbbaddf cdaaf c cfbf abbdbddbcd edec bbb e dddf f cbcba ceba  a a  eacecaaccbeef fbfdfebcbbacdac a f ad fefcbcfcafb abfccbfffbfedaaafbccee bee ab b cad ffbaac dbbedfbfebdbf aabecc b afafead edecf beba ecfebbcfe dfdac bffabdfaafcfacbbccbfeebdeebee fccccdcfdbbeba ff ba f f fcbbebdccfcee eaf b cebabe a dbbebeeafababccefad cbbce feafcfacbbdfeecfcfeae bace cfadd cd fc afd bc bcadeaddfeecaf c ffebaa c bde cdecfa caeabeecdb  c  ffa faad afdcbe ffd dfcb ef e fdcbfaefec ccabbacac fecef dffd d afbdafebe bad bacbd daaefbe deabf  dcefbbcdfdbaf efdbabddffefabbcdb bbfffdbce e ee dcaaecfff faceaca  b bdb  aafaedccfbecbfd bedfaecdfffbfdc eceeebeccd  bf b bbfeb edaf fbcedfd fed baedffefd  beff  fbcfbdbecaaacd\n",
      "----\n",
      "iter 0, loss: 9.732009, smooth loss: 9.729553\n",
      "----\n",
      "ffffaaaaaaabbaabccccccccdcdddde  e eddf faaaaaaaabbcdcceddcdcdff  feafd a aaab abccccfced dddeefffeaaeaafbbbbbccccccdceedeeeedff fafaaaabbab ccdcdcccddef d  aafaafbbbcb bcccccdcededeefbeef ff abbbbbbbbbbbcccddfed e effff aaa aabbbbb  bccccddeeee faffaaabbbcbcccdecd deeeeeeff aaaaaabbdbcccd  cdd eedeeefff f aaaabbcbcccccccccc bddcdeeed efff aaabbbcbcbbcbccdddd dddddfde faaafabbcabcbccccbcc c dee efdfff  aaaabbabcdbccdcd eeeeeeeefaaaa bbbc ccccccdd ddeeefeeefd aa abbbcdccc cedd eeffe eaaafeabbcbcdbccdddcd dddfceefaf aaaaabcbcccccc bdcdd  f  faffafbabbb cccccccdddd dddddeeea da ebabdb bcdbd dd efcfea  afbbbbbccfccbedadd  fef e  aabbaccbccccccdcd ddeeeeefeeefefaaaaaa  bbbbc bccdcc dddf  fffefef abacbbc d dcd cee eeeffeeafeaaaa bbbbcbbccccdcdcdcdeddedefdefeffffaa aaabbbbbbcc cdccedd eefd faaafa abbbbcccdcd  ddcddeefffeeff aaf bbbccd ccddd ddfefffeef aacabbbbbccc dcd dcceee ffa affaaacbabbacdc  ccdedefee efafeaaabbabbcbbccdccddddedeeeeeef fe abbdbbccccccbcdcddded fdeeeef ffcaaaeabbbbdbcbcd bd\n",
      "----\n",
      "iter 100, loss: 5.008057, smooth loss: 9.428379\n",
      "----\n",
      "ffffaaa  abbcbdedddedeeefeeee efaaffaf aaaaaacbccddddccdeeff fff aa abb b bbcccedede e ff ffafaaaa abbabccc dddddded eee fff afeaafa bbabb cccccddddd eeeeeeeffefffaaaafeaacebbbbbbbddddede deeefffffaffafaafaadafccccedd eeeee  faaaffaf aaabbbbccccccc d e eedffafff aafaafbbbbb ccc dfeefeeeff fffffaa afbbbbbbccdd  fdeefeeeefeafbbbbbbcccdcdddeddeeeeeeeeef feff aabbcb ccccd   deeeeeefefaffaabab cccbcd deeddfeefeafffa afaaaaabbbbbbcbccccdd d efeeedff fff aa bbbbb bdddedd efeee ef faaafaaafbb bb dfcdcdeeeeefefeefe aaffffaba bbbccccdcddd  eeeeff ffffafaffbbbbcb ccce eeeedef a faabecc ccddcddf efeffffaffaffaa af ba cccd deee   fafafaa aabbcc cdcdd ee eefeffefffffffaf aaaaaaa bcccdcddeed efef aafa aaaabbbbbbaccccddddddd eeeeefefe ffafa aabbbbbb  cdddddeeeeee dfaffebbcabccdcc dedee eeeee aaaffaeaff  babcdcddddddddceeefeca aaffaabababacccc ccddd e  edfffffffffaffa abbbbbabccccd  deedfdefeeeff afbab  dddcdee e fefedafafbff c  de eeeeeeeff  a aacbbcc cddddedee effdffeffafafaaa aabbbbdccccddddddfee fee\n",
      "----\n",
      "iter 200, loss: 4.166070, smooth loss: 8.925334\n",
      "----\n",
      "aaaffabb bd  dddddeeeffeffff f bbbbbbbbbccccbcdded e feffffffaaaaa bbbb  ccccccdddd eefeeefffffaafaaaaabbbb ccc dc dedeeeffffffffafaaabbb cbccdddddddeed ffaaaaaf bbbbbbdcd  ddeeee ff aaaa a bc cc cdddce  feaaaaaaabbbabbbcccccdcedddd eee ffa aabb  cccccccbeeddeeeeefffff aaaaa cb ccbdc  ee afabaabbbbabcddcddddd eeffe aafaaaaa bbbccc dddde  e  aaafbbacecccccdd deeeeeeefffffaa aa bbbbaccccc  ddd eee e faaaafbbbbcbcccedcddeeeeeeeffafeaaaaabbbabbcccccccd dddd dfeaf  a dbbcccccccc dddddcdefeee faeafaaabbb ccccccccc  ddeeee effffaefaabbbbbbbbbccccc c ddddcd eee  aaaaababbbabbcbcccdcb eee  aaaabb b bddcddd   fffeebbbbbbccccbddc fdfee fff aaaa  b  c cccd ddeeeee f aaaafaacccccccccdddddeeeeeeee ffa  bbb accdbcdddd ee fffffafaafbbb bbccc  ddddd ceff ff aaaaaa bbc cbcdcddc eeefeffff af bbabbbcccccccccd ddddeeeefe fffaaaaaaaaabbbbb ccccdccdddddd eeeeffafff aaaaaabbbcccccdddcedeeeefffaffffaf abbbbbacdcccdcdd e eeffeffffffffffaf bbbb bcbccccccc c de eefefffffaaa bcb  cc ddeeeeee f aaaababbabbcc cddd ee\n",
      "----\n",
      "iter 300, loss: 3.596053, smooth loss: 8.407121\n",
      "----\n",
      "afabcccc ddddd eeeeeeeffeff aa babbbccccc  dcdeeeeee fff aa bb  cc  dd cee f  aaa abaccc ddddd eeeeeedffeffff aaafbbbbb ccccdddddddeeeeeeeeeeeffffff faabbbbbbbccccccc ddd e e ff afa bbbbbbbbb c ddddddeeeeeefeefea aaaab bbcc ddde eeeee ffff aaaaaabbbbcccd ddddddddeef fff aaa bbbbbbbcccc ddddd eeeee ffaaaaafb bbb cccc b eeeeeeeeffafaa aa bbbb bcddd ce eeeff faf aabbbabccccddd efefef fff aaaababbbb cc ddddddeeeeeeee f aaaa babb  dddd d   a  ab dcdddddddd eeefeef aafaab bbb cccc  e eeeeee f fbaaaabdcdcddddeee effffaffffaaaaaaaaaaa bbbccceddddeeeeeeedeffff aaaaaaaaabbbbbbb d ddddd eeefee ffaaaa abbbb ccceddd eeee ffffff  aa  ccdddddddeeeeeeeef aaaaaaaaaafaa bbd  ddeeeeeefeffff aaaa bbbbbbbaccbd d   ffe aaaaaaaaabbbbccccbcdddd eeeeee afffffafaaaf  bbbccccccdc deeeeeeefffe a aabb c dd ddeee eff f  bb b  dddd fe eadab bad cdddd e edef affaa ac ccccccddd eeeeeeefef afaaaaaabbbb cccccdddd fee fffffffaaaab bbaccccccdedddefeeefeffffaffffaffaaaa bbbbbbbbccccccdddddcdedf fffffffafaaaafbabbbbccc d ddd\n",
      "----\n",
      "iter 400, loss: 3.244737, smooth loss: 7.901348\n",
      "----\n",
      "aaabbb  cc dd deeee  aafaaaaaabbbbbbbbbbb c ddddeee eff  aafaaa ccc dddedeeeeeeee afaaaaaabbb cc dddddddee fffff aa bbbabbbccc cdddeeee  fffffffffa aaaabbbbb bcc dddd e ea babbbbbb cdd eeeeeeee fffffffaaaaaabbbbbbbbbbbb ccccbeddddee  afff aaab  ccccbdddddeeeeeeff ffffff fbbabc c  ddddeeeeeeffff aa abbbbbcccb dddeeeeeffeff aafaa bbbbbbbbbb cccccccd d eeeee eaaa bb cb ddd e   fafabb    cddddd e fffaf  bbbbbbbbbabd dde eeee ffffff a bbbbbbbbbbb cccccdddddd eeeeeefffffb abbbbab cccbddddddcee fff  aaaabbbb bccc  dddeedeef afaaaaaa abbdcccc d edee fff a abba  cccccdddde ee ffeafaafaaabbbbbbb ccccccc  dddeeeeede  aaaaabbb  bcc  dede ffeefaaaaaaaab bbbb bddddddd d  ffafffafaab bb cccccccccccdddeeeeeefe aaaaabbbbbbb add d efeeeaff aaaaaaa  bbbcccc dd eeeeeefafff faaaaa bbbbb  ccccddddddd d ffffffaafaabbbbb  ccc dd dde eeffaff aaaaaaaaabbbbc cccbd feeeef fff aaaabbbb bcc ddeeade ff aafa accccccccc eeeeeeeee fff aaaaaaaaaaabbbb dc eeeeeeeffffa aaaa bbbbbbbbbbbbbccccccc dd  eee ffeff aaaaab bbbb bc\n",
      "----\n",
      "iter 500, loss: 2.595299, smooth loss: 7.418433\n",
      "----\n",
      "bbbcccccccdddd dd eeee ffffffff aaaaa acccccccdddd  feee  aaaa bbbbabccc d eee fffffffffffeaaaaaa bbb cccddcddddfd effffffeaaaacc ccccddddddddd ee fe a bbc cc dcddddeeeee fffff  aabbbccccccddddddde e faffff aa bbbbbb ccccc dddddeeeeeee fff baabccccc dddddd dee effa aaaa  b cd d eeee ef aaaaa bbbb c dddd e e   aaa abcc dddd fee ff abbbbbb cccccddddd ee feaffaf  bbb bcccddddd  eeef  f  bbbccccd   ee f a aabbbb ccccccddddddddddeeef fe aacbbbbcccccc d deeefe dcaaaaac ccccddddddd eee  f  bbbbbd cdded e effeafaaaaaa bbb cccccccdcdd  eff ff aaaff cccddddddeee eeeffffffffffaf aa cccccc dddd  eee af aaaaaaa b ddd d eeeeeefdaaaaaaa bb cccccdccde eeefffefffaffaaaab ccccccbd ed eee ff aa bbbbb ccccccdcdd  eeeee fea ababccccdddddddddeeeeeeeff aaf babac ddddd  ee   aaa bbadcc ddddd eeeeefffeffffffbbb  ccc dddddddd feee  cbbb cccccccddddd eeeeeee fa a bbbbbbbcd ee eeeeeffffff aaaaaabb cccccccdddddddd eee fffefa acbbccc dddde eedffff eaaaaa bbec dd  deeeeeff afabbbbccccccdddd eeeedeeefffffffffffffffaaa\n",
      "----\n",
      "iter 600, loss: 2.639489, smooth loss: 6.966076\n",
      "----\n",
      "bbbbbb b d eeeeedeee aaaf b ccccce edeeeeeee ffaaaaaafbabbb b  eeeeeee fffff af bbbbb ddddddd eeeeeefffffffff a bbbbbbc ccc ddddedeeee ffbaaaaafaac  ddd eee ff aaaaabbbbb ec edd eee f af  bcbccdcddddd eeee fffffff aafb b cc  ddeeeeeeee af aaaaaa  c dddddd  ee f aaaa bbb bb cd deeeeefeffff fbab cccccd  eee  ffffff aaaa bbbba c c dddeee efff ab bbba ccc deeeeee  fff aa ba  cc ddddd eeedffffa aa bbbb b dd  eeeeefefe faab bccddcd e e ff aaaaaaaa ccccccdddddddd eeeff a aabbfb cccd dddddeeeeffffeaf a bbbbbbbb ccc ddeeee  ffffff aaf bb b  d  edee ffff  aafabcc  ddde  faffffff aa bbabbccc d d e dff aaaaaa bbbbbbb ccc ddd  eefe d aaa bbbbbbbaccccdddddd eeeeeffffe aaaaa bbbbbbbbb cccd deeeeee ffefff ab bbbbb ccccccc ddd eeeed  babbbbbbb ccc ddddddd eeee fffff aaaaaaaaaa b dd eeeeee fffffff aaaaaaaabbb cc dd ed f ff aaaafacccccccdddd eeeeeeeeffffffff aa bbbbbbbbbc  d eeeee  aaaaabbbbbb  cccccddddddddeee  ffaaaaaaaaa bb cccccddcdd eeee  a acaccc d dee eeeeafaaaaa bb bb ddddddddeeeeefefffa a bbbbbbb\n",
      "----\n",
      "iter 700, loss: 3.163946, smooth loss: 6.542451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "bbbbb cccc dcdd eeeeeff  ab bb bc d  eeeeeeffffffffff aa bbbbbbbbbb cccccccd d ee e aaaaaacbbcaccdddddceceeeffee a bbcccccccccd eeeeeeef ffaaaaaaa bbbbb ccccadddddddeeee ffffff aaaaa bbbbb a cdddcdeedeeeffffffa aaaacbbb  dddddd e eecbbbbbbbbbbcddddd  e fffaeaaaaac bdddddddeeedffff  a bbbbbccccc dddddd eeee f aaaaaabbbbbbb cccccc ddddeeee f a   ccccccccdddddeeeee   aabbbb ccccc dddde eee f aaabbbbb ccd  eeee fffffff aaa bbb  ccccccccddd e e  aaa bbbbbbbb cccc dd eeeeeefffffffaaaaaaa bbb cc c ddde ef ffff aa bbbbbbbb cd  deeefdf aaaaaa bb  b dddddee e fb bbbbbbbbbbba cccccddddd ee ffffffaf afbbbbb  ccccddd  eeef aaaaa bbbbbbbb cdd  eeeee  aaaaaabbb  ccccc  e eeeeffffff aaaaaba d ddd ee effeaaaaaaac cccccdddcd eeedeffff aaabab bdd  eeeeeea f bbbbbb  c   eee fe a bbbbb b  ddddddeeeeed afa b cccccccc ddddd ee fdf aa bbbbbbbbc cccedddddfeee feaaaaaaa bb ccccc  dd eeee fffaffff aaaaa cc ddcfdd e  a bab d dddddeee fffffffffffffffff aaa bbbbb dcccddc  eee ff aaaaa bbbbbbbbcccccccd e  efffffffafa\n",
      "----\n",
      "iter 800, loss: 2.307776, smooth loss: 6.146969\n",
      "----\n",
      "bbbbbbb  d deeee   aaaaaabbbbbbdddddd eeeeeee ff a a bbb ccdddddddd eeee f aaaa bbbbbbcc  dddcd eee ffff aaaa bbb  ccccdddd eeee feaa bbbbbbbb ccccdddddddd e  f aaaaaa b  c dddddcddd ee ffffffffffffffafff aaabbbbb ccccccddddddd eeee ff aa bbbbbbccc dddd ddeeeeeee fffaa b bcbc dddddd eeeee f f aa bcccccccd  eeeeeeee eababc cddd d e eeff aaaaaaccccccd  eeeeeeefffaf aaaa bbbb ccc ddddddddd eeeefffffffafff  bbbfbccc  dddceeeeeefeffffff aafabbbb cccc dddeddeeeeeee ffff abbb cc dd ddeeeeeeffaff aaaaa b cccccddddddd eee f ffff aaa bbb cccc ddedeeeeeeefefffff aaabbbbbbb cccc ddddddcdeeeeee ffffaf aa bbbbb ccccc ddd de e ffffffffff aaaaaabbbbddddddeeedeeeee ffaa  cb  dd e eee ffffafafbba cccccd dddeeeeee f aaaaabbbbbbbddd ddd f e aaaab bb dddddddddeeef ff aaaa bab ccccc   eeeeeeffaffffffaaaaaabbbbbbb c dddddddeeeeeeeef fff aaaaad ccccdd d e effffffff aaafbbbb ccccccdddddddddd ee   aaa bbbbb cccc dde  eeefff a ababb d e eeeeeeeea  bbbbbbb ccc cdddfde  aaaaaaa  b ccccbddddddd eeeeeeef faaaa bb b c\n",
      "----\n",
      "iter 900, loss: 2.187125, smooth loss: 5.779026\n",
      "----\n",
      "b ba ddddd eeeee ff aaaaaa bbbbbbbbdcc dd  eee ffffff aaafbbb cccccc ddd eeeeeefeff aaaaaaaa bbb bc dddedeeeeee ff aaaaaf bbb c ddddd eeeeefefff afbbbbbbbb ccccc  edeee  ffffff aaaaaa bbbb ccc dddd eeeeeee ff aaaaaa bbbbbbb ccccc ddddd eeeeeee ff aaaaa bbbbbbbb ccccdddddddeeeee fffffafffafaaaaa b ccccc ddddd eeeee ffffffff aa bbbbbb b dd dd e fff aaaafa  cccc d d eeeeeee fa aaaafcccccccd  eeeeeeffffffaaaaaaaa bbbbbbb ccccccde eeee efffffff aa bbbbbb b ddddddd eeeeee f aaaaa bb cccccccdcdddeeeeeee   afbbb cc dddddeede   aaaaaaa cbddd  eeee ffefaaf bbbb cccccc dd eeef  a aaaabbbb cccccdd de   fff aaaaaaabbbbb ccccccdddddedeeeeeeeff bfa bbbbb ccccc  ddd eeeeefffaff  abbbbbbb cc dd eeeedee f aaaaa bfbbbbc d  eeeeeeefffffff aaacbbbbbbb cc ddddeeeeeeee  b bbb b  d  eeeeee ff afaaaaa bb cccc edd eeefeffffff aaaaa b bb ccccdddddd eeee ffffffffab bbbb ccbd eeeebff aaaaaaa bbbbbbb cccccddedeeee  ff aaaaa bbb bbeddddddfee ffffffffff aaaaaebbb ccddd d eeeeefefff aaaabbbb b  ddd  eee fff aaaaa bbbbb\n",
      "----\n",
      "iter 1000, loss: 2.620030, smooth loss: 5.439533\n"
     ]
    }
   ],
   "source": [
    "# GRU with Minimal Gated Units\n",
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "# Seed random\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read data and setup maps for integer encoding and decoding.\n",
    "data = open('input.txt', 'r').read()\n",
    "chars = sorted(list(set(data))) # Sort makes model predictable (if seeded).\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) } # dict from enumerate: char and index\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) } # dict from enumerate: index and char\n",
    "\n",
    "# Activation functions\n",
    "# NOTE: Derivatives are calculated using outcomes of their primitives (which are already calculated during forward prop).\n",
    "def sigmoid(input, deriv=False):\n",
    "    if deriv:\n",
    "        return input*(1-input) # sig' = sig(1-sig) - only if claculated in forward prop already!\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def tanh(input, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - input ** 2 # tanh'=1-tanh**2\n",
    "    else:\n",
    "        return np.tanh(input)\n",
    "\n",
    "# Derivative is directly calculated in backprop (in combination with cross-entropy loss function).\n",
    "def softmax(input):\n",
    "    # Subtraction of max value improves numerical stability.\n",
    "    e_input = np.exp(input - np.max(input))\n",
    "    return e_input / e_input.sum()\n",
    "\n",
    "# Hyper parameters\n",
    "N, h_size, o_size = vocab_size, vocab_size, vocab_size # Hidden size is set to vocab_size, assuming that level of abstractness is approximately proportional to vocab_size (but can be set to any other value).\n",
    "seq_length = 5 # Longer sequence lengths allow for lengthier latent dependencies to be trained.\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# Model parameter initialization\n",
    "#if os.path.isfile(\"gru_weights.data\"):\n",
    "    #initfile = open(\"gru_weights.data\", \"rb\")\n",
    "    #init_data = np.load(initfile)\n",
    "    #Wy = init_data['Wy']\n",
    "    #Wh = init_data['Wh']\n",
    "    #Wr = init_data['Wr']\n",
    "    #Wz = init_data['Wz']\n",
    "    #Uh = init_data['Uh']\n",
    "    #Ur = init_data['Ur']\n",
    "    #Uz = init_data['Uz']\n",
    "    #by = init_data['by']\n",
    "    #bh = init_data['bh']\n",
    "    #br = init_data['br']\n",
    "    #bz = init_data['bz']\n",
    "    #initfile.close()\n",
    "#else:\n",
    "Wz = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Uz = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "bz = np.zeros((h_size, 1))\n",
    "\n",
    "Wr = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Ur = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "br = np.zeros((h_size, 1))\n",
    "\n",
    "Wh = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Uh = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "bh = np.zeros((h_size, 1))\n",
    "\n",
    "Wy = np.random.rand(o_size, h_size) * 0.1 - 0.05\n",
    "by = np.zeros((o_size, 1))\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    # Initialize variables\n",
    "    x, z, r, h_hat, h, y, p = {}, {}, {}, {}, {-1: hprev}, {}, {} # Dictionaries contain variables for each timestep.\n",
    "    sequence_loss = 0\n",
    "\n",
    "    # Forward prop\n",
    "    for t in range(len(inputs)):\n",
    "        # Set up one-hot encoded input\n",
    "        x[t] = np.zeros((vocab_size, 1)) # for each time step t a one-hot over vocabulary size - init with all 0\n",
    "        x[t][inputs[t]] = 1 # set t'th input to one (current word in one-hot)\n",
    "        \n",
    "        # Calculate update and reset gates\n",
    "        z[t] = sigmoid(np.dot(Wz, x[t]) + np.dot(Uz, h[t-1]) + bz) # update gate\n",
    "        r[t] = sigmoid(np.dot(Wr, x[t]) + np.dot(Ur, h[t-1]) + br) # reset gate\n",
    "        \n",
    "        # Calculate hidden units\n",
    "        h_hat[t] = tanh(np.dot(Wh, x[t]) + np.dot(Uh, np.multiply(r[t], h[t-1])) + bh)\n",
    "        h[t] = np.multiply(z[t], h[t-1]) + np.multiply((1 - z[t]), h_hat[t]) # sometimes denoted s[t]\n",
    "        \n",
    "        # Regular output unit\n",
    "        y[t] = np.dot(Wy, h[t]) + by\n",
    "        \n",
    "        # Probability distribution\n",
    "        p[t] = softmax(y[t])\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        # targets[t]'s entry of p[t]: since the output is 0 or 1 only one entry contributes (and yj*log(pj) becomes just log(pj))\n",
    "        #loss = -np.sum(np.log(p[t][targets[t]])) # dict p: or time t (=one-hot position), whats the corresponsing target value?\n",
    "        loss = -np.log(p[t][targets[t]])\n",
    "        sequence_loss += loss\n",
    "\n",
    "    # Parameter gradient initialization\n",
    "    dWy, dWh, dWr, dWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n",
    "    dUh, dUr, dUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n",
    "    dby, dbh, dbr, dbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n",
    "    dhnext = np.zeros_like(h[0])\n",
    "    \n",
    "    # Backward prop\n",
    "    for t in reversed(range(len(inputs))): # index t counting down\n",
    "        # âˆ‚loss/âˆ‚y\n",
    "        dy = np.copy(p[t]) # copy output\n",
    "        dy[targets[t]] -= 1 # the current target (truth) is 1 for the current t (an 0 for all other t's)\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wy and âˆ‚loss/âˆ‚by\n",
    "        dWy += np.dot(dy, h[t].T) # weight updates: Wy -> Wy - etha * d loss / dWy -> dWy += etha * d loss / dWy\n",
    "        dby += dy # weight for bias is just 1\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        dh = np.dot(Wy.T, dy) + dhnext\n",
    "        dh_hat = np.multiply(dh, (1 - z[t]))\n",
    "        dh_hat_l = dh_hat * tanh(h_hat[t], deriv=True) # = Wy*dy*(1-zt) * (1-tanh**2(h_hat))\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wh, âˆ‚loss/âˆ‚Uh and âˆ‚loss/âˆ‚bh\n",
    "        dWh += np.dot(dh_hat_l, x[t].T) # Wy*dy*(1-zt) * (1-tanh**2(h_hat)) * xt\n",
    "        dUh += np.dot(dh_hat_l, np.multiply(r[t], h[t-1]).T) # Wy*dy*(1-zt) * (1-tanh**2(h_hat)) * (rt*ht-1)\n",
    "        dbh += dh_hat_l # weight for bias is just 1\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        drhp = np.dot(Uh.T, dh_hat_l) # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat))\n",
    "        dr = np.multiply(drhp, h[t-1]) # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat)) * ht-1\n",
    "        dr_l = dr * sigmoid(r[t], deriv=True) # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat))*ht-1*sig(rt)*(1-sig(rt))\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wr, âˆ‚loss/âˆ‚Ur and âˆ‚loss/âˆ‚br\n",
    "        dWr += np.dot(dr_l, x[t].T) # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat))*sig(rt)*(1-sig(rt))*xt\n",
    "        dUr += np.dot(dr_l, h[t-1].T) # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat))*sig(rt)*(1-sig(rt))*ht-1\n",
    "        dbr += dr_l # # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat))*sig(rt)*(1-sig(rt))\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        dz = np.multiply(dh, h[t-1] - h_hat[t]) # dh * (ht-1 - h_hatt)\n",
    "        dz_l = dz * sigmoid(z[t], deriv=True) # dz * sig(zt)*(1-sig(zt))\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wz, âˆ‚loss/âˆ‚Uz and âˆ‚loss/âˆ‚bz\n",
    "        dWz += np.dot(dz_l, x[t].T) # dz * sig(zt)*(1-sig(zt))*xt\n",
    "        dUz += np.dot(dz_l, h[t-1].T) # # dz * sig(zt)*(1-sig(zt))*ht-1\n",
    "        dbz += dz_l # # dz * sig(zt)*(1-sig(zt))\n",
    "        \n",
    "        # All influences of previous layer to loss\n",
    "        dh_fz_inner = np.dot(Uz.T, dz_l) # Uz * dz * sig(zt)*(1-sig(zt))\n",
    "        dh_fz = np.multiply(dh, z[t]) # dh * zt\n",
    "        dh_fhh = np.multiply(drhp, r[t]) # Uh * Wy*dy*(1-zt) * (1-tanh**2(Wy*dy*(1-zt))) * rt\n",
    "        dh_fr = np.dot(Ur.T, dr_l) # Ur * Uh*Wy*dy*(1-zt)*ht-1*sig(rt)*(1-sig(rt))\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚hð‘¡â‚‹â‚\n",
    "        dhnext = dh_fz_inner + dh_fz + dh_fhh + dh_fr\n",
    "\n",
    "    return sequence_loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, h[len(inputs) - 1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    # Initialize first word of sample ('seed') as one-hot encoded vector.\n",
    "    x = np.zeros((vocab_size, 1)) # np.zeros(shape) -> (vocab_size, 1) = (rows, cols)\n",
    "    x[seed_ix] = 1\n",
    "    ixes = [seed_ix]\n",
    "    \n",
    "    for t in range(n):\n",
    "        # Calculate update and reset gates\n",
    "        z = sigmoid(np.dot(Wz, x) + np.dot(Uz, h) + bz)\n",
    "        r = sigmoid(np.dot(Wr, x) + np.dot(Ur, h) + br)\n",
    "        \n",
    "        # Calculate hidden units\n",
    "        h_hat = tanh(np.dot(Wh, x) + np.dot(Uh, np.multiply(r, h)) + bh)\n",
    "        h = np.multiply(z, h) + np.multiply((1 - z), h_hat)\n",
    "        \n",
    "        # Regular output unit\n",
    "        y = np.dot(Wy, h) + by\n",
    "        \n",
    "        # Probability distribution\n",
    "        p = softmax(y)\n",
    "\n",
    "        # Choose next char according to the distribution\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel()) # ravel returns flattened array, P are the probabilities for choice\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    \n",
    "    return ixes\n",
    "\n",
    "# Initialize sampling parameters and memory gradients (for adagrad)\n",
    "n, p = 0, 0\n",
    "mdWy, mdWh, mdWr, mdWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n",
    "mdUh, mdUr, mdUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n",
    "mdby, mdbh, mdbr, mdbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length\n",
    "\n",
    "print_interval = 100\n",
    "n_runs = 1000\n",
    "\n",
    "while True:\n",
    "    # Reset memory if appropriate\n",
    "    if p + seq_length + 1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((h_size, 1))\n",
    "        p = 0 # current position\n",
    "    \n",
    "    # Get input and target sequence - each an index list of characters\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]] # current position to sequence length\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]] # current position+1 to sequence length+1 (next state)\n",
    "\n",
    "    # Occasionally sample from model and print result\n",
    "    if n % print_interval == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 1000)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n%s\\n----' % (txt, ))\n",
    "\n",
    "    # Get gradients for current model based on input and target sequences\n",
    "    loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    # Occasionally print loss information\n",
    "    if n % print_interval == 0:\n",
    "        print('iter %d, loss: %f, smooth loss: %f' % (n, loss, smooth_loss))\n",
    "\n",
    "    # Update model with adagrad (stochastic) gradient descent\n",
    "    for param, dparam, mem in zip([Wy,  Wh,  Wr,  Wz,  Uh,  Ur,  Uz,  by,  bh,  br,  bz], # zip combines iterables\n",
    "                                  [dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz],\n",
    "                                  [mdWy,mdWh,mdWr,mdWz,mdUh,mdUr,mdUz,mdby,mdbh,mdbr,mdbz]):\n",
    "        np.clip(dparam, -5, 5, out=dparam) # limit values of array - here the changes (dparam has the dWy,...)\n",
    "        mem += dparam * dparam # store the squared change (gradient)\n",
    "        # now actually change the model parameters\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # Small added term for numerical stability\n",
    "\n",
    "    # Prepare for next iteration\n",
    "    p += seq_length\n",
    "    n += 1\n",
    "    \n",
    "    # check exit\n",
    "    if n>n_runs:\n",
    "        break\n",
    "        \n",
    "# save weights\n",
    "#outfile = open(\"gru_weights.data\", \"wb\")\n",
    "#outdata = dict()\n",
    "#outdata['Wy'] = Wy\n",
    "#outdata['Wh'] = Wh\n",
    "#outdata['Wr'] = Wr\n",
    "#outdata['Wz'] = Wz\n",
    "#outdata['Uh'] = Uh\n",
    "#outdata['Ur'] = Ur\n",
    "#outdata['Uz'] = Uz\n",
    "#outdata['by'] = by\n",
    "#outdata['bh'] = bh\n",
    "#outdata['br'] = br\n",
    "#outdata['bz'] = bz\n",
    "#np.savez(outfile, **outdata)\n",
    "#outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
