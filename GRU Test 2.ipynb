{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 576 characters, 7 unique.\n",
      "----\n",
      "a e cbdfd cccda bfafccfcebfddbedaaefcdfcfdefeadb bae fbb abf ffcbad  c aaa   dffbadcf ebadadcbcbdbbef abefffbfa fad efabbeaacbeacfdffedffc bab dbffcadfbcacbacbbcebff ac bedddabdfca d bcf b bdaa dddf dddf decbe ccd abdfd ffcfcfeff abfbafcfadc da   fefdcbfbcbfabd dcaacbcbf  bddbfbedeffedba accef  deeffbbaddf cdaaf c cfbf abbdbddbcd edec bbb e dddf f cbcba ceba  a a  eacecaaccbeef fbfdfebcbbacdac a f ad fefcbcfcafb abfccbfffbfedaaafbccee bee ab b cad ffbaac dbbedfbfebdbf aabecc b afafead edecf beba ecfebbcfe dfdac bffabdfaafcfacbbccbfeebdeebee fccccdcfdbbeba ff ba f f fcbbebdccfcee eaf b cebabe a dbbebeeafababccefad cbbce feafcfacbbdfeecfcfeae bace cfadd cd fc afd bc bcadeaddfeecaf c ffebaa c bde cdecfa caeabeecdb  c  ffa faad afdcbe ffd dfcb ef e fdcbfaefec ccabbacac fecef dffd d afbdafebe bad bacbd daaefbe deabf  dcefbbcdfdbaf efdbabddffefabbcdb bbfffdbce e ee dcaaecfff faceaca  b bdb  aafaedccfbecbfd bedfaecdfffbfdc eceeebeccd  bf b bbfeb edaf fbcedfd fed baedffefd  beff  fbcfbdbecaaacd\n",
      "----\n",
      "iter 0, loss: 9.732009, smooth loss: 9.729553\n",
      "----\n",
      "ffffaaaaaaabbaabccccccccdcdddde  e eddf faaaaaaaabbcdcceddcdcdff  feafd a aaab abccccfced dddeefffeaaeaafbbbbbccccccdceedeeeedff fafaaaabbab ccdcdcccddef d  aafaafbbbcb bcccccdcededeefbeef ff abbbbbbbbbbbcccddfed e effff aaa aabbbbb  bccccddeeee faffaaabbbcbcccdecd deeeeeeff aaaaaabbdbcccd  cdd eedeeefff f aaaabbcbcccccccccc bddcdeeed efff aaabbbcbcbbcbccdddd dddddfde faaafabbcabcbccccbcc c dee efdfff  aaaabbabcdbccdcd eeeeeeeefaaaa bbbc ccccccdd ddeeefeeefd aa abbbcdccc cedd eeffe eaaafeabbcbcdbccdddcd dddfceefaf aaaaabcbcccccc bdcdd  f  faffafbabbb cccccccdddd dddddeeea da ebabdb bcdbd dd efcfea  afbbbbbccfccbedadd  fef e  aabbaccbccccccdcd ddeeeeefeeefefaaaaaa  bbbbc bccdcc dddf  fffefef abacbbc d dcd cee eeeffeeafeaaaa bbbbcbbccccdcdcdcdeddedefdefeffffaa aaabbbbbbcc cdccedd eefd faaafa abbbbcccdcd  ddcddeefffeeff aaf bbbccd ccddd ddfefffeef aacabbbbbccc dcd dcceee ffa affaaacbabbacdc  ccdedefee efafeaaabbabbcbbccdccddddedeeeeeef fe abbdbbccccccbcdcddded fdeeeef ffcaaaeabbbbdbcbcd bd\n",
      "----\n",
      "iter 100, loss: 5.008057, smooth loss: 9.428379\n",
      "----\n",
      "ffffaaa  abbcbdedddedeeefeeee efaaffaf aaaaaacbccddddccdeeff fff aa abb b bbcccedede e ff ffafaaaa abbabccc dddddded eee fff afeaafa bbabb cccccddddd eeeeeeeffefffaaaafeaacebbbbbbbddddede deeefffffaffafaafaadafccccedd eeeee  faaaffaf aaabbbbccccccc d e eedffafff aafaafbbbbb ccc dfeefeeeff fffffaa afbbbbbbccdd  fdeefeeeefeafbbbbbbcccdcdddeddeeeeeeeeef feff aabbcb ccccd   deeeeeefefaffaabab cccbcd deeddfeefeafffa afaaaaabbbbbbcbccccdd d efeeedff fff aa bbbbb bdddedd efeee ef faaafaaafbb bb dfcdcdeeeeefefeefe aaffffaba bbbccccdcddd  eeeeff ffffafaffbbbbcb ccce eeeedef a faabecc ccddcddf efeffffaffaffaa af ba cccd deee   fafafaa aabbcc cdcdd ee eefeffefffffffaf aaaaaaa bcccdcddeed efef aafa aaaabbbbbbaccccddddddd eeeeefefe ffafa aabbbbbb  cdddddeeeeee dfaffebbcabccdcc dedee eeeee aaaffaeaff  babcdcddddddddceeefeca aaffaabababacccc ccddd e  edfffffffffaffa abbbbbabccccd  deedfdefeeeff afbab  dddcdee e fefedafafbff c  de eeeeeeeff  a aacbbcc cddddedee effdffeffafafaaa aabbbbdccccddddddfee fee\n",
      "----\n",
      "iter 200, loss: 4.166070, smooth loss: 8.925334\n",
      "----\n",
      "aaaffabb bd  dddddeeeffeffff f bbbbbbbbbccccbcdded e feffffffaaaaa bbbb  ccccccdddd eefeeefffffaafaaaaabbbb ccc dc dedeeeffffffffafaaabbb cbccdddddddeed ffaaaaaf bbbbbbdcd  ddeeee ff aaaa a bc cc cdddce  feaaaaaaabbbabbbcccccdcedddd eee ffa aabb  cccccccbeeddeeeeefffff aaaaa cb ccbdc  ee afabaabbbbabcddcddddd eeffe aafaaaaa bbbccc dddde  e  aaafbbacecccccdd deeeeeeefffffaa aa bbbbaccccc  ddd eee e faaaafbbbbcbcccedcddeeeeeeeffafeaaaaabbbabbcccccccd dddd dfeaf  a dbbcccccccc dddddcdefeee faeafaaabbb ccccccccc  ddeeee effffaefaabbbbbbbbbccccc c ddddcd eee  aaaaababbbabbcbcccdcb eee  aaaabb b bddcddd   fffeebbbbbbccccbddc fdfee fff aaaa  b  c cccd ddeeeee f aaaafaacccccccccdddddeeeeeeee ffa  bbb accdbcdddd ee fffffafaafbbb bbccc  ddddd ceff ff aaaaaa bbc cbcdcddc eeefeffff af bbabbbcccccccccd ddddeeeefe fffaaaaaaaaabbbbb ccccdccdddddd eeeeffafff aaaaaabbbcccccdddcedeeeefffaffffaf abbbbbacdcccdcdd e eeffeffffffffffaf bbbb bcbccccccc c de eefefffffaaa bcb  cc ddeeeeee f aaaababbabbcc cddd ee\n",
      "----\n",
      "iter 300, loss: 3.596053, smooth loss: 8.407121\n",
      "----\n",
      "afabcccc ddddd eeeeeeeffeff aa babbbccccc  dcdeeeeee fff aa bb  cc  dd cee f  aaa abaccc ddddd eeeeeedffeffff aaafbbbbb ccccdddddddeeeeeeeeeeeffffff faabbbbbbbccccccc ddd e e ff afa bbbbbbbbb c ddddddeeeeeefeefea aaaab bbcc ddde eeeee ffff aaaaaabbbbcccd ddddddddeef fff aaa bbbbbbbcccc ddddd eeeee ffaaaaafb bbb cccc b eeeeeeeeffafaa aa bbbb bcddd ce eeeff faf aabbbabccccddd efefef fff aaaababbbb cc ddddddeeeeeeee f aaaa babb  dddd d   a  ab dcdddddddd eeefeef aafaab bbb cccc  e eeeeee f fbaaaabdcdcddddeee effffaffffaaaaaaaaaaa bbbccceddddeeeeeeedeffff aaaaaaaaabbbbbbb d ddddd eeefee ffaaaa abbbb ccceddd eeee ffffff  aa  ccdddddddeeeeeeeef aaaaaaaaaafaa bbd  ddeeeeeefeffff aaaa bbbbbbbaccbd d   ffe aaaaaaaaabbbbccccbcdddd eeeeee afffffafaaaf  bbbccccccdc deeeeeeefffe a aabb c dd ddeee eff f  bb b  dddd fe eadab bad cdddd e edef affaa ac ccccccddd eeeeeeefef afaaaaaabbbb cccccdddd fee fffffffaaaab bbaccccccdedddefeeefeffffaffffaffaaaa bbbbbbbbccccccdddddcdedf fffffffafaaaafbabbbbccc d ddd\n",
      "----\n",
      "iter 400, loss: 3.244737, smooth loss: 7.901348\n",
      "----\n",
      "aaabbb  cc dd deeee  aafaaaaaabbbbbbbbbbb c ddddeee eff  aafaaa ccc dddedeeeeeeee afaaaaaabbb cc dddddddee fffff aa bbbabbbccc cdddeeee  fffffffffa aaaabbbbb bcc dddd e ea babbbbbb cdd eeeeeeee fffffffaaaaaabbbbbbbbbbbb ccccbeddddee  afff aaab  ccccbdddddeeeeeeff ffffff fbbabc c  ddddeeeeeeffff aa abbbbbcccb dddeeeeeffeff aafaa bbbbbbbbbb cccccccd d eeeee eaaa bb cb ddd e   fafabb    cddddd e fffaf  bbbbbbbbbabd dde eeee ffffff a bbbbbbbbbbb cccccdddddd eeeeeefffffb abbbbab cccbddddddcee fff  aaaabbbb bccc  dddeedeef afaaaaaa abbdcccc d edee fff a abba  cccccdddde ee ffeafaafaaabbbbbbb ccccccc  dddeeeeede  aaaaabbb  bcc  dede ffeefaaaaaaaab bbbb bddddddd d  ffafffafaab bb cccccccccccdddeeeeeefe aaaaabbbbbbb add d efeeeaff aaaaaaa  bbbcccc dd eeeeeefafff faaaaa bbbbb  ccccddddddd d ffffffaafaabbbbb  ccc dd dde eeffaff aaaaaaaaabbbbc cccbd feeeef fff aaaabbbb bcc ddeeade ff aafa accccccccc eeeeeeeee fff aaaaaaaaaaabbbb dc eeeeeeeffffa aaaa bbbbbbbbbbbbbccccccc dd  eee ffeff aaaaab bbbb bc\n",
      "----\n",
      "iter 500, loss: 2.595299, smooth loss: 7.418433\n",
      "----\n",
      "bbbcccccccdddd dd eeee ffffffff aaaaa acccccccdddd  feee  aaaa bbbbabccc d eee fffffffffffeaaaaaa bbb cccddcddddfd effffffeaaaacc ccccddddddddd ee fe a bbc cc dcddddeeeee fffff  aabbbccccccddddddde e faffff aa bbbbbb ccccc dddddeeeeeee fff baabccccc dddddd dee effa aaaa  b cd d eeee ef aaaaa bbbb c dddd e e   aaa abcc dddd fee ff abbbbbb cccccddddd ee feaffaf  bbb bcccddddd  eeef  f  bbbccccd   ee f a aabbbb ccccccddddddddddeeef fe aacbbbbcccccc d deeefe dcaaaaac ccccddddddd eee  f  bbbbbd cdded e effeafaaaaaa bbb cccccccdcdd  eff ff aaaff cccddddddeee eeeffffffffffaf aa cccccc dddd  eee af aaaaaaa b ddd d eeeeeefdaaaaaaa bb cccccdccde eeefffefffaffaaaab ccccccbd ed eee ff aa bbbbb ccccccdcdd  eeeee fea ababccccdddddddddeeeeeeeff aaf babac ddddd  ee   aaa bbadcc ddddd eeeeefffeffffffbbb  ccc dddddddd feee  cbbb cccccccddddd eeeeeee fa a bbbbbbbcd ee eeeeeffffff aaaaaabb cccccccdddddddd eee fffefa acbbccc dddde eedffff eaaaaa bbec dd  deeeeeff afabbbbccccccdddd eeeedeeefffffffffffffffaaa\n",
      "----\n",
      "iter 600, loss: 2.639489, smooth loss: 6.966076\n",
      "----\n",
      "bbbbbb b d eeeeedeee aaaf b ccccce edeeeeeee ffaaaaaafbabbb b  eeeeeee fffff af bbbbb ddddddd eeeeeefffffffff a bbbbbbc ccc ddddedeeee ffbaaaaafaac  ddd eee ff aaaaabbbbb ec edd eee f af  bcbccdcddddd eeee fffffff aafb b cc  ddeeeeeeee af aaaaaa  c dddddd  ee f aaaa bbb bb cd deeeeefeffff fbab cccccd  eee  ffffff aaaa bbbba c c dddeee efff ab bbba ccc deeeeee  fff aa ba  cc ddddd eeedffffa aa bbbb b dd  eeeeefefe faab bccddcd e e ff aaaaaaaa ccccccdddddddd eeeff a aabbfb cccd dddddeeeeffffeaf a bbbbbbbb ccc ddeeee  ffffff aaf bb b  d  edee ffff  aafabcc  ddde  faffffff aa bbabbccc d d e dff aaaaaa bbbbbbb ccc ddd  eefe d aaa bbbbbbbaccccdddddd eeeeeffffe aaaaa bbbbbbbbb cccd deeeeee ffefff ab bbbbb ccccccc ddd eeeed  babbbbbbb ccc ddddddd eeee fffff aaaaaaaaaa b dd eeeeee fffffff aaaaaaaabbb cc dd ed f ff aaaafacccccccdddd eeeeeeeeffffffff aa bbbbbbbbbc  d eeeee  aaaaabbbbbb  cccccddddddddeee  ffaaaaaaaaa bb cccccddcdd eeee  a acaccc d dee eeeeafaaaaa bb bb ddddddddeeeeefefffa a bbbbbbb\n",
      "----\n",
      "iter 700, loss: 3.163946, smooth loss: 6.542451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "bbbbb cccc dcdd eeeeeff  ab bb bc d  eeeeeeffffffffff aa bbbbbbbbbb cccccccd d ee e aaaaaacbbcaccdddddceceeeffee a bbcccccccccd eeeeeeef ffaaaaaaa bbbbb ccccadddddddeeee ffffff aaaaa bbbbb a cdddcdeedeeeffffffa aaaacbbb  dddddd e eecbbbbbbbbbbcddddd  e fffaeaaaaac bdddddddeeedffff  a bbbbbccccc dddddd eeee f aaaaaabbbbbbb cccccc ddddeeee f a   ccccccccdddddeeeee   aabbbb ccccc dddde eee f aaabbbbb ccd  eeee fffffff aaa bbb  ccccccccddd e e  aaa bbbbbbbb cccc dd eeeeeefffffffaaaaaaa bbb cc c ddde ef ffff aa bbbbbbbb cd  deeefdf aaaaaa bb  b dddddee e fb bbbbbbbbbbba cccccddddd ee ffffffaf afbbbbb  ccccddd  eeef aaaaa bbbbbbbb cdd  eeeee  aaaaaabbb  ccccc  e eeeeffffff aaaaaba d ddd ee effeaaaaaaac cccccdddcd eeedeffff aaabab bdd  eeeeeea f bbbbbb  c   eee fe a bbbbb b  ddddddeeeeed afa b cccccccc ddddd ee fdf aa bbbbbbbbc cccedddddfeee feaaaaaaa bb ccccc  dd eeee fffaffff aaaaa cc ddcfdd e  a bab d dddddeee fffffffffffffffff aaa bbbbb dcccddc  eee ff aaaaa bbbbbbbbcccccccd e  efffffffafa\n",
      "----\n",
      "iter 800, loss: 2.307776, smooth loss: 6.146969\n",
      "----\n",
      "bbbbbbb  d deeee   aaaaaabbbbbbdddddd eeeeeee ff a a bbb ccdddddddd eeee f aaaa bbbbbbcc  dddcd eee ffff aaaa bbb  ccccdddd eeee feaa bbbbbbbb ccccdddddddd e  f aaaaaa b  c dddddcddd ee ffffffffffffffafff aaabbbbb ccccccddddddd eeee ff aa bbbbbbccc dddd ddeeeeeee fffaa b bcbc dddddd eeeee f f aa bcccccccd  eeeeeeee eababc cddd d e eeff aaaaaaccccccd  eeeeeeefffaf aaaa bbbb ccc ddddddddd eeeefffffffafff  bbbfbccc  dddceeeeeefeffffff aafabbbb cccc dddeddeeeeeee ffff abbb cc dd ddeeeeeeffaff aaaaa b cccccddddddd eee f ffff aaa bbb cccc ddedeeeeeeefefffff aaabbbbbbb cccc ddddddcdeeeeee ffffaf aa bbbbb ccccc ddd de e ffffffffff aaaaaabbbbddddddeeedeeeee ffaa  cb  dd e eee ffffafafbba cccccd dddeeeeee f aaaaabbbbbbbddd ddd f e aaaab bb dddddddddeeef ff aaaa bab ccccc   eeeeeeffaffffffaaaaaabbbbbbb c dddddddeeeeeeeef fff aaaaad ccccdd d e effffffff aaafbbbb ccccccdddddddddd ee   aaa bbbbb cccc dde  eeefff a ababb d e eeeeeeeea  bbbbbbb ccc cdddfde  aaaaaaa  b ccccbddddddd eeeeeeef faaaa bb b c\n",
      "----\n",
      "iter 900, loss: 2.187125, smooth loss: 5.779026\n",
      "----\n",
      "b ba ddddd eeeee ff aaaaaa bbbbbbbbdcc dd  eee ffffff aaafbbb cccccc ddd eeeeeefeff aaaaaaaa bbb bc dddedeeeeee ff aaaaaf bbb c ddddd eeeeefefff afbbbbbbbb ccccc  edeee  ffffff aaaaaa bbbb ccc dddd eeeeeee ff aaaaaa bbbbbbb ccccc ddddd eeeeeee ff aaaaa bbbbbbbb ccccdddddddeeeee fffffafffafaaaaa b ccccc ddddd eeeee ffffffff aa bbbbbb b dd dd e fff aaaafa  cccc d d eeeeeee fa aaaafcccccccd  eeeeeeffffffaaaaaaaa bbbbbbb ccccccde eeee efffffff aa bbbbbb b ddddddd eeeeee f aaaaa bb cccccccdcdddeeeeeee   afbbb cc dddddeede   aaaaaaa cbddd  eeee ffefaaf bbbb cccccc dd eeef  a aaaabbbb cccccdd de   fff aaaaaaabbbbb ccccccdddddedeeeeeeeff bfa bbbbb ccccc  ddd eeeeefffaff  abbbbbbb cc dd eeeedee f aaaaa bfbbbbc d  eeeeeeefffffff aaacbbbbbbb cc ddddeeeeeeee  b bbb b  d  eeeeee ff afaaaaa bb cccc edd eeefeffffff aaaaa b bb ccccdddddd eeee ffffffffab bbbb ccbd eeeebff aaaaaaa bbbbbbb cccccddedeeee  ff aaaaa bbb bbeddddddfee ffffffffff aaaaaebbb ccddd d eeeeefefff aaaabbbb b  ddd  eee fff aaaaa bbbbb\n",
      "----\n",
      "iter 1000, loss: 2.620030, smooth loss: 5.439533\n"
     ]
    }
   ],
   "source": [
    "# GRU with Minimal Gated Units\n",
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "# Seed random\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read data and setup maps for integer encoding and decoding.\n",
    "data = open('input.txt', 'r').read()\n",
    "chars = sorted(list(set(data))) # Sort makes model predictable (if seeded).\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) } # dict from enumerate: char and index\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) } # dict from enumerate: index and char\n",
    "\n",
    "# Activation functions\n",
    "# NOTE: Derivatives are calculated using outcomes of their primitives (which are already calculated during forward prop).\n",
    "def sigmoid(input, deriv=False):\n",
    "    if deriv:\n",
    "        return input*(1-input) # sig' = sig(1-sig) - only if claculated in forward prop already!\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def tanh(input, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - input ** 2 # tanh'=1-tanh**2\n",
    "    else:\n",
    "        return np.tanh(input)\n",
    "\n",
    "# Derivative is directly calculated in backprop (in combination with cross-entropy loss function).\n",
    "def softmax(input):\n",
    "    # Subtraction of max value improves numerical stability.\n",
    "    e_input = np.exp(input - np.max(input))\n",
    "    return e_input / e_input.sum()\n",
    "\n",
    "# Hyper parameters\n",
    "N, h_size, o_size = vocab_size, vocab_size, vocab_size # Hidden size is set to vocab_size, assuming that level of abstractness is approximately proportional to vocab_size (but can be set to any other value).\n",
    "seq_length = 5 # Longer sequence lengths allow for lengthier latent dependencies to be trained.\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# Model parameter initialization\n",
    "#if os.path.isfile(\"gru_weights.data\"):\n",
    "    #initfile = open(\"gru_weights.data\", \"rb\")\n",
    "    #init_data = np.load(initfile)\n",
    "    #Wy = init_data['Wy']\n",
    "    #Wh = init_data['Wh']\n",
    "    #Wr = init_data['Wr']\n",
    "    #Wz = init_data['Wz']\n",
    "    #Uh = init_data['Uh']\n",
    "    #Ur = init_data['Ur']\n",
    "    #Uz = init_data['Uz']\n",
    "    #by = init_data['by']\n",
    "    #bh = init_data['bh']\n",
    "    #br = init_data['br']\n",
    "    #bz = init_data['bz']\n",
    "    #initfile.close()\n",
    "#else:\n",
    "Wz = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Uz = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "bz = np.zeros((h_size, 1))\n",
    "\n",
    "Wr = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Ur = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "br = np.zeros((h_size, 1))\n",
    "\n",
    "Wh = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Uh = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "bh = np.zeros((h_size, 1))\n",
    "\n",
    "Wy = np.random.rand(o_size, h_size) * 0.1 - 0.05\n",
    "by = np.zeros((o_size, 1))\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    # Initialize variables\n",
    "    x, z, r, h_hat, h, y, p = {}, {}, {}, {}, {-1: hprev}, {}, {} # Dictionaries contain variables for each timestep.\n",
    "    sequence_loss = 0\n",
    "\n",
    "    # Forward prop\n",
    "    for t in range(len(inputs)):\n",
    "        # Set up one-hot encoded input\n",
    "        x[t] = np.zeros((vocab_size, 1)) # for each time step t a one-hot over vocabulary size - init with all 0\n",
    "        x[t][inputs[t]] = 1 # set t'th input to one (current word in one-hot)\n",
    "        \n",
    "        # Calculate update and reset gates\n",
    "        z[t] = sigmoid(np.dot(Wz, x[t]) + np.dot(Uz, h[t-1]) + bz) # update gate\n",
    "        r[t] = sigmoid(np.dot(Wr, x[t]) + np.dot(Ur, h[t-1]) + br) # reset gate\n",
    "        \n",
    "        # Calculate hidden units\n",
    "        h_hat[t] = tanh(np.dot(Wh, x[t]) + np.dot(Uh, np.multiply(r[t], h[t-1])) + bh)\n",
    "        h[t] = np.multiply(z[t], h[t-1]) + np.multiply((1 - z[t]), h_hat[t]) # sometimes denoted s[t]\n",
    "        \n",
    "        # Regular output unit\n",
    "        y[t] = np.dot(Wy, h[t]) + by\n",
    "        \n",
    "        # Probability distribution\n",
    "        p[t] = softmax(y[t])\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        # targets[t]'s entry of p[t]: since the output is 0 or 1 only one entry contributes (and yj*log(pj) becomes just log(pj))\n",
    "        #loss = -np.sum(np.log(p[t][targets[t]])) # dict p: or time t (=one-hot position), whats the corresponsing target value?\n",
    "        loss = -np.log(p[t][targets[t]])\n",
    "        sequence_loss += loss\n",
    "\n",
    "    # Parameter gradient initialization\n",
    "    dWy, dWh, dWr, dWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n",
    "    dUh, dUr, dUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n",
    "    dby, dbh, dbr, dbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n",
    "    dhnext = np.zeros_like(h[0])\n",
    "    \n",
    "    # Backward prop\n",
    "    for t in reversed(range(len(inputs))): # index t counting down\n",
    "        # âˆ‚loss/âˆ‚y\n",
    "        dy = np.copy(p[t]) # copy output\n",
    "        dy[targets[t]] -= 1 # the current target (truth) is 1 for the current t (an 0 for all other t's)\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wy and âˆ‚loss/âˆ‚by\n",
    "        dWy += np.dot(dy, h[t].T) # weight updates: Wy -> Wy - etha * d loss / dWy -> dWy += etha * d loss / dWy\n",
    "        dby += dy # weight for bias is just 1\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        dh = np.dot(Wy.T, dy) + dhnext\n",
    "        dh_hat = np.multiply(dh, (1 - z[t]))\n",
    "        dh_hat_l = dh_hat * tanh(h_hat[t], deriv=True) # = Wy*dy*(1-zt) * (1-tanh**2(h_hat))\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wh, âˆ‚loss/âˆ‚Uh and âˆ‚loss/âˆ‚bh\n",
    "        dWh += np.dot(dh_hat_l, x[t].T) # Wy*dy*(1-zt) * (1-tanh**2(h_hat)) * xt\n",
    "        dUh += np.dot(dh_hat_l, np.multiply(r[t], h[t-1]).T) # Wy*dy*(1-zt) * (1-tanh**2(h_hat)) * (rt*ht-1)\n",
    "        dbh += dh_hat_l # weight for bias is just 1\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        drhp = np.dot(Uh.T, dh_hat_l) # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat))\n",
    "        dr = np.multiply(drhp, h[t-1]) # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat)) * ht-1\n",
    "        dr_l = dr * sigmoid(r[t], deriv=True) # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat))*ht-1*sig(rt)*(1-sig(rt))\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wr, âˆ‚loss/âˆ‚Ur and âˆ‚loss/âˆ‚br\n",
    "        dWr += np.dot(dr_l, x[t].T) # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat))*sig(rt)*(1-sig(rt))*xt\n",
    "        dUr += np.dot(dr_l, h[t-1].T) # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat))*sig(rt)*(1-sig(rt))*ht-1\n",
    "        dbr += dr_l # # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat))*sig(rt)*(1-sig(rt))\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        dz = np.multiply(dh, h[t-1] - h_hat[t]) # dh * (ht-1 - h_hatt)\n",
    "        dz_l = dz * sigmoid(z[t], deriv=True) # dz * sig(zt)*(1-sig(zt))\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wz, âˆ‚loss/âˆ‚Uz and âˆ‚loss/âˆ‚bz\n",
    "        dWz += np.dot(dz_l, x[t].T) # dz * sig(zt)*(1-sig(zt))*xt\n",
    "        dUz += np.dot(dz_l, h[t-1].T) # # dz * sig(zt)*(1-sig(zt))*ht-1\n",
    "        dbz += dz_l # # dz * sig(zt)*(1-sig(zt))\n",
    "        \n",
    "        # All influences of previous layer to loss\n",
    "        dh_fz_inner = np.dot(Uz.T, dz_l) # Uz * dz * sig(zt)*(1-sig(zt))\n",
    "        dh_fz = np.multiply(dh, z[t]) # dh * zt\n",
    "        dh_fhh = np.multiply(drhp, r[t]) # Uh * Wy*dy*(1-zt) * (1-tanh**2(Wy*dy*(1-zt))) * rt\n",
    "        dh_fr = np.dot(Ur.T, dr_l) # Ur * Uh*Wy*dy*(1-zt)*ht-1*sig(rt)*(1-sig(rt))\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚hð‘¡â‚‹â‚\n",
    "        dhnext = dh_fz_inner + dh_fz + dh_fhh + dh_fr\n",
    "\n",
    "    return sequence_loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, h[len(inputs) - 1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    # Initialize first word of sample ('seed') as one-hot encoded vector.\n",
    "    x = np.zeros((vocab_size, 1)) # np.zeros(shape) -> (vocab_size, 1) = (rows, cols)\n",
    "    x[seed_ix] = 1\n",
    "    ixes = [seed_ix]\n",
    "    \n",
    "    for t in range(n):\n",
    "        # Calculate update and reset gates\n",
    "        z = sigmoid(np.dot(Wz, x) + np.dot(Uz, h) + bz)\n",
    "        r = sigmoid(np.dot(Wr, x) + np.dot(Ur, h) + br)\n",
    "        \n",
    "        # Calculate hidden units\n",
    "        h_hat = tanh(np.dot(Wh, x) + np.dot(Uh, np.multiply(r, h)) + bh)\n",
    "        h = np.multiply(z, h) + np.multiply((1 - z), h_hat)\n",
    "        \n",
    "        # Regular output unit\n",
    "        y = np.dot(Wy, h) + by\n",
    "        \n",
    "        # Probability distribution\n",
    "        p = softmax(y)\n",
    "\n",
    "        # Choose next char according to the distribution\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel()) # ravel returns flattened array, P are the probabilities for choice\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    \n",
    "    return ixes\n",
    "\n",
    "# Initialize sampling parameters and memory gradients (for adagrad)\n",
    "n, p = 0, 0\n",
    "mdWy, mdWh, mdWr, mdWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n",
    "mdUh, mdUr, mdUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n",
    "mdby, mdbh, mdbr, mdbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length\n",
    "\n",
    "print_interval = 100\n",
    "n_runs = 1000\n",
    "\n",
    "while True:\n",
    "    # Reset memory if appropriate\n",
    "    if p + seq_length + 1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((h_size, 1))\n",
    "        p = 0 # current position\n",
    "    \n",
    "    # Get input and target sequence - each an index list of characters\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]] # current position to sequence length\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]] # current position+1 to sequence length+1 (next state)\n",
    "\n",
    "    # Occasionally sample from model and print result\n",
    "    if n % print_interval == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 1000)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n%s\\n----' % (txt, ))\n",
    "\n",
    "    # Get gradients for current model based on input and target sequences\n",
    "    loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    # Occasionally print loss information\n",
    "    if n % print_interval == 0:\n",
    "        print('iter %d, loss: %f, smooth loss: %f' % (n, loss, smooth_loss))\n",
    "\n",
    "    # Update model with adagrad (stochastic) gradient descent\n",
    "    for param, dparam, mem in zip([Wy,  Wh,  Wr,  Wz,  Uh,  Ur,  Uz,  by,  bh,  br,  bz], # zip combines iterables\n",
    "                                  [dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz],\n",
    "                                  [mdWy,mdWh,mdWr,mdWz,mdUh,mdUr,mdUz,mdby,mdbh,mdbr,mdbz]):\n",
    "        np.clip(dparam, -5, 5, out=dparam) # limit values of array - here the changes (dparam has the dWy,...)\n",
    "        mem += dparam * dparam # store the squared change (gradient)\n",
    "        # now actually change the model parameters\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # Small added term for numerical stability\n",
    "\n",
    "    # Prepare for next iteration\n",
    "    p += seq_length\n",
    "    n += 1\n",
    "    \n",
    "    # check exit\n",
    "    if n>n_runs:\n",
    "        break\n",
    "        \n",
    "# save weights\n",
    "#outfile = open(\"gru_weights.data\", \"wb\")\n",
    "#outdata = dict()\n",
    "#outdata['Wy'] = Wy\n",
    "#outdata['Wh'] = Wh\n",
    "#outdata['Wr'] = Wr\n",
    "#outdata['Wz'] = Wz\n",
    "#outdata['Uh'] = Uh\n",
    "#outdata['Ur'] = Ur\n",
    "#outdata['Uz'] = Uz\n",
    "#outdata['by'] = by\n",
    "#outdata['bh'] = bh\n",
    "#outdata['br'] = br\n",
    "#outdata['bz'] = bz\n",
    "#np.savez(outfile, **outdata)\n",
    "#outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 576 characters, 7 unique.\n",
      "----\n",
      "a e cbdfd cccda bfafccfcebfddbedaaefcdfcfdefeadb bae fbb abf ffcbad  c aaa   dffbadcf ebadadcbcbdbbef abefffbfa fad efabbeaacbeacfdffedffc bab dbffcadfbcacbacbbcebff ac bedddabdfca d bcf b bdaa dddf dddf decbe ccd abdfd ffcfcfeff abfbafcfadc da   fefdcbfbcbfabd dcaacbcbf  bddbfbedeffedba accef  deeffbbaddf cdaaf c cfbf abbdbddbcd edec bbb e dddf f cbcba ceba  a a  eacecaaccbeef fbfdfebcbbacdac a f ad fefcbcfcafb abfccbfffbfedaaafbccee bee ab b cad ffbaac dbbedfbfebdbf aabecc b afafead edecf beba ecfebbcfe dfdac bffabdfaafcfacbbccbfeebdeebee fccccdcfdbbeba ff ba f f fcbbebdccfcee eaf b cebabe a dbbebeeafababccefad cbbce feafcfacbbdfeecfcfeae bace cfadd cd fc afd bc bcadeaddfeecaf c ffebaa c bde cdecfa caeabeecdb  c  ffa faad afdcbe ffd dfcb ef e fdcbfaefec ccabbacac fecef dffd d afbdafebe bad bacbd daaefbe deabf  dcefbbcdfdbaf efdbabddffefabbcdb bbfffdbce e ee dcaaecfff faceaca  b bdb  aafaedccfbecbfd bedfaecdfffbfdc eceeebeccd  bf b bbfeb edaf fbcedfd fed baedffefd  beff  fbcfbdbecaaacd\n",
      "----\n",
      "iter 0, loss: 9.730961, smooth loss: 9.729552\n",
      "----\n",
      "ffef aabaaacc  bddddadddfaeedef  e c ac ecccdc b cbffddffeba bfe  ec dd a   ac   bbadfcfc babcbeedc cda fedefdffabbbdbfeceeec de dcebcc de a fdfcebbbcbef    abd cededfd  bcdcddbfdedeef dbf ff bcceebcd  cbedbeeffc e befdd ddb aaabcbb  bddddeeffee f eeb  dcde fcaff c cddccccef    abbcdfdeecf  aab cd cccdfe e a abbbeadedcdcdccd  eeaafec  bcdd aa ddcf f  d cbdeef abbaaeaa d  ae eef ad a  b bb b cdd deaeff  aabbcb bdf eaf c fdedcddcd  cb eedf eaaac dc ddeeefabafa ab bdddffeed dfeb eeffd cabaddacce df  eedd c babe bce f cc bbdfcedb bc  caea  f  eacdcfd dde efedccdfdee daaaabbb  c  ed efd  be e cd df ee   bfcdcc eefa  fd cb  faf b  c dd febeebbbce e dceedddedd dac  bba   bbcce  bceba dbcf  eeedfcd bd fdae f fad acc bbccdcc eeddd  bccddc edeefaeaaaafaababcabccfddfcb  a dccd bdc cecbfdb eef  e  adb bcccaeddfce  da aacbcddcced dcf ddafff caecc ccfdffedcf   dbcccccedd fde caadcb ff  cffdaafb aa bdc  bbdfcdfee af edbccdc ecfaaabd acccdeddedadde fd bdcfcbdcdeed dfcfcded fadbbac fedcabeabaa fcd be  d\n",
      "----\n",
      "iter 100, loss: 5.362950, smooth loss: 9.505338\n",
      "----\n",
      "ffff aa  abad efc ad aaaf a a bd bedcd d   aaf bcdd d  cecee ddf d  bcc b    baf f d b de feaf a b bcd eeee eafaaae  cbc eee bfd bfb ca bb ccdcddedde fbdacc cd eef  aabbbadecdc  a feaaf b abbbdfdce ee fabdb dbfefaefba bca b  b bbff f b bcbddedeaaaa b d c  ee eff eefcafebada bdb cfedffeeff fabbb b cedbd d e fe  f a ecccbcc dbeccebfeffaabbd aee deaaaab bbbc  dddfd eeeef  aaaa bbbccd de dd e fba bb bdd  fa fa bbd   ec dc d  ca e aa aca b bdbcc de fef ab bcbba  fe faa ef ca bb dc deaeafe  ba ef a bdcd efaea fa aabedecd  bd ee af aac  cbc ee ffff bafccccbe  bedf faaa ab   dd efff d ba bbf aedeeee fe ffaa be b  cdae  fcc   e e ea  aacbeb bc dc ee d fafaacbbbcdd e  e baa  bec fbddff  ffbc   c  accad  dcb c bdeecdec  ef ecdad  cb ea aaed e a  bdbdddfeafea ac fccccf  e fa  bcbdd dd cc   acd ddef  f aef eedcc d  bccfd    bcdbcd d d daa  a ccc c  b cfddfeeefdeea abbaab bbbbbc  ce  f aba bcd  feef  ffe bff e fafa abadcff f  ff aadbcccdd  c aafabdb ade afaba bdc dd de fdf b  aaaebbfc dcefcd dffa faa\n",
      "----\n",
      "iter 200, loss: 2.915777, smooth loss: 9.048679\n",
      "----\n",
      "aaaff aa ad  cd b f bfcace d f aaaaabaaccccc cede  d cabcccec bdb  aedd  bbe bbddd  eefaaaaabbc bd d  afa a bab ca be cb edddedcc f aabbb e ddeec a bbb  cc ccdcf ef abbfbd  dcecdc ee daaa a ac cd  eee f abbbbbbca ded eeaffa bcbdd d  fee ae  a cc  cd e aa efa fddcbdcddd d a a db cb d   ee ab baad dc ceff adbcc cceea aae bbb  cddeca ebcbd  c  cabdd  ffba bbcc cddddddddd ee a aa a bb cc c   cdc d e b caba dcd efdf aff cbfded  eee ba bbb dbc d fdd ff f bbcb bf  e  a fbbcc ad dc eee aaaafaa  b cce c eee d  babb c  ecec a abbbc cd  ef aaaaabccccc b cedd a daa  bbbbac bdd d e ea ba  ccc  ccdcea a afd bbc   dce ddc  c fb c dc  fafaa dcc  cbc  c  b ba b bbde bc b b ccf  ffef ff bdcccbfa dc a  bba  c b  e f aea b dd efeee daaeced ddd d  bbaa  abfc dc d d ea aab b cd dd  ffffdaaba  e de ea cdab cdbde fecbdcccd  eee aaaa ba ccbdc eabbe bddcdcc d ecfe bbb bbab ad efde ffb ad bbbcdde eeffaf aaa cb ce ddd ee e aaae cccdddddefcf bbab cd cda bba a bc ccdcdcddf e  bfb  bb dcfedd a b bbcbd ed d ee afab cc\n",
      "----\n",
      "iter 300, loss: 3.344526, smooth loss: 8.566109\n",
      "----\n",
      "afaafa b bbc d eeed eabcabb cc c dddefeea aa bbbcccc ccc ec cf  bb acc  dc e aa ba b ccd e fcc cadc b ec feef eaafbbbb  c cdeeaaa bbbcaaddd eeeeeeff fa ccbb ccccddccd bff b bacc cdd eeefba bb b cb dddedeeeefe aaa bbbbc cccd eeef aaaaa bccc cdddddeeeefecf abbbbba bde eee abb bcdddd effa bcdcc ccddd fe aa acc bcb d da aafbdb  ccdd e e aa bbcc  cd e ab bbbcc d f eafaa bbbbbcb  dfefef aaa bbbbcbcdd  d  aaaa bbbcbbb d e ddab c bb  dddd d   a  bb ecddd deee aaafa d bbdd e efa aaa a c ccdbdd e ffaaaa cbc ccccd d efeff aaaa a cccccccc effff fdaeafb cccd ddddf ffaaaaa bcccbdc  f cad caeb ecb dd d e abbbb cccfd e fc a bbbbbc  bc  edffaafa bccbcccce  aaa ba bbd c cad  aabbba ccccddd c e  abbbbbb cc d ea afa ab cccccccddd feee aaaa abcbb d afeeff fabbf  ccccccdcde  aeafa bbcccb c d eb f bd ccccb cee f afa b  cbcb f  baaab b e ccd e f aaaa bbccc  f efaaa cdd eb aaa cbc bddddd afbaa bcbbbcccc fee fefaafa bbbc cb dee feffaaaaf bbd fddd ffffaff abb bbbabcccd eec effab bb d ececefe fcaaafe ecbbcdd e eee\n",
      "----\n",
      "iter 400, loss: 3.106222, smooth loss: 8.070987\n",
      "----\n",
      "a bbbb bdc dd deefe aaad bbbbbdddddd dd d d ba bebc ccc  defeca abb bbbd ea accbb cddb eebfff fa bbbbcbbdd effef ab bbb bccde aaa acccc bccdddddeee ffeaff cbaabc cccc c dc faaaaaa  cdd eeeeefff fdfebeb bbcccddddedaffaaa bbbb ecccced dbfff faab acbcc dbdcdebaaaaab bbbcdd ffd ff b abbbbbcbc cdddd d aacdbbbde   aaaebbbcdd ee fffea aaa bccccc cccccdde f afcbb bbbb cc ea eaa b bbdbdcdd a aabbbbc cbfbe f abbbbbcccd df eff abbbabbcccc d eefaadabbbc d eeeffaaaa cccccccdddd  efffe a bbbaa ccccbcc ddc  cababbb  cccc a defb aaf abbbbbbb accfde e f babb ccc c bd  aabbbbbcccbe eb caaafa fab ccccccd efebaba aa bcbbcca b cbbbbd e  abba bd c deccfecccccb d dddd bfeaaaaa a  ccceddceeef ba bbbbbbbb ccccdeeeeeefa aa aaccbcccc ced d efffaaac bbbccbb  cccdddd ef ee cbaf cccae bcbb bcd e  aaaabbbcccc b dddeef afaadfbaa aab  cc ccc cccd ee fffcd aaaaaa ca cb d ffffaf bbb bb cdc d dee eafc bbabc bbf b ffede ddc fecccccccbccc dddd deed bcccc fcbededeeefffd aaa a cba ccddddddddeeedee ea abbb ccbcc cdcdde feeaa b\n",
      "----\n",
      "iter 500, loss: 1.501875, smooth loss: 7.573275\n",
      "----\n",
      "bb dcdddddddd aaaaaabb cccccccc cdddd deefefdacadb af ccba ccc ccccccccc c eef ffffaaaaa bbbbbbbb ccc cccdc ccfdfa afbccbd ccccef fffddddda eac bbaba b ccf ed fabaaaba bb ccccc  ccccccccba ddeeeeef aababbbf bc dcccccbcccbc ddd efffda babbb ccccdddcd eefeff aaa bbe bbbb c cb c cbcccca e ccdcd edfd e aabb c cb b ddb eff fffa fbbaa bbcccccc cccbbdddee ff faaabac bccc ccccccccd cdcbf  fa bbbccccd aaa abaa bbccdd deeffaf abbbbbbbbdde fc bbebbbbbb ccc d dfecff aaaa bbd cccbdccdede cff af aba ccfbdddfc e abbbbccbbbbc ccc cddddd ebaa  bccbcc cccff fffffeffffaa bbbccccddeddd f faafaaa b cccb acac cd eaffaaaa bba a  cccccccccca dddcde feeeaba bf cccdfd efeaffaaaab dcccdd e fbaaaa bd dd eefef aaa bbcbb a ccccc ccc cc deeaafbcbbcccccccccddde ccfaa bbbab cccb cc b bab bbaf c ccccc cccccddd efffffefa  bbb ccccccccbf ee afdba cbb cccdddec fcfaaeb cb cbccdedddef ff dbcbbcccccc cddcdded ffaaaa bccccccc ccc dddbfe bfcae cbcccce ccaee d ffcbbbbbbf  ec aabbcccec cdceddbfdddd feea bbbcbbbbccccccccdddeeffeaa\n",
      "----\n",
      "iter 600, loss: 2.388193, smooth loss: 7.095778\n",
      "----\n",
      "bbbbbb bbe fddef ffb bbbd c cedcdf fefaabbbb cccccccdfe fffe aabbbb ccbdedb aef baaa  fedcdd  fffaaaaa cccccc c fdc cdf aab cbcccbcccc ce dfcaaf bb bccc ccb ed bebbbcceed fe ffe aaa babd bdfddde fffda babb ccccccc dcfe caab bbbdd eeeff af ababbb  cacccbcb  ccbcaa cb cbc ccbba cecccdfdfcff ffaa bbbbbb bccc bccdded edcc cabc bb cbbbbd e effd bcabbb bccc dfefffe aaaa ba cb bcd eeded fff fafa a b cccc c fd  abaabbbb beccd deefe f b bbcc cccccccc eddddcefee bbbaba ccbc ccddfc aaab cccccccddeeff af baaa bbbb bb d eefffd abebbbb cce ed da ba baa bbbbc  ddfddee aaaaa  cbbdeeff fb ba bbcbc d d f aaa bbbbbb ddcdddd fff aaa abbf bb ccc cccbd ededecdfaa bbbcdddeeee aaa bb cbbcccccc ceef cfcccab ccccde df aaaa bbbccccc ccb cccdb af bbbbbba cbd eddefed aaa acbbcc ddddd fffffaabb ccdccd eeeffff faaa bbbbbb cc dc ec faaa bbbbf ecc ccccccc ccddeeeefeafbbbb cc dbecee bcf ab cccccb cccccddde e aaaaa bcccccccdce dffaaaa bbbb cc ddddcfe ea af ba b cdcdef faaaa bbbbbcccc dcdd ee feaaaa bbbbbbb ddd  a bbbbabb\n",
      "----\n",
      "iter 700, loss: 2.697783, smooth loss: 6.650048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "bbcccccbb bb ccbbccccccb cd df bc caaaacbbbbabbbccccc dd effef bbbbbbbbbbabb cbdc dbeeff afaaba bbbccdcd efeff aaa bbdedded fff fcfe bbc ccbbbbbbb cccccccbdd eeeeeaaa bbbbbbbbb ccccc cbdb aa abbbbbbb ccccccccccbccccdddd ddd fff b bbbbbbcccccccdfdddd cf fff baaaa cbbcccdcdddd cffea ababbbbbddd d febabb bbbb cad dcfbeab bbb bcccccccdddeeee faa a cbbbbbbbccbddeeeee aaaaccccbcccccbb ccdbeec fabbbcccccbccd cfd caacbbbbc cdd ddc aaabbbbbbbbb cbb ccccbd deceff baac ccacccccccccccccccccccccccc dd dadbab ccbddee df ccbbbbbbbce bbdc fcd eaaaaa bb bbbbbbccccbccccbcd dde ebcbbabbbbbbcccc ccbddeefe f afbbbbb bbbb cdc cceff aaaaa bbbbbcccbdfe aaabab bbcccccdcc befeff af bbbbbbf cc cccccdb f cfaabbaaba cccccccdcebeeeeaa babbbbbbbbc cccd dcaf aaaaaba bbcbbbb ccb c  adef fa aaaa bb ba ccccccdccccc cdd e fffff aaabbbbb bb ccc ccbe eefffff aaafa bbbdccccccccccccc ccccccccb ccccccc cdc eeff abbbb cb ccbf cbba cbcbb ebb cdddd cfefeaa bbbbbbccccccccbbbcde ffdaff aaaaa bbbcccccccccddedef faaaaa bb bcccccccccc\n",
      "----\n",
      "iter 800, loss: 1.582103, smooth loss: 6.230779\n",
      "----\n",
      "bbbccccc d dfeff aaabbbbbccccccddddd  fef ccbabbaa bbbbb cccccccccc cccc c dcdd cfffe ff aabbbbabb bcccc ccb acccb ccccddc ababb dccc ccdcc ddcee cfcbabbbb c af dfd bcaba c ccccccccc ccbdddd eefffff babbb bccccccccccbccccccccccccccd fe ff aea bbcccac cd ddf fbaaa bbbbb c cdcc ffdea aaaebb cbd ddddeee ffea acbbbbbbb ccd ef ffdb bbbabbcd eddd ffffe bb abbb cccccf e fff aaf bbbbb cccccccccbcbb ddedeee fff aabbfbccca dddcefffaaf bbbccc ccfcccdc fefa bbbdbbb ccccc cddd efff ff bbabbbbbbbbcd ed faaabab cccccccccccc ddd fcffff babaabb cccc cce fedaaa cccccde fcefa abbbbbb baccd efccff bfbaa bbbb bb ccbbb ccccccccb ce f feaaa bbbb cccccccccced fffff aaaa babb ca fccbcc cbdcc eccf feff aabbbbbc cccdcdfd  faaaabbbb ccccded eee f aaaa bbbbb cccccccccccce dd ffff aaa aaab b bba bbbdd eefefe abbbbccccccccc cccccbcdc ddeeef aaaa bbbbd ccccccbc c cdddd eee ffcff aaaa bbbbbbb cccccccccb bddd fffff fff  bbf bbbbbcc c cdbdd f faf bbbbbbb  ccccccccccc ccbdf e aaabbbbb bb ccccccccccccbcddddd fafaabb bcbb c\n",
      "----\n",
      "iter 900, loss: 1.485817, smooth loss: 5.848238\n",
      "----\n",
      "b ccbbbdde dfaaa bbabb bbc ccccccccddd ee aaaa bbbbbb bbbfb cacccccccccc cdddddedd aaaabbbbb cccb cbcdde fffff  faaa bbbe ccc c fccfc fa bbcccccb fdceeff f bbbbb acccccb cdcccc cdddee ffff aaa bbbb ccccbbc de cffffa aaaccccbb cccbddd edffff aaaa babbbb cdde fef bbbbbba cccddded ffe babbbbcb ccc d effe aaa bbbbbbbb cccccccc de ffffe aaaa abb c cdd dcd fea baaa cbbbb cccccccc ccccee feff caaaa bbcdddd ebaabb bb cccccccccccccccf cee  aaaaaa bbbb cccccd d ffefbaa bbbbbb c cccccbcc edfeff faabbccccddd dabfe babb cccccdd e e ababbbbbd edd aaaa abbacccd ceea aaaa bbbb ccce cc cccccccccccccccc cd ccefb bbbbbbbbbb cccccccccccccd fffffffff bfa bbcbb cccccb ccccccccccccbdd  fffabaa bbb dc efde baaf bbbbb cfddddd e ccaaaa bcccccccbcccdddeeeee aa bbbcdccedd e ababbc ccbb cccccccccc dfddeed cf bbbbbdad eefffeaa bb cccbc c dedc ffff aabbbbb bbb ddfee faaaa abb cbfccc dd eeeee f faaabbb cccccccead fe aaa bbbbb cccb cfccccccfed eeff aaabb cccccddde ffff aa bbbcccccccbb bbdefd caaba baddc fee aaaba dede \n",
      "----\n",
      "iter 1000, loss: 2.191889, smooth loss: 5.481131\n"
     ]
    }
   ],
   "source": [
    "# GRU with Minimal Gated Units with Residual (adding h one step back) - seems to converge a bit faster\n",
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "# Seed random\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read data and setup maps for integer encoding and decoding.\n",
    "data = open('input.txt', 'r').read()\n",
    "chars = sorted(list(set(data))) # Sort makes model predictable (if seeded).\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) } # dict from enumerate: char and index\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) } # dict from enumerate: index and char\n",
    "\n",
    "# Activation functions\n",
    "# NOTE: Derivatives are calculated using outcomes of their primitives (which are already calculated during forward prop).\n",
    "def sigmoid(input, deriv=False):\n",
    "    if deriv:\n",
    "        return input*(1-input) # sig' = sig(1-sig) - only if claculated in forward prop already!\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def tanh(input, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - input ** 2 # tanh'=1-tanh**2\n",
    "    else:\n",
    "        return np.tanh(input)\n",
    "\n",
    "# Derivative is directly calculated in backprop (in combination with cross-entropy loss function).\n",
    "def softmax(input):\n",
    "    # Subtraction of max value improves numerical stability.\n",
    "    e_input = np.exp(input - np.max(input))\n",
    "    return e_input / e_input.sum()\n",
    "\n",
    "# Hyper parameters\n",
    "N, h_size, o_size = vocab_size, vocab_size, vocab_size # Hidden size is set to vocab_size, assuming that level of abstractness is approximately proportional to vocab_size (but can be set to any other value).\n",
    "seq_length = 5 # Longer sequence lengths allow for lengthier latent dependencies to be trained.\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# Model parameter initialization\n",
    "#if os.path.isfile(\"gru_weights.data\"):\n",
    "    #initfile = open(\"gru_weights.data\", \"rb\")\n",
    "    #init_data = np.load(initfile)\n",
    "    #Wy = init_data['Wy']\n",
    "    #Wh = init_data['Wh']\n",
    "    #Wr = init_data['Wr']\n",
    "    #Wz = init_data['Wz']\n",
    "    #Uh = init_data['Uh']\n",
    "    #Ur = init_data['Ur']\n",
    "    #Uz = init_data['Uz']\n",
    "    #by = init_data['by']\n",
    "    #bh = init_data['bh']\n",
    "    #br = init_data['br']\n",
    "    #bz = init_data['bz']\n",
    "    #initfile.close()\n",
    "#else:\n",
    "Wz = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Uz = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "bz = np.zeros((h_size, 1))\n",
    "\n",
    "Wr = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Ur = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "br = np.zeros((h_size, 1))\n",
    "\n",
    "Wh = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Uh = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "bh = np.zeros((h_size, 1))\n",
    "\n",
    "Wy = np.random.rand(o_size, h_size) * 0.1 - 0.05\n",
    "by = np.zeros((o_size, 1))\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    # Initialize variables\n",
    "    x, z, r, h_hat, h, y, p = {}, {}, {}, {}, {-1: hprev}, {}, {} # Dictionaries contain variables for each timestep.\n",
    "    sequence_loss = 0\n",
    "\n",
    "    # Forward prop\n",
    "    for t in range(len(inputs)):\n",
    "        # Set up one-hot encoded input\n",
    "        x[t] = np.zeros((vocab_size, 1)) # for each time step t a one-hot over vocabulary size - init with all 0\n",
    "        x[t][inputs[t]] = 1 # set t'th input to one (current word in one-hot)\n",
    "        \n",
    "        # Calculate update and reset gates\n",
    "        if t>1:\n",
    "            z[t] = sigmoid(np.dot(Wz, x[t]) + np.dot(Uz, h[t-1]) + 0.5*np.dot(Uz, h[t-2]) + bz) # update gate\n",
    "            r[t] = sigmoid(np.dot(Wr, x[t]) + np.dot(Ur, h[t-1]) + 0.5*np.dot(Ur, h[t-2]) + br) # reset gate\n",
    "        else:\n",
    "            z[t] = sigmoid(np.dot(Wz, x[t]) + np.dot(Uz, h[t-1]) + bz) # update gate\n",
    "            r[t] = sigmoid(np.dot(Wr, x[t]) + np.dot(Ur, h[t-1]) + br) # reset gate\n",
    "        \n",
    "        # Calculate hidden units\n",
    "        if t>1:\n",
    "            h_hat[t] = tanh(np.dot(Wh, x[t]) + np.dot(Uh, np.multiply(r[t], h[t-1])) + 0.5*np.dot(Uh, np.multiply(r[t], h[t-2])) + bh)\n",
    "            h[t] = np.multiply(z[t], h[t-1]) + 0.5*np.multiply(z[t], h[t-2]) + np.multiply((1 - z[t]), h_hat[t]) # sometimes denoted s[t]\n",
    "        else: \n",
    "            h_hat[t] = tanh(np.dot(Wh, x[t]) + np.dot(Uh, np.multiply(r[t], h[t-1])) + bh)\n",
    "            h[t] = np.multiply(z[t], h[t-1]) + np.multiply((1 - z[t]), h_hat[t]) # sometimes denoted s[t]\n",
    "        \n",
    "        # Regular output unit\n",
    "        if t>1:\n",
    "            y[t] = np.dot(Wy, h[t]) + 0.5*np.dot(Wy, h[t-1]) + by\n",
    "        else:\n",
    "            y[t] = np.dot(Wy, h[t]) + by\n",
    "        \n",
    "        # Probability distribution\n",
    "        p[t] = softmax(y[t])\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        # targets[t]'s entry of p[t]: since the output is 0 or 1 only one entry contributes (and yj*log(pj) becomes just log(pj))\n",
    "        #loss = -np.sum(np.log(p[t][targets[t]])) # dict p: or time t (=one-hot position), whats the corresponsing target value?\n",
    "        loss = -np.log(p[t][targets[t]])\n",
    "        sequence_loss += loss\n",
    "\n",
    "    # Parameter gradient initialization\n",
    "    dWy, dWh, dWr, dWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n",
    "    dUh, dUr, dUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n",
    "    dby, dbh, dbr, dbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n",
    "    dhnext = np.zeros_like(h[0])\n",
    "    \n",
    "    # Backward prop\n",
    "    for t in reversed(range(len(inputs))): # index t counting down\n",
    "        # âˆ‚loss/âˆ‚y\n",
    "        dy = np.copy(p[t]) # copy output\n",
    "        dy[targets[t]] -= 1 # the current target (truth) is 1 for the current t (an 0 for all other t's)\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wy and âˆ‚loss/âˆ‚by\n",
    "        if t>1:\n",
    "            dWy += np.dot(dy, h[t].T) + 0.5*np.dot(dy, h[t-1].T)\n",
    "        else:\n",
    "            dWy += np.dot(dy, h[t].T) # weight updates: Wy -> Wy - etha * d loss / dWy -> dWy += etha * d loss / dWy\n",
    "        dby += dy # weight for bias is just 1\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        dh = np.dot(Wy.T, dy) + dhnext\n",
    "        dh_hat = np.multiply(dh, (1 - z[t]))\n",
    "        dh_hat_l = dh_hat * tanh(h_hat[t], deriv=True)\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wh, âˆ‚loss/âˆ‚Uh and âˆ‚loss/âˆ‚bh\n",
    "        dWh += np.dot(dh_hat_l, x[t].T)\n",
    "        if t>1:\n",
    "            dUh += np.dot(dh_hat_l, np.multiply(r[t], h[t-1]).T) + 0.5*np.dot(dh_hat_l, np.multiply(r[t], h[t-2]).T)\n",
    "        else:\n",
    "            dUh += np.dot(dh_hat_l, np.multiply(r[t], h[t-1]).T)\n",
    "        dbh += dh_hat_l # weight for bias is just 1\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        drhp = np.dot(Uh.T, dh_hat_l)\n",
    "        if t>1:\n",
    "            dr = np.multiply(drhp, h[t-1]) + 0.5*np.multiply(drhp, h[t-2])\n",
    "        else:\n",
    "            dr = np.multiply(drhp, h[t-1]) \n",
    "        dr_l = dr * sigmoid(r[t], deriv=True) \n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wr, âˆ‚loss/âˆ‚Ur and âˆ‚loss/âˆ‚br\n",
    "        dWr += np.dot(dr_l, x[t].T)\n",
    "        if t>1:\n",
    "            dUr += np.dot(dr_l, h[t-1].T) + 0.5*np.dot(dr_l, h[t-2].T) \n",
    "        else:\n",
    "            dUr += np.dot(dr_l, h[t-1].T) \n",
    "        dbr += dr_l\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        if t>1:\n",
    "            dz = np.multiply(dh, h[t-1] - h_hat[t]) + 0.5*np.multiply(dh, h[t-2] - h_hat[t])\n",
    "        else:\n",
    "            dz = np.multiply(dh, h[t-1] - h_hat[t]) \n",
    "        dz_l = dz * sigmoid(z[t], deriv=True) \n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wz, âˆ‚loss/âˆ‚Uz and âˆ‚loss/âˆ‚bz\n",
    "        dWz += np.dot(dz_l, x[t].T)\n",
    "        if t>1:\n",
    "            dUz += np.dot(dz_l, h[t-1].T) + 0.5*np.dot(dz_l, h[t-2].T)\n",
    "        else:\n",
    "            dUz += np.dot(dz_l, h[t-1].T) \n",
    "        dbz += dz_l \n",
    "        \n",
    "        # All influences of previous layer to loss\n",
    "        dh_fz_inner = np.dot(Uz.T, dz_l) \n",
    "        dh_fz = np.multiply(dh, z[t]) \n",
    "        dh_fhh = np.multiply(drhp, r[t]) \n",
    "        dh_fr = np.dot(Ur.T, dr_l) \n",
    "        \n",
    "        # âˆ‚loss/âˆ‚hð‘¡â‚‹â‚\n",
    "        dhnext = dh_fz_inner + dh_fz + dh_fhh + dh_fr\n",
    "\n",
    "    return sequence_loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, h[len(inputs) - 1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    # Initialize first word of sample ('seed') as one-hot encoded vector.\n",
    "    x = np.zeros((vocab_size, 1)) # np.zeros(shape) -> (vocab_size, 1) = (rows, cols)\n",
    "    x[seed_ix] = 1\n",
    "    ixes = [seed_ix]\n",
    "    \n",
    "    for t in range(n):\n",
    "        # Calculate update and reset gates\n",
    "        z = sigmoid(np.dot(Wz, x) + np.dot(Uz, h) + bz)\n",
    "        r = sigmoid(np.dot(Wr, x) + np.dot(Ur, h) + br)\n",
    "        \n",
    "        # Calculate hidden units\n",
    "        h_hat = tanh(np.dot(Wh, x) + np.dot(Uh, np.multiply(r, h)) + bh)\n",
    "        h = np.multiply(z, h) + np.multiply((1 - z), h_hat)\n",
    "        \n",
    "        # Regular output unit\n",
    "        y = np.dot(Wy, h) + by\n",
    "        \n",
    "        # Probability distribution\n",
    "        p = softmax(y)\n",
    "\n",
    "        # Choose next char according to the distribution\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel()) # ravel returns flattened array, P are the probabilities for choice\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    \n",
    "    return ixes\n",
    "\n",
    "# Initialize sampling parameters and memory gradients (for adagrad)\n",
    "n, p = 0, 0\n",
    "mdWy, mdWh, mdWr, mdWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n",
    "mdUh, mdUr, mdUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n",
    "mdby, mdbh, mdbr, mdbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length\n",
    "\n",
    "print_interval = 100\n",
    "n_runs = 1000\n",
    "\n",
    "while True:\n",
    "    # Reset memory if appropriate\n",
    "    if p + seq_length + 1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((h_size, 1))\n",
    "        p = 0 # current position\n",
    "    \n",
    "    # Get input and target sequence - each an index list of characters\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]] # current position to sequence length\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]] # current position+1 to sequence length+1 (next state)\n",
    "\n",
    "    # Occasionally sample from model and print result\n",
    "    if n % print_interval == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 1000)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n%s\\n----' % (txt, ))\n",
    "\n",
    "    # Get gradients for current model based on input and target sequences\n",
    "    loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    # Occasionally print loss information\n",
    "    if n % print_interval == 0:\n",
    "        print('iter %d, loss: %f, smooth loss: %f' % (n, loss, smooth_loss))\n",
    "\n",
    "    # Update model with adagrad (stochastic) gradient descent\n",
    "    for param, dparam, mem in zip([Wy,  Wh,  Wr,  Wz,  Uh,  Ur,  Uz,  by,  bh,  br,  bz], # zip combines iterables\n",
    "                                  [dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz],\n",
    "                                  [mdWy,mdWh,mdWr,mdWz,mdUh,mdUr,mdUz,mdby,mdbh,mdbr,mdbz]):\n",
    "        np.clip(dparam, -5, 5, out=dparam) # limit values of array - here the changes (dparam has the dWy,...)\n",
    "        mem += dparam * dparam # store the squared change (gradient)\n",
    "        # now actually change the model parameters\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # Small added term for numerical stability\n",
    "\n",
    "    # Prepare for next iteration\n",
    "    p += seq_length\n",
    "    n += 1\n",
    "    \n",
    "    # check exit\n",
    "    if n>n_runs:\n",
    "        break\n",
    "        \n",
    "# save weights\n",
    "#outfile = open(\"gru_weights.data\", \"wb\")\n",
    "#outdata = dict()\n",
    "#outdata['Wy'] = Wy\n",
    "#outdata['Wh'] = Wh\n",
    "#outdata['Wr'] = Wr\n",
    "#outdata['Wz'] = Wz\n",
    "#outdata['Uh'] = Uh\n",
    "#outdata['Ur'] = Ur\n",
    "#outdata['Uz'] = Uz\n",
    "#outdata['by'] = by\n",
    "#outdata['bh'] = bh\n",
    "#outdata['br'] = br\n",
    "#outdata['bz'] = bz\n",
    "#np.savez(outfile, **outdata)\n",
    "#outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
