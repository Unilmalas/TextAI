{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "622a4f5f-d985-490e-af73-1497b64f8c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.25021142338544655\n",
      "Epoch: 1000, Loss: 0.005402412220044528\n",
      "Epoch: 2000, Loss: 0.0018887338702984376\n",
      "Epoch: 3000, Loss: 0.0011047734946855738\n",
      "Epoch: 4000, Loss: 0.0007712445192718557\n",
      "Epoch: 5000, Loss: 0.0005888514888261304\n",
      "Epoch: 6000, Loss: 0.0004745582561519108\n",
      "Epoch: 7000, Loss: 0.0003965162250563846\n",
      "Epoch: 8000, Loss: 0.00033997735949601373\n",
      "Epoch: 9000, Loss: 0.00029720384265399455\n",
      "Epoch: 10000, Loss: 0.0002637562985685239\n",
      "Epoch: 11000, Loss: 0.00023691034963321258\n",
      "Epoch: 12000, Loss: 0.00021490373704381174\n",
      "Epoch: 13000, Loss: 0.00019654728113833103\n",
      "Epoch: 14000, Loss: 0.00018101012031186987\n",
      "Epoch: 15000, Loss: 0.00016769460182468348\n",
      "Epoch: 16000, Loss: 0.00015616006499328207\n",
      "Epoch: 17000, Loss: 0.00014607462087768688\n",
      "Epoch: 18000, Loss: 0.00013718364041651073\n",
      "Epoch: 19000, Loss: 0.00012928857534488206\n",
      "Epoch: 20000, Loss: 0.00012223236977219383\n",
      "Epoch: 21000, Loss: 0.00011588919120944194\n",
      "Epoch: 22000, Loss: 0.00011015706134608055\n",
      "Epoch: 23000, Loss: 0.00010495247563506221\n",
      "Epoch: 24000, Loss: 0.00010020641337430891\n",
      "Epoch: 25000, Loss: 9.586133697154925e-05\n",
      "Epoch: 26000, Loss: 9.186890605712182e-05\n",
      "Epoch: 27000, Loss: 8.818821564867497e-05\n",
      "Epoch: 28000, Loss: 8.478442356734439e-05\n",
      "Epoch: 29000, Loss: 8.16276704833372e-05\n",
      "Epoch: 30000, Loss: 7.86922224095994e-05\n",
      "Epoch: 31000, Loss: 7.5955784039383e-05\n",
      "Epoch: 32000, Loss: 7.339894455086274e-05\n",
      "Epoch: 33000, Loss: 7.100472703670504e-05\n",
      "Epoch: 34000, Loss: 6.875821966891802e-05\n",
      "Epoch: 35000, Loss: 6.664627183317209e-05\n",
      "Epoch: 36000, Loss: 6.465724228086166e-05\n",
      "Epoch: 37000, Loss: 6.278078921304381e-05\n",
      "Epoch: 38000, Loss: 6.1007694382591344e-05\n",
      "Epoch: 39000, Loss: 5.932971496111576e-05\n",
      "Epoch: 40000, Loss: 5.773945819596336e-05\n",
      "Epoch: 41000, Loss: 5.623027487465832e-05\n",
      "Epoch: 42000, Loss: 5.479616838938111e-05\n",
      "Epoch: 43000, Loss: 5.3431716803641464e-05\n",
      "Epoch: 44000, Loss: 5.213200580569588e-05\n",
      "Epoch: 45000, Loss: 5.0892570817266074e-05\n",
      "Epoch: 46000, Loss: 4.970934683347523e-05\n",
      "Epoch: 47000, Loss: 4.857862481734324e-05\n",
      "Epoch: 48000, Loss: 4.749701367221437e-05\n",
      "Epoch: 49000, Loss: 4.6461406978234826e-05\n",
      "Predictions:\n",
      "[[0.00698481]\n",
      " [0.99352722]\n",
      " [0.99320281]\n",
      " [0.00670742]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return x * (1 - x)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"Initializes the neural network with random weights.\"\"\"\n",
    "        self.weights1 = np.random.randn(input_size, hidden_size)  # Weights between input and hidden layer\n",
    "        self.bias1 = np.random.randn(1, hidden_size)       # Biases for the hidden layer\n",
    "        self.weights2 = np.random.randn(hidden_size, output_size) # Weights between hidden and output layer\n",
    "        self.bias2 = np.random.randn(1, output_size)      # Biases for the output layer\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Performs forward propagation.\"\"\"\n",
    "        self.hidden_layer_output = sigmoid(np.dot(X, self.weights1) + self.bias1)\n",
    "        self.output = sigmoid(np.dot(self.hidden_layer_output, self.weights2) + self.bias2)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        \"\"\"Performs backpropagation and updates weights.\"\"\"\n",
    "\n",
    "        # Output layer\n",
    "        error_output = y - self.output\n",
    "        delta_output = error_output * sigmoid_derivative(self.output)\n",
    "        \n",
    "        # Hidden layer\n",
    "        error_hidden = delta_output.dot(self.weights2.T)\n",
    "        delta_hidden = error_hidden * sigmoid_derivative(self.hidden_layer_output)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights2 += self.hidden_layer_output.T.dot(delta_output) * learning_rate\n",
    "        self.bias2 += np.sum(delta_output, axis=0, keepdims=True) * learning_rate\n",
    "        self.weights1 += X.T.dot(delta_hidden) * learning_rate\n",
    "        self.bias1 += np.sum(delta_hidden, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, X, y, epochs=10000, learning_rate=0.1):\n",
    "        \"\"\"Trains the neural network.\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, learning_rate)\n",
    "            if epoch % 1000 == 0:  # Print loss every 1000 epochs (optional)\n",
    "                loss = np.mean(np.square(y - output)) # Example loss function (MSE)\n",
    "                print(f\"Epoch: {epoch}, Loss: {loss}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Makes predictions.\"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "\n",
    "# Example usage (XOR problem):\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input data\n",
    "y = np.array([[0], [1], [1], [0]])  # Target output\n",
    "\n",
    "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1) # 2 inputs, 4 hidden nodes, 1 output\n",
    "nn.train(X, y, epochs=50000, learning_rate=0.5) # Train the model\n",
    "\n",
    "# Make predictions:\n",
    "predictions = nn.predict(X)\n",
    "print(\"Predictions:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23bfb0c3-abb4-494d-88e3-c74039abb9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 4, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[0, 0, 0],\n",
    "       [0, 1, 0],\n",
    "       [0, 2, 0],\n",
    "       [1, 0, 0],\n",
    "       [1, 1, 0]])\n",
    "\n",
    "np.sum(a, axis=0, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "d844d0b6-cbc0-4649-aa11-5c4e7ae86ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.25197498248415096\n",
      "Epoch: 2000, Loss: 0.0020451762524309127\n",
      "Epoch: 4000, Loss: 0.0007178594998462216\n",
      "Epoch: 6000, Loss: 0.000419011496692839\n",
      "Epoch: 8000, Loss: 0.00029131202452414254\n",
      "Epoch: 10000, Loss: 0.00022137813534038093\n",
      "Epoch: 12000, Loss: 0.0001775598364971579\n",
      "Epoch: 14000, Loss: 0.0001476687526316816\n",
      "Epoch: 16000, Loss: 0.0001260458523787279\n",
      "Epoch: 18000, Loss: 0.00010971688813369982\n",
      "Epoch: 20000, Loss: 9.697362009387609e-05\n",
      "Epoch: 22000, Loss: 8.676709430930543e-05\n",
      "Epoch: 24000, Loss: 7.84186283954392e-05\n",
      "Epoch: 26000, Loss: 7.147025568697961e-05\n",
      "Epoch: 28000, Loss: 6.560207743886969e-05\n",
      "Epoch: 30000, Loss: 6.0584077865832056e-05\n",
      "Epoch: 32000, Loss: 5.624675289432194e-05\n",
      "Epoch: 34000, Loss: 5.24625205257304e-05\n",
      "Epoch: 36000, Loss: 4.913356968327758e-05\n",
      "Epoch: 38000, Loss: 4.618369256586239e-05\n",
      "Epoch: 40000, Loss: 4.355265868396584e-05\n",
      "Epoch: 42000, Loss: 4.119225505351232e-05\n",
      "Epoch: 44000, Loss: 3.906344505898866e-05\n",
      "Epoch: 46000, Loss: 3.71342945918336e-05\n",
      "Epoch: 48000, Loss: 3.5378434619320445e-05\n",
      "Predictions:\n",
      "[[0.00550215]\n",
      " [0.99280388]\n",
      " [0.99575711]\n",
      " [0.00591909]]\n"
     ]
    }
   ],
   "source": [
    "# mathematically correct backprop  - numerically unstable\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_stable(x):\n",
    "    \"Numerically stable sigmoid function.\"\n",
    "    if x.any() >= 0:\n",
    "        z = np.exp(-x + 1e-3)\n",
    "        return 1 / (1 + z)\n",
    "    else:\n",
    "        # if x is less than zero then z will be small, denom can't be\n",
    "        # zero because it's 1+z.\n",
    "        z = np.exp(x)\n",
    "        return z / (1 + z)\n",
    "\n",
    "def exp_normalize(x):  # use exp-normalize trick\n",
    "    b = np.max(x)\n",
    "    y = np.exp(x - b)\n",
    "    return y / y.sum()\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    xs = sigmoid(x)\n",
    "    return xs * (1 - xs)\n",
    "\n",
    "def sigmoid_d_sigin(x):\n",
    "    \"\"\"Derivative of the sigmoid function - x has to be sigmoid already\"\"\"\n",
    "    return x * (1 - x)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "    return x * (x > 0)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"Derivative of the ReLU function.\"\"\"\n",
    "    return 1. * (x > 0)\n",
    "    \n",
    "    \n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"Initializes the neural network with random weights.\"\"\"\n",
    "        self.weights1 = np.random.randn(input_size, hidden_size)  # Weights between input and hidden layer\n",
    "        self.bias1 = np.random.randn(1, hidden_size)       # Biases for the hidden layer\n",
    "        self.weights2 = np.random.randn(hidden_size, output_size) # Weights between hidden and output layer\n",
    "        self.bias2 = np.random.randn(1, output_size)      # Biases for the output layer\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Performs forward propagation.\"\"\"\n",
    "        self.hidden_layer_output = sigmoid(np.dot(X, self.weights1) + self.bias1)\n",
    "        self.output = sigmoid(np.dot(self.hidden_layer_output, self.weights2) + self.bias2)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        \"\"\"Performs backpropagation and updates weights.\"\"\"\n",
    "        #print(self.weights1, self.weights2)\n",
    "        # Output layer\n",
    "        error_output = y - self.output\n",
    "        delta_output = error_output * sigmoid_d_sigin(self.output)  # careful: need to use x * (1-x) here as sig_deriv as we pass the output (sig)\n",
    "        #delta_output_full = error_output * sigmoid_derivative(np.dot(self.hidden_layer_output, self.weights2))  # instability is here\n",
    "        #delta_output_full = error_output * relu_derivative(self.output)  # this still keeps the bias in even as the deriv with respect to w2 would lose it\n",
    "        \n",
    "        # Hidden layer\n",
    "        error_hidden = delta_output.dot(self.weights2.T)\n",
    "        delta_hidden = error_hidden * sigmoid_d_sigin(self.hidden_layer_output)\n",
    "        #delta_hidden_full = delta_output_full.dot(self.weights2.T) * sigmoid_derivative(np.dot(X, self.weights1) + self.bias1)  # bias-term stabilizes here\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights2 += self.hidden_layer_output.T.dot(delta_output) * learning_rate\n",
    "        #self.weights2 += self.hidden_layer_output.T.dot(delta_output_full) * learning_rate\n",
    "        self.bias2 += np.sum(delta_output, axis=0, keepdims=True) * learning_rate\n",
    "        #self.bias2 += np.sum(error_output, axis=0, keepdims=True) * learning_rate\n",
    "        self.weights1 += X.T.dot(delta_hidden) * learning_rate\n",
    "        #self.weights1 += (np.dot(self.weights2.T, delta_hidden_full).T * X).T * learning_rate  # the problem is here\n",
    "        self.bias1 += np.sum(delta_hidden, axis=0, keepdims=True) * learning_rate\n",
    "        #self.bias1 += np.sum(np.dot(delta_hidden_full.T, self.weights2), axis=0, keepdims=True) * learning_rate\n",
    "        #self.bias1 += np.sum(delta_hidden_full, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "\n",
    "    def train(self, X, y, epochs=10000, learning_rate=0.1, output_every=1000):\n",
    "        \"\"\"Trains the neural network.\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, learning_rate)\n",
    "            if epoch % output_every == 0:  # Print loss every 1000 epochs (optional)\n",
    "                loss = np.mean(np.square(y - output)) # Example loss function (MSE)\n",
    "                print(f\"Epoch: {epoch}, Loss: {loss}\")\n",
    "                # optional pruning\n",
    "                #self.weights1[abs(self.weights1) < 1e-2] = 0\n",
    "                #self.weights2[abs(self.weights2) < 1e-2] = 0\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Makes predictions.\"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "\n",
    "# Example usage (XOR problem):\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input data\n",
    "y = np.array([[0], [1], [1], [0]])  # Target output\n",
    "# input variable standardization omitted  [xi - min(x)] / [max(x) - min(x)]\n",
    "\n",
    "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)  # 2 inputs, 4 hidden nodes, 1 output\n",
    "nn.train(X, y, epochs=50000, learning_rate=0.5, output_every=2000)  # Train the model\n",
    "\n",
    "# Make predictions:\n",
    "predictions = nn.predict(X)\n",
    "print(\"Predictions:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "10f8cc74-b6dc-4562-8c20-8a2e9919dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pyimagesearch.com/2021/05/06/backpropagation-from-scratch-with-python/\n",
    "# import the necessary packages\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, alpha=0.1):\n",
    "        # initialize the list of weights matrices, then store the\n",
    "        # network architecture and learning rate\n",
    "        self.W = []\n",
    "        self.layers = layers\n",
    "        self.alpha = alpha\n",
    "        # start looping from the index of the first layer but\n",
    "        # stop before we reach the last two layers\n",
    "        for i in np.arange(0, len(layers) - 2):\n",
    "            # randomly initialize a weight matrix connecting the\n",
    "            # number of nodes in each respective layer together,\n",
    "            # adding an extra node for the bias\n",
    "            w = np.random.randn(layers[i] + 1, layers[i + 1] + 1)\n",
    "            self.W.append(w / np.sqrt(layers[i]))\n",
    "        # the last two layers are a special case where the input\n",
    "        # connections need a bias term but the output does not\n",
    "        w = np.random.randn(layers[-2] + 1, layers[-1])\n",
    "        self.W.append(w / np.sqrt(layers[-2]))\n",
    "        \n",
    "    def __repr__(self):\n",
    "        # construct and return a string that represents the network\n",
    "        # architecture\n",
    "        return \"NeuralNetwork: {}\".format(\"-\".join(str(l) for l in self.layers))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        # compute and return the sigmoid activation value for a\n",
    "        # given input value\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_deriv(self, x):\n",
    "        # compute the derivative of the sigmoid function ASSUMING\n",
    "        # that x has already been passed through the 'sigmoid'\n",
    "        # function\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def fit(self, X, y, epochs=1000, displayUpdate=100):\n",
    "        # insert a column of 1's as the last entry in the feature\n",
    "        # matrix -- this little trick allows us to treat the bias\n",
    "        # as a trainable parameter within the weight matrix\n",
    "        X = np.c_[X, np.ones((X.shape[0]))]\n",
    "        # loop over the desired number of epochs\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            # loop over each individual data point and train\n",
    "            # our network on it\n",
    "            for (x, target) in zip(X, y):\n",
    "                self.fit_partial(x, target)\n",
    "            # check to see if we should display a training update\n",
    "            if epoch == 0 or (epoch + 1) % displayUpdate == 0:\n",
    "                loss = self.calculate_loss(X, y)\n",
    "                print(\"[INFO] epoch={}, loss={:.7f}\".format(\n",
    "                    epoch + 1, loss))\n",
    "\n",
    "    def fit_partial(self, x, y):\n",
    "        # construct our list of output activations for each layer\n",
    "        # as our data point flows through the network; the first\n",
    "        # activation is a special case -- it's just the input\n",
    "        # feature vector itself\n",
    "        A = [np.atleast_2d(x)]\n",
    "        # FEEDFORWARD:\n",
    "        # loop over the layers in the network\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # feedforward the activation at the current layer by\n",
    "            # taking the dot product between the activation and\n",
    "            # the weight matrix -- this is called the \"net input\"\n",
    "            # to the current layer\n",
    "            net = A[layer].dot(self.W[layer])\n",
    "            # computing the \"net output\" is simply applying our\n",
    "            # nonlinear activation function to the net input\n",
    "            out = self.sigmoid(net)\n",
    "            # once we have the net output, add it to our list of\n",
    "            # activations\n",
    "            A.append(out)\n",
    "        # BACKPROPAGATION\n",
    "        # the first phase of backpropagation is to compute the\n",
    "        # difference between our *prediction* (the final output\n",
    "        # activation in the activations list) and the true target\n",
    "        # value\n",
    "        error = A[-1] - y\n",
    "        # from here, we need to apply the chain rule and build our\n",
    "        # list of deltas 'D'; the first entry in the deltas is\n",
    "        # simply the error of the output layer times the derivative\n",
    "        # of our activation function for the output value\n",
    "        D = [error * self.sigmoid_deriv(A[-1])]\n",
    "        # once you understand the chain rule it becomes super easy\n",
    "        # to implement with a 'for' loop -- simply loop over the\n",
    "        # layers in reverse order (ignoring the last two since we\n",
    "        # already have taken them into account)\n",
    "        for layer in np.arange(len(A) - 2, 0, -1):\n",
    "            # the delta for the current layer is equal to the delta\n",
    "            # of the *previous layer* dotted with the weight matrix\n",
    "            # of the current layer, followed by multiplying the delta\n",
    "            # by the derivative of the nonlinear activation function\n",
    "            # for the activations of the current layer\n",
    "            delta = D[-1].dot(self.W[layer].T)\n",
    "            delta = delta * self.sigmoid_deriv(A[layer])\n",
    "            D.append(delta)\n",
    "        # since we looped over our layers in reverse order we need to\n",
    "        # reverse the deltas\n",
    "        D = D[::-1]\n",
    "        # WEIGHT UPDATE PHASE\n",
    "        # loop over the layers\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # update our weights by taking the dot product of the layer\n",
    "            # activations with their respective deltas, then multiplying\n",
    "            # this value by some small learning rate and adding to our\n",
    "            # weight matrix -- this is where the actual \"learning\" takes\n",
    "            # place\n",
    "            self.W[layer] += -self.alpha * A[layer].T.dot(D[layer])\n",
    "\n",
    "    def predict(self, X, addBias=True):\n",
    "        # initialize the output prediction as the input features -- this\n",
    "        # value will be (forward) propagated through the network to\n",
    "        # obtain the final prediction\n",
    "        p = np.atleast_2d(X)\n",
    "        # check to see if the bias column should be added\n",
    "        if addBias:\n",
    "            # insert a column of 1's as the last entry in the feature\n",
    "            # matrix (bias)\n",
    "            p = np.c_[p, np.ones((p.shape[0]))]\n",
    "        # loop over our layers in the network\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # computing the output prediction is as simple as taking\n",
    "            # the dot product between the current activation value 'p'\n",
    "            # and the weight matrix associated with the current layer,\n",
    "            # then passing this value through a nonlinear activation\n",
    "            # function\n",
    "            p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "        # return the predicted value\n",
    "        return p\n",
    "\n",
    "    def calculate_loss(self, X, targets):\n",
    "        # make predictions for the input data points then compute\n",
    "        # the loss\n",
    "        targets = np.atleast_2d(targets)\n",
    "        predictions = self.predict(X, addBias=False)\n",
    "        loss = 0.5 * np.sum((predictions - targets) ** 2)\n",
    "        # return the loss\n",
    "        return loss\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "7152c320-d7e9-45ca-8690-65e1b6afb95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork: 2-2-1\n"
     ]
    }
   ],
   "source": [
    "#from pyimagesearch.nn import NeuralNetwork\n",
    "\n",
    "nn = NeuralNetwork([2, 2, 1])\n",
    "print(nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "912ff201-963f-404e-82a3-17758f3d720e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch=1, loss=0.5052719\n",
      "[INFO] epoch=500, loss=0.4899362\n",
      "[INFO] epoch=1000, loss=0.2722973\n",
      "[INFO] epoch=1500, loss=0.1639889\n",
      "[INFO] epoch=2000, loss=0.1404335\n",
      "[INFO] epoch=2500, loss=0.1338330\n",
      "[INFO] epoch=3000, loss=0.1310129\n",
      "[INFO] epoch=3500, loss=0.1294984\n",
      "[INFO] epoch=4000, loss=0.1285673\n",
      "[INFO] epoch=4500, loss=0.1279422\n",
      "[INFO] epoch=5000, loss=0.1274958\n",
      "[INFO] epoch=5500, loss=0.1271622\n",
      "[INFO] epoch=6000, loss=0.1269040\n",
      "[INFO] epoch=6500, loss=0.1266986\n",
      "[INFO] epoch=7000, loss=0.1265315\n",
      "[INFO] epoch=7500, loss=0.1263932\n",
      "[INFO] epoch=8000, loss=0.1262767\n",
      "[INFO] epoch=8500, loss=0.1261775\n",
      "[INFO] epoch=9000, loss=0.1260919\n",
      "[INFO] epoch=9500, loss=0.1260174\n",
      "[INFO] epoch=10000, loss=0.1259519\n",
      "[INFO] epoch=10500, loss=0.1258940\n",
      "[INFO] epoch=11000, loss=0.1258424\n",
      "[INFO] epoch=11500, loss=0.1257961\n",
      "[INFO] epoch=12000, loss=0.1257544\n",
      "[INFO] epoch=12500, loss=0.1257166\n",
      "[INFO] epoch=13000, loss=0.1256822\n",
      "[INFO] epoch=13500, loss=0.1256508\n",
      "[INFO] epoch=14000, loss=0.1256220\n",
      "[INFO] epoch=14500, loss=0.1255954\n",
      "[INFO] epoch=15000, loss=0.1255709\n",
      "[INFO] epoch=15500, loss=0.1255482\n",
      "[INFO] epoch=16000, loss=0.1255270\n",
      "[INFO] epoch=16500, loss=0.1255074\n",
      "[INFO] epoch=17000, loss=0.1254890\n",
      "[INFO] epoch=17500, loss=0.1254718\n",
      "[INFO] epoch=18000, loss=0.1254557\n",
      "[INFO] epoch=18500, loss=0.1254405\n",
      "[INFO] epoch=19000, loss=0.1254262\n",
      "[INFO] epoch=19500, loss=0.1254127\n",
      "[INFO] epoch=20000, loss=0.1254000\n",
      "[INFO] data=[0 0], ground-truth=0, pred=0.0067, step=0\n",
      "[INFO] data=[0 1], ground-truth=1, pred=0.9880, step=1\n",
      "[INFO] data=[1 0], ground-truth=1, pred=0.9877, step=1\n",
      "[INFO] data=[1 1], ground-truth=0, pred=0.5005, step=1\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "#from pyimagesearch.nn import NeuralNetwork\n",
    "import numpy as np\n",
    "\n",
    "# construct the XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "# define our 2-2-1 neural network and train it\n",
    "nn = NeuralNetwork([2, 2, 1], alpha=0.5)\n",
    "nn.fit(X, y, epochs=20000, displayUpdate=500)\n",
    "# now that our network is trained, loop over the XOR data points\n",
    "for (x, target) in zip(X, y):\n",
    "\t# make a prediction on the data point and display the result\n",
    "\t# to our console\n",
    "\tpred = nn.predict(x)[0][0]\n",
    "\tstep = 1 if pred > 0.5 else 0\n",
    "    \n",
    "\tprint(\"[INFO] data={}, ground-truth={}, pred={:.4f}, step={}\".format(\n",
    "\t\tx, target[0], pred, step))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "9aba4100-c41d-411d-8834-5d1bc17b926d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch=1, loss=0.5880017\n",
      "[INFO] epoch=500, loss=0.1150901\n",
      "[INFO] epoch=1000, loss=0.0139236\n",
      "[INFO] epoch=1500, loss=0.0063214\n",
      "[INFO] epoch=2000, loss=0.0039370\n",
      "[INFO] epoch=2500, loss=0.0028123\n",
      "[INFO] epoch=3000, loss=0.0021682\n",
      "[INFO] epoch=3500, loss=0.0017545\n",
      "[INFO] epoch=4000, loss=0.0014680\n",
      "[INFO] epoch=4500, loss=0.0012587\n",
      "[INFO] epoch=5000, loss=0.0010995\n",
      "[INFO] epoch=5500, loss=0.0009746\n",
      "[INFO] epoch=6000, loss=0.0008742\n",
      "[INFO] epoch=6500, loss=0.0007918\n",
      "[INFO] epoch=7000, loss=0.0007230\n",
      "[INFO] epoch=7500, loss=0.0006648\n",
      "[INFO] epoch=8000, loss=0.0006150\n",
      "[INFO] epoch=8500, loss=0.0005718\n",
      "[INFO] epoch=9000, loss=0.0005341\n",
      "[INFO] epoch=9500, loss=0.0005009\n",
      "[INFO] epoch=10000, loss=0.0004714\n",
      "[INFO] epoch=10500, loss=0.0004451\n",
      "[INFO] epoch=11000, loss=0.0004215\n",
      "[INFO] epoch=11500, loss=0.0004001\n",
      "[INFO] epoch=12000, loss=0.0003808\n",
      "[INFO] epoch=12500, loss=0.0003632\n",
      "[INFO] epoch=13000, loss=0.0003470\n",
      "[INFO] epoch=13500, loss=0.0003322\n",
      "[INFO] epoch=14000, loss=0.0003186\n",
      "[INFO] epoch=14500, loss=0.0003060\n",
      "[INFO] epoch=15000, loss=0.0002944\n",
      "[INFO] epoch=15500, loss=0.0002835\n",
      "[INFO] epoch=16000, loss=0.0002734\n",
      "[INFO] epoch=16500, loss=0.0002640\n",
      "[INFO] epoch=17000, loss=0.0002552\n",
      "[INFO] epoch=17500, loss=0.0002469\n",
      "[INFO] epoch=18000, loss=0.0002392\n",
      "[INFO] epoch=18500, loss=0.0002319\n",
      "[INFO] epoch=19000, loss=0.0002250\n",
      "[INFO] epoch=19500, loss=0.0002185\n",
      "[INFO] epoch=20000, loss=0.0002124\n"
     ]
    }
   ],
   "source": [
    "# define our 2-2-1 neural network and train it\n",
    "nn = NeuralNetwork([2, 2, 1], alpha=0.5)\n",
    "nn.fit(X, y, epochs=20000, displayUpdate=500)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "92ab99a2-d195-4f34-aade-97c90bfe3268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch=1, loss=0.6030635\n",
      "[INFO] epoch=500, loss=0.5007939\n",
      "[INFO] epoch=1000, loss=0.5007938\n",
      "[INFO] epoch=1500, loss=0.5007938\n",
      "[INFO] epoch=2000, loss=0.5007938\n",
      "[INFO] epoch=2500, loss=0.5007938\n",
      "[INFO] epoch=3000, loss=0.5007938\n",
      "[INFO] epoch=3500, loss=0.5007938\n",
      "[INFO] epoch=4000, loss=0.5007938\n",
      "[INFO] epoch=4500, loss=0.5007938\n",
      "[INFO] epoch=5000, loss=0.5007938\n",
      "[INFO] epoch=5500, loss=0.5007938\n",
      "[INFO] epoch=6000, loss=0.5007938\n",
      "[INFO] epoch=6500, loss=0.5007938\n",
      "[INFO] epoch=7000, loss=0.5007938\n",
      "[INFO] epoch=7500, loss=0.5007938\n",
      "[INFO] epoch=8000, loss=0.5007938\n",
      "[INFO] epoch=8500, loss=0.5007938\n",
      "[INFO] epoch=9000, loss=0.5007938\n",
      "[INFO] epoch=9500, loss=0.5007938\n",
      "[INFO] epoch=10000, loss=0.5007938\n",
      "[INFO] epoch=10500, loss=0.5007938\n",
      "[INFO] epoch=11000, loss=0.5007938\n",
      "[INFO] epoch=11500, loss=0.5007938\n",
      "[INFO] epoch=12000, loss=0.5007938\n",
      "[INFO] epoch=12500, loss=0.5007938\n",
      "[INFO] epoch=13000, loss=0.5007938\n",
      "[INFO] epoch=13500, loss=0.5007938\n",
      "[INFO] epoch=14000, loss=0.5007938\n",
      "[INFO] epoch=14500, loss=0.5007938\n",
      "[INFO] epoch=15000, loss=0.5007938\n",
      "[INFO] epoch=15500, loss=0.5007938\n",
      "[INFO] epoch=16000, loss=0.5007938\n",
      "[INFO] epoch=16500, loss=0.5007938\n",
      "[INFO] epoch=17000, loss=0.5007938\n",
      "[INFO] epoch=17500, loss=0.5007938\n",
      "[INFO] epoch=18000, loss=0.5007938\n",
      "[INFO] epoch=18500, loss=0.5007938\n",
      "[INFO] epoch=19000, loss=0.5007938\n",
      "[INFO] epoch=19500, loss=0.5007938\n",
      "[INFO] epoch=20000, loss=0.5007938\n"
     ]
    }
   ],
   "source": [
    "# define our 2-1 neural network and train it\n",
    "nn = NeuralNetwork([2, 1], alpha=0.5)\n",
    "nn.fit(X, y, epochs=20000, displayUpdate=500)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "9b33a6a7-d6a1-4d9d-9cbb-640e4c314077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading MNIST (sample) dataset...\n",
      "[INFO] samples: 1797, dim: 64\n",
      "[INFO] training network...\n",
      "[INFO] NeuralNetwork: 64-32-16-10\n",
      "[INFO] epoch=1, loss=605.2862818\n",
      "[INFO] epoch=100, loss=7.7118261\n",
      "[INFO] epoch=200, loss=3.0804622\n",
      "[INFO] epoch=300, loss=2.5305348\n",
      "[INFO] epoch=400, loss=1.8910584\n",
      "[INFO] epoch=500, loss=1.7701411\n",
      "[INFO] epoch=600, loss=1.7087344\n",
      "[INFO] epoch=700, loss=1.6699285\n",
      "[INFO] epoch=800, loss=1.6430435\n",
      "[INFO] epoch=900, loss=1.6233045\n",
      "[INFO] epoch=1000, loss=1.6082008\n",
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        44\n",
      "           1       0.96      1.00      0.98        46\n",
      "           2       0.95      0.97      0.96        38\n",
      "           3       0.97      0.91      0.94        35\n",
      "           4       0.91      0.98      0.94        41\n",
      "           5       0.96      0.98      0.97        47\n",
      "           6       1.00      0.97      0.98        58\n",
      "           7       0.98      0.96      0.97        51\n",
      "           8       0.97      0.95      0.96        41\n",
      "           9       0.98      0.98      0.98        49\n",
      "\n",
      "    accuracy                           0.97       450\n",
      "   macro avg       0.97      0.97      0.97       450\n",
      "weighted avg       0.97      0.97      0.97       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "#from pyimagesearch.nn import NeuralNetwork\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets\n",
    "\n",
    "# load the MNIST dataset and apply min/max scaling to scale the\n",
    "# pixel intensity values to the range [0, 1] (each image is\n",
    "# represented by an 8 x 8 = 64-dim feature vector)\n",
    "print(\"[INFO] loading MNIST (sample) dataset...\")\n",
    "digits = datasets.load_digits()\n",
    "data = digits.data.astype(\"float\")\n",
    "data = (data - data.min()) / (data.max() - data.min())\n",
    "print(\"[INFO] samples: {}, dim: {}\".format(data.shape[0],\n",
    "\tdata.shape[1]))\n",
    "# construct the training and testing splits\n",
    "(trainX, testX, trainY, testY) = train_test_split(data,\n",
    "\tdigits.target, test_size=0.25)\n",
    "# convert the labels from integers to vectors\n",
    "trainY = LabelBinarizer().fit_transform(trainY)\n",
    "testY = LabelBinarizer().fit_transform(testY)\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "nn = NeuralNetwork([trainX.shape[1], 32, 16, 10])\n",
    "print(\"[INFO] {}\".format(nn))\n",
    "nn.fit(trainX, trainY, epochs=1000)\n",
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = nn.predict(testX)\n",
    "predictions = predictions.argmax(axis=1)\n",
    "print(classification_report(testY.argmax(axis=1), predictions))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3058a7-317d-4b34-9d7f-f6f68af75154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/rasbt/MachineLearning-QandAI-book?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeada01-903b-473f-8702-eeea50b379c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bdev)",
   "language": "python",
   "name": "bdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
