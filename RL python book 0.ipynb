{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip3 install tensorflow`\n",
    "\n",
    "or\n",
    "\n",
    "`pip3 install tensorflow-gpu`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI Gym installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On OSX: \n",
    "\n",
    "`brew install cmake boost boost-python sdl2 swig wget`\n",
    " \n",
    "On Ubuntu 16.04:\n",
    "\n",
    "`apt-get install -y python-pyglet python3-opengl zlib1g-dev libjpeg-dev patchelf cmake swig libboost-all-dev libsdl2-dev libosmesa6-dev xvfb ffmpeg`\n",
    "\n",
    "On Ubuntu 18.04\n",
    "\n",
    "`sudo apt install -y python3-dev zlib1g-dev libjpeg-dev cmake swig python-pyglet python3-opengl libboost-all-dev libsdl2-dev libosmesa6-dev patchelf ffmpeg xvfb `\n",
    "\n",
    "Then:\n",
    "\n",
    "```\n",
    "git clone https://github.com/openai/gym.git \n",
    "\n",
    "cd gym\n",
    "\n",
    "pip install -e '.[all]'\n",
    "```\n",
    "\n",
    "PyBox2D:\n",
    "\n",
    "```\n",
    "git clone https://github.com/pybox2d/pybox2d\n",
    "cd pybox2d\n",
    "pip3 install -e .\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duckietown installation\n",
    "\n",
    "```\n",
    "git clone https://github.com/duckietown/gym-duckietown.git\n",
    "cd gym-duckietown\n",
    "pip3 install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roboschool installation\n",
    "\n",
    "```\n",
    "git clone https://github.com/openai/roboschool\n",
    "cd roboschool\n",
    "ROBOSCHOOL_PATH=`pwd`\n",
    "git clone https://github.com/olegklimov/bullet3 -b roboschool_self_collision\n",
    "mkdir bullet3/build\n",
    "cd    bullet3/build\n",
    "cmake -DBUILD_SHARED_LIBS=ON -DUSE_DOUBLE_PRECISION=1 -DCMAKE_INSTALL_PREFIX:PATH=$ROBOSCHOOL_PATH/roboschool/cpp-household/bullet_local_install -DBUILD_CPU_DEMOS=OFF -DBUILD_BULLET2_DEMOS=OFF -DBUILD_EXTRAS=OFF  -DBUILD_UNIT_TESTS=OFF -DBUILD_CLSOCKET=OFF -DBUILD_ENET=OFF -DBUILD_OPENGL3_DEMOS=OFF ..\n",
    "\n",
    "make -j4\n",
    "make install\n",
    "cd ../..\n",
    "pip3 install -e $ROBOSCHOOL_PATH\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "\n",
    "# create the environment \n",
    "env = gym.make(\"CartPole-v1\")\n",
    "# reset the environment before starting\n",
    "env.reset()\n",
    "\n",
    "# loop 10 times\n",
    "for i in range(10):\n",
    "    # take a random action\n",
    "    env.step(env.action_space.sample())\n",
    "    # render the game\n",
    "    env.render()\n",
    "\n",
    "time.sleep(10)\n",
    "    \n",
    "# close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished, reward:38\n",
      "Episode 1 finished, reward:31\n",
      "Episode 2 finished, reward:24\n",
      "Episode 3 finished, reward:9\n",
      "Episode 4 finished, reward:19\n",
      "Episode 5 finished, reward:61\n",
      "Episode 6 finished, reward:9\n",
      "Episode 7 finished, reward:16\n",
      "Episode 8 finished, reward:38\n",
      "Episode 9 finished, reward:17\n",
      "Episode 10 finished, reward:22\n",
      "Episode 11 finished, reward:19\n",
      "Episode 12 finished, reward:34\n",
      "Episode 13 finished, reward:11\n",
      "Episode 14 finished, reward:10\n",
      "Episode 15 finished, reward:19\n",
      "Episode 16 finished, reward:33\n",
      "Episode 17 finished, reward:18\n",
      "Episode 18 finished, reward:10\n",
      "Episode 19 finished, reward:20\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "\n",
    "# create and initialize the environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "\n",
    "# play 10 games\n",
    "for i in range(20):\n",
    "    # initialize the variables\n",
    "    done = False\n",
    "    game_rew = 0\n",
    "\n",
    "    while not done:\n",
    "        # choose a random action\n",
    "        action = env.action_space.sample()\n",
    "        # take a step in the environment\n",
    "        new_obs, rew, done, info = env.step(action)\n",
    "        game_rew += rew\n",
    "        \n",
    "        env.render() # show what we are doing\n",
    "        #time.sleep(1)\n",
    "    \n",
    "        # when is done, print the cumulative reward of the game and reset the environment\n",
    "        if done:\n",
    "            print('Episode %d finished, reward:%d' % (i, game_rew))\n",
    "            env.reset()\n",
    "            \n",
    "# close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "print(env.observation_space) # Discrete (space that allows a fixed range of non-neg numbers) or Box (n-dim array) class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space) # Discrete(2): actions have values 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.sample()) # draw a sample from the action space\n",
    "print(env.action_space.sample())\n",
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Discrete' object has no attribute 'low'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4df1bfb271ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# min/max allowed in the Box space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Discrete' object has no attribute 'low'"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.low) # min/max allowed in the Box space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# create two constants: a and b\n",
    "#a = tf.constant(4)\n",
    "#b = tf.constant(3)\n",
    "\n",
    "# perform a computation\n",
    "#c = a + b\n",
    "#print(c) # print the shape of c\n",
    "\n",
    "# create a session\n",
    "#session = tf.Session()\n",
    "#session = tf.compat.v1.Session() \n",
    "# run the session. It compute the sum\n",
    "#res = session.run(c)\n",
    "#print(res) # print the actual result\n",
    "\n",
    " # Launch the graph in a session.\n",
    "with tf.compat.v1.Session() as ses:\n",
    "    # Build a graph.\n",
    "    a = tf.constant(5.0)\n",
    "    b = tf.constant(6.0)\n",
    "    c = a + b\n",
    "\n",
    "    # Evaluate the tensor `c`.\n",
    "    print(ses.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the graph\n",
    "#tf.reset_default_graph()\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(1) # tensors are arrays of any number of dimensions\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "# array of five elements\n",
    "b = tf.constant([1,2,3,4,5])\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#NB: a can be of any type of tensor\n",
    "#a = tf.constant([1,2,3,4,5])\n",
    "#first_three_elem = a[:3]\n",
    "#fourth_elem = a[3]\n",
    "\n",
    "#sess = tf.Session()\n",
    "#sess = tf.compat.v1.Session()\n",
    "#print(sess.run(first_three_elem))\n",
    "\n",
    "with tf.compat.v1.Session() as ses:\n",
    "    a = tf.constant([1,2,3,4,5])\n",
    "    first_three_elem = a[:3]\n",
    "    fourth_elem = a[3]\n",
    "    print(ses.run(first_three_elem))\n",
    "    print(ses.run(fourth_elem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1.  1.1 2.1 3.1], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1.0, 1.1, 2.1, 3.1], dtype=tf.float32, name='a_const') # constants are immutable\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.1 10.2 10.3]]\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as ses:\n",
    "    a = tf.compat.v1.placeholder(shape=(1,3), dtype=tf.float32) # placeholder is a tensor that is fed at runtime\n",
    "    b = tf.constant([[10,10,10]], dtype=tf.float32) # e.g. input for models, useful when number of training examples not known\n",
    "\n",
    "    c = a + b\n",
    "\n",
    "    #sess = tf.Session()\n",
    "    #sess = tf.compat.v1.Session()\n",
    "    res = ses.run(c, feed_dict={a:[[0.1,0.2,0.3]]}) # feed_dict allows to override the value of tensors in the graph\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(None, 3), dtype=float32)\n",
      "[[10.1 10.2 10.3]]\n",
      "[[7. 7. 7.]\n",
      " [7. 7. 7.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with tf.compat.v1.Session() as ses:\n",
    "    # NB: the fist dimension is 'None', meaning that it can be of any lenght\n",
    "    a = tf.compat.v1.placeholder(shape=(None,3), dtype=tf.float32)\n",
    "    b = tf.compat.v1.placeholder(shape=(None,3), dtype=tf.float32)\n",
    "\n",
    "    c = a + b\n",
    "\n",
    "    print(a)\n",
    "\n",
    "    #sess = tf.Session()\n",
    "    #sess = tf.compat.v1.Session()\n",
    "    print(ses.run(c, feed_dict={a:[[0.1,0.2,0.3]], b:[[10,10,10]]}))\n",
    "\n",
    "    v_a = np.array([[1,2,3],[4,5,6]])\n",
    "    v_b = np.array([[6,5,4],[3,2,1]])\n",
    "    print(ses.run(c, feed_dict={a:v_a, b:v_b}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.1 10.2 10.3]]\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as ses:\n",
    "    #sess = tf.Session()\n",
    "    #sess = tf.compat.v1.Session()\n",
    "    print(ses.run(c, feed_dict={a:[[0.1,0.2,0.3]], b:[[10,10,10]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "[[-0.9595651   0.21389782 -0.43376774]]\n",
      "[[4 5]]\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "with tf.compat.v1.Session() as ses:\n",
    "    # variable initialized using the glorot uniform initializer; variable is a mutable tensor that can be trained via optimizer\n",
    "    var = tf.compat.v1.get_variable(\"first_variable\", shape=[1,3], dtype=tf.float32, initializer=tf.initializers.GlorotUniform())\n",
    "    # e.g. weights and biases in NNs\n",
    "    \n",
    "    # variable initialized with constant values\n",
    "    init_val = np.array([4,5])\n",
    "    var2 = tf.compat.v1.get_variable(\"second_variable\", shape=[1,2], dtype=tf.int32, initializer=tf.constant_initializer(init_val))\n",
    "\n",
    "    # create the session\n",
    "    #sess = tf.Session()\n",
    "    #sess = tf.compat.v1.Session()\n",
    "    # initialize all the variables\n",
    "    ses.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "    print(ses.run(var))\n",
    "\n",
    "    print(ses.run(var2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not trainable variable\n",
    "var2 = tf.compat.v1.get_variable(\"variable\", shape=[1,2], trainable=False, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'first_variable:0' shape=(1, 3) dtype=float32>, <tf.Variable 'second_variable:0' shape=(1, 2) dtype=int32>]\n"
     ]
    }
   ],
   "source": [
    "print(tf.compat.v1.global_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7184286\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph() # graph represents low-level computations of dependencies between operations\n",
    "\n",
    "with tf.compat.v1.Session() as ses:\n",
    "    const1 = tf.constant(3.0, name='constant1')\n",
    "\n",
    "    var = tf.compat.v1.get_variable(\"variable1\", shape=[1,2], dtype=tf.float32)\n",
    "    var2 = tf.compat.v1.get_variable(\"variable2\", shape=[1,2], trainable=False, dtype=tf.float32)\n",
    "\n",
    "    op1 = const1 * var\n",
    "    op2 = op1 + var2\n",
    "    op3 = tf.reduce_mean(op2)\n",
    "\n",
    "    #sess = tf.Session()\n",
    "    #sess = tf.compat.v1.Session()\n",
    "    ses.run(tf.compat.v1.global_variables_initializer())\n",
    "    print(ses.run(op3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0, MSE: 4572.0996, W: 1.295, b: -0.797\n",
      "Epoch:  40, MSE: 5.8447, W: 0.497, b: -1.052\n",
      "Epoch:  80, MSE: 4.8327, W: 0.533, b: -0.243\n",
      "Epoch: 120, MSE: 4.2012, W: 0.515, b: 0.458\n",
      "Epoch: 160, MSE: 3.8903, W: 0.507, b: 1.004\n",
      "Epoch: 200, MSE: 3.7699, W: 0.502, b: 1.372\n",
      "Final weight: 0.501, bias: 1.433\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "with tf.compat.v1.Session() as ses:\n",
    "\n",
    "    np.random.seed(10)\n",
    "    tf.compat.v1.set_random_seed(10)\n",
    "\n",
    "    W, b = 0.5, 1.4\n",
    "    # create a dataset of 100 examples\n",
    "    X = np.linspace(0,100, num=100)\n",
    "    # add random noise to the y labels\n",
    "    y = np.random.normal(loc=W * X + b, scale=2.0, size=len(X))\n",
    "\n",
    "    # create the placeholders\n",
    "    x_ph = tf.compat.v1.placeholder(shape=[None,], dtype=tf.float32)\n",
    "    y_ph = tf.compat.v1.placeholder(shape=[None,], dtype=tf.float32)\n",
    "\n",
    "    # create the variables.\n",
    "    v_weight = tf.compat.v1.get_variable(\"weight\", shape=[1], dtype=tf.float32)\n",
    "    v_bias = tf.compat.v1.get_variable(\"bias\", shape=[1], dtype=tf.float32)\n",
    "\n",
    "    # linear computation\n",
    "    out = v_weight * x_ph + v_bias\n",
    "\n",
    "    # compute the Mean Squared Error\n",
    "    loss = tf.reduce_mean((out - y_ph)**2)\n",
    "\n",
    "    # optimizer\n",
    "    opt = tf.compat.v1.train.AdamOptimizer(0.4).minimize(loss)\n",
    "\n",
    "    # create the session\n",
    "    #session = tf.Session()\n",
    "    #sess = tf.compat.v1.Session()\n",
    "    ses.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "    # loop to train the parameters\n",
    "    for ep in range(210):\n",
    "        # run the optimizer and get the loss\n",
    "        train_loss, _ = ses.run([loss, opt], feed_dict={x_ph:X, y_ph:y})\n",
    "\n",
    "        # print epoch number and loss\n",
    "        if ep % 40 == 0:\n",
    "            print('Epoch: %3d, MSE: %.4f, W: %.3f, b: %.3f' % (ep, train_loss, ses.run(v_weight), ses.run(v_bias)))\n",
    "\n",
    "    print('Final weight: %.3f, bias: %.3f' % (ses.run(v_weight), ses.run(v_bias)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .. with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0, MSE: 4572.0996, W: 1.295, b: -0.797\n",
      "Epoch:  40, MSE: 5.8447, W: 0.497, b: -1.052\n",
      "Epoch:  80, MSE: 4.8327, W: 0.533, b: -0.243\n",
      "Epoch: 120, MSE: 4.2012, W: 0.515, b: 0.458\n",
      "Epoch: 160, MSE: 3.8903, W: 0.507, b: 1.004\n",
      "Epoch: 200, MSE: 3.7699, W: 0.502, b: 1.372\n",
      "Final weight: 0.501, bias: 1.433\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "with tf.compat.v1.Session() as ses:\n",
    "\n",
    "    np.random.seed(10)\n",
    "    tf.compat.v1.set_random_seed(10)\n",
    "\n",
    "    W, b = 0.5, 1.4\n",
    "    # create a dataset of 100 examples\n",
    "    X = np.linspace(0,100, num=100)\n",
    "    # add random noise to the y labels\n",
    "    y = np.random.normal(loc=W * X + b, scale=2.0, size=len(X))\n",
    "\n",
    "    # create the placeholders\n",
    "    x_ph = tf.compat.v1.placeholder(shape=[None,], dtype=tf.float32)\n",
    "    y_ph = tf.compat.v1.placeholder(shape=[None,], dtype=tf.float32)\n",
    "\n",
    "    # create the variables.\n",
    "    v_weight = tf.compat.v1.get_variable(\"weight\", shape=[1], dtype=tf.float32)\n",
    "    v_bias = tf.compat.v1.get_variable(\"bias\", shape=[1], dtype=tf.float32)\n",
    "\n",
    "    # linear computation\n",
    "    out = v_weight * x_ph + v_bias\n",
    "\n",
    "    # compute the Mean Squared Error\n",
    "    loss = tf.reduce_mean((out - y_ph)**2)\n",
    "\n",
    "    # optimizer\n",
    "    opt = tf.compat.v1.train.AdamOptimizer(0.4).minimize(loss)\n",
    "\n",
    "    tf.summary.scalar('MSEloss', loss)\n",
    "    tf.summary.histogram('model_weight', v_weight)\n",
    "    tf.summary.histogram('model_bias', v_bias)\n",
    "    all_summary = tf.compat.v1.summary.merge_all()\n",
    "\n",
    "    now = datetime.now()\n",
    "    clock_time = \"{}_{}.{}.{}\".format(now.day, now.hour, now.minute, now.second)\n",
    "    file_writer = tf.compat.v1.summary.FileWriter('log_dir/'+clock_time, tf.compat.v1.get_default_graph())\n",
    "\n",
    "    # create the session\n",
    "    #session = tf.Session()\n",
    "    #sess = tf.compat.v1.Session()\n",
    "    ses.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "    # loop to train the parameters\n",
    "    for ep in range(210):\n",
    "        # run the optimizer and get the loss\n",
    "        #train_loss, _, train_summary = ses.run([loss, opt, all_summary], feed_dict={x_ph:X, y_ph:y})\n",
    "        train_loss, _ = ses.run([loss, opt], feed_dict={x_ph:X, y_ph:y})\n",
    "        #file_writer.add_summary(train_summary, ep)\n",
    "\n",
    "        # print epoch number and loss\n",
    "        if ep % 40 == 0:\n",
    "            print('Epoch: %3d, MSE: %.4f, W: %.3f, b: %.3f' % (ep, train_loss, ses.run(v_weight), ses.run(v_bias)))\n",
    "        \n",
    "    print('Final weight: %.3f, bias: %.3f' % (ses.run(v_weight), ses.run(v_bias)))\n",
    "    #file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0  delta: 0.33333\n",
      "Iter: 1  delta: 0.1463\n",
      "Iter: 2  delta: 0.10854\n",
      "Iter: 3  delta: 0.08717\n",
      "Iter: 4  delta: 0.06736\n",
      "Iter: 5  delta: 0.05212\n",
      "Iter: 6  delta: 0.04085\n",
      "Iter: 7  delta: 0.03384\n",
      "Iter: 8  delta: 0.02956\n",
      "Iter: 9  delta: 0.0268\n",
      "Iter: 10  delta: 0.02425\n",
      "Iter: 11  delta: 0.02195\n",
      "Iter: 12  delta: 0.02061\n",
      "Iter: 13  delta: 0.01962\n",
      "Iter: 14  delta: 0.019\n",
      "Iter: 15  delta: 0.01872\n",
      "Iter: 16  delta: 0.01825\n",
      "Iter: 17  delta: 0.01765\n",
      "Iter: 18  delta: 0.01698\n",
      "Iter: 19  delta: 0.01627\n",
      "Iter: 20  delta: 0.01554\n",
      "Iter: 21  delta: 0.01481\n",
      "Iter: 22  delta: 0.01408\n",
      "Iter: 23  delta: 0.01337\n",
      "Iter: 24  delta: 0.01268\n",
      "Iter: 25  delta: 0.01202\n",
      "Iter: 26  delta: 0.01138\n",
      "Iter: 27  delta: 0.01078\n",
      "Iter: 28  delta: 0.0102\n",
      "Iter: 29  delta: 0.00965\n",
      "Iter: 30  delta: 0.00912\n",
      "Iter: 31  delta: 0.00863\n",
      "Iter: 32  delta: 0.00816\n",
      "Iter: 33  delta: 0.00771\n",
      "Iter: 34  delta: 0.00729\n",
      "Iter: 35  delta: 0.00689\n",
      "Iter: 36  delta: 0.00651\n",
      "Iter: 37  delta: 0.00615\n",
      "Iter: 38  delta: 0.00581\n",
      "Iter: 39  delta: 0.0055\n",
      "Iter: 40  delta: 0.0052\n",
      "Iter: 41  delta: 0.00498\n",
      "Iter: 42  delta: 0.00483\n",
      "Iter: 43  delta: 0.00473\n",
      "Iter: 44  delta: 0.00464\n",
      "Iter: 45  delta: 0.00453\n",
      "Iter: 46  delta: 0.00442\n",
      "Iter: 47  delta: 0.0043\n",
      "Iter: 48  delta: 0.00418\n",
      "Iter: 49  delta: 0.00405\n",
      "Iter: 50  delta: 0.00393\n",
      "Iter: 51  delta: 0.0038\n",
      "Iter: 52  delta: 0.00367\n",
      "Iter: 53  delta: 0.00354\n",
      "Iter: 54  delta: 0.00341\n",
      "Iter: 55  delta: 0.00329\n",
      "Iter: 56  delta: 0.00316\n",
      "Iter: 57  delta: 0.00304\n",
      "Iter: 58  delta: 0.00292\n",
      "Iter: 59  delta: 0.00281\n",
      "Iter: 60  delta: 0.00269\n",
      "Iter: 61  delta: 0.00258\n",
      "Iter: 62  delta: 0.00248\n",
      "Iter: 63  delta: 0.00237\n",
      "Iter: 64  delta: 0.00227\n",
      "Iter: 65  delta: 0.00218\n",
      "Iter: 66  delta: 0.00208\n",
      "Iter: 67  delta: 0.00199\n",
      "Iter: 68  delta: 0.00191\n",
      "Iter: 69  delta: 0.00183\n",
      "Iter: 70  delta: 0.00175\n",
      "Iter: 71  delta: 0.00167\n",
      "Iter: 72  delta: 0.00159\n",
      "Iter: 73  delta: 0.00152\n",
      "Iter: 74  delta: 0.00146\n",
      "Iter: 75  delta: 0.00139\n",
      "Iter: 76  delta: 0.00133\n",
      "Iter: 77  delta: 0.00127\n",
      "Iter: 78  delta: 0.00121\n",
      "Iter: 79  delta: 0.00116\n",
      "Iter: 80  delta: 0.0011\n",
      "Iter: 81  delta: 0.00105\n",
      "Iter: 82  delta: 0.00101\n",
      "Iter: 83  delta: 0.00096\n",
      "Iter: 84  delta: 0.00092\n",
      "Iter: 85  delta: 0.00087\n",
      "Iter: 86  delta: 0.00083\n",
      "Iter: 87  delta: 0.0008\n",
      "Iter: 88  delta: 0.00076\n",
      "Iter: 89  delta: 0.00072\n",
      "Iter: 90  delta: 0.00069\n",
      "Iter: 91  delta: 0.00066\n",
      "Iter: 92  delta: 0.00063\n",
      "Iter: 93  delta: 0.0006\n",
      "Iter: 94  delta: 0.00057\n",
      "Iter: 95  delta: 0.00054\n",
      "Iter: 96  delta: 0.00052\n",
      "Iter: 97  delta: 0.0005\n",
      "Iter: 98  delta: 0.00047\n",
      "Iter: 99  delta: 0.00045\n",
      "Iter: 100  delta: 0.00043\n",
      "Iter: 101  delta: 0.00041\n",
      "Iter: 102  delta: 0.00039\n",
      "Iter: 103  delta: 0.00037\n",
      "Iter: 104  delta: 0.00035\n",
      "Iter: 105  delta: 0.00034\n",
      "Iter: 106  delta: 0.00032\n",
      "Iter: 107  delta: 0.00031\n",
      "Iter: 108  delta: 0.00029\n",
      "Iter: 109  delta: 0.00028\n",
      "Iter: 110  delta: 0.00027\n",
      "Iter: 111  delta: 0.00025\n",
      "Iter: 112  delta: 0.00024\n",
      "Iter: 113  delta: 0.00023\n",
      "Iter: 114  delta: 0.00022\n",
      "Iter: 115  delta: 0.00021\n",
      "Iter: 116  delta: 0.0002\n",
      "Iter: 117  delta: 0.00019\n",
      "Iter: 118  delta: 0.00018\n",
      "Iter: 119  delta: 0.00017\n",
      "Iter: 120  delta: 0.00016\n",
      "Iter: 121  delta: 0.00016\n",
      "Iter: 122  delta: 0.00015\n",
      "Iter: 123  delta: 0.00014\n",
      "Iter: 124  delta: 0.00014\n",
      "Iter: 125  delta: 0.00013\n",
      "Iter: 126  delta: 0.00012\n",
      "Iter: 127  delta: 0.00012\n",
      "Iter: 128  delta: 0.00011\n",
      "Iter: 129  delta: 0.00011\n",
      "Iter: 130  delta: 0.0001\n",
      "Won 85 of 100 games!\n",
      "[[0.54083394 0.49722378 0.46884941 0.45487071]\n",
      " [0.55739213 0.         0.35755091 0.        ]\n",
      " [0.5909355  0.64245898 0.61466487 0.        ]\n",
      " [0.         0.74129273 0.86262154 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def eval_state_action(V, s, a, gamma=0.99):\n",
    "    return np.sum([p * (rew + gamma*V[next_s]) for p, next_s, rew, _ in env.P[s][a]])\n",
    "\n",
    "def value_iteration(eps=0.0001):\n",
    "    '''\n",
    "    Value iteration algorithm\n",
    "    '''\n",
    "    V = np.zeros(nS)\n",
    "    it = 0\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # update the value of each state using as \"policy\" the max operator\n",
    "        for s in range(nS):\n",
    "            old_v = V[s]\n",
    "            V[s] = np.max([eval_state_action(V, s, a) for a in range(nA)])\n",
    "            delta = max(delta, np.abs(old_v - V[s]))\n",
    "\n",
    "        if delta < eps:\n",
    "            break\n",
    "        else:\n",
    "            print('Iter:', it, ' delta:', np.round(delta, 5))\n",
    "        it += 1\n",
    "\n",
    "    return V\n",
    "\n",
    "def run_episodes(env, V, num_games=100):\n",
    "    '''\n",
    "    Run some test games\n",
    "    '''\n",
    "    tot_rew = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for _ in range(num_games):\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = np.argmax([eval_state_action(V, state, a) for a in range(nA)])\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            tot_rew += reward \n",
    "            if done:\n",
    "                state = env.reset()\n",
    "\n",
    "    print('Won %i of %i games!'%(tot_rew, num_games))\n",
    "\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    # create the environment\n",
    "    env = gym.make('FrozenLake-v0')\n",
    "    # enwrap it to have additional information from it\n",
    "    env = env.unwrapped\n",
    "\n",
    "    # spaces dimension\n",
    "    nA = env.action_space.n\n",
    "    nS = env.observation_space.n\n",
    "\n",
    "    # Value iteration\n",
    "    V = value_iteration(eps=0.0001)\n",
    "    # test the value function on 100 games\n",
    "    run_episodes(env, V, 100)\n",
    "    # print the state values\n",
    "    print(V.reshape((4,4)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 7 policy iterations\n",
      "Won 81 of 100 games!\n",
      "[[0.54091157 0.49730529 0.46893217 0.4549538 ]\n",
      " [0.55745963 0.         0.35758788 0.        ]\n",
      " [0.59098844 0.64249454 0.61469305 0.        ]\n",
      " [0.         0.74131715 0.86263385 0.        ]]\n",
      "[[0. 3. 3. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def eval_state_action(V, s, a, gamma=0.99):\n",
    "    return np.sum([p * (rew + gamma*V[next_s]) for p, next_s, rew, _ in env.P[s][a]])\n",
    "\n",
    "def policy_evaluation(V, policy, eps=0.0001):\n",
    "    '''\n",
    "    Policy evaluation. Update the value function until it reach a steady state\n",
    "    '''\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # loop over all states\n",
    "        for s in range(nS):\n",
    "            old_v = V[s]\n",
    "            # update V[s] using the Bellman equation\n",
    "            V[s] = eval_state_action(V, s, policy[s])\n",
    "            delta = max(delta, np.abs(old_v - V[s]))\n",
    "\n",
    "        if delta < eps:\n",
    "            break\n",
    "\n",
    "def policy_improvement(V, policy):\n",
    "    '''\n",
    "    Policy improvement. Update the policy based on the value function\n",
    "    '''\n",
    "    policy_stable = True\n",
    "    for s in range(nS):\n",
    "        old_a = policy[s]\n",
    "        # update the policy with the action that bring to the highest state value\n",
    "        policy[s] = np.argmax([eval_state_action(V, s, a) for a in range(nA)])\n",
    "        if old_a != policy[s]: \n",
    "            policy_stable = False\n",
    "\n",
    "    return policy_stable\n",
    "\n",
    "\n",
    "def run_episodes(env, policy, num_games=100):\n",
    "    '''\n",
    "    Run some games to test a policy\n",
    "    '''\n",
    "    tot_rew = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for _ in range(num_games):\n",
    "        done = False\n",
    "        while not done:\n",
    "            # select the action accordingly to the policy\n",
    "            next_state, reward, done, _ = env.step(policy[state])\n",
    "                \n",
    "            state = next_state\n",
    "            tot_rew += reward \n",
    "            if done:\n",
    "                state = env.reset()\n",
    "\n",
    "    print('Won %i of %i games!'%(tot_rew, num_games))\n",
    "\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    # create the environment\n",
    "    env = gym.make('FrozenLake-v0')\n",
    "    # enwrap it to have additional information from it\n",
    "    env = env.unwrapped\n",
    "\n",
    "    # spaces dimension\n",
    "    nA = env.action_space.n\n",
    "    nS = env.observation_space.n\n",
    "    \n",
    "    # initializing value function and policy\n",
    "    V = np.zeros(nS)\n",
    "    policy = np.zeros(nS)\n",
    "\n",
    "    # some useful variable\n",
    "    policy_stable = False\n",
    "    it = 0\n",
    "\n",
    "    while not policy_stable:\n",
    "        policy_evaluation(V, policy)\n",
    "        policy_stable = policy_improvement(V, policy)\n",
    "        it += 1\n",
    "\n",
    "    print('Converged after %i policy iterations'%(it))\n",
    "    run_episodes(env, policy)\n",
    "    print(V.reshape((4,4)))\n",
    "    print(policy.reshape((4,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
