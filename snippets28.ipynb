{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdd3iUVfrw8e+Zkt5DegJJSOgQem9iQ8HeUUBXZRdRsfcX3f2tyq676FoAFUVRsAsKCkpHkA4JoYaSXia9tynn/WNIIIQySSaNnM91zcVkyvOchJnnPvU+QkqJoiiK0vFoWrsAiqIoSutQAUBRFKWDUgFAURSlg1IBQFEUpYNSAUBRFKWD0rV2ARqiU6dOMjw8vLWLoSiK0q7s3bs3V0rpd+7j7SoAhIeHs2fPntYuhqIoSrsihEg+3+OqC0hRFKWDUgFAURSlg1IBQFEUpYNqV2MAiqIoHZ3RaCQtLY3Kysp6zzk5OREaGoper7fpWCoAKIqitCNpaWm4u7sTHh6OEKL2cSkleXl5pKWlERERYdOxVBeQoihKO1JZWYmvr2+diz+AEAJfX9/ztgwuRAUApUGyThWxd00SWaeKWrsoitJhnXvxv9TjF6K6gBSbZZ0qYsW8/ZjNFnQ6DTc9OYDASM/WLpaiKI2kWgCKzdITCjCbLCDBbLaQnlDQ2kVSFKUJVABQbBbSzbv2vtCIOj8ritJyLrSRV0M3+FIBQLGZp59z7f3uwwJV94+itAInJyfy8vLqXexrZgE5OTnZfCw1BqDYzJBYDIBWr6Ekz/aZBoqi2E9oaChpaWnk5OTUe65mHYCtVABQbJaVWITQCLoNCeD4HgNmswWtVjUiFaUl6fV6m+f5X4r69io2MyQW4xviSufevpiqLeSklLR2kRRFaQIVABSbSIskO6mYwAhPgqKsff+Zx9VaAEVpz1QAUGxSkFVOdaWZgEgPXD0d8fR3JuNEYWsXS1GUJlABQLFJVqK1th8Q7gFAcLQXmScKkZaGTTtTFKXtUAFAsYkhsRhHFx1e/i6ANQBUlZvIzyxr5ZIpitJYKgAoNjEkFhMQ7oHQWHONBEd5AZBxXHUDKUp7pQKAcknVlSbyM0oJiPCofczd1wk3b0cVABSlHVMBQLmknOQSpISAiDMrf4UQBEV5kXGisMHLzxVFaRtUAFAu6dwB4BrB0V6UF1VTlFPRGsVSFKWJVABQLsmQWIynvzNObnW3mVPjAIrSvqkAoFyUlBJDonUB2Lm8g1xwctOTqdYDKEq7pAKAclGlBVWUF1fXGQCuIYQgOMpLtQAUpZ1SAUC5qJqtH88XAACCojwpzq2ktKCqJYulKIodqACgXJQhqRitXoNvqNt5nw+Oto4DqG4gRWl/Wj0ACCG0Qoj9QohVrV0WpT7DqWL8O7tfMO1zp1A39E5a1Q2kKO1QqwcAYDZwpLULodRnNlnISS25YPcPgEarISjSUyWGU5R2qFUDgBAiFJgELGrNcijnl5deitloqbMA7HyCor3IzyijstTYQiVTFMUeWrsF8A7wHGC50AuEEDOEEHuEEHvOtwWa0nyyTlm3gLxYCwDOjAOoVoCitC+tFgCEEJOBbCnl3ou9Tkr5kZRysJRysJ+fXwuVTgEwJBXh4umAm7fjRV8X0MUDrU6jAoCitDOt2QIYBdwohEgCvgYmCCG+bMXyKOcwnLIuABNCXPR1Wr2GgAgPMtVAsKK0K60WAKSUL0opQ6WU4cDdwAYp5X2tVR6lrspSI0U5FZfs/qkRHO1FTmop1ZWmZi6Zoij20tpjAEobVZsAzsYAEBTlibTI2oVjiqK0fW0iAEgpN0kpJ7d2OZQzDEnFCAF+nd1ten1gpCdCI8g8oQKAorQXbSIAKG2PIbEYnxA3HJx0Nr3ewUmHX5ibWhCmKO2ICgBKPdIiyU4qtrn7p0ZQtBeGxGLMxgvO6lUUpQ1RAUCppzC7nKpyE4ENDADBUV6YTRYMycXNVDJFUexJBQClHkPi6QVg4RdfAXwutUGMorQvKgAo9WQlFuPgpMU70KVB73Ny0+MT7KoygypKO6ECgFKPIbEI/3APhObiC8DOJzjKi8yTRVjMahxAUdo6FQCUOoxVZvLSywiMbFj3T43gaC+MlWZy00rtXDJFUexNBQCljpyUYqRFNngGUI2gqJoNYtR6AEVp61QAUOrIqh0AblwAcPN2xKOTkxoIVpR2QAUApQ5DYjEefs44uzs0+hjB0V5knChESmnHkimKYm8qACh1GBKLG137rxEU5UVlqZGCrHI7lUpRlOagAoBSq7SgkrLCKgIjmxYAajeIUd1AitKmqQCg1GrsArBzefo54+LpoAKAorRxKgAotbISi9HqNHQKc2vScYQQ1vUAahxAUdo0FQCUWobEIjqFuaHVNf1jERztRWlBFSV5lXYomaIozUEFAAUAs9lCTnIJgRFN6/6poTaKV5S2TwUABYD89DJMRkujF4CdyyfIFUcXndonWFHaMBUAFMDa/QO2bwF5KUIjCIryIkOtCFaUNksFAAWwzgBydtfj7utkt2MGR3lRaCinrKjKbsdUFMV+VABQAOsMoIAIT4RoeAbQCwmKto4nqLxAitI2qQCgUFlmpNBQ3uQFYOfy6+yOzkGjBoIVpY1SAUAhO6lpCeAuRKvVEBjpqRaEKUobpQKAYs0AKsDfzgEArNNB89JLqSo32v3YiqI0jQoACobEYnyCXHFw0tn92MFRXiAh86QaB1CUtkYFgA5OSokhqYhAO03/PFdAhAcarVDdQIrSBqkA0MEVZVdQVWYiwE4rgM+lc9Di38VDbRSvKG1QqwUAIYSTEGKXECJOCHFICPH31ipLR2bvBWDnExztRXZSCcZqc7OdQ1GUhmvNFkAVMEFKGQP0ByYKIYa3Ynk6JENiMXpHLd5Brs12juBoLywWieGUGgdQlLak1QKAtCo9/aP+9E3lDm5hWYnF+Id7oNHYbwHYuQK7eoJApYVQlDamVccAhBBaIUQskA2slVLuPM9rZggh9ggh9uTk5LR8IS9jpmozeWmlzdr9A+DorKNTqJsaCFaUNqZVA4CU0iyl7A+EAkOFEH3O85qPpJSDpZSD/fz8Wr6Ql7GclBIsFtlsM4DOFhztheFUEWaTpdnPpSiKbdrELCApZSGwCZjYykXpUAw1K4CbaQbQ2YKjvDAZLeSklDT7uRRFsU1rzgLyE0J4nb7vDFwFHG2t8nREWaeKcfd1wsXDodnPFRSlNopXlLamNVsAQcBGIcQBYDfWMYBVrVieDseQ2HwLwM7l4uGAd6CLSgynKG2I/df+20hKeQAY0Frn7+jKCqsoLahqke6fGkFRXpzYm43FIpt11pGiKLZpE2MASsszJNb0/7dMCwCsA8HVFSbyM0ov/WJFUZqdCgAdlCGpCI1W0CnMrcXOWbtRvBoHUJQ2QQWADirrVDGdwtzR6bUtdk53HyfcfBzJOK4WhClKW6ACQAdkMVvITilp0e6fGsHRXmScKERKtehbUVqbCgAdUH5mGaYqc4vNADpbcJQXFcXVFGVXtPi5FUWpSwWADqg1BoBr1I4DqOmgitLqVADogLISi3Fy0+PRybnFz+0V4IKzu14NBCtKG6ACQAdkSCwmIMIDIVp+Lr4QguAoL7VBjKK0ASoAdDBVFSYKsspapf+/RlCUF8W5lZTkV7ZaGRRFUQGgw8lOKgYJAeEttwL4XDXjAKoVoCitSwWADsaQWAQC/FuxBeAb6oaDk1ZtEKPYTeaJQnavSiRL7TrXIK2WC0hpHVmJxXgHuODo3Hr/9RqNILCrlxoIVuwi61QRy+ftR1oke1YncfNTAwjq6tXaxWoXVAugA5FSWgeAI1uv+6dGcLQnBZllVJRWt3ZRlHYuOT4PabEuLLSYJes/O6zGl2ykAkAHUpxbQWWpsVUHgGsER3sDkKm6gZQmKi+pAkAI0GgFpYXVfP2PnRzelqFWnF+C6gLqQFpzAdi5/Lu4o9VryDheSGR/tdWn0jgmo5nEuFwCIz0I79eJkG7eOLs7sGHJETZ+cZRTsTlccW8PXL0cW7uobZJqAXQgWYnF6By1+AS5tnZR0Oo0BEZ4qHEApUkSdhqoKDEy7KauDJoYTmCkJ55+ztz85ABG3xFN2tECvvrHTo7tzFKtgfNQAaADMSQW49/ZHY22bfy3B0V7kZtaQnWlqbWLorRD0iKJXZeCX2d3QrrVHfQVGkHMlWHc/cpQvANdWLf4MGs+PEh5sRpzOpvNVwIhhLMQontzFkZpPiajmdzUEgIjW7/7p0ZwlBdSQtZJNQ6gNFzywTwKssrpf3XYBVe1ewW4cMszgxhxa1eSDuby1T92cmJvdguXtO2yKQAIIW4AYoE1p3/uL4T4uTkLpthXbmopFrNs1QVg5wqM9ESjEaobSGmU/WtTcPNxpOtA/4u+TqMRDLymC3e9NBQPXyd++/ggvy06SGWpsYVK2nbZ2gJ4DRgKFAJIKWOB8OYpktIc2tIAcA29oxa/Lu4qM6jSYIakYjKOFxIzIQytjV2aPsGu3PrcIIbdGMGp/Tks+8dOEuNymrmkbZutAcAkpVTt9HbMkFiEm7djm5sNERTlhSGpGJPR3NpFUdqR2HUpODjr6DU6uEHv02o1DL4+gjteHIyLuwO/Lohn3WeHqSrvmK0BWwPAQSHEFEArhIgWQrwH/NmM5VLsLCuxmICIttP9UyM42guLSVpzFCmKDYpzKzi5N5veY4JxcGrcTPZOoe7c8eJgBl8fTsIuA1/9YxfJh/LsXNK2z9YA8BjQG6gClgFFwBPNVSjFvsqLqynJq2xT3T81grp6gkDtE6zYLG5DKkII+l0R1qTjaHUaht0YyW3PDcLBWceq9+LY+OXRDjUr7ZLhUwihBf4upXwWeLn5i6TYmyHRenFtCyuAz+Xkqsc32FWNAyg2qSwzcnhbJtFDA3Dztk93ZkC4B3e+NJhdKxOJXZtC6uF8JkzrQWgPH7scvy27ZAtASmkGBrVAWZRmkpVYjEYj8Ovs3tpFOa/gKC+yThZhMVtauyhKG3foj3RMVWb6X9XZrsfV6bWMvDWKW58dhEYn+OmdWLZ8dQxj1eU9NmVrF9B+IcTPQoipQohba27NWjLFbgyJxfiGuqFz0LZ2Uc4rKNoLY5WZ3LTS1i6K0oaZjRYObEwjrJcPnULdmuUcgZGe3PXKUPpNCCV+czpf/3PXZd06tTUA+AB5wATghtO3yU05sRAiTAixUQhxRAhxSAgxuynHU87PYrEOsLbF7p8atRvFq/UAykUk7DZQXlTNADvX/s+ld9Ay5s5u3PzUAJCS5f/dx9bvj2OqvvxaAzYNoUspH2iGc5uAp6WU+4QQ7sBeIcRaKeXhZjhXh1WQWYaxymzXAeD5sfN5pP8jdjueq6cjnn7OZBwvtHvTXrk8SGlN++Ab4kZoT+8WOWdIN2/uemUo2388Sdy6VJLj87jy/p4EtsHZdI1l60rgxUKIT8+9NeXEUspMKeW+0/dLgCNASFOOqdR3ZgGYfT60JouJBXEL7HKsswVHe5F5oqg2r7uinC3lcD75GWUMuEjah/mx8+1+XgcnHeOmdOfG2f0xVZv58d972b78JOkJBexdk9TudyCzdRLtqrPuOwG3ABn2KoQQIhwYAOw8z3MzgBkAnTur2mFDGRKLcHTV4env3KTjZJRm8OPxH1l+fDkAN664kf5+/Ynxi6G/f38iPCPQiMYnmQuK8uLIn5nkZ5XhG9w8/btK+xW7NgVXL0eiBgdc8DUL4hbYtWV6trCePtw9ZxjbvjvOvt+S2fdbMiDR6bXc9OQAAtvAJkuNYWsX0A9n/yyE+ApYZ48CCCHcgB+AJ6SU9VYDSSk/Aj4CGDx4sKoeNlBWYjEB4Z4XrDVdjNFiZEvqFr47/h3b0rfVeS6xKJHEokSWn7AGBHcHd/r59bMGBL/+9O3UFzcH2y/ktRvFHy9UAUCpIyelhLSjBYy4tStaXf1KhtFs5NuEbwF4bstzuOhccNG71P7rqnOt/dlZ71zneVe99TkHjcMlvyOOzjomTOuJxSI5tiMLEJjNFtITCi7vAHAe0UCTq+NCCD3Wi/9SKeWPTT2eUld1pYn8zLJLJss6V1pJmrW2f2I5uRW5+Lv4MzNmJrdE3UKQWxB9P+9L/PR4pJSklKQQmx1LbE4scTlxLIhdgEQiEER7R9e2EPr79SfM/cLNd49OTrh6OZJxoog+40Lt8esrl4nYdSnonbT0HlO3h1hKyXNbnmNN0prax1YnrgZAJ3SYpO0LurRCWy9AuOpdrfdP/+ysc8ZF74JTiDdogsAiEAhCurXMmERzsCkACCFKgLNr31nA8005sbBeCT4Bjkgp5zXlWMr5ZScVg7RtAZjRbGRj6ka+T/ie7Znb0QgNY0PGcnu32xkVMgqdpv5HRQhBF48udPHowk1RNwFQUl1CfG48cdlxxOXEsTpxNd8lfAeAt6M3Mf4xta2E3p1646xzrj1WcJQnGccLkVI2qsWiXH5K8is5viebfhNCcXQ+8xk8kneEt/a8xe6s3UR4RvDM4GeYtX4W8dPja19jkRYqTZWUm8qpMFZQbiqn3FROmbGMcqP1/tn/VpgqrM+d9Xh2eTYVpgrKjeWUmcooM5YBENArnHEn78ajshO3rr2R+4bd1WzdT83J1i6g5lhBNAqYCsQLIWJPP/aSlPLXZjhXh2Q4nV/HP/zCASClOIUfjv/AihMryK/MJ8g1iEf6P8ItUbcQ6Bp43vfMjJl5weO5O7gzMngkI4NHAtYv4cnCk8TlxBGbbW0lbErdBFhrad19ute2Etw7d6ZsTxXFuZV4+p0Zs7D3rCOl/TiwIRWAmAnWtA/Z5dm8u+9dfj75M16OXrw87GVu63Ybeo2+3ns1QmPt6tG7QNOGwGpJKak0V5JbkcsdX03h7tiXuCZ9GmNDh9nnBC3M1hbAKCBWSlkmhLgPGAj8T0qZ3NgTSym3Aqqa14yyThXjFeCCk2vdL0e1uZoNKRv4PuF7dmbtRCu0jAsdx+3dbmdk8Ei0mosvGGvIxVgjNER7RxPtHc3t3W4HoLCykAO5B2oDwvITy1l2dBne5YHcxYvMW7WAyKG+xPjF0Mu3V7MO7iltV1WFiUNbM4ga5I/W3cKC2AUsPrQYk8XE/b3v56F+D+HhcKZyc7GKib0IIXDWORPmHkaZYyEh4x3Rb4jgpS/fYOyogcwaMKu2Vdse2DoGsACIEULEAM9h7bpZAoxrroIpTSOlxJBUTOdeZ/KZJBUl8cPxH/jpxE8UVBUQ7BrMYwMe4+aom/F3adg4QVN4OXkxNnQsY0PHAtappQkFCcRmx5F7pJrKVA3/0fwHoLZmZzQb0Wvr1/KUy9fhPzIwVpop7pnIDSseJ7s8m2u6XMMTg54gzL1+IriWriTMjJnJrX3H8/WRnVyVeh+L4/8f61PW89rI1xgW1D5aBMKWjZKFEPuklAOFEHOAdCnlJzWPNX8Rzxg8eLDcs2dPS56y3SrOreCLV7Yz8q5IMroc4ofjP7A7azc6oeOKzldwe/TtDA8e3qSpm83h1wUHyM8oI+vGbXx++PN6z8+MmalaAx2A2WThkxc3k+2Qyjfd3qKPbx+eG/ocA/wHtHbR6sk4Xsjy/+4jaLQDn7i8SUpJCrdG38rTg5+u00JpTUKIvVLKwec+bmsLoEQI8SJwHzD2dIZQVR1rww4cOg7AnBPPkpRylFC3UGYPnM3NUTfTyblTK5fuwoKjvUiMy2Vmt8d4ZsgzAPT9vC/OOmdc9a4MDxreyiVUmltycTKLfviOwJKBHOm/jblj5nJdxHVtrrJSIzjaix7DA0nYbuCTF79gmeEzlhxawh9pf/DysJe5ssuVrV3EC7L1L3oX1r0AHpRSZmFdsftWs5VKabD5sfOpNFWy8uRKpq+ezhebvsekqaZb1y58dPVH/HLrLzzU96E2ffEH64IwqJ8XaOn1S3HRufDgbw/y1dGvsKXlqrQvRVVF/Hv3v7l5xc04HAwE7yo+fvBtJkVOarMX/xojb4tC76hlx7dJPDnwSZZOWoqPkw9PbHqCpzY9RW5FbmsX8bxs+qtKKbOklPOklH+c/jlFSrmkeYum2OpEwQkWxC1gwncTeGnrS+RW5NLfMoLALt78d8J/GBE8os1/gWr4hbmhc9SSeVYAmBkzk2jvaL6a/BWjQkbxxs43eGXbK1SaKluxpIq9GC1Glh5ZyqTlk/jy8Jfc7jIdn7JgJtwQY53B0w44uzsw/OaupCcUkrDLQG/f3nw1+StmD5zN5tTN3LjiRpYfX97mKi625gIaLoTYLYQoFUJUCyHMQoj2nQTjMrH44GJu+fkWAEYHj+aTaz7hp8k/I/KcCY3ybeXSNZxGqyGoqycZJ858vGr6/D0cPHh3wrvMjJnJzyd/ZtrqaWSU2i0jidLCpJRsTNnIrT/dytxdc+nh04PvbviOfhnjcfFwoNuQ809Dbqt6jQ7GP9yDbT+coKrciF6j56G+D/H9jd8T7RXNnD/nMGPtDFJLUlu7qLVsrRa+D9wDHMc6o/Yh4IPmKpRyaVJKpv46lXl7z6yhW520mgd/f5APN36O2WRp0ymgLyY4ypO8jFIqy+pv1K0RGh7p/wjvT3if1JJU7lp1Fzsyd7RCKZWmOJp/lId+f4jHNz6OEIIPrvyAj6/+GN/yEFIO59NvQihafftotdbQaATjp3SnsqSanT+dqn08wjOCxRMX88qwVziQc4Dbfr6NJYeWYLa0fnppm//CUsoTgFZKaZZSLgbGN1uplIsyWUy8tv01YnNiubPbncROta6ji58eT/z0eMY4XA3QJvcAtkVwtBdIyDx54UbmuLBxfD35a3ydfPnr2r+y+ODiNte8VurLLs9mzrY53LnyThIKEnhp2Ev8cOMPjA0dixCCuHUp6Bzrp31oL/w6u9NnfCjxW9LJTj6T2kwjNNzV4y5+uvknhgQO4a09bzFt9TSOFxxvxdLaHgDKhRAOQKwQ4t9CiCcB12Ysl3IBVeYqnt38LD8e/5EZ/WbwyvBX6i3cyjpVjKuXI27eTq1UyqbxD/dAoxN1xgHOp4tHF5ZNWsaVna9k3t55PLvlWcqN5S1USqUhKkwVLIxbyOTlk1l5aiXTe0/nl1t/4Z4e99Su9SgtqCJht4Feo4LqLV5sT4bdGImzuwOblx3Dck5680DXQN6f8D5zx8wltSSVO1fdyQexH1Btrm6VstoaAKaefu2jQBkQBtzWXIVSzq/MWMasdbNYl7KO54Y8x2MDHqvNmXP2KkhDUnG7rf2DdX/WgHAPm7bic9G78N9x/+XJQU+yNnkt9/56LynFKS1QSsUWH+z/gJUnVzJ5+WQ+iP2A0SGj+fmmn887Rz5+UyrSImvTPrRXjs46Rt8eRXZyCYe31h+jEkIwKXISK25ewbXh17IwbiF3rryTuJy4Fi+rrbOAkrGmbQiSUv5dSvnU6S4hpYUUVBbw4G8PssewhzdGv8HUXlPrPF8zUFpRUk1xTkW7DgBg3Sg+J7nEpk25hRD8pc9fWHDVAnIqcrh71d1sSdvSAqVULmZP1h4WHljIS1tfws/Zj88nfs688fMI86h/ga+uNHFwSwZdB/nj0an9pFK4kOghAYR092bHipOUF5+/du/j5MPcMXP54MoPKDOVMfXXqczdNbdFW7G2zgK6AYgF1pz+ub8Q4ufmLJhyRlZZFtNWT+NE4QneueIdbuh6wwVfW5MArr0OANcIjvbCYpFkJdo+2Wxk8Ei+mfwNoe6hPLr+URbELcAiLc1YSuVCvjr6FQ/8Zt1J9s0xb7Js0jIGBlw4ccDhrRlUV5gYcPXlsemTEIJx93TDWGXmzx8vXlceGzqWFTet4K7ud7H0yFJu+ekW/kz/s0XKaWsX0GvAUKAQQEoZC4Q3T5GUs50qOsXU1VPJrchl4VULGR82/qKvNyQWIzQCv87tOwAERnoiRMM3ig9xC2HJdUuYHDmZ+bHzmb1xNiXVJc1USuV8Hl3/KG/sfKP25xf/eJGYJTEX3LLRbLYQtyGV4Ggv/Lu078/t2bwDXRlwdWeO7cgiPaHgoq911bvy8vCX+Xzi5zhoHfjrur/y8taXKao6UwFqji0vbQ0AJimlmvffwg7lHuL+1fdTba5m8cTFDA6sl8qjHkNiEb4hrugdL57Rs61zcNbRKcydTBvGAc7lpHPi9dGv88LQF9iatpUpv0zhZOHJZiilcq6VJ1eyJW0Lo0JGsfe+vcCZ2WkXyuF0cl82pflVl03t/2yDrg/H3ceJzV8lYDZfujU6MGAg39/4PQ/3fZhfT/3KjStuZE3SGqSUzbIXt60B4KAQYgqgFUJECyHeA1qmjdJB7czcyV9++wvOOmeWXLeEHj49LvkeaZEYEovttgF8awuO9iLrVDFmU8O7cYQQ3NvzXhZdu4iS6hLu+eUefk/6vRlKqdT4Pel3Xtn2CkMCh/DO+Hdw0Dpc8j1SSmLXpuId6EKXPu1v4eKl6B20jLm7GwWZZcSts20BmKPWkccHPs7Xk78m0DWQZzc/y+MbH2+W8tkaAB4DemPNB7QMKAJmN0uJFNYnr2fmupkEuwWz5LoldPHoYtP7CrLKqa40E3CRDWDak+AoL8xGC398k0DWqcY1QAcFDOKbyd8Q7R3N05uf5u29b7eJBTiXm02pm3h+y/PE+MXw3oT3cNJZpyBfKkd/ekIhOSkl9L+qM0JzeW4PEtGvExExndj9SyIl+banL+nu050xIWMAajdR6vt5X/p+3tdu3UG2BoBep286wAm4CdhtlxIodSw/vpynNj9FT9+efDbxMwJcA2x+ryHJepEMjLw8AoBWZ70gHPojg5/e3t/oIBDgGsDiaxdzR7c7+PTgp8xcN5PCyoZ3LSnnty19G09teooePj2Yf+X8Ovl7LpW6O3ZtCs7ueroNs/1z3h6NvjMaJGz9tmELvx4d8Cjx0+PZcpd1VtulutMaytYAsBT4FLgVmHz6duGpKEqjLD64mDl/zmF40HA+vvpjPB0b1pWTlViMg7MOL//2kUDrUnLTS2vvm0yWSw6kXYyD1oE5I+bw95F/Z49hD3f/cjdH8o7Yo5gd2u6s3czeOJtIz0gWXr0QNwc3m9+bl1FK8qGIKkwAACAASURBVME8+l0Rik7fvsesLsXD15nBk8I5FZtDUnzDM4N6OzXPxvO2BoAcKeVKKWWilDK55tYsJeqApJTM2zuPeXvnMTF8Iu9PeL9RWRCt/f8el01TOqSb95l8MBLskenh1uhb+Xzi55gsJqaunsrKkyubftAOKjY7llnrZxHqFspH13zU4ApL3LpUdHoNfcaGNlMJ25b+V3XGO9CFP75JwFjd8G7I5tjy0tYA8KoQYpEQ4h4hxK01N7uXpgOqyeuz+OBi7ux2J3PHzG3U1ofVlSby00vb/QKwswVGenLzkwMYMikcn2BXdq1M5OS+7CYft69fX76Z/A19O/Xlpa0v8ebONzFa6ieeUy7sUO4hZq6bib+LP4uuXYSPk8+l33SWsqIqju3KoufIIJzc2m/ah4bQ6jSMu6c7xbmV7FvT8Ppzc+yEZ2sAeADoD0zE2vVzA9ZuIKUJbMnrY6uclBKk5LIZAK4RGOnJ0Bsiue25QQRGePDbokOc2Nv0IODr7MtH13zEfT3vY9nRZTz020O1m3Y0x3zry8mx/GPMWDsDT0dPFl2zqFGbDMVvTMNilsRc1b7TPjRUSHdvug0LYN/vyRRklbV2cWwOADFSysFSyulSygdO3/7SrCW7zF0sr09jGBJrVgBfHlNAz+XgpGPyYzEERnjw+yf2CQJ6jZ7nhz7P3DFzOZx3mLtW3kVcTlyzzLe+XJwsPMmMtTNw1jmz6JpFBLo2PGe/Ne1DOl37++Hpd3mMVzXEyFuj0Om1bPk6odUz2NoaAHYIIXo1a0k6kEvl9WkMQ2Ixnn7Ol3VzujYIRNovCABMipzEl9d/iV6r5/4199vlmJej5OJkHv79YTRCwyfXfkKoe+P67o9uz6Sq3ET/y3Dhly1cPR0ZflMkaUcLOLHHPp/hxrI1AIzGmgr6mBDigBAiXghxoDkLdrlqSF4fW0lpzZkTcJlM/7wYBycdkx89EwSO7zHY5bjrU9aTXpqOyWIC7D/fur1LL03nod8fwmQxseiaRTavTTmXxWwhbn0qQV09CYy8PFurtug9NgS/zu5s/e44VRWmViuHrQFgIhANXMOZ/n81DbSBGprXx1alBVWUF1UTEN4xvlBnB4G1nx62SxB4pP8jxE+PZ+3tawFr3vYNd2xoloG39iarLIsHf3uQMmMZH13zEV29ujb6WKdicynOreywtf8aGo1g3JTulJdUs2vlqUu/obnKYcuLzp76qaaBNk5j8vrYqrb/vwO0AGrUBIGgrp52CwJAbZ92UVURj254tMNvMJNbkcvDvz9MYVUhH139kU0pSS5ESsn+35Px9HcmvF/DB44vNwHhHvQZE0L8xjRyUlonYWH72nSznWpMXp+GMCQWodVp8A2xfRHO5cDBScekWf2sQeCTQxzfbZ8gMDNmJm+NfYuj+Ud54Y8XOmzqiILKAh7+/WEM5QYWXLWAPp36NOl4mSeKyE62pn3QXCZrVZpq2E2ROLnp2fzVMaSl5QeEWzUACCE+FUJkCyEONtc5VuxP5/o5y/l21CSum7OcFfvTm+tU59XYvD4NYUgsxq+zO1pdx4vntS2BKC/WfnqIhN1ZTT7mI/0fYVzYOJ4b8hwbUzcyb+88O5S0rhX70xk1dwMRL/zCqLkbWvxzeSlFVUXMWDuD1JJU3p/wPgP8B9j+5q3vQOI5G/IkbmH/NxtxctPTfXjDZw5drpxc9Yy6LQpDYjGHt9XfPay5tfYV4zOs4wvNYsX+dF78MZ6rd6ygd14iE3at5MUf41vkyzY/dn6T8vrYZOs7mE9stm4CI7DmykncYv0CdiB6Ry2TH40hONqLdZ8eJmFX04MAwL0972VKjyksObyEb45+Y5djwpnPZXmWgbl/zKc8y9Bin0tblFaXMnPdTE4WnuSdK95haNDQhh0gZCB8d/+ZIJC4hYKlz5OU5knfcSHoHS7vtA8N1W1YIMHRXmxffpKKkpbdG1i09jxUIUQ4sEpKecn25eDBg+WePXtsPnZc7744mOuPsFdrdcQcim9AKRuu7+d9AesuVW+Pf7tRqR0uxXJyM5sWbuBIyTgQoNPCTf5vEDh1DkSMtfv52jpjlZlf5seRkVDIlff3ovuwptc0zRYzj298nK3pW3l/wvuMCR3T5GOOmruB9MIKZsX+wPVJO/g1fDgf9L+NEC9ntr0wocnHb4pyYzkz183kQM4B5o2fxxWdr2j4QapKYfXzELcMPEKgLIeNDm9zLCWIaa+PwMXTyf4Fb+fyMkr59p+76TY8kCun9bT78YUQe6WU9QYe23wAEELMAGYAdO7ceVBysu1jzwOf+IoH41cyOjMeh9PT+wodXHlr0D10v+FqJvUNZnikDzqtfRtC7+57l4/jP2Zi+ETeGP1Go1I7XEr2wQQ2fnmM3EJXQAICgZlhvU4xaNp14NUxZ1kYq8388oF9g0C5sZzpa6aTUpzCkuuW0N2ne5OOF9erb+3n8WxVGh3/e/YTuvi6Eu7rQngnV8J9XQn1drb7Z/R8Kk2VPLr+UXYbdvPvsf/m2vBrG3YAUzXs/Qy2/BvKcsCrCxQmU27xZkn2Qno4b2S831IIioHg/hA8AIL6g08kaFq7M6L1bV9+gn2/pXDLMwMJjvKy67HbbQA4W0NbAKPmbuDmTV9yfdIOjBoteouJao0OB4uZDRHDWNTjWnQ+vkzsE8ikfkEMi/BF24TBqfmx88+7inRmzEz7TCc0VlJ9YBW7ViVxIDMGZ00xfUOPsDdtCGaLQCvM3OT9KoEOx6BTd4i6CqKvgs4jQd9xal3WIHCAjIQCuwUBQ5mBKb9OQSBYNmkZ/i7+DT6GlJLVB7OY8+lmXtn+Cd0K06j5tFmAk96hJEXGsNU9nP3uYRi1OgB0GkGotzNdfF2J6ORKl3OCg96G4LBifzpv/XaMjMIKgr2cefba7tw8IKT2+WpzNbM3zmZb+jZeH/16w9anWMwQ/z1s/CcUpkD4GOh5E2x+EwY/yM7fc9hTeANT7szGu2IPZMSC4SCYTufGd/SAwH4dPigYq8wse20HDs467nx5CFo7Bv0OGQBW7E8n/5knydW7szpiONcl7iCwqpCuA3rh8/sKLA6ObBtzK++6xVBqhk5ujlzfN5DJ/YIZ3MW7wTMV9mTt4eHfH2ZY8DC2pW8jfrqdupky42D/lyTvOMLm3PsosfjTu2s2I+4bhmNlKllf/IN033sJyVtK4A0PQGUhHF8LydvAXA16F+uXMuoqiLoSfBs/j7u9qAkC6QkFXDW9J92HBzX5mEfzjzJt9TTCPcL5bOJnDerWi08r4v9WHWZ3Yi6PH1/DxMMbkIBRo0NnMXHKK5QQP3ecTx4DiwUcnTD17kdut34khPbggHMASXkVJOeVU1p1pvWgPSs4hPu6EO7rSngnl9PBwQUHnaZ2zKHCeGY2k7Ney5u39uXmASEYLUae2fQMG1I38NqI17it2222/VJSQsJvsP4fkH3IehG/6jXQ6OD7B+COzzCGjGLJ85sJEvu5/rEhZ7omzUbIOWoNBpmxkLEfsg6Cucr6vKOHtaUQFGMNCsEDwDuiflDY+o51zOHsLs/ELZC+D0Y/YfP/T1txKjaH1QvjGXV7FP2vsl8rvkMGALhwzafq5EkMb7xJ2bZt6Lt2JXnKTL6XgWw4mk2VyUKAhyPX9w1icr8gBoRdOhhklGZw96q78XT0ZOmkpYz6alTTAkBFARz4DvZ/QXl6MltLH+Z4xSi8fWD8/f0J7uZj/aB/dz/c8Zn1C3Duz9VlkLQNTqyFE+sg//SCE5/I08HgKmtgcLg887EYq838Ov8AaccKuHJ6T3rYIQhsTt3M4xsfZ1zoON4e//Ylk/dlF1fy1m/H+H5fGoEOMO/Ecrz2bqPaP4ht3l35Pngot2XsZoiHZOgXH2MuKaF8927Ktu+gbPufVJ+w7mWs9fTEZdgwXEYMpzpmEGnOviTlV5CUW0ZS3ulbbt3goBEQ4u1MdnEVVefZVjPEy5ktz43jhT9eYE3SGl4c+iJTek6x7Q+R/Ces+zuk7gCfrjDhFeh1s/UCfdZF+eDmNDZ/lcAt92oJFvsvflE+Oyhk7LcGhksFhcI0+OGBC38H2hkpJb/MP0BGQiFTXhuGm7d9Wu5tMgAIIb4CxgOdAAPwqpTykwu9vjEB4GKklJRu2IDhzbkY09Jwv/pq3J98mk3FOn45kMmmhByqTRaCPZ2swSAmmJhQz3pJ28qN5UxbPY2M0gyWTlpKhGcE82PnN7zbx2KBxM2w/ws4sgppquKowzS2GW7EaNYx6LpwBl3b5UyO/IbWfvJOwon11mCQuAVMFaB1hC4jIfpqa0Do1A2akJSurakTBKb1pMeIpgeBpUeWMnfXXKb2mspzQ54772sqjWYW/XGK+ZtOYjJL/hbjxY3fzsN46BABL76Az7RptpU/O5vynTsp+3M7ZTt2YMrMBEAXFITriBG4jhiO6/Dh6Pz8kFKSX1ZdGwysgaGclXHnn14osDBl8g5+PvkzTw96mvv73H/pAmXFW2v8x38H9yAY9zwMuA/OM85lsUiWvboDJzc9tz03qHHJDmuDwv4zrYVzg4JXF8g7bv38Jm2F2z+1tnTbqeLcCpb9fSfhfX2ZOKOvXY7ZJgNAQ9k7ANSwVFWRv3gxuR9+BBYLvg8+iO/DD1EmdKw7YuCXA5lsTsjBaJaEejszqV8Qk/sG0yfEuvL26c1Psy55HR9c+UHjZokUpkDsMti/FIpSwMmLwsj72XTqStKTTARFeTL+3h74BLna75c2VkLKn9aAcHwt5B6zPu7Z2frlibrKGlicTq8ubsdNbVO1mV9OB4EJU3vSc2TTg8DcXXNZemQpLw97mbt73F37uJSSVQcymbv6KOmFFVzbO4AXejpifv5JTPn5hPznLdyvbNzFSUqJMTmZsh07KPtzO+U7d2Iusm6T6Rgdhctwa0BwGTIErbt77ftqZh15Vxbzwu4vmTvkPgqc3PHpvBKj65/M6j+Lv8X87eInzz8FG9+w9vU7ecDop2DojIu2Hk/tz2H1h/Fc+3AfogY1fMzkgsxGyD5yuuso9sy/8nQXl9Bauzn9e4JfT/DvAf69rC3fZpiQ0Rz2/JrEzp9PMfmxGLr09m3y8VQAsIExK4vst/5D8S+/oAsKIuD553C/9lqEEBRVGFl72MCqAxlsPZ6LySLp4utCWMQ24sq+4alBT/FAnwcuOdh25mSVcOwX2PcFnNpkfSxyPOZ+97E/rT971qSh1WsYeWtXeo0Kbv5dvgpTzrQOTm2C6lJrX27nEdaA4OwD6//ebpvapmozvy44QOrRAiZM7UHPkcFNOp7ZYmb2xtn8kf4H7014j7GhY4lLLeT/Vh1mT3IBPYM8+H+TexKTc4K0xx5HODkSNn8Bzn2btpr2bNJiofLIEcq3b6ds+w7K9+5FVlaCVotznz64jBiO64iRrBN+vLDyGH/Z/W3ttNNPrnVA77ONh/o+xOMDHr9w7bzEYJ3Vs/cz0Ohh+EwY9Tg4X3qLwq/+sZOywiquf6Sf3We11FHzWex5I8R/Bz0mWbs/s4+c7vY8fY3T6K0tXP8eZwWHnuAdDo3ch6O5mI0Wvv7nLiwWyT1zhjZ5y0wVABqgfPdusl5/g6qjR3EZOpSAl1/GqXu32ucLy6v5/ZCBL+JXkaSdj7FoAEHVD9DN341NCbl1+lvPHmwDIPMA7P8SDnxjHaz1DIP+90L/KWQVeLHxy6PkZ5TRdaAfY+7qhqunY7P/vvWYqiFtl7VlcGI9GE6PZTh5g7EcIsdbWw/jXoCIMdZmuJOn9WavL1IztDhM1WZ+XRhP6pF8rrivB71GNS0IlBvLuX/N/SQVJxOjfYm1sTo6uTnwzDXduWNwGCUrVpA5Zw6OEeGELVyIPuQ8FQE7slRXU7E/lrId2ynfvoOK+HgwXziNhVGrodeB+PNPMa0ohD/fhR0LrBMJBk6Hcc+B+8VnVFnMFpLi89i7JonsJGt+G51ew01PDmie7J+XGgczVkBuAmQfhezD1u6k7MPWCk8NndPpwNDrTGvBr4f1u9mKg86pR/P5+Z1YhkwKZ+gNkU06lgoADSTNZgq/+46ct9/BXFKC9z334PfYo2i9rDWZhIIE7vv1PsLdI5ns909+O5RLn8TPOCAj2W7pXXucEZpDXOl8goeuHQL7lkDWAdA6QM8brH2nEeOprrawY8Up4jen4eblyNi7uxER49civ6dNijPh5OnWwdFfz/S/no+Du7WLoCYgOHnWDRD1nvOs+5zudMC71Be7kUzVZlYvjCfFDkGgotrMO5t2szT1GZBwa+C/efaqIbg56sh5913yFizEdeQIQv73vzpdMi3FXFpK+a7dlGxYT8ma37CUlgJQpYOEPiG8GfgXrhvXl9dv7nOmBWCsgF0fwR/zrBWUPrfDFS9dcuZYWVEVR7ZlcOiPDEoLqtA7aTFWWoOP0MCwGyMZNDHc/r9kYy/IVaWQcwxyjlhbCtlHrMGh+KzV2A5u4Ne9bldSZQn8+nSLtYR//+QQp/bncPf/G4pXQOMna6gA0EjmwkJy3n2Pgq+/Ruvhgd+TTyInT2DKmvuoNlfz9eSva+eE3/PSv3lf/y6PGh9nh6UnD2p/5VndtwgkDsIMgX1hwDToezu4WPdQTYzLYcvXCZQWVtF3fCjDb4zEwVnXor+jzWo+7H1us7Zgxj4P3l2gqhgqi866FVsvHjU/n/28rD8bpQ6d05mAAJCfaB2kzoqHOz+3y5fMZDSzekE8KYfzuWJqw4OAlJKf4zL41+qjZBRVMqZ3FUfFXCI8w1k84SMKX32d4lWr8Lz9NoJefRWhb/1+5/RX51D07Xe1vSGed9zOl6PuZcGmkzxxVTRPXBEJsV/Cpn9BSQZEXQ1XzoGgfhc8ppSSjOOFHNyczqn9OVgskrCe3vQZF4qTq46V78ZhNlvQapuxBWBvFYXWwHB2ayH7KJSdtXGL3tVaCfKNgoJE6H49BPQ5p7LjUfe+g3uj1jWU/f4+S1f1wifYnfAYP0K7exMo4hrc4lABoIkqjx3D8M/XKd+9G0OoKwsnmHjp4SX08zvzBRk1dwOdi3ezSP9fTGjxFOWUSkd+003gtodetE5fO62sqIo/vk7g5P4cfIJdueK+Hm37C2KPGrmU1r7Zs4NFneBReDp4nPW84ZD1y6d1tH7gh/2tNng2hcl4uiVw6HRLYLRtQWB/SgH/t+ow+1IK6R3swf+b3Ivhkb5sSdvCi6se5R8rXQk+WYjfk0/iO+PhJm3zaQ8WaeGZTc/Q7+01FLhBthdM3SDJd4O4D2eRdHIUZbE/Mtf7JzzKkiF0KFz1KoSPvuAxqypMHNuRxcEt6RRkluHooqPHyCD6jAmpU0vNOlVEekIBId282/Zn2xZleXVbC8d/g6I06xobaTmzqO2CxOmA4HHhIFHn/ulKUG4CfyyL50DJNU1K96ICgB1IKfn8vb/S9Ys/6FQCHpMn4//sM+gDrEneft5zCu3Ps5ik+ROANeYhzDbOYnTPUBZNG4wQAmmRHNqawfblJzEbLQyZHE7/qzvbddVfs2iNWUA1Qab79dYWh7naWvsa8hcY8Ri4Ny25njUIHCTlUB7j7+1O7zEX7qPPLKrg32uOsXx/Op3cHHnu2u7cNii0duV4dXIyh+6fgiY7nwN/HcfUxxc2qWxNJaVke8Z23tn3Dkfyj9DNuxuzB85m1vpZbLI8Tfa//oXPLVfiFx6PJjOWY5ZQSke/xKCrp1xwGnBuWgnxm9NJ2GXAVGXGv4s7fcaFEj3YH11HSvBW87kc/CDs+cRaCQobZq28VBWfqcjUVm7OuX9uxafm/gVax3tKb2Nn6RRAY033MlbPoCnjG1TkCwWANtrX0DZ9f/x7/uu5nb/Mm8Z9+13JW/QJJRs20Omvf8Xnzhu4Me5voNlJBY58bLqeqbr1TA0wsOiIA6+sOMhTQyPY/NUxMk8UEdLdm/FTujepX69Fne8iHzG2+WYAndvC6HcnfDPV2ora/gHs/AgGToVRsxud90in13Ld3/qweuFBNi21ToM9NwiUV5v4cPMpPtxyEouER8Z35ZEronBzPPPVKd+3j7RHZuEKbH7xOt41r0V39Cvu6XFPI3/5pjmYe5B39r7DzqydhLiF8Ib/OCZ1vx1NqPX/yueqPhjXOJG/fD36URrcHpnPSzvCiN9SxhdR+QyLPDPt0GQ0c3JfDgc3p5N1qgitXkO3IQH0GReCf5eOswFRrXM/lxFj6v7s1sixu5rW8XmCRmhyJXt/NWO2WNBqBSHDG5Ca+xJUC8BGew17eei3hxgWPIwPJnyAVqOlOi2N7H/9i5K169B7QED/Ity6aDBNXED6298S+uSdaNfPZlnoP/hllx8jqvQ4O+sYdXs0PUYEtnr3QJt2sRZHzxtg2zsQ+xUgod9dMPpJ6BTdqFOZjGbWfHiQ5IN5HPaA3GojFV46+vb3Y/3hHLKKK5nUN4gXrutBmE/dgF38669kvPAi+uBgwj5ciDYslCc2PsGW9C2100NbSmJRIu/tf4+1yWvxcfJhRr8Z3NHtDhxSdlgvUlf/k/mHPuWRE7uRFkHaof6UHskh9P33MA0bze0L/yS7pIrv/jaCYJ2eQ3+kc3hbJpWlRjz9nek7LpTuwwNxcm39MY1W00ot4TrpXhqR7Vd1ATVBRmkG9/xyDx4OHiydtBQPh7NqPknbKJ13H4ZdjlQXClwH90bj34WS1avxuusu5JXj2bSyjIJiJw7rTZj6efK/+wfj1MR5vQrWPtg/34O9n1v7YHvfDGOetg62N9DyPans/ewYQSYNEokE4hzMFHhreeaefozoUbdmJ6Uk78OPyHnnHZwHDyL0vffQeVvnxtdMD00uTrZL9tBLMZQZWBC3gBUnVuCodWR67+lM7z0dV71rTWHh12dh98egcQCtDu5YjCV0LMnT76fq+HG6fLGEvJCuzH57O91KILRKIIQgIqYTfcaGENrdu/nXoij12Wk2nAoAjXS+NA+14r6Bn2aBTwTyzmUcveIWMJsp8oggz6cXpa7B5Pr1x6kyj6uencCWklLm/HSQYRE+LJo+pE43gtIEpTmwYz7s+hiqS6DbRBjzDIQNsfkQo+ZuICzTyKgqHRoEEomoydUpwDvABf9wD/y7eOAf6oLxs7cp/eF7PCZPJuiN19E4ONQ5XnZ5NlN+mYJEsuz6ZfbfDAjrrl2fHvyUpUeWYpZm7up+Fw/3fRhf57NWjpbmwE+PWFM3+HSF/JMw9jmY8DIAptxcjt37IGnOvcjqfh2lRSbKNJJkL8GLjw4hNLjlp68qZ7FTi0MFgEaQUp4/zYOUsPlfsOlNazK1u77A4uBJ4YkMDv7vGw6a+iKFtYYfosvg2peuwTnEegH4KTadp76No0+wB589MBRvV4cLnV5pqIpCaxDYMR8q8q3/N2OfgYhxl8xvFP7CLwSbNNxZ6oAGa3rmFS7VCAFvXdkDQ1IJhqRiKoqtOzYJiwkvFyPBQ7sSEO6Bf7gH3oGudZIGHss/xrTV0+ji0aXB2UMvptJUybKjy1gUv4jS6lImRU5iVv9ZhLqH1n3hifWw/G/WPuXBD1hXyZ4euJS3LybTEsPBzemc3GvAYgGfymQGPTie/CAPpn22i15BHix7eBguDqqi0t6pANAIC+MW8kHsBzw96Gnu7TaV0vxKSrJLKNn0OSXJyZR4DKHEpS8l+VWUFVZR708pzfR0OM6E9+omhVt32MAjy/bRxceFLx8aRoBHx8nV3yKqSq2pC/58D0qzIGSwNRB0m1gvEBSUVTPn50O1CdOCTRrCTBpSdRYydJY6u3RVp6Vx/NFnyS3QYLnuXoocg8hOLq5d8KR31OLX2R3/cA9rUOjizv6KXTy+8XHGhozlnSveuWT20IsxWUysOLGCBbELyK7IZkzIGGYPnF2/i8lUbU3bsf196wKmEY/AutfIGrmYlIIwjDnJpMamklcdioOzjh4jAunqnU/xUzNwjokh7NNPWHs8n5lf7mVcNz8+mjbYpj0HlLZLBYCLkFJSVW6iJK+Sknzr7VBSAntOxBFGBJ7VnagoNdZ5jxASN29n3H2dcPdxqv03e+k3HJF9kEKLMFUz8OiHDP7lc7QedWdMbD+Zx0Of78bHzYGlDw6ns287mQ3UnhgrIXapdcC4MMW6WGfMU6fTFmtZf8TACz/GU1BWzdW9Ath4LJtK4/nTeFTEHyT1kZnIyipC33sX1+HDAZAWSWF2OYakYrKTSshOLiYntQSLyfq9cnbXY/ItZbtpA717dmXWNX/B2d3a6rN1nryUknUp63h337skFScR4xfD7P6zifEeQHWlGWOVCWOV2Xo/OxXj1g+pLsjGGDIOY/g1VCftp8joR/JJaispnt4wsFsa0VPuRe9oDUpFq34h45ln8Jg0ieC3/s3Xe9J48cd4bhsYyn/u6KcmLbRjHToAZJ4oJCk+D49OTjg46awX+bMu9iV5lRir6uZMMWmqqXYpp1tYOB6dXHB3Lsf90ALcq47gft0TuA6/Fc0FakU1X2xfcxYVzz+M6/DhhH24EKGtW/uLSy1k+uJdOGg1fPnQMLoFqP7WZmE2WrNYbp0HuQmYfbryg/MdvHSyF1GB3vznjhj6hHheMJFfyfr1pD/9DDpfX8I++hDHrhdPi2A2WchLLyU7qRhDcgnZScXkZZTWjim4+zrh4etM5slCLBaJRiPoNyEUZzeH0xdy6wXdWGkmuyiP1Pw0jJVmXHDDXXghjBpM1ZdYUX2Wmgt87WdcWFMzDL4uvN5rcz/6mJx58/CdMQP/p57kf+uO8/a6BGaO78rzE3vYfE6lbemwASDrVBE//mdvvTUWji66M7X3s2rw0s3IU3sfpVRTxNc3nE7zkLITvr7HulDj7mXW1AQ2Kvj2W7LmvIrPAw8Q8Hz93PEJhhLuW7STarOFzx4YSv+wZsya2NFZLBzduBSx9b90l4kUOQTiGHASDwAAIABJREFUMuEp9IOmwc4P6w22yVObKfj8cwzf7sCpb1/C5n+ArlOnRp26oryKV39+k8zEAq51uZmKZC3V5fX3BUZYL9hCLymWhRTLQoSDJNQ7mDDfEBycdDg4atE76XBw0qJ31KLXVOFw4BP0KRvRh/TA4bo56DsFWZ9z0CI0gqxTRfz09v5LpmaQUpL16msUfvstgX//O1533sErKw6ydGcKcyb34i+jI+qXWWnzOmwA2LsmiR0/narZN51+V4Qy7MZIHJzqD2wZLUb+tvZvxGbHsnjiYmuah4M/wPKZ4BkCU76DTlENLnfW//2TgqVLCXrzTbxuubne8yl55dz7yQ7yS6v5ePpgRnZt3EVGubDyahNzVx9lyfZkIn1d+GhkAVFHP7TuaOXqDz2uh8M/1+Ybkic2YXh2BgVHtLhfcw3B/5qLxtm5aWU4a3rou70/Zv8n+VhMFjRaDdfO6E1odx8yKtP4IPYDVietxtPRk4f7PszdPe7GUXuBrLApO+CHh61JzCa8DKOeuGBGVpu7nEwmUmfNomzrNsIWzMd59BhmLd3HmkNZvHvPAG6MaVoWVaXlddgAYGvNB+D1Ha/z9bGveWP0G9wQOdnaZbD+HxA23Frzd23cxgzSaCTl4RlU7N1Lly+W4Ny/f73XGIormfrJTpLyypk/ZSBX9bL/tMGOak9SPk9/F0dyXjl/GRXBs9d2x9lBa+0QT94GW/4DpzbC/2/vvsOjqtIHjn/f9AYhJATSA0gVBKSs4P5sWNBdRQEBpReVpiCgCIpioQgKukAoohAJKk3RRVfEtuqqKCCgFFEgkEY6IQkkk0nO7487SIAkJGSSm2TO53l4mJncmftemJz33nPPeY+rNwVnFAm7myH5qZxJdKbhyJEETp2C2GmB8uLDQxe3XcVXO3bQ96Y7cQkqYPne5Ww+vBlXZ1cGtxnMiHYjqOdWSrdgUaER93/nGWWL+70FoZf8fl+xotxcYocMwRJ7nMiYtdCiFcPe+ondJzJZPbwbf2+hT1JqE4dNAFC+M5+Nhzfywg8vMKztMKZeOxG2Pm4szdiuH/ReCq6VG6ljzcwktv8AivLO0nTjRlybXFpXPTPXwvDVP/Fb4mkW9u9A745VWz++rssrKGTR9sOs/PYoIQ08WdCvA92bl5LEE3bBN6+SEP0dp2O9AKHJ+IH4jZ95RVUcy1J8eOjBjIM81P4hYg7GUFBYQN+WfXnkmkdo5FVGSYFTcfD+w8aaDO37wz9ePb9ymx0VJKcQO3AgWK1Ern+PM36NGLDiB+IyzvDew91pH1rLC7w5EIdOAJdzQZmH7rNx3jTCWJv3hifg5qfttkZu/h9/EDtgIG5NmxKxLgYnj0uTSk6+ldHRP7PjWAYv9G7HkOsi7LJvR7Mv/hRTNuzlj5QcHvxbODPualPmxLtDHTqi8i9d50CcofXKMcaiPd72O+v9Nv5bJnw5gSLbzak7I+9kQqcJhNe/TF2j/Vvg348ZVwD/eBU6DCx7+0rKO3yY4w8OwjWoCRHr1pGGG32iviffWsjmsT2I8LfjMqValSktATj84N7EnEQmfz2Z0HqhzL/mUZzX3GV0C/SOgluesesC6e4tWhD8yivkHThA0jMzKSn5+ri7sGZEN3q2DmTmlt9Y+tWfdtu/I7BYi1i4/TD3RX1Pdp6VNSO6Mue+9peddd10yQzE9fz/h7i7Ur+phauG+8L2Z2FhG9g0Eo59y6UTPiomak8U474Y91fjD/Cf2P+w9ejWMg4sFz56FDYOM+rQj/m2yht/AI+WLQldspj82OPEPzaRQA8n3h7VjcIixZA3fyI1u4zFgbQaz6GvAC4o89BlBk0/fBys+TBgLTS70W77uVjaipWkLlpEoymTCXjooRK3KSgsYurGvXy4J5ExNzZnWq9Wehz2ZRw6eZopG/ayP/E0fTqF8NzdV+PrdfnCZcpqJW7gXeT+FgciiJsbymKhwV3/R9B9bYwJZLvWwN53jFm1/i2g83Do+GCl1yZoH92eX4f9WvZGSftg8yhI+8OY/n/z09W+uPmpLVtIemo6vr17EzRvLnviTvHgGzto1sib9x6+jnoeDlwgrhbQ5aAvopRi5v9mcjjzMEtbDqXphtFGKdfhW41l4KqQ/8MPkf/776QuXIT7VVdR7+abL9nG1dmJRf07Us/DheX/PcLpvAJe7N3ur/rz2nmFRYqV3xxl0fbDxr/X4M70alf22rXFJc+fT+5vcbi3aYNnx474DehP5voNWFNTz9dbuXOesVDK/g9g52r47GljgMDV90LnERB+nV2vFgHjSuPHZfD5c+DZEIZuMdZjNkGDe++lICGBtMVLcA0JodNjj7Js8LWMjt7JmJhdvDW8K+4uusBhraOUqjV/OnfurOxl+Z7lqt2admr11lFKPeer1MpblMpOsdvnX07hmTPqaJ++6tC1nVXeH3+Uul1RUZF6+T8HVcS0rerRd3Yri7Ww2mKsDY6kZKt7l36nIqZtVWPW7lRp2XkVen/Gu++pA61aq5Nz5lZsxyd/U2rrFKXmhCr1XH2llnRT6odlSp3JqNDHLP1lack/yE5Wam1f47PXDVAqJ61i8VWBoqIilTB9hjrQqrXK3LRZKaXU5l1xKmLaVjV+3S5VWFhkcoRaaYCdqoQ21SG7gL448QWTvprEP92CmPP7DqTNPdBnJbhWbpx3RRWcPMmxfvfj5OVF0w3r/1pwviTLvj7Cy58e4pbWgUQNutbhy0kXFSne/iGWeZ8ewt3FmRd6X809HYIr1E2W++OPnBj9EN7X9yAsKuqSmdrlYsk15orsXA2Ju8HFE9r1Ma4KQrtc2VXBn58bc0/ysuCO2dB1tP2vLq6QKigg7pEx5P70E2ErluNz/fWs+O8R5v7nEMN7RPLc3W11V2UNpEcB2RzOPMyQT4bQ3FrE6tjDuPeYCD1n2X2oX3md+eUXTgwdhmeXzoS/8QbiUnqv3Lodx3lmy290i2zIqmFdHLbfNS7jDE9u2scPR9O5pXUgc/u0r3BBvfxjx4gd+ACugY2IePddnH18Kh9Y0l4jEfy6ESw5Ru2hzsON1cw8yjFk0ppvdCudK+LW701ofHXl47Kzwpwcjj84iIKEBCLeWYd7y5bM/vggq747xpO9WjHupopPltRKV1qJkorQCQDIzMvkgX/3x5KTzHsJiQT2WmCUyTXZqQ+2kDR9On5DhtDk6RllbvvhngSmbNhLm6D6RI/sRkMHKietlGL9z3G8uPUAIsKz/2zL/V1CK3zGWZiVReyAgRRmZRG5cQNuoaGXf1NF5GcbSWDnaji5z1g4vF1f6DLSKDdRUo33Pe/C57OM6qVdR8PtL1X7FWlFFJw8SWz/ASBC5Ib1ODcKZPKGPWzZk8iCftdwf5cws0OsE7b8ksBT7+8rtUhhedXIBCAivYDXAWdglVJqXlnbVzgBFPtFW/zLYvbEfceejP2sTs7gmr5vw1W3Vip+e0qeO4+M6GiavPgCfvffX+a2Xx5KZmzMbsIaejH4unDe+OZYpc4OysseZyJXur/G9T3w83Ll4MlsejT3Z36/awj1q3gFVVVQwImHH+bMzl1ErFmNV+fOVRD5uZ0po1to52qjm6jgjLGmccTfYe+7RtmJyP8zhpl+/y9w8zG6Ilv/o+pisqO8Q4c4PmgwrmFhRMSspdDDi1HRP/P9kXRGXh/JJ7+erPPfyyvZX2GR4tQZC+m5FtJy8snItZCeYyE9J5+0XAsZORbSc/NJz7FwLC0XBfjlneapn2OY13UwmR71LyhTXh41LgGIiDNwGLgNiAd+Bh5QSh0o7T0VTgDFlk9r/814AOakZnD3LXPh2qGViN7+lNX6V99qeRqmH4+mM+zNHVgKjeULz7mSs4Py2PJLAtPf/5WzBeerplbVvkrbH0Dfa0NY0K/DBQuvlJdSipMvvMCpd98jaM4cGvS5z17hXl5eFuzbYCSDlP3g4gEoaBAJab9D4/YwaAPUr111dnK+/Y64MWOMirfLl5FbJNz52jfEZZ69YLu6/L30dHVi5t1tua6pP+m5RkOeXkajnnnGQlEJza4INPRyo6G3G/4+bvj7uPPxviQAHtu9gTtO/MwnkdextGNfBDg2r/wnCjUxAXQHZiml7rA9nw6glJpb2nuuqAvo2Dds+nAYz/v5MDwrmyk9XzOG7tVAhVlZxPYfQGF2Nk03bcQ1uOzGoOvsz0uciOPm7ESncPtWFf3lxCkshZeWIK6KfZW1v4qe+RSXEbOO5Jdewn/0KAKnTq1siFdGKYj/2UgE+9aDKjSuAoZ+WGoRt5ru1KZNJD0zE99+fQl68UW6z/uSk1l5l2xXl7+Xpanv4UKAjzv+PucadncCvM8/9vdxI8DHnYbebvh5uV0yzHvv1e1xK7y0aqzF2YUO+y8zf6SYmjgPIASIK/Y8HvjbxRuJyMPAwwDh4ZeZJn+RqD1RLNu7DPyMG3xrfOuxZudMxhYkMq7juMu8u/o5+/oSuiyK2P4DiBs/gch1MTh5ld7NkVbKLMyKfEHLq7TPrIp9lfW5iafOlvj65eR89z+S58zBp2dPGk2eXJnQKkcEwroZi9gf/tSYzbtvvTH7vAKLfNckDfr1wxIfT/ryFbiFhpKcVXL/f13+XgIsGtABf+/zjbqflxtuLpUbXJI8bQ5N5k7HWRXiBOQ5u7Ij5BpCpk+jQ6U+2WBmAijpGv6SyxGl1EpgJRhXABXZwbiO4xjn2w42Dqd9Y29+Tc6F+9fU6F8092bNCFn4KnGPjCFx+gxCXltU6k3O4AaeJJTQIIY08GT9I93tGtf1876stn2Vtb/gBhW/MZp/5AgJjz+Oe8uWhMx/2W6VPa/Yua5JW+lpWt35V1dlTf5ulqXRxIkUJCaS+trr9Pm/YWz2b3/JNnX5exnSwJP7Otl3MIHl+HEiV87H4uoCliLynZxxK7RyTctgetx86b/vlTDzNyEeKH6qEAok2nUPxe4BAMbfG4cbr9dgPjfcQODUqWRv20basmWlbvfEHa3wvGg+gKerM0/cYf+ZzNW5L3vuz5qZSdzYcYibG2FRS3HyrgHFyxJ2X9jYN73BeJ6w28yoKkVECH7pJby6dWPU9+vomnkUv7zTvPxtFH55p/X3soIKkpM5MXIUWK3U63wtDR8YSOv3N9LwgYGEcWVXwSUx8x6AC8ZN4J5AAsZN4AeVUvtLe09lRgFF7Ykyun2OfWP8op2b4l9DKaVIeuopsj78iJDF/6L+bbeVuF11joCobaMtlMXCiVGjObt3L+HRa/Dq1KnKYtUMhVlZxD44iLNJJ9kV2Iousb/w31Z/J3jWc/p7WU7WzEyODxmCNTGJ8Og1eLav/Nl+jbsJDCAidwGvYQwDfUspNbus7auqHHRNVZSfz/EhQ8n/808i330Hj1ZVW6OoLlFKkTRzJlmbNhO8YD6+d99tdkgO49A1HVAWyyWvi7s7rffuMSGi2qMoN5fjI0aSf+gQYStX4n3dJbdFr0iNLAetlPpEKdVSKdX8co2/I3Jydyd08WKcfXyIHzsOa0aG2SHVGhnR0WRt2oz/2DG68a9mzT/fjtff//7Xc3F3p/7d/+Sqz7ebGFXNV2SxEDdhAnn79xOyaKHdGv+yOPx6ADWda+NAQpcuwZqWRsJjE0s8s9IulP3116S8PJ96t99Oo0cfNTsch+MaGIhbaMhf9YuMhXYEl0ZlrHLm4JTVSuKUqZz54UeCZr9EvZ49q2W/OgHUAp7t2xM0+yXO7NzJyTlzzA6nRss7fJjEyVPwaNOG4HlzzR/x46Csaek0GDiQ4FdeATc3sj/7DEtsrNlh1UhKKZKefY7s7dtpPGM6De6tvnlKDrseQG3je/fd5B8+TPobq/Bo1Qq/Bx4wO6Qax5qeTvzYcTh5exO6LKrMORRa1Qpbsvivx+5XNefEiJEcHzKU8Og1uDdrZmJkNYtSipSX55P1/vsEjB9Pw6HVW6FAnx7VIo0mTcLnxhs5OXsOuT/uMDucGqXIYiH+0cewpqURGhWFa+PGZoek2Xi0bk3E29EopTg+ZCh5hw+bHVKNkb5iBRlr1uA3eDABE8ZX+/51AqhFxNmZ4FdfwS0igoRJk7DExV3+TQ5AKcXJmc9ydvdugufNxbN9O7ND0i7i3qIFEW9HI05OnBg2nLxDh8wOyXQZ77xD6muvU/+eu2k8Y7op6yjoBFDLOPv4EBa1FKUU8ePGU5iTa3ZIpktftYqsDz8k4NEJ1L/zTrPD0Urh3qwZEWvfRtzdOT5sOGd/K3XKT52X9e+tJL/4Ej4330zw7Nmm3avSCaAWcouIIHTRQvKPHiVx2jQKkpOJHTzEWMPWwWR//jmpCxdR/667CBhX8+o7aRdyi4wkImYtzt7enBgxgrN795odUrXL/vprEqdPx6tLF0IWLURczVvYSSeAWsq7Rw8aT5tGzhdfEDduPGd37SJ1aZTZYVWrvIMHSXhyGh7t2xM0Z7ZeirCWcAsNNZJAgwacGDmKM7trbwmMijqzcycJEyfh0bKlMVDBo2Ir2dmbQ60IVtcc6tDRNsb6Qo4w49Kamsqx/gNAKSI3rMc1MNDskLQKKkhO5sSw4RSkpBC2bBnef+tmdkhVKu/AAY4PHYZLo0ZErIvBpWHDatt3jZwJrFVO8+2fUe+uO+HcYuYi+PTsWednXBbl5RE3YQKFp04RFrVUN/61lGvjxoS/HY1rcBBxjzxC7vffmx1Slck/dowTox/CqV49wt96s1ob/7LoBFCLuQYG4ly/PhQVgYsLKEXON99wZtcus0OrMkopkp5+hry9+wie/zIebduaHZJWCa6BgURER+MWHk7cmLHkfPut2SHZXUFSEidGjQIg/M03cQ0KMjmi83QCqOXOzbhsumkj9e+5GycvTxImPU7SzGcpOmu/srE1RdqyZZz++GMaPf54qRVStdrFxd+f8Og1uF3VnPhx48n+8iuzQ7Iba0YGJ0aNpuh0NmFvrMS9WVOzQ7qAvgdQx6iCAlL/tZj0Vatwa96MkIUL8WjZ0uyw7OL0p5+SMOlxfHvfQ9C8efqmbx1TmJXFidEPkXfwICGvvkr9O243O6RKKczJ4cSw4eT/+Sfhq97Aq2tX02LR9wAchLi6EjhlMmGr3qDwVBax9/cn8733qE2JviRnf/2NxKem49mpE01efFE3/nWQs68v4W+9iWe7diRMnszpTz4xO6QrVpSXR/zYceT9/jshr79mauNfFp0A6iif66+n2ZYP8OralZOznidh4iQKs7LMDqvCClJSODZgAHFjxuDc0I/QJYtxcnMzOyytijjXq0fYqlV4dupIwtQnyProI7NDqjBVUEDC45M5s3MnwfPmUe+mm8wOqVQ6AdRhLgEBhK1cQeATT5D95Zccve++WjfmOnXxEvL27jNG/Cxbjou/v9khaVXM2ceb8JUr8eralcRpT3Fq82azQyo3VVRE4tNPk/PVVzR5dia+//yH2SGVSSeAOk6cnPAfNZLId9Yhzi4cHzKUtOXLUYWFZodWpkMdOnKwdRuyNm40Xigs5Fjv3hzq0NHcwLRq4eTlRdjyZXj36EHS08+Q+d56s0O6LKUUyXPmcvqjf9No0sRaUbFXJwAH4XnNNTT94H3q9+pF6muvc2LkKAqSU8wO6xKqoIDTn36K+0XLX4qHh15VysE4eXoSGrXUqIA7axYZa2PMDqlMaUuWkhkTQ8Phw/F/5BGzwykXnQAciLOPD8GvLCBo9mzO7tvHsXvvJfvrr80OCzCGy6UtX8Gft91OwqTHKczIwKNjRxBB3N1R+fk4efvoVaUcjLEs6r/wubUnybNnk/7WarNDKlHG22+TtnQpvn36EDjtyVozSEEnAAcjIjTo24emmzfhEhhI/JixJM+dR5FJS03mHThA4oyn+fOmm0l97TXcmzUlNCqK5ts+xSUggAYDBxK5/j0aDByINS3NlBg1c4mbG6GLFlGvVy9S5s8nbcVKs0O6wKktW0ieM5d6t91K0AvP15rGH/Q8AIdWlJ9PyvwFZK5bh0fbtoQsfBW3yMgq36+yWsn+/HMyYmI4u3MX4umJ7729aThoEO5XXVXl+9dqJ2W1kvjUdE5v3UrAhAkEjB9nemOb/cUXxD82Ea9uXQlbvhwnd3dT4ylNafMA9JKQDszJ3Z0mM5/Bu0d3kmY8zbE+fWky6zl877mnSvZnzczk1IaNZL77LtaTJ3ENDSVw2jQa9O1jlLTQtDKIiwvBL89DXFxIW7IEZS2g0cSJpiSBgpQU4kY/RP6xY3hcfTWhi5fU2Ma/LDoBaNTr2ROPLW1JeOIJEp+cRu7/vqfJszNx8va2y+fnHTxIxtoYTm/dirJY8O7RnSbPPovPjTcg5wrZaVo5iLOzUfrb1YX05StQlgICn5harUmg6OxZTs56nvzDh3GqX5+wFctx9rHP70p10wlAA8A1KIiINWtIW7actGXLOLtnD8ELX8Xz6quv6POMbp4vyIhZe76bp899NBw8WHfzaJUiTk40ef55xNWVjLfeQhUU4D96NAlTphC6aOEVDRQoys+nMC0Na3o61rS0v/4UphV7np5GwfETF77v9Gn+6N6j1pZg1/cAtEvk/vQTiU88SWFGBoFPTMVvyJByn2GV1M3jN2gQDfrch7OvbxVHrjkSpRQp8+aREf02blddheXIERoMGEDQrOeMn1ssWDMysKYajXdhWhrWcw16ehqFqecb/KLs7BL34eTri0tAAC7+/rgEBCBenuT9tp/8I0egoADx8KDebbfS+Mkna/QItdLuAegEoJXImplJ0gxjRqPPTTcRNHcOLn5+pW6fd/AgGTExnP73+W4ev8GD8bnxRt3No1WZ0hZFKouTjw8uAQE4B/jjEtDIaNwbBeBsa+TP/XH29y+x7EjSrFmcWr8BcXNDWSwXJJ2aqkbdBBaR+4FZQBugm1JKt+o1jIufH6FRS8lcG0PKggUc630vwQsW4NY0koTJxqW2s59fyd08gwbh3qKF2YegOYDm2z8j+eX5ZG/bBlYriODSuDFe1/0Nt7Awo4EP8Lc16AG4BPhXehnGcyXY/Qb0J3P9hlq9FrcpVwAi0gYoAlYAU8ubAPQVgDnyDhwgYfIULMeP4962LfkHDuDRvj3W1FSsSUlGN8+DDxqjeXQ3j1bNauMZeXWrUVcASqmDgOljeLXy8WjbloLERFCK/P37Acjbt8/4oYsLzbd9qrt5NNPUpTPy6mbqPQAR+ZrLXAGIyMPAwwDh4eGdjx8/Xk3RacUVpKSQMn8+p7d9Ztz8cnen3u231fibX5qmmXAFICKfA01K+NHTSqkPy/s5SqmVwEowuoDsFJ5WQa6BgTj5+IDVatTmsVh0bR5Nq+WqLAEopW6tqs/WzKEvtTWtbtETwbRyC1uy+K/HQc89a2IkmqbZgynVQEXkPhGJB7oDH4vINjPi0DRNc2RmjQL6APjAjH1rmqZpBr0egKZpmoPSCUDTNM1B6QSgaZrmoHQC0DRNc1C1qhqoiKQCVzoVOACoy4vK1uXj08dWe9Xl46tNxxahlLpk1matSgCVISI7S5oKXVfU5ePTx1Z71eXjqwvHpruANE3THJROAJqmaQ7KkRLASrMDqGJ1+fj0sdVedfn4av2xOcw9AE3TNO1CjnQFoGmaphWjE4CmaZqDcogEICK9ROR3EflTRJ4yOx57EZEwEflKRA6KyH4RmWh2TPYmIs4i8ouIbDU7FnsTkQYisklEDtn+D7ubHZO9iMjjtu/kbyLyrohUbiV2k4nIWyKSIiK/FXutoYhsF5E/bH/7mRnjlajzCUBEnIGlwJ1AW+ABEWlrblR2YwWmKKXaANcB4+vQsZ0zEThodhBV5HXgU6VUa6ADdeQ4RSQEeAzoopRqBzgDA82NqtLWAL0ueu0p4AulVAvgC9vzWqXOJwCgG/CnUuqoUsoCvAf0Njkmu1BKJSmldtseZ2M0ICHmRmU/IhIK/ANYZXYs9iYi9YEbgDcBlFIWpdQpc6OyKxfAU0RcAC8g0eR4KkUp9Q2QcdHLvYFo2+No4N5qDcoOHCEBhABxxZ7HU4cayXNEJBLoBOwwNxK7eg14EigyO5Aq0AxIBVbburhWiYi32UHZg1IqAXgFOAEkAVlKqc/MjapKNFZKJYFxMgYEmhxPhTlCApASXqtTY19FxAfYDExSSp02Ox57EJF/AilKqV1mx1JFXIBrgWVKqU5ALrWwC6Ektr7w3kBTIBjwFpHB5kallcQREkA8EFbseSi1/HK0OBFxxWj81yml3jc7Hju6HrhHRGIxuu1uEZEYc0Oyq3ggXil17optE0ZCqAtuBY4ppVKVUgXA+0APk2OqCskiEgRg+zvF5HgqzBESwM9ACxFpKiJuGDejPjI5JrsQEcHoQz6olFpodjz2pJSarpQKVUpFYvyffamUqjNnkUqpk0CciLSyvdQTOGBiSPZ0ArhORLxs39Ge1JEb3Bf5CBhmezwM+NDEWK6IKWsCVyellFVEJgDbMEYjvKWU2m9yWPZyPTAE+FVE9them6GU+sTEmLTyexRYZzsxOQqMMDkeu1BK7RCRTcBujJFqv1DLyyaIyLvATUCAiMQDzwHzgA0iMgoj6d1vXoRXRpeC0DRNc1CO0AWkaZqmlUAnAE3TNAelE4CmaZqD0glA0zTNQekEoGma5qB0AtC0yxCRyOJVICv5WauKF+wTkekiMkhEbhCR3SJiFZF+xX7eUUR+sFXW3CciA+wRh6aBA8wD0LSaRCk1+qKXbgf6A97AcGDqRT8/AwxVSv0hIsHALhHZVscKx2km0VcAWp0lIltEZJft7PnhYq/niMhsEdkrIj+KSGPb681tz38WkRdEJKeEz3QWkQW2bfaJyCMlbBNpq/Efbdtmk4h42X72tYh0sT2uD7jZSibEKqX2cVHhO6XUYaXUH7bHiRjlBhrZ719Jc2Q6AWh12UilVGegC/CYiPjbXvcGflRKdQBnWYKSAAAB5UlEQVS+AR6yvf468LpSqiul14sahVHdsivQFXhIRJqWsF0rYKVS6hrgNDCuhG1uxagjXy4i0g1wA46U9z2aVhadALS67DER2Qv8iFEQsIXtdQtwboWxXUCk7XF3YKPt8TulfObtwFBb6Y0dgH+xzy0uTin1P9vjGODvJWzTC/hPeQ7EVmxsLTBCKVUXy2NrJtD3ALQ6SURuwjjD7q6UOiMiXwPnliUsUOdroBRSsd8DAR5VSm27zHYX11gpqeZKN2DsZXdodBV9DDyjlPqxXFFqWjnoKwCtrvIFMm2Nf2uMJTMv50egr+1xaUsYbgPG2spwIyItS1nIJbzYGr8PAN8V/6GIXA0cUkoVlhWQrVDcB8DbSqmNZW2raRWlE4BWV30KuIjIPuBFjMb9ciYBk0XkJyAIyCphm1UYZZt324aGrqDkK4iDwDDb/hsCyy76+Z22GAEQka62KpP3AytE5FzF2v4YS0cOF5E9tj8dy3EsmnZZuhqoptnYRuqcVUopERkIPKCUqvD60bblObfaFkQvbZvtGMM7k640Xk2rLH0PQNPO6wwssS1icgoYWVU7UkrdVlWfrWnlpa8ANE3THJS+B6BpmuagdALQNE1zUDoBaJqmOSidADRN0xyUTgCapmkO6v8Bg2JdzopydRAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# various norms and distances graph\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def l1_norm(v):\n",
    "    '''L1 distance'''\n",
    "    norm = np.sum(v)\n",
    "    return v / norm\n",
    "\n",
    "def l2_norm(v):\n",
    "    '''L2 distance'''\n",
    "    norm = np.sqrt(np.sum(np.square(v)))\n",
    "    return v / norm\n",
    "\n",
    "def froto(p, q):\n",
    "    '''vector from p to q'''\n",
    "    return np.array(p) - np.array(q)\n",
    "\n",
    "def hellingm(p, q):\n",
    "    '''returns Hellinger metric for distributions p and q'''\n",
    "    if len(p) != len(q):\n",
    "        return -1\n",
    "    hm = 0.\n",
    "    for idx, elp in enumerate(p):\n",
    "        hm += (math.sqrt(elp) - math.sqrt(max(q[idx], 0.)))**2\n",
    "    return math.sqrt(hm) / math.sqrt(2.)\n",
    "\n",
    "def euclid(p, q):\n",
    "    '''Euclidean metric for distributions p and q'''\n",
    "    if len(p) != len(q):\n",
    "        return -1\n",
    "    ec = 0.\n",
    "    for idx, elp in enumerate(p):\n",
    "        ec += (elp - q[idx]) ** 2\n",
    "    return math.sqrt(ec) \n",
    "\n",
    "def minkovfr(p, q, pp):\n",
    "    '''Minkowski and fractional metric for distributions p and q'''\n",
    "    if len(p) != len(q):\n",
    "        return -1\n",
    "    ec = 0.\n",
    "    for idx, elp in enumerate(p):\n",
    "        ec += (elp - q[idx]) ** pp\n",
    "    return ec ** (1. / pp)\n",
    "\n",
    "def cosine(p, q):\n",
    "    '''cosine similarity for distributions p and q'''\n",
    "    if len(p) != len(q):\n",
    "        return -1\n",
    "    cs = 0.\n",
    "    pn = 0.\n",
    "    qn = 0.\n",
    "    for idx, elp in enumerate(p):\n",
    "        cs += elp * q[idx]\n",
    "        pn += elp * elp\n",
    "        qn += q[idx] * q[idx]\n",
    "    if cs > 0. and pn > 0. and qn > 0.:\n",
    "        return math.sqrt(cs) / math.sqrt(pn) / math.sqrt(qn)\n",
    "    return 0\n",
    "\n",
    "def jaccind(a, b):\n",
    "    '''returns the Jaccard index for two sets a, b'''\n",
    "    return len(a.intersection(b)) / len(a.union(b))\n",
    "\n",
    "def kullei(p, q):\n",
    "    '''Kullback-Leibler divergence (modified)'''\n",
    "    return np.sum(np.dot(p, np.array([np.log(p / (q0 + 1) + 1) for q0 in q])))\n",
    "\n",
    "def main():\n",
    "    vx = np.array([1., 0.])\n",
    "    xc = range(12)\n",
    "    yc = []\n",
    "    yh = []\n",
    "    ye = []\n",
    "    yd = []\n",
    "    ykl = []\n",
    "    for phi in range(0,12):\n",
    "        vy = np.array([math.cos(phi), math.sin(phi)])\n",
    "        yc.append(cosine(vx, vy))\n",
    "        yh.append(hellingm(vx, vy))\n",
    "        ye.append(euclid(vx, vy))\n",
    "        yd.append(np.dot(vx, vy))\n",
    "        ykl.append(kullei(vx, vy))\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set(xlabel='angle pi/12', ylabel='measure')\n",
    "    plt.plot(xc, yc, marker='o') # Cosine\n",
    "    plt.plot(xc, yh, marker='x') # Helling\n",
    "    plt.plot(xc, ye, marker='+') # Euclid\n",
    "    plt.plot(xc, yd, marker='*') # dot\n",
    "    plt.plot(xc, ykl, marker='.') # KL\n",
    "    ax.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4.5, 'm', 0.45, '10m')\n",
      "(13.2, '100m', 13200.0, 'dm')\n",
      "(12.2, 'm', 1220.0, 'cm')\n",
      "(3.06, 'cm', 30.6, 'mm')\n",
      "(7.0, 'm', 700.0, 'cm')\n",
      "(5.0, 'm', 0.5, '10m')\n",
      "(7.44, 'm', 744.0, 'cm')\n",
      "(1.38, 'mm', 0.0001, '10m')\n",
      "(7.17, 'dm', 71.7, 'cm')\n",
      "(1.5, 'dm', 0.0002, 'km')\n",
      "\n",
      "(9.33, 'dm2', 0.0933, 'm2')\n",
      "(13.29, 'dm2', 13.29, 'dm2')\n",
      "(4.61, 'ha', 4.61, 'ha')\n",
      "(38.5, 'cm2', 0.00385, 'm2')\n",
      "(4.31, 'ha', 0.0431, 'km2')\n",
      "(30.0, 'km2', 3000000000.0, 'dm2')\n",
      "(6.56, 'a', 6560000.0, 'cm2')\n",
      "(2.0, 'ha', 200000000.0, 'cm2')\n",
      "(16.67, 'a', 16.67, 'a')\n",
      "(30.0, 'a', 30000000.0, 'cm2')\n",
      "\n",
      "(1.56, '1000000m3', 1560.0, '1000m3')\n",
      "(4.25, '1000m3', 0.00425, '1000000m3')\n",
      "(10.8, 'mm3', 0.0, 'km3')\n",
      "(6.43, '1000m3', 6430000.0, 'dm3')\n",
      "(2.8, 'm3', 0.0028, '1000m3')\n",
      "(5.11, 'km3', 5110000000000.0, 'dm3')\n",
      "(2.8, 'km3', 2800000000.0, 'm3')\n",
      "(3.45, 'm3', 3450000.0, 'cm3')\n",
      "(22.0, '1000000m3', 2.2e+16, 'mm3')\n",
      "(1.53, '1000000m3', 1.53, '1000000m3')\n"
     ]
    }
   ],
   "source": [
    "# Auri examples\n",
    "import random\n",
    "\n",
    "def dist():\n",
    "    dfact = [1000, 100, 10, 1, 0.1, 0.01, 0.001]\n",
    "    dnms = ['km', '100m', '10m', 'm', 'dm', 'cm', 'mm']\n",
    "    i0 = random.randint(0, len(dfact)-1)\n",
    "    i1 = random.randint(0, len(dfact)-1)\n",
    "    n0 = round(random.randint(5, 100) / random.randint(1, 20), 2)\n",
    "    return (n0, dnms[i0], round(n0 * dfact[i0] / dfact[i1], 4), dnms[i1])\n",
    "\n",
    "def area():\n",
    "    dfact = [1000000, 10000, 100, 1, 0.01, 0.0001, 0.000001]\n",
    "    dnms = ['km2', 'ha', 'a', 'm2', 'dm2', 'cm2', 'mm2']\n",
    "    i0 = random.randint(0, len(dfact)-1)\n",
    "    i1 = random.randint(0, len(dfact)-1)\n",
    "    n0 = round(random.randint(5, 100) / random.randint(1, 20), 2)\n",
    "    return (n0, dnms[i0], round(n0 * dfact[i0] / dfact[i1], 6), dnms[i1])\n",
    "\n",
    "def vol():\n",
    "    dfact = [1000000000, 1000000, 1000, 1, 0.001, 0.000001, 0.000000001]\n",
    "    dnms = ['km3', '1000000m3', '1000m3', 'm3', 'dm3', 'cm3', 'mm3']\n",
    "    i0 = random.randint(0, len(dfact)-1)\n",
    "    i1 = random.randint(0, len(dfact)-1)\n",
    "    n0 = round(random.randint(5, 100) / random.randint(1, 20), 2)\n",
    "    return (n0, dnms[i0], round(n0 * dfact[i0] / dfact[i1], 8), dnms[i1])\n",
    "\n",
    "def main():\n",
    "    for i in range(10):\n",
    "        print(dist())\n",
    "    print()\n",
    "    for i in range(10):\n",
    "        print(area())\n",
    "    print()\n",
    "    for i in range(10):\n",
    "        print(vol())\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1 0.  0.3]\n",
      " [0.1 0.2 0. ]\n",
      " [0.  0.2 0.3]]\n",
      "[[0.07 0.05 0.1 ]]\n",
      "[[1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# test q,k,v-attention\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"compute softmax values for each sets of scores in x\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def main():\n",
    "    q = np.array([[0.1, 0.2, 0.2]])\n",
    "    k = np.array([[0.1, 0, 0.3], [0.1, 0.2, 0], [0, 0.2, 0.3]])\n",
    "    v = np.array([[1., 0., 1.]])\n",
    "    print(k)\n",
    "    print(np.dot(q, k.T))\n",
    "    print(softmax(np.dot(q, k.T)) * v)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text: this is a test\n",
      "++++++++++++++++++++++++++++++++++++++\n",
      "2.6455933144511468\n"
     ]
    }
   ],
   "source": [
    "# text entropy 2\n",
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def entropy(s):\n",
    "    '''calclate information entropy of given text'''\n",
    "    p, lns = Counter(s), float(len(s)) # Counter({'4': 4, '3': 3, '2': 2, '1': 1})\n",
    "    return -sum( count/lns * math.log(count/lns, 2) for count in p.values())\n",
    "        \n",
    "def main():\n",
    "    piece = input(\"Enter text: \").replace('\\n', ' ').replace('\\r', ' ')\n",
    "    print('++++++++++++++++++++++++++++++++++++++')\n",
    "    #print(uniqc(piece))\n",
    "    print(entropy(piece))\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replied the little old\n",
      "woman as her only friend. No, I cannot do that, she replied, but I will give you some supper and a\n",
      "place to pass the night with you, if you will only hold fast to the tip of my tail. \n"
     ]
    }
   ],
   "source": [
    "# Markov chain text generator\n",
    "import sys\n",
    "import random\n",
    " \n",
    "def readdata(file):\n",
    "    '''Read file and return contents.'''\n",
    "    with open(file) as f:\n",
    "        contents = f.read()\n",
    "    return contents\n",
    " \n",
    "def makerule(data, context):\n",
    "    '''Make a rule dict for given data.'''\n",
    "    rule = {}\n",
    "    words = data.split(' ')\n",
    "    index = context\n",
    " \n",
    "    for word in words[index:]:\n",
    "        key = ' '.join(words[index-context:index])\n",
    "        if key in rule:\n",
    "            rule[key].append(word)\n",
    "        else:\n",
    "            rule[key] = [word]\n",
    "        index += 1\n",
    " \n",
    "    return rule\n",
    " \n",
    "def makestring(rule, length):    \n",
    "    '''Use a given rule to make a string.'''\n",
    "    oldwords = random.choice(list(rule.keys())).split(' ') #random starting words\n",
    "    string = ' '.join(oldwords) + ' '\n",
    " \n",
    "    for i in range(length):\n",
    "        try:\n",
    "            key = ' '.join(oldwords)\n",
    "            newword = random.choice(rule[key])\n",
    "            string += newword + ' '\n",
    " \n",
    "            for word in range(len(oldwords)):\n",
    "                oldwords[word] = oldwords[(word + 1) % len(oldwords)]\n",
    "            oldwords[-1] = newword\n",
    " \n",
    "        except KeyError:\n",
    "            return string\n",
    "    return string\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    # Usage: markov.py source.txt context length\n",
    "    #data = readdata(sys.argv[1])\n",
    "    data = '''As soon as they heard her orders they ran away in every direction as fast as they could, Dorothy\n",
    "only stopping once to pick a beautiful flower; and after a time the ladder was ready. The Scarecrow\n",
    "climbed up the ladder first, but he was so anxious to get the new house and my wife as soon as\n",
    "possible. The Lion hesitated no longer, but drank till the dish was empty. How do you do? I'm pretty\n",
    "well, thank you, replied Dorothy politely. How do you do? I'm not feeling well, said the wolf, and\n",
    "he dashed away at full speed, followed by the others. It was lucky the Scarecrow and the Tin\n",
    "Woodman, for we certainly must climb over the wall. When they were on, Dorothy could not take them\n",
    "off had she wished, but of course she did not wish to leave her little dog behind. Toto had run into\n",
    "the crowd to bark at the birds sitting there. Dorothy went to the Witch's castle, where he was\n",
    "placed in a small yard with a high arched room, the walls of which glistened with countless\n",
    "emeralds. Before them stood a little man about the same height as herself; and when she had made out\n",
    "the proper way of nursing it, (which was to twist it up into a sort of lullaby to it as she did not\n",
    "notice when the Scarecrow stumbled into a hole and rolled over to the other side of the Tin Woodman,\n",
    "sadly; for he is much too heavy to carry I shall have to think about that, replied the little old\n",
    "woman as her only friend. No, I cannot do that, she replied, but I will give you some supper and a\n",
    "place to pass the night with you, if you will only hold fast to the tip of my tail.'''\n",
    "    #rule = makerule(data, int(sys.argv[2]))\n",
    "    #string = makestring(rule, int(sys.argv[3]))\n",
    "    rule = makerule(data, 10)\n",
    "    string = makestring(rule, 50)\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.derstandard.at/story/2000115810293/aktuelle-zahlen-zum-coronavirus\n",
      "https://www.sozialministerium.at/\n",
      "https://www.ages.at/themen/krankheitserreger/coronavirus/\n",
      "https://www.who.int/emergencies/diseases/novel-coronavirus-2019/advice-for-public\n",
      "https://en.wikipedia.org/wiki/2019%E2%80%9320_coronavirus_pandemic\n"
     ]
    }
   ],
   "source": [
    "# search test time constraints on google\n",
    "# googlesearch.search(query, tld='com', lang='en', tbs='0',\n",
    "# safe='off', num=10, start=0, stop=None, domains=None, pause=2.0,\n",
    "# tpe='', country='', extra_params=None, user_agent=None)[source]\n",
    "# tbs (str) – Time limits (i.e “qdr:h” => last hour, “qdr:d” => last 24 hours, “qdr:m” => last month)\n",
    "# A specific time range, for example from March 2 1984 to June 5 1987: tbs=cdr:1,cd_min:3/2/1984,cd_max:6/5/1987\n",
    "# extra_params (dict) – A dictionary of extra HTTP GET parameters,\n",
    "# which must be URL encoded. For example if you don’t want Google to filter similar\n",
    "# results you can set the extra_params to {‘filter’: ‘0’} which will append ‘&filter=0’ to every query.\n",
    "try: \n",
    "    from googlesearch import search \n",
    "except ImportError:  \n",
    "    print(\"No module named 'google' found\") \n",
    "  \n",
    "# to search \n",
    "query = \"covid-19\"\n",
    "  \n",
    "#for j in search(query, tld=\"co.in\", num=5, stop=5, pause=1):\n",
    "#for j in search(query, tld=\"co.in\", tbs='cdr:2Ccd_min:2F30%2F2020:Ccd_max:2F11%2F2020', num=5, stop=5, pause=2):\n",
    "for j in search(query, tld=\"co.in\", tbs='cdr:1,cd_min:3/1/2020,cd_max:3/20/2020', num=5, stop=5, pause=2): \n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the h5py package if it isn't installed yet with running in your command line interface\n",
    "# sudo pip install h5py\n",
    "\n",
    "# https://github.com/totalgood/nlpia/tree/master/src/nlpia/book/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# code is adopted to Francois Chollet's example: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "\n",
    "def instantiate_seq2seq_model(num_encoder_tokens, num_decoder_tokens, num_neurons=256):\n",
    "\n",
    "    encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = LSTM(num_neurons, return_state=True)\n",
    "    _, state_h, state_c = encoder(encoder_inputs)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "    decoder_lstm = LSTM(num_neurons, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(num_neurons,))\n",
    "    decoder_state_input_c = Input(shape=(num_neurons,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "\n",
    "    return model, encoder_model, decoder_model\n",
    "\n",
    "\n",
    "def construct_seq2seq_model(num_encoder_tokens, num_decoder_tokens, num_neurons=256):\n",
    "\n",
    "    encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = LSTM(num_neurons, return_state=True)\n",
    "    _, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "    decoder_lstm = LSTM(num_neurons, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from ch10 import instantiate_seq2seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\pugnlp\\constants.py:129: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  FLOAT_TYPES = tuple([t for t in set(np.typeDict.values()) if t.__name__.startswith('float')] + [float])\n",
      "D:\\Anaconda\\lib\\site-packages\\pugnlp\\constants.py:130: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  FLOAT_DTYPES = tuple(set(np.dtype(typ) for typ in FLOAT_TYPES))\n",
      "D:\\Anaconda\\lib\\site-packages\\pugnlp\\constants.py:131: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  INT_TYPES = tuple([t for t in set(np.typeDict.values()) if t.__name__.startswith('int')] + [int])\n",
      "D:\\Anaconda\\lib\\site-packages\\pugnlp\\constants.py:132: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  INT_DTYPES = tuple(set(np.dtype(typ) for typ in INT_TYPES))\n",
      "D:\\Anaconda\\lib\\site-packages\\pugnlp\\constants.py:134: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  NUMERIC_DTYPES = tuple(set(np.dtype(typ) for typ in NUMERIC_TYPES))\n",
      "D:\\Anaconda\\lib\\site-packages\\pugnlp\\constants.py:136: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  DATETIME_TYPES = tuple([t for t in set(np.typeDict.values()) if t.__name__.startswith('datetime')] +\n",
      "D:\\Anaconda\\lib\\site-packages\\pugnlp\\constants.py:141: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  VECTOR_TYPES = (list, tuple, np.matrix, np.ndarray)\n",
      "D:\\Anaconda\\lib\\site-packages\\pugnlp\\constants.py:159: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime instead.\n",
      "  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n",
      "D:\\Anaconda\\lib\\site-packages\\pugnlp\\constants.py:167: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  INF = pd.np.inf\n",
      "D:\\Anaconda\\lib\\site-packages\\pugnlp\\constants.py:168: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  NAN = pd.np.nan\n",
      "INFO:nlpia.constants:Starting logger in nlpia.constants...\n",
      "INFO:__main__:No BIGDATA index found in d:\\berniedata\\deepl\\nlpia\\src\\nlpia\\data\\bigdata_info.csv so copy d:\\berniedata\\deepl\\nlpia\\src\\nlpia\\data\\bigdata_info.latest.csv to d:\\berniedata\\deepl\\nlpia\\src\\nlpia\\data\\bigdata_info.csv if you want to \"freeze\" it.\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('d:\\\\berniedata\\\\deepl\\\\nlpia\\\\src\\\\nlpia\\\\data\\\\mavis-batey-greetings.csv',), **{'low_memory': False})`...\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('d:\\\\berniedata\\\\deepl\\\\nlpia\\\\src\\\\nlpia\\\\data\\\\sms-spam.csv',), **{'low_memory': False})`...\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, unicode_literals, division, absolute_import\n",
    "from builtins import (bytes, dict, int, list, object, range, str,  # noqa\n",
    "    ascii, chr, hex, input, next, oct, open, pow, round, super, filter, map, zip)\n",
    "from future import standard_library\n",
    "standard_library.install_aliases()  # noqa\n",
    "from past.builtins import basestring\n",
    "\n",
    "# from traceback import format_exc\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import shutil\n",
    "from traceback import format_exc\n",
    "from zipfile import ZipFile\n",
    "from math import ceil\n",
    "from itertools import product, zip_longest\n",
    "from requests.exceptions import ConnectionError, InvalidURL, InvalidSchema, InvalidHeader, MissingSchema\n",
    "from urllib.error import URLError\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import ftplib\n",
    "import spacy\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.keyedvectors import REAL, Vocab\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from pugnlp.util import clean_columns\n",
    "\n",
    "from nlpia.constants import DATA_PATH, BIGDATA_PATH\n",
    "from nlpia.constants import DATA_INFO_FILE, BIGDATA_INFO_FILE, BIGDATA_INFO_LATEST\n",
    "from nlpia.constants import INT_MIN, INT_NAN, MIN_DATA_FILE_SIZE\n",
    "from nlpia.constants import EOL  # noqa (not used)\n",
    "from nlpia.constants import tqdm, no_tqdm\n",
    "from nlpia.futil import mkdir_p, path_status, find_files  # from pugnlp.futil\n",
    "from nlpia.futil import find_filepath, expand_filepath, normalize_filepath, normalize_ext, ensure_open\n",
    "from nlpia.futil import read_json, read_text, read_csv\n",
    "from nlpia.web import get_url_filemeta\n",
    "from nlpia.web import dropbox_basename, get_url_title, try_parse_url  # noqa (not used)\n",
    "from nlpia.web import requests_get\n",
    "\n",
    "\n",
    "_parse = None  # placeholder for SpaCy parser + language model\n",
    "\n",
    "np = pd.np\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# logging.config.dictConfig(LOGGING_CONFIG)\n",
    "# # doesn't display line number, etc\n",
    "# if os.environ.get('DEBUG'):\n",
    "#     logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "# SMALLDATA_URL = 'http://totalgood.org/static/data'\n",
    "\n",
    "W2V_FILES = [\n",
    "    'GoogleNews-vectors-negative300.bin.gz',\n",
    "    'glove.6B.zip', 'glove.twitter.27B.zip', 'glove.42B.300d.zip', 'glove.840B.300d.zip',\n",
    "]\n",
    "# You probably want to `rm nlpia/src/nlpia/data/bigdata_info.csv` if you modify any of these\n",
    "# so they don't overwrite what you hardcode within loaders.py here:\n",
    "ZIP_FILES = {\n",
    "    'GoogleNews-vectors-negative300.bin.gz': None,\n",
    "    'glove.6B.zip': ['glove.6B.50d.w2v.txt', 'glove.6B.100d.w2v.txt', 'glove.6B.200d.w2v.txt', 'glove.6B.300d.w2v.txt'],\n",
    "    'glove.twitter.27B.zip': None,\n",
    "    'glove.42B.300d.zip': None,\n",
    "    'glove.840B.300d.zip': None,\n",
    "}\n",
    "ZIP_PATHS = [[os.path.join(BIGDATA_PATH, fn) for fn in ZIP_FILES[k]] if ZIP_FILES[k] else k for k in ZIP_FILES.keys()]\n",
    "\n",
    "harry_docs = [\"The faster Harry got to the store, the faster and faster Harry would get home.\",\n",
    "              \"Harry is hairy and faster than Jill.\",\n",
    "              \"Jill is not as hairy as Harry.\"]\n",
    "\n",
    "\n",
    "def load_imdb_df(dirpath=os.path.join(BIGDATA_PATH, 'aclImdb'), subdirectories=(('train', 'test'), ('pos', 'neg', 'unsup'))):\n",
    "    \"\"\" Walk directory tree starting at `path` to compile a DataFrame of movie review text labeled with their 1-10 star ratings\n",
    "    Returns:\n",
    "      DataFrame: columns=['url', 'rating', 'text'], index=MultiIndex(['train_test', 'pos_neg_unsup', 'id'])\n",
    "    TODO:\n",
    "      Make this more robust/general by allowing the subdirectories to be None and find all the subdirs containing txt files\n",
    "    >> imdb_df().head()\n",
    "                                                          url  rating                                               text\n",
    "    index0 index1 index2\n",
    "    train  pos    0       http://www.imdb.com/title/tt0453418       9  Bromwell High is a cartoon comedy. It ran at t...\n",
    "                  1       http://www.imdb.com/title/tt0210075       7  If you like adult comedy cartoons, like South ...\n",
    "                  2       http://www.imdb.com/title/tt0085688       9  Bromwell High is nothing short of brilliant. E...\n",
    "                  3       http://www.imdb.com/title/tt0033022      10  \"All the world's a stage and its people actors...\n",
    "                  4       http://www.imdb.com/title/tt0043137       8  FUTZ is the only show preserved from the exper...\n",
    "    \"\"\"\n",
    "    dfs = {}\n",
    "    for subdirs in tqdm(list(product(*subdirectories))):\n",
    "        urlspath = os.path.join(dirpath, subdirs[0], 'urls_{}.txt'.format(subdirs[1]))\n",
    "        if not os.path.isfile(urlspath):\n",
    "            if subdirs != ('test', 'unsup'):  # test/ dir doesn't usually have an unsup subdirectory\n",
    "                logger.warning('Unable to find expected IMDB review list of URLs: {}'.format(urlspath))\n",
    "            continue\n",
    "        df = pd.read_csv(urlspath, header=None, names=['url'])\n",
    "        # df.index.name = 'id'\n",
    "        df['url'] = series_strip(df.url, endswith='/usercomments')\n",
    "\n",
    "        textsdir = os.path.join(dirpath, subdirs[0], subdirs[1])\n",
    "        if not os.path.isdir(textsdir):\n",
    "            logger.warning('Unable to find expected IMDB review text subdirectory: {}'.format(textsdir))\n",
    "            continue\n",
    "        filenames = [fn for fn in os.listdir(textsdir) if fn.lower().endswith('.txt')]\n",
    "        df['index0'] = subdirs[0]  # TODO: column names more generic so will work on other datasets\n",
    "        df['index1'] = subdirs[1]\n",
    "        df['index2'] = np.array([int(fn[:-4].split('_')[0]) for fn in filenames])\n",
    "        df['rating'] = np.array([int(fn[:-4].split('_')[1]) for fn in filenames])\n",
    "        texts = []\n",
    "        for fn in filenames:\n",
    "            with ensure_open(os.path.join(textsdir, fn)) as f:\n",
    "                texts.append(f.read())\n",
    "        df['text'] = np.array(texts)\n",
    "        del texts\n",
    "        df.set_index('index0 index1 index2'.split(), inplace=True)\n",
    "        df.sort_index(inplace=True)\n",
    "        dfs[subdirs] = df\n",
    "    return pd.concat(dfs.values())\n",
    "\n",
    "\n",
    "def load_glove(filepath, batch_size=1000, limit=None, verbose=True):\n",
    "    r\"\"\" Load a pretrained GloVE word vector model\n",
    "    First header line of GloVE text file should look like:\n",
    "        400000 50\\n\n",
    "    First vector of GloVE text file should look like:\n",
    "        the .12 .22 .32 .42 ... .42\n",
    "    >>> wv = load_glove(os.path.join(BIGDATA_PATH, 'glove_test.txt'))\n",
    "    >>> wv.most_similar('and')[:3]\n",
    "    [(',', 0.92...),\n",
    "     ('.', 0.91...),\n",
    "     ('of', 0.86...)]\n",
    "    \"\"\"\n",
    "    num_dim = isglove(filepath)\n",
    "    tqdm_prog = tqdm if verbose else no_tqdm\n",
    "    wv = KeyedVectors(num_dim)\n",
    "\n",
    "    if limit:\n",
    "        vocab_size = int(limit)\n",
    "    else:\n",
    "        with ensure_open(filepath) as fin:\n",
    "            for i, line in enumerate(fin):\n",
    "                pass\n",
    "        vocab_size = i + 1\n",
    "\n",
    "    wv.vectors = np.zeros((vocab_size, num_dim), REAL)\n",
    "\n",
    "    with ensure_open(filepath) as fin:\n",
    "        batch, words = [], []\n",
    "        for i, line in enumerate(tqdm_prog(fin, total=vocab_size)):\n",
    "            line = line.split()\n",
    "            word = line[0]\n",
    "            vector = np.array(line[1:]).astype(float)\n",
    "            # words.append(word)\n",
    "            # batch.append(vector)\n",
    "            wv.index2word.append(word)\n",
    "            wv.vocab[word] = Vocab(index=i, count=vocab_size - i)\n",
    "            wv.vectors[i] = vector\n",
    "            if len(words) >= batch_size:\n",
    "                # wv[words] = np.array(batch)\n",
    "                batch, words = [], []\n",
    "            if i >= vocab_size - 1:\n",
    "                break\n",
    "        if words:\n",
    "            wv[words] = np.array(batch)\n",
    "    return wv\n",
    "\n",
    "\n",
    "def load_glove_df(filepath, **kwargs):\n",
    "    \"\"\" Load a GloVE-format text file into a dataframe\n",
    "    >>> df = load_glove_df(os.path.join(BIGDATA_PATH, 'glove_test.txt'))\n",
    "    >>> df.index[:3]\n",
    "    Index(['the', ',', '.'], dtype='object', name=0)\n",
    "    >>> df.iloc[0][:3]\n",
    "    1    0.41800\n",
    "    2    0.24968\n",
    "    3   -0.41242\n",
    "    Name: the, dtype: float64\n",
    "    \"\"\"\n",
    "    pdkwargs = dict(index_col=0, header=None, sep=r'\\s', skiprows=[0], verbose=False, engine='python')\n",
    "    pdkwargs.update(kwargs)\n",
    "    return pd.read_csv(filepath, **pdkwargs)\n",
    "\n",
    "\n",
    "# def load_glove_format(filepath):\n",
    "#     \"\"\" https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python#45894001 \"\"\"\n",
    "#     # glove_input_file = os.path.join(BIGDATA_PATH, filepath)\n",
    "#     word2vec_output_file = os.path.join(BIGDATA_PATH, filepath.split(os.path.sep)[-1][:-4] + '.w2v.txt')\n",
    "#     if not os.path.isfile(word2vec_output_file):  # TODO: also check file size\n",
    "#         glove2word2vec(glove_input_file=filepath, word2vec_output_file=word2vec_output_file)\n",
    "#     return KeyedVectors.load_word2vec_format(word2vec_output_file)\n",
    "\n",
    "\n",
    "def get_en2fr(url='http://www.manythings.org/anki/fra-eng.zip'):\n",
    "    \"\"\" Download and parse English->French translation dataset used in Keras seq2seq example \"\"\"\n",
    "    download_unzip(url)\n",
    "    return pd.read_table(url, compression='zip', header=None, skip_blank_lines=True, sep='\\t', skiprows=0, names='en fr'.split())\n",
    "\n",
    "\n",
    "def load_anki_df(language='deu'):\n",
    "    \"\"\" Load into a DataFrame statements in one language along with their translation into English\n",
    "    >>> df = get_data('zsm')\n",
    "    >>> list(list(df.columns)[:2])\n",
    "    ['eng', 'zsm']\n",
    "    >>> len(df) > 100\n",
    "    True\n",
    "    >> get_data('zsm').head(2)\n",
    "                    eng                                zsm\n",
    "    0      Are you new?                         Awak baru?\n",
    "    1        Forget it.                        Lupakanlah.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(language):\n",
    "        filepath = language\n",
    "        lang = re.search('[a-z]{3}-eng/', filepath).group()[:3].lower()\n",
    "    else:\n",
    "        lang = (language or 'deu').lower()[:3]\n",
    "        filepath = os.path.join(BIGDATA_PATH, '{}-eng'.format(lang), '{}.txt'.format(lang))\n",
    "    df = pd.read_table(filepath, skiprows=1, header=None)\n",
    "    for i, newc in enumerate(['eng', lang, 'license']):\n",
    "        df.columns = [newc if str(c).lower().strip().startswith(newc) else c for c in df.columns]\n",
    "        if newc not in df.columns and i < len(df.columns):\n",
    "            columns = list(df.columns)\n",
    "            columns[i] = newc\n",
    "            df.columns = columns\n",
    "    return df\n",
    "\n",
    "\n",
    "BIG_URLS = {\n",
    "    'w2v': (\n",
    "        'https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1',\n",
    "        1647046227,\n",
    "        'GoogleNews-vectors-negative300.bin.gz',\n",
    "        KeyedVectors.load_word2vec_format,\n",
    "        {'binary': True},\n",
    "    ),\n",
    "    'words_google_news': (\n",
    "        'https://www.dropbox.com/s/9pm0js9qdjr04jy/words_google_news.txt.gz?dl=1',\n",
    "        3015517,\n",
    "    ),\n",
    "    'glove_twitter': (\n",
    "        'https://nlp.stanford.edu/data/glove.twitter.27B.zip',\n",
    "        1520408563,\n",
    "    ),\n",
    "    'glove_small': (\n",
    "        'https://nlp.stanford.edu/data/glove.6B.zip',\n",
    "        862182613,\n",
    "        os.path.join('glove.6B', 'glove.6B.50d.txt'),\n",
    "        load_glove,\n",
    "    ),\n",
    "    'glove_large': (\n",
    "        'https://nlp.stanford.edu/data/glove.840B.300d.zip',\n",
    "        2176768927,\n",
    "    ),\n",
    "    'glove_medium': (\n",
    "        'https://nlp.stanford.edu/data/glove.42B.300d.zip',\n",
    "        1877800501,\n",
    "    ),\n",
    "    'slang': (\n",
    "        'https://www.dropbox.com/s/43c22018fbfzypd/slang.csv.gz?dl=1',\n",
    "        117633024,\n",
    "    ),\n",
    "    'tweets': (\n",
    "        'https://www.dropbox.com/s/5gpb43c494mc8p0/tweets.csv.gz?dl=1',\n",
    "        311725313,\n",
    "    ),\n",
    "    'crimedata': (\n",
    "        'https://www.dropbox.com/s/mg4yokpifu3n6u5/crimedata.csv.gz?dl=1',\n",
    "        2126689,\n",
    "    ),\n",
    "    'cities': (\n",
    "        'https://www.dropbox.com/s/tcri5eyzpabhnyy/cities.csv.gz?dl=1',\n",
    "        8396891,\n",
    "    ),\n",
    "    'cities_us_wordvectors': (\n",
    "        'https://www.dropbox.com/s/7ujezmo03b637q3/cities_us_wordvectors.csv.gz?dl=1',\n",
    "        8451128,\n",
    "    ),\n",
    "    'dialog': (\n",
    "        'https://www.dropbox.com/s/5543bkihxflzry9/dialog.csv.gz?dl=1',\n",
    "        4415234,\n",
    "    ),\n",
    "    'cornellmovies': (\n",
    "        'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip',\n",
    "        9916637,\n",
    "        'cornell_movie_dialogs_corpus',\n",
    "\n",
    "    ),\n",
    "    'save_dialog_tweets': (\n",
    "        'https://www.dropbox.com/s/tlrr9bm45uzm9yl/save_dialog_tweets.txt.gz?dl=1',\n",
    "        4517000,\n",
    "    ),\n",
    "    'lsa_tweets': (\n",
    "        'https://www.dropbox.com/s/rpjt0d060t4n1mr/lsa_tweets_5589798_2003588x200.tar.gz?dl=1',\n",
    "        3112841563,\n",
    "    ),\n",
    "    'lsa_tweets_pickle': (\n",
    "        'https://www.dropbox.com/s/7k0nvl2dx3hsbqp/lsa_tweets_5589798_2003588x200.pkl.projection.u.npy?dl=1',\n",
    "        2900000000,\n",
    "    ),\n",
    "    'ubuntu_dialog_1500k': (\n",
    "        'https://www.dropbox.com/s/krvi79fbsryytc2/ubuntu_dialog_1500k.csv.gz?dl=1',\n",
    "        296098788,\n",
    "    ),\n",
    "    'ubuntu_dialog_test': (\n",
    "        'https://www.dropbox.com/s/47mqbx0vgynvnnj/ubuntu_dialog_test.csv.gz?dl=1',\n",
    "        31273,\n",
    "    ),\n",
    "    'imdb': (\n",
    "        'https://www.dropbox.com/s/yviic64qv84x73j/aclImdb_v1.tar.gz?dl=1',\n",
    "        84125825,\n",
    "        'aclImdb',  # directory for extractall\n",
    "        load_imdb_df,  # postprocessor to combine text files into a single DataFrame\n",
    "    ),\n",
    "    'imdb_test': (\n",
    "        'https://www.dropbox.com/s/cpgrf3udzkbmvuu/aclImdb_test.tar.gz?dl=1',\n",
    "        10858,\n",
    "        'aclImdb_test',  # directory for extractall\n",
    "        load_imdb_df,\n",
    "    ),\n",
    "    'alice': (\n",
    "        # 'https://www.dropbox.com/s/py952zad3mntyvp/aiml-en-us-foundation-alice.v1-9.zip?dl=1',\n",
    "        'https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/' \\\n",
    "        'aiml-en-us-foundation-alice/aiml-en-us-foundation-alice.v1-9.zip',\n",
    "        8249482,\n",
    "    ),\n",
    "    # BRFSS annual mental health survey\n",
    "    'cdc': (\n",
    "        'https://www.cdc.gov/brfss/annual_data/2016/files/LLCP2016ASC.zip',\n",
    "        52284490,\n",
    "    ),\n",
    "}\n",
    "for yr in range(2011, 2017):\n",
    "    BIG_URLS['cdc' + str(yr)[-2:]] = ('https://www.cdc.gov/brfss/annual_data/{yr}/files/LLCP{yr}ASC.zip'.format(yr=yr), None)\n",
    "\n",
    "\n",
    "# Aliases for bigurls. Canonical name given on line by itself.\n",
    "BIG_URLS['cornell'] = BIG_URLS['cornellmoviedialog'] = BIG_URLS['cornellmoviedialogs'] = BIG_URLS['cornell_movie_dialog'] = \\\n",
    "    BIG_URLS['cornell_movie_dialogs'] = BIG_URLS['cornell_movie_dialog_corpus'] = BIG_URLS['cornell_movie_dialogs_corpus'] = \\\n",
    "    BIG_URLS['cornellmovies']\n",
    "BIG_URLS['word2vec'] = BIG_URLS['wv'] = \\\n",
    "    BIG_URLS['w2v']\n",
    "BIG_URLS['glove'] = BIG_URLS['glovesm'] = BIG_URLS['glove-sm'] = BIG_URLS['glove_sm'] = BIG_URLS['glove-small'] = \\\n",
    "    BIG_URLS['glove_small']\n",
    "BIG_URLS['ubuntu'] = BIG_URLS['ubuntu_dialog'] = \\\n",
    "    BIG_URLS['ubuntu_dialog_1500k']\n",
    "BIG_URLS['glovelg'] = BIG_URLS['glove_lg'] = BIG_URLS['glove-lg'] = BIG_URLS['glove-large'] = \\\n",
    "    BIG_URLS['glove_large']\n",
    "BIG_URLS['glovemed'] = BIG_URLS['glove_med'] = BIG_URLS['glove-med'] = BIG_URLS['glove-medium'] = \\\n",
    "    BIG_URLS['glove_medium']\n",
    "\n",
    "\n",
    "def generate_big_urls_glove(bigurls=None):\n",
    "    \"\"\" Generate a dictionary of URLs for various combinations of GloVe training set sizes and dimensionality \"\"\"\n",
    "    bigurls = bigurls or {}\n",
    "    for num_dim in (50, 100, 200, 300):\n",
    "        # not all of these dimensionality, and training set size combinations were trained by Stanford\n",
    "        for suffixes, num_words in zip(\n",
    "                ('sm -sm _sm -small _small'.split(),\n",
    "                 'med -med _med -medium _medium'.split(),\n",
    "                 'lg -lg _lg -large _large'.split()),\n",
    "                (6, 42, 840)\n",
    "        ):\n",
    "            for suf in suffixes[:-1]:\n",
    "                name = 'glove' + suf + str(num_dim)\n",
    "                dirname = 'glove.{num_words}B'.format(num_words=num_words)\n",
    "                # glove.42B.300d.w2v.txt\n",
    "                filename = dirname + '.{num_dim}d.w2v.txt'.format(num_dim=num_dim)\n",
    "                # seed the alias named URL with the URL for that training set size's canonical name\n",
    "                bigurl_tuple = BIG_URLS['glove' + suffixes[-1]]\n",
    "                bigurls[name] = list(bigurl_tuple[:2])\n",
    "                bigurls[name].append(os.path.join(dirname, filename))\n",
    "                bigurls[name].append(load_glove)\n",
    "                bigurls[name] = tuple(bigurls[name])\n",
    "    return bigurls\n",
    "\n",
    "\n",
    "BIG_URLS.update(generate_big_urls_glove())\n",
    "\n",
    "ANKI_LANGUAGES = 'afr arq ara aze eus bel ben ber bul yue cat cbk cmn chv hrv ces dan nld est fin fra glg kat ' \\\n",
    "                 'deu ell heb hin hun isl ind ita jpn kha khm kor lvs lit nds mkd zsm mal mri mar max nob pes ' \\\n",
    "                 'pol por ron rus srp slk slv spa swe tgl tam tat tha tur ukr urd uig vie'.split()\n",
    "ANKI_LANGUAGE_SYNONYMS = list(zip('fre esp ger french spanish german turkish turkey dut dutch'.split(),\n",
    "                                  'fra spa deu fra    spa     deu    tur     tur    dan dan'.split()))\n",
    "LANG2ANKI = dict((lang[:2], lang) for lang in ANKI_LANGUAGES)\n",
    "\"\"\"\n",
    ">>> len(ANKI_LANGUAGES) - len(LANG2ANKI)\n",
    "9\n",
    "\"\"\"\n",
    "ENGLISHES = 'eng usa us bri british american aus australian'.split()\n",
    "for lang in ANKI_LANGUAGES:\n",
    "    for eng in ENGLISHES:\n",
    "        BIG_URLS[lang] = ('http://www.manythings.org/anki/{}-eng.zip'.format(lang), 1000, '{}-{}'.format(lang, eng), load_anki_df)\n",
    "        BIG_URLS[lang + '-eng'] = ('http://www.manythings.org/anki/{}-eng.zip'.format(lang),\n",
    "                                   1000, '{}-{}'.format(lang, eng), load_anki_df)\n",
    "\n",
    "for syn, lang in ANKI_LANGUAGE_SYNONYMS:\n",
    "    BIG_URLS[syn] = BIG_URLS[lang]\n",
    "    for eng in ENGLISHES:\n",
    "        BIG_URLS[lang + '-' + eng] = BIG_URLS[lang + '-eng']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Google N-Gram Viewer meta data is from:\n",
    "* [GOOGLE_NGRAM files](https://storage.googleapis.com/books/ngrams/books/datasetsv2.html)\n",
    "* [GOOGLE_NGRAM data format](https://books.google.com/ngrams/info)\n",
    "\"\"\"\n",
    "GOOGLE_NGRAM_URL = 'http://storage.googleapis.com/books/ngrams/books/'\n",
    "GOOGLE_NGRAM_NAMES = '0 1 2 3 4 5 6 7 8 9 a b c d e f g h i j k l m n o other p pos punctuation q r s t u v w x y z'.split()\n",
    "GOOGLE_NGRAM_FILE = 'googlebooks-eng-all-1gram-20120701-{}.gz'\n",
    "\n",
    "for name in GOOGLE_NGRAM_NAMES:\n",
    "    BIG_URLS['1gram_{}'.format(name)] = (GOOGLE_NGRAM_URL + GOOGLE_NGRAM_FILE.format(name),\n",
    "                                         1000, GOOGLE_NGRAM_FILE.format(name),\n",
    "                                         pd.read_table,\n",
    "                                         {'sep': '\\t', 'header': None, 'names': 'term_pos year term_freq book_freq'.split()})\n",
    "\n",
    "try:\n",
    "    BIGDATA_INFO = pd.read_csv(BIGDATA_INFO_FILE, header=0)\n",
    "    logger.warning('Found BIGDATA index in {default} so it will overwrite nlpia.loaders.BIG_URLS !!!'.format(\n",
    "        default=BIGDATA_INFO_FILE))\n",
    "except (IOError, pd.errors.EmptyDataError):\n",
    "    BIGDATA_INFO = pd.DataFrame(columns='name url file_size'.split())\n",
    "    logger.info('No BIGDATA index found in {default} so copy {latest} to {default} if you want to \"freeze\" it.'.format(\n",
    "        default=BIGDATA_INFO_FILE, latest=BIGDATA_INFO_LATEST))\n",
    "BIG_URLS.update(dict(zip(BIGDATA_INFO.name, zip(BIGDATA_INFO.url, BIGDATA_INFO.file_size))))\n",
    "BIGDATA_INFO = pd.DataFrame(list(\n",
    "    zip(BIG_URLS.keys(), list(zip(*BIG_URLS.values()))[0], list(zip(*BIG_URLS.values()))[1])),\n",
    "    columns='name url file_size'.split())\n",
    "BIGDATA_INFO.to_csv(BIGDATA_INFO_LATEST)\n",
    "\n",
    "\n",
    "# FIXME: consolidate with DATA_INFO or BIG_URLS\n",
    "DATA_NAMES = {\n",
    "    'pointcloud': os.path.join(DATA_PATH, 'pointcloud.csv.gz'),\n",
    "    'hutto_tweets0': os.path.join(DATA_PATH, 'hutto_ICWSM_2014/tweets_GroundTruth.csv.gz'),\n",
    "    'hutto_tweets': os.path.join(DATA_PATH, 'hutto_ICWSM_2014/tweets_GroundTruth.csv'),\n",
    "    'hutto_nyt': os.path.join(DATA_PATH, 'hutto_ICWSM_2014/nytEditorialSnippets_GroundTruth.csv.gz'),\n",
    "    'hutto_movies': os.path.join(DATA_PATH, 'hutto_ICWSM_2014/movieReviewSnippets_GroundTruth.csv.gz'),\n",
    "    'hutto_products': os.path.join(DATA_PATH, 'hutto_ICWSM_2014/amazonReviewSnippets_GroundTruth.csv.gz'),\n",
    "}\n",
    "\n",
    "# FIXME: put these in BIG_URLS, and test/use them with get_data()\n",
    "DDL_DS_QUESTIONS_URL = 'http://minimum-entropy.districtdatalabs.com/api/questions/?format=json'\n",
    "DDL_DS_ANSWERS_URL = 'http://minimum-entropy.districtdatalabs.com/api/answers/?format=json'\n",
    "\n",
    "\n",
    "# Files to load into local variables like loaders.kite_text loaders.kite_history\n",
    "TEXTS = ['kite_text.txt', 'kite_history.txt']\n",
    "CSVS = ['mavis-batey-greetings.csv', 'sms-spam.csv']\n",
    "\n",
    "DATA_INFO = pd.read_csv(DATA_INFO_FILE, header=0)\n",
    "\n",
    "\n",
    "def rename_file(source, dest):\n",
    "    \"\"\" Rename (mv) file(s) from source to dest\n",
    "    >>> from tempfile import mkdtemp\n",
    "    >>> tmpdir = mkdtemp(suffix='doctest_rename_file', prefix='tmp')\n",
    "    >>> fout = ensure_open(os.path.join(tmpdir, 'fake_data.bin.gz'), 'w')\n",
    "    >>> fout.write(b'fake nlpia.loaders.rename_file')\n",
    "    30\n",
    "    >>> fout.close()\n",
    "    >>> dest = rename_file(os.path.join(tmpdir, 'fake_data.bin.gz'), os.path.join(tmpdir, 'Fake_Data.bin.gz'))\n",
    "    >>> os.path.isfile(os.path.join(tmpdir, 'Fake_Data.bin.gz'))\n",
    "    True\n",
    "    \"\"\"\n",
    "    logger.debug('nlpia.loaders.rename_file(source={}, dest={})'.format(source, dest))\n",
    "    if not isinstance(source, str):\n",
    "        dest = [dest] if isinstance(dest, str) else dest\n",
    "        return [rename_file(s, d) for (s, d) in zip_longest(source, dest, fillvalue=[source, dest][int(len(source) > len(dest))])]\n",
    "    logger.debug('nlpia.loaders.os.rename(source={}, dest={})'.format(source, dest))\n",
    "    if source == dest:\n",
    "        return dest\n",
    "    os.rename(source, dest)\n",
    "    return dest\n",
    "\n",
    "\n",
    "def normalize_ext_rename(filepath):\n",
    "    \"\"\" normalize file ext like '.tgz' -> '.tar.gz' and '300d.txt' -> '300d.glove.txt' and rename the file\n",
    "    >>> pth = os.path.join(DATA_PATH, 'sms_slang_dict.txt')\n",
    "    >>> pth == normalize_ext_rename(pth)\n",
    "    True\n",
    "    \"\"\"\n",
    "    logger.warn('normalize_ext.filepath=' + str(filepath))\n",
    "    new_file_path = normalize_ext(filepath)\n",
    "    logger.warn('download_unzip.new_filepaths=' + str(new_file_path))\n",
    "    # FIXME: fails when name is a url filename\n",
    "    filepath = rename_file(filepath, new_file_path)\n",
    "    logger.warn('download_unzip.filepath=' + str(filepath))\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def untar(fname, verbose=True):\n",
    "    \"\"\" Uunzip and untar a tar.gz file into a subdir of the BIGDATA_PATH directory \"\"\"\n",
    "    if fname.lower().endswith(\".tar.gz\"):\n",
    "        dirpath = os.path.join(BIGDATA_PATH, os.path.basename(fname)[:-7])\n",
    "        if os.path.isdir(dirpath):\n",
    "            return dirpath\n",
    "        with tarfile.open(fname) as tf:\n",
    "            members = tf.getmembers()\n",
    "            for member in tqdm(members, total=len(members)):\n",
    "                tf.extract(member, path=BIGDATA_PATH)\n",
    "        dirpath = os.path.join(BIGDATA_PATH, members[0].name)\n",
    "        if os.path.isdir(dirpath):\n",
    "            return dirpath\n",
    "    else:\n",
    "        logger.warning(\"Not a tar.gz file: {}\".format(fname))\n",
    "\n",
    "\n",
    "def series_rstrip(series, endswith='/usercomments', ignorecase=True):\n",
    "    \"\"\" Strip a suffix str (`endswith` str) from a `df` columns or pd.Series of type str \"\"\"\n",
    "    return series_strip(series, startswith=None, endswith=endswith, startsorendswith=None, ignorecase=ignorecase)\n",
    "\n",
    "\n",
    "def series_lstrip(series, startswith='http://', ignorecase=True):\n",
    "    \"\"\" Strip a suffix str (`endswith` str) from a `df` columns or pd.Series of type str \"\"\"\n",
    "    return series_strip(series, startswith=startswith, endswith=None, startsorendswith=None, ignorecase=ignorecase)\n",
    "\n",
    "\n",
    "def series_strip(series, startswith=None, endswith=None, startsorendswith=None, ignorecase=True):\n",
    "    \"\"\" Strip a suffix/prefix str (`endswith`/`startswith` str) from a `df` columns or pd.Series of type str \"\"\"\n",
    "    if ignorecase:\n",
    "        mask = series.str.lower()\n",
    "        endswith = endswith.lower()\n",
    "    else:\n",
    "        mask = series\n",
    "    if not (startsorendswith or endswith or startswith):\n",
    "        logger.warning('In series_strip(): You must specify endswith, startswith, or startsorendswith string arguments.')\n",
    "        return series\n",
    "    if startsorendswith:\n",
    "        startswith = endswith = startsorendswith\n",
    "    if endswith:\n",
    "        mask = mask.str.endswith(endswith)\n",
    "        series[mask] = series[mask].str[:-len(endswith)]\n",
    "    if startswith:\n",
    "        mask = mask.str.endswith(startswith)\n",
    "        series[mask] = series[mask].str[len(startswith):]\n",
    "    return series\n",
    "\n",
    "\n",
    "def endswith_strip(s, endswith='.txt', ignorecase=True):\n",
    "    \"\"\" Strip a suffix from the end of a string\n",
    "    >>> endswith_strip('http://TotalGood.com', '.COM')\n",
    "    'http://TotalGood'\n",
    "    >>> endswith_strip('http://TotalGood.com', endswith='.COM', ignorecase=False)\n",
    "    'http://TotalGood.com'\n",
    "    \"\"\"\n",
    "    if ignorecase:\n",
    "        if s.lower().endswith(endswith.lower()):\n",
    "            return s[:-len(endswith)]\n",
    "    else:\n",
    "        if s.endswith(endswith):\n",
    "            return s[:-len(endswith)]\n",
    "    return s\n",
    "\n",
    "\n",
    "def startswith_strip(s, startswith='http://', ignorecase=True):\n",
    "    \"\"\" Strip a prefix from the beginning of a string\n",
    "    >>> startswith_strip('HTtp://TotalGood.com', 'HTTP://')\n",
    "    'TotalGood.com'\n",
    "    >>> startswith_strip('HTtp://TotalGood.com', startswith='HTTP://', ignorecase=False)\n",
    "    'HTtp://TotalGood.com'\n",
    "    \"\"\"\n",
    "    if ignorecase:\n",
    "        if s.lower().startswith(startswith.lower()):\n",
    "            return s[len(startswith):]\n",
    "    else:\n",
    "        if s.endswith(startswith):\n",
    "            return s[len(startswith):]\n",
    "    return s\n",
    "\n",
    "\n",
    "def combine_dfs(dfs, index_col='index0 index1 index2'.split()):\n",
    "    if isinstance(dfs, 'dict'):\n",
    "        dfs = list(dfs.values())\n",
    "\n",
    "\n",
    "def get_longest_table(url='https://www.openoffice.org/dev_docs/source/file_extensions.html', header=0):\n",
    "    \"\"\" Retrieve the HTML tables from a URL and return the longest DataFrame found\n",
    "    >>> get_longest_table('https://en.wikipedia.org/wiki/List_of_sovereign_states').columns\n",
    "    Index(['Common and formal names', 'Membership within the UN System[a]',\n",
    "       'Sovereignty dispute[b]',\n",
    "       'Further information on status and recognition of sovereignty[d]'],\n",
    "      dtype='object')\n",
    "    \"\"\"\n",
    "    dfs = pd.read_html(url, header=header)\n",
    "    return longest_table(dfs)\n",
    "\n",
    "\n",
    "def get_leet_map():\n",
    "    r\"\"\" Retrieve mapping from English letters to l33t like E => 3 or A => /\\ or /-\\ or @ \"\"\"\n",
    "    df = get_longest_table(\n",
    "        'https://sites.google.com/site/inhainternetlanguage/different-internet-languages/l33t/list-of-l33ts', header=None)\n",
    "    df = df.drop(index=0).iloc[:, :2]\n",
    "    df.columns = ['eng', 'l33t']\n",
    "    df['l33t'] = df['l33t'].str.split(',')\n",
    "    table = []\n",
    "    for i, row in df.iterrows():\n",
    "        for s in row['l33t']:\n",
    "            table.append((row['eng'].strip(), s.strip()))\n",
    "    table = pd.DataFrame(table, columns=df.columns)\n",
    "    leet_path = os.path.join(DATA_PATH, 'l33t.csv')\n",
    "    logger.info('Saving l33t dictionary (character mapping) to {}'.format(leet_path))\n",
    "    table.to_csv(leet_path)\n",
    "    return table\n",
    "\n",
    "\n",
    "def get_netspeak_map():\n",
    "    \"\"\" Retrieve mapping from chat/text abbreviations and acronyms like LMK => Let Me Know \"\"\"\n",
    "    dfs = pd.read_html('https://www.webopedia.com/quick_ref/textmessageabbreviations.asp')\n",
    "    df = dfs[0].drop(index=0)\n",
    "    df.columns = ['abbrev', 'definition']\n",
    "    csv_path = os.path.join(DATA_PATH, 'netspeak.csv')\n",
    "    logger.info('Saving netspeak dictionary (word mapping) to {}'.format(csv_path))\n",
    "    df.to_csv(csv_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "# more nontabular lists at 'https://simple.wikipedia.org/wiki/Leet\n",
    "\n",
    "\n",
    "def longest_table(dfs):\n",
    "    \"\"\" Return this single longest DataFrame that among an array/list/tuple of DataFrames\n",
    "    Useful for automagically finding the DataFrame you want when using pd.read_html() on a Wikipedia page.\n",
    "    \"\"\"\n",
    "    sorted_indices = sorted((len(df if hasattr(df, '__len__') else []), i) for i, df in enumerate(dfs))\n",
    "    return dfs[sorted_indices[-1][1]]\n",
    "\n",
    "\n",
    "def get_filename_extensions(url='https://www.webopedia.com/quick_ref/fileextensionsfull.asp'):\n",
    "    \"\"\" Load a DataFrame of filename extensions from the indicated url\n",
    "    >>> df = get_filename_extensions('https://www.openoffice.org/dev_docs/source/file_extensions.html')\n",
    "    >>> df.head(2)\n",
    "        ext                      description\n",
    "    0    .a        UNIX static library file.\n",
    "    1  .asm  Non-UNIX assembler source file.\n",
    "    \"\"\"\n",
    "    df = get_longest_table(url)\n",
    "    columns = list(df.columns)\n",
    "    columns[0] = 'ext'\n",
    "    columns[1] = 'description'\n",
    "    if len(columns) > 2:\n",
    "        columns[2] = 'details'\n",
    "    df.columns = columns\n",
    "    return df\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# Populate some local string variables with text files from DATA_PATH\n",
    "for filename in TEXTS:\n",
    "    with ensure_open(os.path.join(DATA_PATH, filename)) as fin:\n",
    "        locals()[filename.split('.')[0]] = fin.read()\n",
    "del fin\n",
    "\n",
    "\n",
    "for filename in CSVS:\n",
    "    locals()['df_' + filename.split('.')[0].replace('-', '_')] = read_csv(\n",
    "        os.path.join(DATA_PATH, filename))\n",
    "\n",
    "\n",
    "def migrate_big_urls(big_urls=BIG_URLS, inplace=True):\n",
    "    r\"\"\" Migrate the big_urls table schema/structure from a dict of lists to a dict of dicts\n",
    "    >>> big_urls = {'x': (1, 2, 3, \"4x\"), 'y': (\"yme\", \"cause\")}\n",
    "    >>> inplace = migrate_big_urls(big_urls=big_urls)\n",
    "    >>> inplace\n",
    "    {'x': {0: 1, 1: 2, 2: 3, 3: '4x'}, 'y': {0: 'yme', 1: 'cause'}}\n",
    "    >>> inplace is big_urls\n",
    "    True\n",
    "    >>> big_urls = {'x': [1, 2, 3, \"4x\"], 'y': [\"yme\", \"cause\"]}\n",
    "    >>> copied = migrate_big_urls(big_urls=big_urls, inplace=False)\n",
    "    >>> copied\n",
    "    {'x': {0: 1, 1: 2, 2: 3, 3: '4x'}, 'y': {0: 'yme', 1: 'cause'}}\n",
    "    >>> copied is big_urls\n",
    "    False\n",
    "    >>> copied['x'] is big_urls['x']\n",
    "    False\n",
    "    >>> 1 is copied['x'][0] is big_urls['x'][0]\n",
    "    True\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        big_urls = deepcopy(big_urls)\n",
    "    for name, meta in big_urls.items():\n",
    "        big_urls[name] = dict(zip(range(len(meta)), meta))\n",
    "        big_urls[name] = dict(zip(range(len(meta)), meta))\n",
    "        # big_urls[name]['filenames'] = [normalize_ext(big_urls)]\n",
    "    return big_urls\n",
    "\n",
    "\n",
    "BIG_URLS = migrate_big_urls(BIG_URLS)\n",
    "\n",
    "\n",
    "def normalize_glove(filepath):\n",
    "    r\"\"\" https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python#45894001 \"\"\"\n",
    "    # FIXME\n",
    "    filepath = expand_filepath(filepath)\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def unzip(filepath, verbose=True):\n",
    "    r\"\"\" Unzip GloVE models and convert to word2vec binary models (gensim.KeyedVectors)\n",
    "    The only kinds of files that are returned are \"*.asc\" and \"*.txt\" and only after renaming.\n",
    "    \"\"\"\n",
    "    filepath = expand_filepath(filepath)\n",
    "    filename = os.path.basename(filepath)\n",
    "    tqdm_prog = tqdm if verbose else no_tqdm\n",
    "    z = ZipFile(filepath)\n",
    "\n",
    "    unzip_dir = filename.split('.')[0] if filename.split('.')[0] else os.path.splitext(filename)[0]\n",
    "    unzip_dir = os.path.join(BIGDATA_PATH, unzip_dir)\n",
    "    if not os.path.isdir(unzip_dir) or not len(os.listdir(unzip_dir)) == len(z.filelist):\n",
    "        z.extractall(path=unzip_dir)\n",
    "\n",
    "    logger.info('unzip_dir contains: {}'.format(os.listdir(unzip_dir)))\n",
    "    # for f in os.listdir(unzip_dir):\n",
    "    #     if f.lower().endswith('about.txt'):\n",
    "    #         os.remove(os.path.join(unzip_dir, f))\n",
    "    for f in tqdm_prog(os.listdir(unzip_dir)):\n",
    "        if f[-1] in ' \\t\\r\\n\\f':\n",
    "            bad_path = os.path.join(unzip_dir, f)\n",
    "            logger.warning('Stripping whitespace from end of filename: {} -> {}'.format(\n",
    "                repr(bad_path), repr(bad_path.rstrip())))\n",
    "            shutil.move(bad_path, bad_path.rstrip())\n",
    "            # rename_file(source=bad_path, dest=bad_path.rstrip())\n",
    "    anki_paths = [os.path.join(unzip_dir, f) for f in os.listdir(unzip_dir)\n",
    "                  if f.lower()[:3] in ANKI_LANGUAGES and f.lower()[3:] == '.txt']\n",
    "    logger.info('anki_paths: {}'.format(anki_paths))\n",
    "\n",
    "    w2v_paths = [os.path.join(BIGDATA_PATH, f[:-4] + '.w2v.txt') for f in os.listdir(unzip_dir)\n",
    "                 if f.lower().endswith('.txt') and 'glove' in f.lower()]\n",
    "    for f, word2vec_output_file in zip(os.listdir(unzip_dir), w2v_paths):\n",
    "        glove_input_file = os.path.join(unzip_dir, f)\n",
    "        logger.info('Attempting to converting GloVE format to Word2vec: {} -> {}'.format(\n",
    "            repr(glove_input_file), repr(word2vec_output_file)))\n",
    "        try:\n",
    "            glove2word2vec(glove_input_file=glove_input_file, word2vec_output_file=word2vec_output_file)\n",
    "        except:  # noqa\n",
    "            logger.info('Failed to convert GloVE format to Word2vec: {} -> {}'.format(\n",
    "                repr(glove_input_file), repr(word2vec_output_file)))\n",
    "\n",
    "    txt_paths = [os.path.join(BIGDATA_PATH, f.lower()[:-4] + '.txt') for f in os.listdir(unzip_dir) if f.lower().endswith('.asc')]\n",
    "    for f, txt_file in zip(os.listdir(unzip_dir), txt_paths):\n",
    "        if f.lower().endswith('.asc'):\n",
    "            input_file = os.path.join(unzip_dir, f)\n",
    "            logger.info('Renaming .asc file to .txt: {} -> {}'.format(\n",
    "                repr(input_file), repr(txt_file)))\n",
    "            shutil.move(input_file, txt_file)\n",
    "\n",
    "    return anki_paths + txt_paths + w2v_paths\n",
    "\n",
    "\n",
    "def create_big_url(name):\n",
    "    \"\"\" If name looks like a url, with an http, add an entry for it in BIG_URLS \"\"\"\n",
    "    # BIG side effect\n",
    "    global BIG_URLS\n",
    "    filemeta = get_url_filemeta(name)\n",
    "    if not filemeta:\n",
    "        return None\n",
    "    filename = filemeta['filename']\n",
    "    remote_size = filemeta['remote_size']\n",
    "    url = filemeta['url']\n",
    "    name = filename.split('.')\n",
    "    name = (name[0] if name[0] not in ('', '.') else name[1]).replace(' ', '-')\n",
    "    name = name.lower().strip()\n",
    "    BIG_URLS[name] = (url, int(remote_size or -1), filename)\n",
    "    return name\n",
    "\n",
    "\n",
    "def get_ftp_filemeta(parsed_url, username='anonymous', password='nlpia@totalgood.com'):\n",
    "    \"\"\" FIXME: Get file size, hostname, path metadata from FTP server using parsed_url (urlparse)\"\"\"\n",
    "    return dict(\n",
    "        url=parsed_url.geturl(), hostname=parsed_url.hostname, path=parsed_url.path,\n",
    "        username=(parsed_url.username or username),\n",
    "        remote_size=-1,\n",
    "        filename=os.path.basename(parsed_url.path))\n",
    "    ftp = ftplib.FTP(parsed_url.hostname)\n",
    "    ftp.login(username, password)\n",
    "    ftp.cwd(parsed_url.path)\n",
    "    ftp.retrbinary(\"RETR \" + filename, open(filename, 'wb').write)\n",
    "    ftp.quit()\n",
    "\n",
    "\n",
    "def download_unzip(names=None, normalize_filenames=False, verbose=True):\n",
    "    r\"\"\" Download CSV or HTML tables listed in `names`, unzip and to DATA_PATH/`names`.csv .txt etc\n",
    "    TODO: move to web or data_utils or futils\n",
    "    Also normalizes file name extensions (.bin.gz -> .w2v.bin.gz).\n",
    "    Uses table in data_info.csv (internal DATA_INFO) to determine URL or file path from dataset name.\n",
    "    Also looks\n",
    "    If names or [names] is a valid URL then download it and create a name\n",
    "        from the url in BIG_URLS (not yet pushed to data_info.csv)\n",
    "    \"\"\"\n",
    "    names = [names] if isinstance(names, (str, basestring)) else names\n",
    "    # names = names or list(BIG_URLS.keys())  # download them all, if none specified!\n",
    "    file_paths = {}\n",
    "    for name in names:\n",
    "        created = create_big_url(name)\n",
    "        name = (created or name).lower().strip()\n",
    "\n",
    "        if name in BIG_URLS:\n",
    "            filepath = download_name(name, verbose=verbose)\n",
    "            if not filepath:\n",
    "                continue\n",
    "            file_paths[name] = normalize_ext_rename(filepath)\n",
    "            logger.debug('downloaded name={} to filepath={}'.format(name, file_paths[name]))\n",
    "            fplower = file_paths[name].lower()\n",
    "            if fplower.endswith('.tar.gz'):\n",
    "                logger.info('Extracting {}'.format(file_paths[name]))\n",
    "                file_paths[name] = untar(file_paths[name], verbose=verbose)\n",
    "                logger.debug('download_untar.filepaths=' + str(file_paths))\n",
    "            elif file_paths[name].lower().endswith('.zip'):\n",
    "                file_paths[name] = unzip(file_paths[name], verbose=verbose)\n",
    "                logger.debug('download_unzip.filepaths=' + str(file_paths))\n",
    "        else:\n",
    "            df = pd.read_html(DATA_INFO['url'][name], **DATA_INFO['downloader_kwargs'][name])[-1]\n",
    "            df.columns = clean_columns(df.columns)\n",
    "            file_paths[name] = os.path.join(DATA_PATH, name + '.csv')\n",
    "            df.to_csv(file_paths[name])\n",
    "            file_paths[name] = normalize_ext_rename(file_paths[name])\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "download = download_unzip\n",
    "\n",
    "\n",
    "def download_file(url, data_path=BIGDATA_PATH, filename=None, size=None, chunk_size=4096, normalize_filename=False, verbose=True):\n",
    "    \"\"\"Uses stream=True and a reasonable chunk size to be able to download large (GB) files over https\n",
    "    Downloading this small file takes 1.5 sec. All subsequent \"downloads\" takes .6 sec to verify path and size.\n",
    "    >>> import time\n",
    "    >>> meta = BIG_URLS['ubuntu_dialog_test']\n",
    "    >>> pathend = os.path.join(*('nlpia/src/nlpia/bigdata/ubuntu_dialog_test.csv.gz'.split('/')))\n",
    "    >>> download_file(url=meta[0], verbose=False).endswith(pathend)\n",
    "    True\n",
    "    >>> t0 = time.time()\n",
    "    >>> localpath = download_file(url=BIG_URLS['ubuntu_dialog_test'][0], verbose=False)\n",
    "    >>> t1 = time.time()\n",
    "    >>> localpath is None or ((0.015 < (t1 - t0) < 5.0) and localpath.endswith(pathend))\n",
    "    True\n",
    "    >>> t0 = time.time()\n",
    "    >>> download_file(url=meta[0], size=meta[1], verbose=False).endswith(pathend)\n",
    "    True\n",
    "    >>> time.time() - t0 < 0.02\n",
    "    True\n",
    "    \"\"\"\n",
    "    if isinstance(url, (list, tuple)):\n",
    "        return [\n",
    "            download_file(\n",
    "                s, data_path=data_path, filename=filename, size=size, chunk_size=chunk_size, verbose=verbose)\n",
    "            for s in url]\n",
    "    if url.endswith('dl=0'):\n",
    "        url = url[:-1] + '1'  # noninteractive Dropbox download\n",
    "    remote_size = size\n",
    "\n",
    "    # figure out what filename to expect after download and how big it should be\n",
    "    if filename is None:\n",
    "        filename = dropbox_basename(url)\n",
    "    filepath = os.path.join(data_path, filename)\n",
    "    if normalize_filename:\n",
    "        filepath = normalize_filepath(filepath)\n",
    "    logger.info('expanded+normalized file path: {}'.format(filepath))\n",
    "    tqdm_prog = tqdm if verbose else no_tqdm\n",
    "    logger.info('requesting URL: {}'.format(url))\n",
    "\n",
    "    logger.info('remote_size: {}'.format(remote_size))\n",
    "    stat = path_status(filepath)\n",
    "    local_size = stat.get('size', None)\n",
    "    logger.info('local_size: {}'.format(local_size))\n",
    "\n",
    "    r = None\n",
    "    if not remote_size or not stat['type'] == 'file' or not local_size >= remote_size or not stat['size'] > MIN_DATA_FILE_SIZE:\n",
    "        try:\n",
    "            r = requests_get(url, stream=True, allow_redirects=True, timeout=5)\n",
    "            remote_size = r.headers.get('Content-Length', -1)\n",
    "        except ConnectionError:\n",
    "            logger.error('ConnectionError for url: {} => request {}'.format(url, r))\n",
    "            remote_size = -1 if remote_size is None else remote_size\n",
    "        except (InvalidURL, InvalidSchema, InvalidHeader, MissingSchema) as e:\n",
    "            logger.warn(e)\n",
    "            logger.warn('HTTP Error for url: {}\\n request: {}\\n traceback: {}'.format(url, r, format_exc()))\n",
    "            logger.warn('This can happen for Google Word Vector download links to Dropbox or Google Docs.')\n",
    "    try:\n",
    "        remote_size = int(remote_size)\n",
    "    except ValueError:\n",
    "        remote_size = -1\n",
    "\n",
    "    # remote_size has changed so need to check it again\n",
    "    # TODO: check md5 or get the right size of remote file\n",
    "    if stat['type'] == 'file' and local_size >= remote_size and stat['size'] > MIN_DATA_FILE_SIZE:\n",
    "        r = r.close() if r else r\n",
    "        logger.info('retained: {}'.format(filepath))\n",
    "        return filepath\n",
    "\n",
    "    filedir = os.path.dirname(filepath)\n",
    "    created_dir = mkdir_p(filedir)\n",
    "    logger.info('data path created: {}'.format(created_dir))\n",
    "    assert os.path.isdir(filedir)\n",
    "    assert created_dir.endswith(filedir)\n",
    "    bytes_downloaded = 0\n",
    "    if r:\n",
    "        logger.info('downloading to: {}'.format(filepath))\n",
    "        with open(filepath, 'wb') as f:\n",
    "            for chunk in tqdm_prog(r.iter_content(chunk_size=chunk_size), total=ceil(remote_size / float(chunk_size))):\n",
    "                bytes_downloaded += len(chunk)\n",
    "                if chunk:  # filter out keep-alive chunks\n",
    "                    f.write(chunk)\n",
    "        r.close()\n",
    "    else:\n",
    "        logger.error(f'Unable to requests.get(url={url}) using request object {r}')\n",
    "        return None\n",
    "\n",
    "    logger.debug('nlpia.loaders.download_file: bytes={}'.format(bytes_downloaded))\n",
    "    stat = path_status(filepath)\n",
    "    logger.info(\"local file stat {}\".format(stat))\n",
    "    logger.debug(\"filepath={}: local_size={}, remote_size={}, downloaded_bytes={}\".format(\n",
    "        filepath, size, remote_size, bytes_downloaded))\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def download_name(name, verbose=True, **kwargs):\n",
    "    meta = BIG_URLS[name]\n",
    "    size = meta[1] or -1\n",
    "    url = meta[0]\n",
    "    return download_file(url=url, size=size, verbose=verbose, normalize_filename=True, **kwargs)\n",
    "    # for filename in meta['filenames']\n",
    "\n",
    "\n",
    "def read_named_csv(name, data_path=DATA_PATH, nrows=None, verbose=True):\n",
    "    \"\"\" Convert a dataset in a local file (usually a CSV) into a Pandas DataFrame\n",
    "    TODO: should be called read_named_dataset\n",
    "    Args:\n",
    "    `name` is assumed not to have an extension (like \".csv\"), alternative extensions are tried automatically.file\n",
    "    \"\"\"\n",
    "    if os.path.isfile(name):\n",
    "        try:\n",
    "            return read_json(name)\n",
    "        except (IOError, UnicodeDecodeError, json.JSONDecodeError):\n",
    "            pass\n",
    "        try:\n",
    "            return read_csv(name, nrows=nrows)\n",
    "        except (IOError, pd.errors.ParserError):\n",
    "            pass\n",
    "        try:\n",
    "            return read_text(name, nrows=nrows)\n",
    "        except (IOError, UnicodeDecodeError):\n",
    "            pass\n",
    "    data_path = expand_filepath(data_path)\n",
    "    if os.path.isfile(os.path.join(data_path, name)):\n",
    "        return read_csv(os.path.join(data_path, name), nrows=nrows)\n",
    "    if name in DATASET_NAME2FILENAME:\n",
    "        name = DATASET_NAME2FILENAME[name]\n",
    "        if name.lower().endswith('.txt') or name.lower().endswith('.txt.gz'):\n",
    "            return read_text(os.path.join(data_path, name), nrows=nrows)\n",
    "        else:\n",
    "            return read_csv(os.path.join(data_path, name), nrows=nrows)\n",
    "    try:\n",
    "        return read_csv(os.path.join(data_path, name + '.csv.gz'), nrows=nrows)\n",
    "    except IOError:\n",
    "        pass\n",
    "    try:\n",
    "        return read_csv(os.path.join(data_path, name + '.csv'), nrows=nrows)\n",
    "    except IOError:\n",
    "        pass\n",
    "    try:\n",
    "        return read_json(os.path.join(data_path, name + '.json'))\n",
    "    except IOError:\n",
    "        pass\n",
    "    try:\n",
    "        return read_text(os.path.join(data_path, name + '.txt'), verbose=verbose)\n",
    "    except IOError:\n",
    "        pass\n",
    "\n",
    "    # FIXME: mapping from short name to uncompressed filename\n",
    "    # BIGDATA files are usually not loadable into dataframes\n",
    "    try:\n",
    "        return KeyedVectors.load_word2vec_format(os.path.join(BIGDATA_PATH, name + '.bin.gz'), binary=True)\n",
    "    except IOError:\n",
    "        pass\n",
    "    except ValueError:\n",
    "        pass\n",
    "    try:\n",
    "        return read_text(os.path.join(BIGDATA_PATH, name + '.txt'), verbose=verbose)\n",
    "    except IOError:\n",
    "        pass\n",
    "\n",
    "\n",
    "def get_data(name='sms-spam', nrows=None, limit=None):\n",
    "    \"\"\" Load data from a json, csv, or txt file if it exists in the data dir.\n",
    "    References:\n",
    "      [cities_air_pollution_index](https://www.numbeo.com/pollution/rankings.jsp)\n",
    "      [cities](http://download.geonames.org/export/dump/cities.zip)\n",
    "      [cities_us](http://download.geonames.org/export/dump/cities_us.zip)\n",
    "    >>> from nlpia.data.loaders import get_data\n",
    "    >>> words = get_data('words_ubuntu_us')\n",
    "    >>> len(words)\n",
    "    99171\n",
    "    >>> list(words[:8])\n",
    "    ['A', \"A's\", \"AA's\", \"AB's\", \"ABM's\", \"AC's\", \"ACTH's\", \"AI's\"]\n",
    "    >>> get_data('ubuntu_dialog_test').iloc[0]\n",
    "    Context      i think we could import the old comments via r...\n",
    "    Utterance    basically each xfree86 upload will NOT force u...\n",
    "    Name: 0, dtype: object\n",
    "    >>> get_data('imdb_test').info()\n",
    "    <class 'pandas.core.frame.DataFrame'>\n",
    "    MultiIndex: 20 entries, (train, pos, 0) to (train, neg, 9)\n",
    "    Data columns (total 3 columns):\n",
    "    url       20 non-null object\n",
    "    rating    20 non-null int64\n",
    "    text      20 non-null object\n",
    "    dtypes: int64(1), object(2)\n",
    "    memory usage: 809.0+ bytes\n",
    "    \"\"\"\n",
    "    nrows = nrows or limit\n",
    "    if name in BIG_URLS:\n",
    "        logger.info('Downloading {}'.format(name))\n",
    "        filepaths = download_unzip(name, normalize_filenames=True)\n",
    "        logger.debug('nlpia.loaders.get_data.filepaths=' + str(filepaths))\n",
    "        filepath = filepaths[name][0] if isinstance(filepaths[name], (list, tuple)) else filepaths[name]\n",
    "        logger.debug('nlpia.loaders.get_data.filepath=' + str(filepath))\n",
    "        filepathlow = filepath.lower()\n",
    "\n",
    "        if len(BIG_URLS[name]) >= 4:\n",
    "            kwargs = BIG_URLS[name][4] if len(BIG_URLS[name]) >= 5 else {}\n",
    "            return BIG_URLS[name][3](filepath, **kwargs)\n",
    "        if filepathlow.endswith('.w2v.txt'):\n",
    "            try:\n",
    "                return KeyedVectors.load_word2vec_format(filepath, binary=False, limit=nrows)\n",
    "            except (TypeError, UnicodeError):\n",
    "                pass\n",
    "        if filepathlow.endswith('.w2v.bin') or filepathlow.endswith('.bin.gz') or filepathlow.endswith('.w2v.bin.gz'):\n",
    "            try:\n",
    "                return KeyedVectors.load_word2vec_format(filepath, binary=True, limit=nrows)\n",
    "            except (TypeError, UnicodeError):\n",
    "                pass\n",
    "        if filepathlow.endswith('.gz'):\n",
    "            try:\n",
    "                filepath = ensure_open(filepath)\n",
    "            except:  # noqa\n",
    "                pass\n",
    "        if re.match(r'.json([.][a-z]{0,3}){0,2}', filepathlow):\n",
    "            return read_json(filepath)\n",
    "        if filepathlow.endswith('.tsv.gz') or filepathlow.endswith('.tsv'):\n",
    "            try:\n",
    "                return pd.read_table(filepath)\n",
    "            except:  # noqa\n",
    "                pass\n",
    "        if filepathlow.endswith('.csv.gz') or filepathlow.endswith('.csv'):\n",
    "            try:\n",
    "                return read_csv(filepath)\n",
    "            except:  # noqa\n",
    "                pass\n",
    "        if filepathlow.endswith('.txt'):\n",
    "            try:\n",
    "                return read_text(filepath)\n",
    "            except (TypeError, UnicodeError):\n",
    "                pass\n",
    "        return filepaths[name]\n",
    "    elif name in DATASET_NAME2FILENAME:\n",
    "        return read_named_csv(name, nrows=nrows)\n",
    "    elif name in DATA_NAMES:\n",
    "        return read_named_csv(DATA_NAMES[name], nrows=nrows)\n",
    "    elif os.path.isfile(name):\n",
    "        return read_named_csv(name, nrows=nrows)\n",
    "    elif os.path.isfile(os.path.join(DATA_PATH, name)):\n",
    "        return read_named_csv(os.path.join(DATA_PATH, name), nrows=nrows)\n",
    "\n",
    "    msg = 'Unable to find dataset \"{}\"\" in {} or {} (*.csv.gz, *.csv, *.json, *.zip, or *.txt)\\n'.format(\n",
    "        name, DATA_PATH, BIGDATA_PATH)\n",
    "    msg += 'Available dataset names include:\\n{}'.format('\\n'.join(DATASET_NAMES))\n",
    "    logger.error(msg)\n",
    "    raise IOError(msg)\n",
    "\n",
    "\n",
    "def multifile_dataframe(paths=['urbanslang{}of4.csv'.format(i) for i in range(1, 5)], header=0, index_col=None):\n",
    "    \"\"\"Like pandas.read_csv, but loads and concatenates (df.append(df)s) DataFrames together\"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for p in paths:\n",
    "        df = df.append(read_csv(p, header=header, index_col=index_col), ignore_index=True if not index_col else False)\n",
    "    if index_col and df.index.name == index_col:\n",
    "        del df[index_col]\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_wikidata_qnum(wikiarticle, wikisite):\n",
    "    \"\"\"Retrieve the Query number for a wikidata database of metadata about a particular article\n",
    "    >>> print(get_wikidata_qnum(wikiarticle=\"Andromeda Galaxy\", wikisite=\"enwiki\"))\n",
    "    Q2469\n",
    "    \"\"\"\n",
    "    resp = requests_get('https://www.wikidata.org/w/api.php', timeout=5, params={\n",
    "        'action': 'wbgetentities',\n",
    "        'titles': wikiarticle,\n",
    "        'sites': wikisite,\n",
    "        'props': '',\n",
    "        'format': 'json'\n",
    "    }).json()\n",
    "    return list(resp['entities'])[0]\n",
    "\n",
    "\n",
    "DATASET_FILENAMES = [f['name'] for f in find_files(DATA_PATH, ext='.csv.gz', level=0)]\n",
    "DATASET_FILENAMES += [f['name'] for f in find_files(DATA_PATH, ext='.csv', level=0)]\n",
    "DATASET_FILENAMES += [f['name'] for f in find_files(DATA_PATH, ext='.json', level=0)]\n",
    "DATASET_FILENAMES += [f['name'] for f in find_files(DATA_PATH, ext='.txt', level=0)]\n",
    "DATASET_NAMES = [\n",
    "    f[:-4] if f.endswith('.csv') else f for f in [os.path.splitext(f)[0] for f in DATASET_FILENAMES]]\n",
    "DATASET_NAME2FILENAME = dict(sorted(zip(DATASET_NAMES, DATASET_FILENAMES)))\n",
    "\n",
    "\n",
    "def str2int(s):\n",
    "    s = ''.join(c for c in s if c in '0123456789')\n",
    "    return int(s or INT_MIN)\n",
    "\n",
    "\n",
    "def clean_toxoplasmosis(url='http://www.rightdiagnosis.com/t/toxoplasmosis/stats-country.htm'):\n",
    "    dfs = pd.read_html('http://www.rightdiagnosis.com/t/toxoplasmosis/stats-country.htm', header=0)\n",
    "    df = dfs[0].copy()\n",
    "    df.columns = normalize_column_names(df.columns)\n",
    "    df = df.dropna().copy()\n",
    "    df['extrapolated_prevalence'] = df['extrapolated_prevalence'].apply(str2int)\n",
    "    df['population_estimated_used'] = df['population_estimated_used'].apply(str2int)\n",
    "    df['frequency'] = df.extrapolated_prevalence.astype(float) / df.population_estimated_used\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_column_names(df):\n",
    "    r\"\"\" Clean up whitespace in column names. See better version at `pugnlp.clean_columns`\n",
    "    >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=['Hello World', 'not here'])\n",
    "    >>> normalize_column_names(df)\n",
    "    ['hello_world', 'not_here']\n",
    "    \"\"\"\n",
    "    columns = df.columns if hasattr(df, 'columns') else df\n",
    "    columns = [c.lower().replace(' ', '_') for c in columns]\n",
    "    return columns\n",
    "\n",
    "\n",
    "def clean_column_values(df, inplace=True):\n",
    "    r\"\"\" Convert dollar value strings, numbers with commas, and percents into floating point values\n",
    "    >>> df = get_data('us_gov_deficits_raw')\n",
    "    >>> df2 = clean_column_values(df, inplace=False)\n",
    "    >>> df2.iloc[0]\n",
    "    Fiscal year                                                               10/2017-3/2018\n",
    "    President's party                                                                      R\n",
    "    Senate majority party                                                                  R\n",
    "    House majority party                                                                   R\n",
    "    Top-bracket marginal income tax rate                                                38.3\n",
    "    National debt millions                                                       2.10896e+07\n",
    "    National debt millions of 1983 dollars                                       8.47004e+06\n",
    "    Deficit\\n(millions of 1983 dollars)                                               431443\n",
    "    Surplus string in 1983 dollars                                                       NaN\n",
    "    Deficit string in 1983 dollars ($ = $10B)    $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "    Net surplus in 1983 dollars ($B)                                                    -430\n",
    "    Name: 0, dtype: object\n",
    "    \"\"\"\n",
    "    dollars_percents = re.compile(r'[%$,;\\s]+')\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "    for c in df.columns:\n",
    "        values = None\n",
    "        if df[c].dtype.char in '<U S O'.split():\n",
    "            try:\n",
    "                values = df[c].copy()\n",
    "                values = values.fillna('')\n",
    "                values = values.astype(str).str.replace(dollars_percents, '')\n",
    "                # values = values.str.strip().str.replace(dollars_percents, '').str.strip()\n",
    "                if values.str.len().sum() > .2 * df[c].astype(str).str.len().sum():\n",
    "                    values[values.isnull()] = np.nan\n",
    "                    values[values == ''] = np.nan\n",
    "                    values = values.astype(float)\n",
    "            except ValueError:\n",
    "                values = None\n",
    "            except:  # noqa\n",
    "                logger.error('Error on column {} with dtype {}'.format(c, df[c].dtype))\n",
    "                raise\n",
    "\n",
    "        if values is not None:\n",
    "            if values.isnull().sum() < .6 * len(values) and values.any():\n",
    "                df[c] = values\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_geonames(filepath='http://download.geonames.org/export/dump/cities1000.zip'):\n",
    "    \"\"\"Clean the table of city metadata from download.geoname.org/export/dump/{filename}\n",
    "    Reference:\n",
    "      http://download.geonames.org/export/dump/readme.txt\n",
    "    'cities1000.txt' and 'allCountries.txt' have the following tab-separated fields:\n",
    "    0  geonameid         : integer id of record in geonames database\n",
    "    1  name              : name of geographical point (utf8) varchar(200)\n",
    "    2  asciiname         : name of geographical point in plain ascii characters, varchar(200)\n",
    "    3  alternatenames    : alternatenames, comma separated, ascii names automatically transliterated,\n",
    "                           convenience attribute from alternatename table, varchar(10000)\n",
    "    4  latitude          : latitude in decimal degrees (wgs84)\n",
    "    5  longitude         : longitude in decimal degrees (wgs84)\n",
    "    6  feature class     : see http://www.geonames.org/export/codes.html, char(1)\n",
    "    7  feature code      : see http://www.geonames.org/export/codes.html, varchar(10)\n",
    "    8  country code      : ISO-3166 2-letter country code, 2 characters\n",
    "    9  cc2               : alternate country codes, comma separated, ISO-3166 2-letter country code, 200 characters\n",
    "    10 admin1 code       : fipscode (subject to change to iso code), see exceptions below,\n",
    "                           see file admin1Codes.txt for display names of this code; varchar(20)\n",
    "    11 admin2 code       : code for the second administrative division, a county in the US,\n",
    "                           see file admin2Codes.txt; varchar(80)\n",
    "    12 admin3 code       : code for third level administrative division, varchar(20)\n",
    "    13 admin4 code       : code for fourth level administrative division, varchar(20)\n",
    "    14 population        : bigint (8 byte int)\n",
    "    15 elevation         : in meters, integer\n",
    "    16 dem               : digital elevation model, srtm3 or gtopo30, average elevation of\n",
    "                           (3''x3''ca 90mx90m) or 30''x30''(ca 900mx900m) area in meters, integer.\n",
    "                           srtm processed by cgiar/ciat.\n",
    "    17 timezone          : the iana timezone id (see file timeZone.txt) varchar(40)\n",
    "    18 modification date : date of last modification in yyyy-MM-dd format\n",
    "    \"\"\"\n",
    "    columns = ['geonameid', 'name', 'asciiname', 'alternatenames', 'latitude', 'longitude', 'feature class',\n",
    "               'feature code', 'country code']\n",
    "    columns += ['cc2', 'admin1_code', 'admin2_code', 'admin3_code', 'admin4_code', 'population', 'elevation',\n",
    "                'dem', 'timezone', 'modification date']\n",
    "    columns = normalize_column_names(columns)\n",
    "    df = pd.read_csv(filepath, sep='\\t', index_col=None, low_memory=False, header=None)\n",
    "    df.columns = columns\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_geo_adwords(filename='AdWords API Location Criteria 2017-06-26.csv.gz'):\n",
    "    \"\"\" WARN: Not a good source of city names. This table has many errors, even after cleaning\"\"\"\n",
    "    df = pd.read_csv(filename, header=0, index_col=0, low_memory=False)\n",
    "    df.columns = [c.replace(' ', '_').lower() for c in df.columns]\n",
    "    canonical = pd.DataFrame([list(row) for row in df.canonical_name.str.split(',').values])\n",
    "\n",
    "    def cleaner(row):\n",
    "        cleaned = pd.np.array(\n",
    "            [s for i, s in enumerate(row.values) if s not in ('Downtown', None) and (i > 3 or row[i + 1] != s)])\n",
    "        if len(cleaned) == 2:\n",
    "            cleaned = [cleaned[0], None, cleaned[1], None, None]\n",
    "        else:\n",
    "            cleaned = list(cleaned) + [None] * (5 - len(cleaned))\n",
    "        if not pd.np.all(pd.np.array(row.values)[:3] == pd.np.array(cleaned)[:3]):\n",
    "            logger.info('{} => {}'.format(row.values, cleaned))\n",
    "        return list(cleaned)\n",
    "\n",
    "    cleancanon = canonical.apply(cleaner, axis=1)\n",
    "    cleancanon.columns = 'city region country extra extra2'.split()\n",
    "    df['region'] = cleancanon.region\n",
    "    df['country'] = cleancanon.country\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_cornell_movies(filename='cornell_movie_dialogs_corpus.zip', subdir='cornell movie-dialogs corpus'):\n",
    "    \"\"\" Load a dataframe of ~100k raw (uncollated) movie lines from the cornell movies dialog corpus\n",
    "    >>> local_filepath = download_file(BIG_URLS['cornell_movie_dialogs_corpus'][0])\n",
    "    >>> df = clean_cornell_movies(filename='cornell_movie_dialogs_corpus.zip')\n",
    "    >>> df.describe(include='all')\n",
    "              user   movie  person utterance\n",
    "    count   304713  304713  304713    304446\n",
    "    unique    9035     617    5356    265783\n",
    "    top      u4525    m289    JACK     What?\n",
    "    freq       537    1530    3032      1684\n",
    "    \"\"\"\n",
    "    fullpath_zipfile = find_filepath(filename)\n",
    "    dirname = os.path.basename(filename)\n",
    "    subdir = 'cornell movie-dialogs corpus'\n",
    "    if fullpath_zipfile.lower().endswith('.zip'):\n",
    "        retval = unzip(fullpath_zipfile)\n",
    "        logger.debug(f'unzip({fullpath_zipfile}) return value: {retval}')\n",
    "        dirname = dirname[:-4]\n",
    "    fullpath_movie_lines = os.path.join(BIGDATA_PATH, dirname, subdir, 'movie_lines.txt')\n",
    "    dialog = pd.read_csv(\n",
    "        fullpath_movie_lines, sep=r'\\+\\+\\+\\$\\+\\+\\+', engine='python', header=None, index_col=0)\n",
    "    dialog.columns = 'user movie person utterance'.split()\n",
    "    dialog.index.name = 'line'\n",
    "    dialog.index = [int(s.strip()[1:]) for s in dialog.index.values]\n",
    "    dialog.sort_index(inplace=True)\n",
    "    for col in dialog.columns:\n",
    "        dialog[col] = dialog[col].str.strip()\n",
    "    return dialog\n",
    "\n",
    "\n",
    "def isglove(filepath):\n",
    "    \"\"\" Get the first word vector in a GloVE file and return its dimensionality or False if not a vector\n",
    "    >>> isglove(os.path.join(DATA_PATH, 'cats_and_dogs.txt'))\n",
    "    False\n",
    "    \"\"\"\n",
    "\n",
    "    with ensure_open(filepath, 'r') as f:\n",
    "        header_line = f.readline()\n",
    "        vector_line = f.readline()\n",
    "    try:\n",
    "        num_vectors, num_dim = header_line.split()\n",
    "        return int(num_dim)\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "    vector = vector_line.split()[1:]\n",
    "    if len(vector) % 10:\n",
    "        print(vector)\n",
    "        print(len(vector) % 10)\n",
    "        return False\n",
    "    try:\n",
    "        vector = np.array([float(x) for x in vector])\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "    if np.all(np.abs(vector) < 12.):\n",
    "        return len(vector)\n",
    "    return False\n",
    "\n",
    "\n",
    "def nlp(texts, lang='en', linesep=None, verbose=True):\n",
    "    r\"\"\" Use the SpaCy parser to parse and tag natural language strings.\n",
    "    Load the SpaCy parser language model lazily and share it among all nlpia modules.\n",
    "    Probably unnecessary, since SpaCy probably takes care of this with `spacy.load()`\n",
    "    >>> _parse is None\n",
    "    True\n",
    "    >>> doc = nlp(\"Domo arigatto Mr. Roboto.\")\n",
    "    >>> doc.text\n",
    "    'Domo arigatto Mr. Roboto.'\n",
    "    >>> doc.ents\n",
    "    (Roboto,)\n",
    "    >>> docs = nlp(\"Hey Mr. Tangerine Man!\\nPlay a song for me.\\n\", linesep='\\n')\n",
    "    >>> doc = docs[0]\n",
    "    >>> [t for t in doc]\n",
    "    [Hey, Mr., Tangerine, Man, !]\n",
    "    >>> [tok.text for tok in doc]\n",
    "    ['Hey', 'Mr.', 'Tangerine', 'Man', '!']\n",
    "    >>> [(tok.text, tok.tag_) for tok in doc]\n",
    "    [('Hey', 'UH'),\n",
    "     ('Mr.', 'NNP'),\n",
    "     ('Tangerine', 'NNP'),\n",
    "     ('Man', 'NN'),\n",
    "     ('!', '.')]\n",
    "    >>> [(ent.text, ent.ent_id, ent.has_vector, ent.vector[:3].round(3)) for ent in doc.ents]\n",
    "    [('Tangerine Man', 0, True, array([0.72 , 1.913, 2.675], dtype=float32))]\n",
    "    \"\"\"\n",
    "    # doesn't let you load a different model anywhere else in the module\n",
    "    linesep = os.linesep if linesep in ('default', True, 1, 'os') else linesep\n",
    "    tqdm_prog = no_tqdm if (not verbose or (hasattr(texts, '__len__') and len(texts) < 3)) else tqdm\n",
    "    global _parse\n",
    "    if not _parse:\n",
    "        try:\n",
    "            _parse = spacy.load(lang)\n",
    "        except (OSError, IOError):\n",
    "            try:\n",
    "                spacy.cli.download(lang)\n",
    "            except URLError:\n",
    "                logger.warning(\"Unable to download Spacy language model '{}' so nlp(text) just returns text.split()\".format(lang))\n",
    "    parse = _parse or str.split\n",
    "    # TODO: reverse this recursion (str first then sequence) to allow for sequences of sequences of texts\n",
    "    if isinstance(texts, str):\n",
    "        if linesep:\n",
    "            return nlp(texts.split(linesep))\n",
    "        else:\n",
    "            return nlp([texts])\n",
    "    if hasattr(texts, '__len__'):\n",
    "        if len(texts) == 1:\n",
    "            return parse(texts[0])\n",
    "        elif len(texts) > 1:\n",
    "            return [(parse or str.split)(text) for text in tqdm_prog(texts)]\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        # return generator if sequence of strings doesn't have __len__ which means its an iterable or generator itself\n",
    "        return (parse(text) for text in tqdm_prog(texts))\n",
    "    # TODO: return the same type as the input, e.g. `type(texts)(texts)`\n",
    "\n",
    "\n",
    "def clean_win_tsv(filepath=os.path.join(DATA_PATH, 'Products.txt'),\n",
    "                  index_col=False, sep='\\t', lineterminator='\\r', error_bad_lines=False, **kwargs):\n",
    "    \"\"\" Load and clean tab-separated files saved on Windows OS ('\\r\\n') \"\"\"\n",
    "    df = pd.read_csv(filepath, index_col=index_col, sep=sep, lineterminator=lineterminator,\n",
    "                     error_bad_lines=error_bad_lines, **kwargs)\n",
    "    index_col = df.columns[0]\n",
    "    original_len = len(df)\n",
    "    if df[index_col].values[-1] == '\\n':\n",
    "        df.iloc[-1, 0] = np.nan\n",
    "        original_len = len(df) - 1\n",
    "    df.dropna(how='all', inplace=True)\n",
    "    df[index_col] = df[index_col].str.strip().apply(lambda x: x if x else str(INT_MIN)).astype(int)\n",
    "    df = df[~(df[index_col] == INT_NAN)]\n",
    "    df.set_index(index_col, inplace=True)\n",
    "    if len(df) != original_len:\n",
    "        logger.warning(('Loaded {} rows from tsv. Original file, \"{}\", contained {} seemingly valid lines.' +\n",
    "                        'Index column: {}').format(len(df), original_len, filepath, index_col))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "#from ch10 import construct_seq2seq_model\n",
    "#from nlpia.loaders import get_data, DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "num_samples = 10000\n",
    "data_path = os.path.join(DATA_PATH, 'movie_dialog.txt')  # preprocessed CMU movie dialogue samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "\n",
    "from io import open\n",
    "\n",
    "with open(\"D:\\BernieData\\DeepL\\data\\movie_characters_stats.pkl\", \"rb\") as filehandler: # movie_char_stats characters_stats.pkl\n",
    "    input_characters, target_characters, input_token_index, target_token_index = pickle.load(filehandler)\n",
    "\n",
    "with open(\"D:\\BernieData\\DeepL\\data\\movie_encoder_decoder_stats.pkl\", \"rb\") as filehandler: # encoder_decoder_stats\n",
    "    num_encoder_tokens, num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length = pickle.load(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "lines = open(data_path).read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = construct_seq2seq_model(num_encoder_tokens, num_decoder_tokens)\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "batch_size = 64    # <1>\n",
    "epochs = 100       # <2>\n",
    "num_neurons = 256  # <3>\n",
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(num_neurons, return_state=True)\n",
    "_, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(DATA_PATH, 'ch10_train_seq2seq_keras.h5')\n",
    "model.save(model_path + '_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(model_path + '_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "\n",
    "from io import open\n",
    "\n",
    "with open(\"D:\\BernieData\\DeepL\\data\\movie_characters_stats.pkl\", \"rb\") as filehandler: # movie_char_stats characters_stats.pkl\n",
    "    input_characters, target_characters, input_token_index, target_token_index = pickle.load(filehandler)\n",
    "\n",
    "with open(\"D:\\BernieData\\DeepL\\data\\movie_encoder_decoder_stats.pkl\", \"rb\") as filehandler: # encoder_decoder_stats\n",
    "    num_encoder_tokens, num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length = pickle.load(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict to look up predicted tokens\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, encoder_model, decoder_model = instantiate_seq2seq_model(num_encoder_tokens, num_decoder_tokens, num_neurons=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model.load_weights('D:\\BernieData\\DeepL\\data\\encoder_seq2seq.hd5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decoder_model.load_weights('D:\\BernieData\\DeepL\\data\\decoder_seq2seq.hd5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def response(input_text):\n",
    "    input_text = input_text.lower()\n",
    "    input_seq = np.zeros((1, max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "    for t, char in enumerate(input_text):\n",
    "        input_seq[0, t, input_token_index[char]] = 1.\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded sentence: i don't know anymore.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  \n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:30: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_text = 'hello. how are you?'\n",
    "response(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded sentence: i don't know anymore.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  \n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:30: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_text = 'Do you cheer for football?'\n",
    "response(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded sentence: i don't know. i think it's a problem with you to talk to him.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  \n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:30: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n"
     ]
    }
   ],
   "source": [
    "input_text = 'What about basketball?'\n",
    "response(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
