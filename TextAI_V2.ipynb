{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1548\n"
     ]
    }
   ],
   "source": [
    "# prepare text file as corpus (lower case, remove stopwords)\n",
    "import os\n",
    "import math\n",
    "import re # regex\n",
    "import numpy as np\n",
    "#import scipy.sparse as sp\n",
    "\n",
    "def read1k():\n",
    "    return f.read(1024)\n",
    "\n",
    "def process_data(chunk, text):\n",
    "    #print('processed: {0}'.format(ctr))\n",
    "    text.append(unicode(chunk, errors='ignore')) # 'utf8' codec can't decode byte 0xc3\n",
    "\n",
    "def rmsword(corpus, stopwords): # remove stopwords from corpus\n",
    "    i = 0\n",
    "    for elem in corpus:\n",
    "        for sword in stopwords:\n",
    "            if elem == sword:\n",
    "                while True:\n",
    "                    try:\n",
    "                        corpus.remove(elem) # this throws an error if elem not in corpus (might have been removed already)\n",
    "                        i += 1\n",
    "                    except:\n",
    "                        break\n",
    "    return i # returns number of stopwords removed\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # read corpus\n",
    "    os.chdir('C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "    #f = open('vcpos.txt')\n",
    "    #f = open('todinvenedig.txt')\n",
    "    #f = open('todinvenedigshrt.txt') # shorter version for tests\n",
    "    #f = open('DUO-AIBaseRaw-08Feb18.txt')\n",
    "    #f = open('AbstractsFull.txt')\n",
    "    f = open('AbstractsFull2012.txt')\n",
    "    text = []\n",
    "    for piece in iter(read1k, ''):\n",
    "        process_data(piece, text)\n",
    "    \n",
    "    corpus = []\n",
    "    for elem in text:\n",
    "        corpus += elem.split() # splits on all whitespaces\n",
    "    corpus = [t.lower() for t in corpus] # convert to lower case\n",
    "    \n",
    "    # remove special characters\n",
    "    corpus = [re.sub(r'[^\\w]', ' ', t) for t in corpus]\n",
    "    \n",
    "    # remove digits\n",
    "    corpus = [re.sub(r'\\b\\d+\\b', '', t) for t in corpus]\n",
    "    \n",
    "    #print corpus\n",
    "    \n",
    "    text = []\n",
    "    # get stopwords\n",
    "    #f = open('gerstopw0.txt') # German stopwords\n",
    "    f = open('englstopwrds.txt') # English stopwords\n",
    "    for piece in iter(read1k, ''):\n",
    "        process_data(piece, text)\n",
    "    stopwrds = []\n",
    "    for elem in text:\n",
    "        stopwrds += elem.split()\n",
    "    stopwrds = [t.lower() for t in stopwrds]\n",
    "    #print stopwrds\n",
    "    \n",
    "    #print('length of corpus: {0}'.format(len(corpus)))\n",
    "    print rmsword(corpus, stopwrds) # remove stopwords from corpus\n",
    "    #print('length of corpus: {0}'.format(len(corpus)))\n",
    "    \n",
    "    # save corpus as separate file\n",
    "    fout = open('bsc_corpus0.txt', 'w')\n",
    "    fout.write(' '.join(corpus)) # requires a string\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda2\\lib\\site-packages\\gensim-3.2.0-py2.7-win-amd64.egg\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2018-05-09 14:16:55,673 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2018-05-09 14:16:55,677 : INFO : collecting all words and their counts\n",
      "2018-05-09 14:16:55,680 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-05-09 14:16:55,683 : INFO : collected 1075 word types from a corpus of 4592 raw words and 1 sentences\n",
      "2018-05-09 14:16:55,684 : INFO : Loading a fresh vocabulary\n",
      "2018-05-09 14:16:55,687 : INFO : min_count=5 retains 190 unique words (17% of original 1075, drops 885)\n",
      "2018-05-09 14:16:55,687 : INFO : min_count=5 leaves 3366 word corpus (73% of original 4592, drops 1226)\n",
      "2018-05-09 14:16:55,690 : INFO : deleting the raw counts dictionary of 1075 items\n",
      "2018-05-09 14:16:55,691 : INFO : sample=0.001 downsamples 85 most-common words\n",
      "2018-05-09 14:16:55,694 : INFO : downsampling leaves estimated 1714 word corpus (50.9% of prior 3366)\n",
      "2018-05-09 14:16:55,694 : INFO : estimated required memory for 190 words and 150 dimensions: 323000 bytes\n",
      "2018-05-09 14:16:55,697 : INFO : resetting layer weights\n",
      "2018-05-09 14:16:55,703 : INFO : training model with 3 workers on 190 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-05-09 14:16:55,720 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-09 14:16:55,721 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-09 14:16:55,723 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-09 14:16:55,724 : INFO : training on 22960 raw words (8470 effective words) took 0.0s, 528331 effective words/s\n",
      "2018-05-09 14:16:55,726 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# import modules and set up logging\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# load up unzipped corpus from http://mattmahoney.net/dc/text8.zip\n",
    "os.chdir('C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "sentences = word2vec.Text8Corpus('bsc_corpus0.txt')\n",
    "# train the skip-gram model; default window=5\n",
    "model = word2vec.Word2Vec(sentences, size=150)\n",
    "# ... and some hours later... just as advertised...\n",
    "#print model.most_similar(positive=['a', 'b'], negative=['c'], topn=1)\n",
    " \n",
    "# pickle the entire model to disk, so we can load&resume training later\n",
    "#model.save('todvenshrt0.model')\n",
    "# store the learned weights, in a format the original C tool understands\n",
    "#model.save_word2vec_format('todvenshrt0.model.bin', binary=True)\n",
    "# or, import word weights created by the (faster) C word2vec\n",
    "# this way, you can switch between the C/Python toolkits easily\n",
    "#model = word2vec.Word2Vec.load_word2vec_format('todvenshrt0.bin', binary=True)\n",
    " \n",
    "# \"boy\" is to \"father\" as \"girl\" is to ...?\n",
    "#print model.most_similar(['a', 't0'], ['b'], topn=3)\n",
    "    \n",
    "# which word doesn't go with the others?\n",
    "#print model.doesnt_match(\"a b c\".split())\n",
    "\n",
    "# to read out vocabulary\n",
    "#model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'b', 0.9993299841880798), (u's', 0.999323844909668), (u'fluctuation', 0.9992915987968445), (u'disease', 0.999285101890564), (u'clinical', 0.9992791414260864), (u'p', 0.9992711544036865), (u'parkinson', 0.9992598295211792), (u'diurnal', 0.9992573261260986), (u'patients', 0.9992554187774658), (u'levodopa', 0.9992510080337524)]\n",
      "[(u'b', 0.9987132549285889), (u'diurnal', 0.998710572719574), (u'p', 0.9986894130706787), (u'levodopa', 0.9986794590950012), (u'disease', 0.9986448287963867), (u'clinical', 0.9986404180526733), (u'h4', 0.9986352920532227), (u's', 0.998630702495575), (u'fluctuation', 0.9986303448677063), (u'parkinson', 0.9986236691474915)]\n"
     ]
    }
   ],
   "source": [
    "print model.wv.most_similar([u'pain'], topn=10)\n",
    "print model.wv.most_similar([u'depression'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in corpus (after stopwords removed): 1051\n",
      "number of phrases: 215\n"
     ]
    }
   ],
   "source": [
    "# co-occurance test large files\n",
    "import os\n",
    "import math\n",
    "import re # regex\n",
    "import numpy as np\n",
    "#import scipy.sparse as sp\n",
    "\n",
    "def read1k():\n",
    "    return f.read(1024)\n",
    "\n",
    "def process_data(chunk, text):\n",
    "    #print('processed: {0}'.format(ctr))\n",
    "    text.append(unicode(chunk, errors='ignore')) # 'utf8' codec can't decode byte 0xc3\n",
    "    \n",
    "def valinlst(val, lst): # checks if value is in a list\n",
    "    for elem in lst:\n",
    "        if elem == val:\n",
    "            #print('valinlst elem: {0} val: {1}'.format(elem, val))\n",
    "            #re.match( val, elem, re.I)\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def findphrases(corpus, win2): # returns all phrases from corpus for given window as a list\n",
    "    i = 0\n",
    "    phrase = []\n",
    "    phrases = []\n",
    "    for elem in corpus:\n",
    "        if i > -win2-1 and i < len(corpus)-win2:\n",
    "            for j in range(i-win2,i+win2+1): # for skip-gram we would need to omit the i-value\n",
    "                phrase.append(corpus[j])\n",
    "            phrases.append(phrase)\n",
    "            phrase = []\n",
    "        #i += 1\n",
    "        i += ( 2 * win2 + 1 )\n",
    "    return phrases\n",
    "\n",
    "def rmsword(corpus, stopwords): # remove stopwords from corpus\n",
    "    i = 0\n",
    "    for elem in corpus:\n",
    "        for sword in stopwords:\n",
    "            if elem == sword:\n",
    "                while True:\n",
    "                    try:\n",
    "                        corpus.remove(elem) # this throws an error if elem not in corpus (might have been removed already)\n",
    "                        i += 1\n",
    "                    except:\n",
    "                        break\n",
    "    return i # returns number of stopwords removed\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # read corpus\n",
    "    os.chdir('C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "    #f = open('todinvenedig.txt')\n",
    "    #f = open('todinvenedigshrt.txt') # shorter version for tests\n",
    "    #f = open('vecsemtst0.txt') # test file with fixed similarities\n",
    "    f = open('bsc_corpus0.txt') # this file is already preprocessed, symbols and stopwords removed\n",
    "    \n",
    "    text = []\n",
    "    for piece in iter(read1k, ''): # read file\n",
    "        process_data(piece, text)\n",
    "    \n",
    "    corpus = []\n",
    "    for elem in text:\n",
    "        corpus += elem.split() # splits on all whitespaces: add text data to corpus\n",
    "    corpus = [t.lower() for t in corpus] # convert to lower case\n",
    "    \n",
    "    # fill words set (unique!) from corpus\n",
    "    words = set() # words as set: each entry unique\n",
    "    for elem in corpus:\n",
    "        words.add(elem)\n",
    "        \n",
    "    # todo: for very large datasets: process chunks and save to file in between ********************\n",
    "        \n",
    "    print('number of words in corpus (after stopwords removed): {0}'.format(len(words)))\n",
    "    \n",
    "    # note: +/- 1-3: syntactic, +/- 4-10: semantic context\n",
    "    phrases = findphrases(corpus, 4) # extract phrases from corpus (window +/- the given size)\n",
    "    #print phrases\n",
    "    print('number of phrases: {0}'.format(len(phrases)))\n",
    "    \n",
    "    wordcont = np.zeros((len(words),len(phrases)))\n",
    "    k = 1.5 # Laplacian smoothing\n",
    "    \n",
    "    # fill word-context (or word-phrase) matrix\n",
    "    i = 0 # word index\n",
    "    wordlist = []\n",
    "    for word in words:\n",
    "        j = 0 # phrease index\n",
    "        wordlist.append(word) # to allow for index-access later\n",
    "        for phrase in phrases:\n",
    "            if valinlst(word, phrase):\n",
    "                wordcont[i][j] += 1 + k # k for Laplacian smoothing\n",
    "                #if wordcont[i][j] >= 1.0:\n",
    "                #    print('wordcont[{0}][{1}]: {2} word: {3} phrease: {4}'.format(i,j,wordcont[i][j],word,phrase))\n",
    "            j += 1\n",
    "        i += 1\n",
    "    \n",
    "    #print sp.issparse(wordcont)\n",
    "    #print wordcont\n",
    "    #print np.nonzero(wordcont)\n",
    "    #print(\"word set: {0} word list: {1}\".format(next(iter(words)), wordlist[0]))\n",
    "    \n",
    "    # calculate pointwise mutual information\n",
    "    fijsum = 0.0\n",
    "    pistar = []\n",
    "    pstarj = []\n",
    "    for i in range(len(words)):\n",
    "        pistar.append(0.0)\n",
    "        for j in range(len(phrases)):\n",
    "            if i is 0:\n",
    "                pstarj.append(0.0)\n",
    "            fijsum += wordcont[i][j]\n",
    "            pistar[i] += wordcont[i][j]\n",
    "            pstarj[j] += wordcont[i][j]\n",
    "    \n",
    "    #print fijsum\n",
    "    #print pistar\n",
    "    #print pstarj\n",
    "    \n",
    "    ppmi = np.zeros((len(words),len(phrases))) # init ppmi matrix with 0s\n",
    "    for i in range(len(words)):\n",
    "        for j in range(len(phrases)):\n",
    "            if wordcont[i][j] > 0.0:\n",
    "                ppmi[i][j] = max(math.log((wordcont[i][j]*fijsum)/pistar[i]/pstarj[j], 2.0), 0.0)\n",
    "    \n",
    "    #print ppmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppmi mean: 0.052522468248 and stdev: 0.590914387169\n",
      "ppmi max: 8.28848161179 and min: 0.0\n",
      "wcounter - no. of ppmi above limit: 1876\n",
      "ppmi 7.70 for word: aided and phrase: [u'rapien', u'wirken', u'nms', u'auswahl', u'individuellen', u'device', u'aided', u'therapie', u'hilfreich'] idx 0 , 50\n",
      "ppmi 6.70 for word: wochen and phrase: [u'wochen', u'monate', u'beginn', u'levodopa', u'therapie', u'basierend', u'zeitliche', u'muster', u'beziehung'] idx 1 , 80\n",
      "ppmi 7.07 for word: wochen and phrase: [u'wochen', u'andauern', u'anhaltende', u'depression', u'woche', u'lnger', u'anhaltende', u'depression', u'vegetativen'] idx 1 , 203\n",
      "ppmi 7.70 for word: dynamik and phrase: [u'erklren', u'lsst', u'weitere', u'studien', u'untersuchung', u'zeitliche', u'dynamik', u'angst', u'motorischen'] idx 2 , 172\n",
      "ppmi 6.70 for word: stren and phrase: [u'parkinson', u'patienten', u'fllen', u'dyskinesien', u'schwerwiegend', u'normale', u'bewegung', u'stren', u'lebensqualitt'] idx 3 , 110\n",
      "ppmi 6.70 for word: stren and phrase: [u'aktivitten', u'stren', u'anhaltende', u'halluzinationen', u'wahnvorstellungen', u'floridale', u'psychosen', u'lage', u'sorgen'] idx 3 , 201\n",
      "ppmi 5.70 for word: rating and phrase: [u'studie', u'wurden', u'zweiundsechzig', u'pd', u'patienten', u'aufgenommen', u'einschtzungen', u'rating', u'agenturen'] idx 4 , 139\n",
      "ppmi 5.70 for word: rating and phrase: [u'lebens', u'adl', u'subscore', u'un', u'ified', u'parkinson', u's', u'disease', u'rating'] idx 4 , 143\n",
      "ppmi 5.70 for word: rating and phrase: [u'klinischen', u'bewertung', u'unterzogen', u'einschlielich', u'unified', u'parkinson', u's', u'disease', u'rating'] idx 4 , 160\n",
      "ppmi 6.07 for word: rating and phrase: [u'depression', u'rating', u'scale', u'hamd', u'hamilton', u'anxiety', u'rating', u'scale', u'hars'] idx 4 , 162\n",
      "ppmi 7.70 for word: ganz and phrase: [u'lsen', u'bentigt', u'hilfe', u'persnlichen', u'pflege', u'verlassen', u'ganz', u'allein', u'gedankenstrung'] idx 5 , 198\n",
      "ppmi 7.70 for word: gesehen and phrase: [u'verbunden', u'verschlechterung', u'pdq', u'p', u'dyskinesien', u'wurden', u'gesehen', u'globale', u'qol'] idx 6 , 102\n",
      "ppmi 7.87 for word: halluzina and phrase: [u'beibehaltung', u'einsicht', u'gelegentliche', u'hufige', u'halluzina', u'tionen', u'wahnvorstellungen', u'einsicht', u'tglichen'] idx 7 , 200\n",
      "ppmi 7.70 for word: komplikationssubtypen and phrase: [u'regressionsanalysen', u'wurden', u'durchgefhrt', u'assoziationen', u'motorischen', u'komplikationssubtypen', u'qol', u'testen', u'ergebnisse'] idx 8 , 98\n",
      "ppmi 5.70 for word: edl and phrase: [u'm', u'edl', u'zeigte', u'bessere', u'sensitivitt', u'probleme', u'identifizieren', u'tgliche', u'aktivitten'] idx 9 , 51\n",
      "ppmi 5.70 for word: edl and phrase: [u'wurde', u'm', u'edl', u'qol', u'gefunden', u'mae', u'sowohl', u'motorischen', u'nichtmotorischen'] idx 9 , 54\n",
      "ppmi 5.87 for word: edl and phrase: [u'edl', u'm', u'edl', u'erklrten', u'patienten', u'stichprobe', u'schwierigkeiten', u'skala', u'enthaltenen'] idx 9 , 56\n",
      "ppmi 5.87 for word: edl and phrase: [u'hhere', u'leistung', u'erklren', u'm', u'edl', u'probleme', u'tglichen', u'leistung', u'erfassen'] idx 9 , 61\n",
      "ppmi 7.70 for word: manahmen and phrase: [u'funktionellen', u'manahmen', u'studie', u'zeigte', u'hohe', u'korrelation', u'bewertungsskalen', u'behinderung', u'gleichzeitig'] idx 10 , 52\n",
      "ppmi 6.12 for word: mild and phrase: [u'intellektuelle', u'beeintrchtigung', u'keine', u'mild', u'konsequente', u'vergesslichkeit', u'teilerinnerung', u'ereignissen', u'schwierigkeiten'] idx 11 , 193\n"
     ]
    }
   ],
   "source": [
    "# read out ppmi-matrix and display words and context/phrases\n",
    "import numpy as np\n",
    "\n",
    "def getwordi(words, indx): # access words set\n",
    "    i = 0\n",
    "    for elem in words:\n",
    "        if i == indx:\n",
    "            return elem\n",
    "        i += 1\n",
    "\n",
    "print('ppmi mean: {0} and stdev: {1}'.format(np.mean(ppmi), np.std(ppmi)))\n",
    "print('ppmi max: {0} and min: {1}'.format(np.amax(ppmi), np.amin(ppmi)))\n",
    "\n",
    "# todo: keep only PMI-entries with values > 2 * length of context (e.g. 2 * 7 = 14)\n",
    "\n",
    "isigword = [] # index array for significant words\n",
    "isiphrs = [] # index array for significant words\n",
    "\n",
    "wcounter = 0\n",
    "for i in range(len(wordlist)):\n",
    "    for j in range(len(phrases)):\n",
    "        if ppmi[i][j] > 0.3: # note limit\n",
    "            #print('ppmi = {0} for word: {1} and phrase: {2}'.format(ppmi[i][j], wordlist[i], phrases[j]))\n",
    "            isigword.append(i)\n",
    "            isiphrs.append(j)\n",
    "            wcounter += 1\n",
    "            \n",
    "print('wcounter - no. of ppmi above limit: {0}'.format(wcounter))\n",
    "\n",
    "#for i in range(len(isigword)):\n",
    "for i in range(20):\n",
    "    print('ppmi {0:.2f} for word: {1} and phrase: {2} idx {3} , {4}'.format(ppmi[isigword[i]][isiphrs[i]], wordlist[isigword[i]], phrases[isiphrs[i]], isigword[i], isiphrs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# measure similarity between words for given context/phrases\n",
    "import numpy as np\n",
    "\n",
    "def cosine(wordv, wordw): # cosine similarity for two word vectors\n",
    "    sumv = 0.\n",
    "    sumw = 0.\n",
    "    sumvw = 0.\n",
    "    for elemv, elemw in zip(wordv, wordw):\n",
    "        #print('for: {0} {1}'.format(elemv, elemw))\n",
    "        sumv += elemv * elemv\n",
    "        sumw += elemw * elemw\n",
    "        sumvw += elemv * elemw\n",
    "    sumv = math.sqrt(sumv)\n",
    "    sumw = math.sqrt(sumw)\n",
    "    #print('{0} {1}'.format(sumv, sumw))\n",
    "    if sumv > 0. and sumw > 0.:\n",
    "        return sumvw / sumv / sumw\n",
    "    else: return -1.\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    #for i in range(len(isigword)):\n",
    "        #for j in range(i+1,len(isiphrs)):\n",
    "    for i in range(10):\n",
    "        for j in range(i+1,10):\n",
    "            thiscosine = cosine(ppmi[isigword[i]], ppmi[isigword[j]])\n",
    "            if thiscosine > 0.1 and wordlist[isigword[i]] != wordlist[isigword[j]]:\n",
    "                print('cosine {0} and {1}: {2:.3f}'.format(wordlist[isigword[i]], wordlist[isigword[j]], thiscosine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppmi_red mean: 0.049045852587 and stdev: 0.357699780101\n",
      "ppmi_red max: 7.54989504224 and min: -1.33049926166\n"
     ]
    }
   ],
   "source": [
    "# Dense vectors - applying SVD\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "W, S, C = linalg.svd(ppmi, overwrite_a=True, full_matrices=False)\n",
    "Sc = np.diag(S)\n",
    "\n",
    "#print W.shape, Sc.shape, C.shape\n",
    "#np.allclose(ppmi, np.dot(W, np.dot(Sc, C)))\n",
    "#np.dot(W, np.dot(Sc, C))\n",
    "\n",
    "# need to reduce matrices to e.g. 50\n",
    "k = 50 # number of singular values we want to keep\n",
    "Cred = C[:k, :]\n",
    "Sred = Sc[:k, :k]\n",
    "Wred = W[:, :k]\n",
    "ppmi_red = np.dot(Wred, np.dot(Sred, Cred))\n",
    "\n",
    "print('ppmi_red mean: {0} and stdev: {1}'.format(np.mean(ppmi_red), np.std(ppmi_red)))\n",
    "print('ppmi_red max: {0} and min: {1}'.format(np.amax(ppmi_red), np.amin(ppmi_red)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4309\n"
     ]
    }
   ],
   "source": [
    "# measure similarity between words for given context/phrases\n",
    "import numpy as np\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    isigword = [] # index array for significant words\n",
    "    isiphrs = [] # index array for significant words\n",
    "\n",
    "    wcounter = 0\n",
    "    for i in range(len(wordlist)):\n",
    "        for j in range(len(phrases)):\n",
    "            if ppmi_red[i][j] > 1.0: # note limit\n",
    "                isigword.append(i)\n",
    "                isiphrs.append(j)\n",
    "                wcounter += 1\n",
    "    \n",
    "    print wcounter\n",
    "    \n",
    "    #for i in range(len(isigword)):\n",
    "        #for j in range(i+1,len(isiphrs)):\n",
    "    for i in range(10):\n",
    "        for j in range(i+1,10):\n",
    "            thiscosine = cosine(ppmi_red[isigword[i]], ppmi_red[isigword[j]])\n",
    "            if thiscosine > 0.3 and wordlist[isigword[i]] != wordlist[isigword[j]]:\n",
    "                print('cosine {0} and {1}: {2:.3f}'.format(wordlist[isigword[i]], wordlist[isigword[j]], thiscosine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppmi 0.00 for word: aided and phrase: [u'auslsende', u'moment', u'therapie', u'verndern', u'oft', u'kommt', u'patient', u'ihnen', u'verndert'] idx 0 , 3\n",
      "ppmi 0.00 for word: aided and phrase: [u'cholinerge', u'pd', u'gestrt', u'fhren', u'neuropsychiatrischen', u'strungen', u'gesamtbelastung', u'nms', u'mp'] idx 0 , 30\n",
      "ppmi 0.00 for word: aided and phrase: [u'lebensqualitt', u'bei', u'prvalenz', u'nms', u'beziehung', u'lebensqualitt', u'ndert', u'gesamten', u'krankheitsverlaufs'] idx 0 , 36\n",
      "ppmi 0.00 for word: aided and phrase: [u'apd', u'patienten', u'vs', u'oraler', u'therapie', u'duodopa', u'verhilft', u'verlsslich', u'langanhaltend'] idx 0 , 40\n",
      "ppmi 7.70 for word: aided and phrase: [u'rapien', u'wirken', u'nms', u'auswahl', u'individuellen', u'device', u'aided', u'therapie', u'hilfreich'] idx 0 , 50\n",
      "ppmi 0.00 for word: aided and phrase: [u'wochen', u'monate', u'beginn', u'levodopa', u'therapie', u'basierend', u'zeitliche', u'muster', u'beziehung'] idx 0 , 80\n",
      "ppmi 0.00 for word: aided and phrase: [u'harnwegsprobleme', u'bezogen', u'pdq3', u'9', u'domains', u'gesamtlast', u'nms', u'parkinson', u'bezug'] idx 0 , 131\n",
      "ppmi 0.00 for word: wochen and phrase: [u'direkt', u'lcig', u'behandlu', u'ngsbeginn', u'sowie', u'monate', u'danach', u'ausschlielich', u'lcig'] idx 1 , 45\n",
      "ppmi 6.70 for word: wochen and phrase: [u'wochen', u'monate', u'beginn', u'levodopa', u'therapie', u'basierend', u'zeitliche', u'muster', u'beziehung'] idx 1 , 80\n",
      "ppmi 0.00 for word: wochen and phrase: [u'depression', u'rating', u'scale', u'hamd', u'hamilton', u'anxiety', u'rating', u'scale', u'hars'] idx 1 , 162\n",
      "ppmi 0.00 for word: wochen and phrase: [u'depression', u'p', u'perioden', u'traurigkeit', u'schuld', u'grer', u'normal', u'niemals', u'tage'] idx 1 , 202\n",
      "ppmi 7.07 for word: wochen and phrase: [u'wochen', u'andauern', u'anhaltende', u'depression', u'woche', u'lnger', u'anhaltende', u'depression', u'vegetativen'] idx 1 , 203\n",
      "ppmi 0.00 for word: wochen and phrase: [u'symptomen', u'schlaflosigkeit', u'anorexie', u'gewichtsverlust', u'verlust', u'interesses', u'anhaltende', u'depression', u'vegetativen'] idx 1 , 204\n",
      "ppmi 0.00 for word: dynamik and phrase: [u'grere', u'studien', u'aspekte', u'interkulturellen', u'umfeld', u'untersuchen', u'knapp', u'ziel', u'beurteilung'] idx 2 , 92\n",
      "ppmi 7.70 for word: dynamik and phrase: [u'erklren', u'lsst', u'weitere', u'studien', u'untersuchung', u'zeitliche', u'dynamik', u'angst', u'motorischen'] idx 2 , 172\n",
      "ppmi 0.00 for word: stren and phrase: [u'symptome', u'angstzustnde', u'apathie', u'kognition', u'halluzinationen', u'schlafstrungen', u'harnwegsprobleme', u'tragen', u'teil'] idx 3 , 35\n",
      "ppmi 6.70 for word: stren and phrase: [u'parkinson', u'patienten', u'fllen', u'dyskinesien', u'schwerwiegend', u'normale', u'bewegung', u'stren', u'lebensqualitt'] idx 3 , 110\n",
      "ppmi 0.00 for word: stren and phrase: [u'erregende', u'stimmung', u'apathie', u'kognitive', u'beeintrchtigungen', u'halluzinationen', u'psychosen', u'schlafstrungen', u'tagesschlfrigkeit'] idx 3 , 130\n",
      "ppmi 6.70 for word: stren and phrase: [u'aktivitten', u'stren', u'anhaltende', u'halluzinationen', u'wahnvorstellungen', u'floridale', u'psychosen', u'lage', u'sorgen'] idx 3 , 201\n",
      "ppmi 0.00 for word: rating and phrase: [u'verbindungen', u'wurden', u'klinischen', u'studien', u'vielversprechenden', u'ergebnissen', u'getestet', u'gegensatz', u'adrenerge'] idx 4 , 121\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print('ppmi {0:.2f} for word: {1} and phrase: {2} idx {3} , {4}'.format(ppmi[isigword[i]][isiphrs[i]], wordlist[isigword[i]], phrases[isiphrs[i]], isigword[i], isiphrs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 0.003s.\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 0.017s.\n",
      "Extracting tf features for LDA...\n",
      "done in 0.015s.\n",
      "\n",
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.210s.\n",
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: disease exploratory effects efficacy ejw emptying entacapone enzyme essential evaluation evolution experience expression disorder extension factors family feature features findings\n",
      "Topic #1: parkinson years exploratory effects efficacy ejw emptying entacapone enzyme essential evaluation evolution experience expression investigate extension factors family feature features\n",
      "Topic #2: h4 years experience effects efficacy ejw emptying entacapone enzyme essential evaluation evolution exploratory fmri expression extension factors family feature features\n",
      "Topic #3: patients years experience effects efficacy ejw emptying entacapone enzyme essential evaluation evolution exploratory investigate expression extension factors family feature features\n",
      "Topic #4: fluctuation years exploratory effects efficacy ejw emptying entacapone enzyme essential evaluation evolution experience expression year extension factors family feature features\n",
      "Topic #5: diurnal years expression efficacy ejw emptying entacapone enzyme essential evaluation evolution experience exploratory extension effect factors family feature features findings\n",
      "Topic #6: motor years experience effects efficacy ejw emptying entacapone enzyme essential evaluation evolution exploratory fmri expression extension factors family feature features\n",
      "Topic #7: histamine experience effect effects efficacy ejw emptying entacapone enzyme essential evaluation evolution exploratory fmri expression extension factors family feature features\n",
      "Topic #8: neurodegenerative years exploratory effects efficacy ejw emptying entacapone enzyme essential evaluation evolution experience expression fmri extension factors family feature features\n",
      "Topic #9: levodopa years exploratory effects efficacy ejw emptying entacapone enzyme essential evaluation evolution experience expression early extension factors family feature features\n",
      "\n",
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.054s.\n",
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: disease exploratory effects efficacy ejw emptying entacapone enzyme essential evaluation evolution experience expression disorder extension factors family feature features findings\n",
      "Topic #1: parkinson years exploratory effects efficacy ejw emptying entacapone enzyme essential evaluation evolution experience expression investigate extension factors family feature features\n",
      "Topic #2: h4 years experience effects efficacy ejw emptying entacapone enzyme essential evaluation evolution exploratory fmri expression extension factors family feature features\n",
      "Topic #3: patients years experience effects efficacy ejw emptying entacapone enzyme essential evaluation evolution exploratory investigate expression extension factors family feature features\n",
      "Topic #4: fluctuation years exploratory effects efficacy ejw emptying entacapone enzyme essential evaluation evolution experience expression year extension factors family feature features\n",
      "Topic #5: diurnal night hdc advanced rhythms subthalamic div juvenile lewy characteristics feature features efficacy ejw emptying entacapone enzyme essential evaluation evolution\n",
      "Topic #6: motor role long brain year therapy term rhythm tremor respect extension expression findings effect effects efficacy features ejw feature emptying\n",
      "Topic #7: histamine parkinsonism stimulation gait controls parkinsonian dystonia induced response onset rate pain data case marked effect bodies higher wake emptying\n",
      "Topic #8: neurodegenerative study fluctuations disorders nucleus decarboxylase expression histidine treatment diseases human day pd tmn h3 van ad dopa tuberomammillary early\n",
      "Topic #9: levodopa clinical mrna dementia enzyme production neuronal hd results deep carbidopa time analysis disorder sleep symptoms quantitative huntington levels infusion\n",
      "\n",
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n",
      "done in 1.012s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: h4 motor neurodegenerative decarboxylase dopa early lewy marked citation design abstract maintenance agonist gel comparison bilateral pet 18f implications trials\n",
      "Topic #1: fluctuations disorders mrna advanced controls dystonia year carbidopa quantitative limiting future dj cycle hypothalamic efficacy state depression chinese characterized stalevo\n",
      "Topic #2: diurnal treatment histamine parkinsonism nucleus enzyme juvenile day ad tuberomammillary alzheimer huntington wamelen supports key hybridization plays freezing patient temporal\n",
      "Topic #3: study expression hdc human rhythms induced versus bodies investigate markedly postmortem situ objectives abnormalities apomorphine hereditary pharmacokinetics reference dyskinesia evaluation\n",
      "Topic #4: clinical night diseases van hd h3 pain analysis levels receptor hofman conclusion tissue obtained humans shows quality someren entacapone gastric\n",
      "Topic #5: disease fluctuation levodopa parkinsonian time cognitive blood rasagiline movement measures cyclohydrolase fmri cognition gender plays respect taiwanese treatment life presence\n",
      "Topic #6: parkinson patients subthalamic deep term onset rate swaab essential rating emptying transl recessive approach findings benefit function d1 advances mood\n",
      "Topic #7: histidine neuronal tmn gait rhythm therapy effect tremor symptoms therapeutic keywords bao covering significantly health vigilance rodents drug chronic gene\n",
      "Topic #8: parkins pd long brain div response sleep non disorder studies df regulating respect clock higher wake disturbances major report scale\n",
      "Topic #9: stimulation dementia role production results characteristics dopamine data case infusion divh4 ejw ma shan daytime death class life effects model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import os\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "#from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "def read1k():\n",
    "    return f.read(1024)\n",
    "\n",
    "def process_data(chunk, text):\n",
    "    #print('processed: {0}'.format(ctr))\n",
    "    text.append(unicode(chunk, errors='ignore')) # 'utf8' codec can't decode byte 0xc3\n",
    "    \n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "#dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "#                             remove=('headers', 'footers', 'quotes'))\n",
    "#data_samples = dataset.data[:n_samples]\n",
    "#print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# read corpus\n",
    "os.chdir('C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "#f = open('todinvenedig.txt')\n",
    "#f = open('todinvenedigshrt.txt') # shorter version for tests\n",
    "#f = open('vecsemtst0.txt') # test file with fixed similarities\n",
    "f = open('bsc_corpus0.txt') # this file is already preprocessed, symbols and stopwords removed\n",
    "\n",
    "text = []\n",
    "for piece in iter(read1k, ''): # read file\n",
    "    process_data(piece, text)\n",
    "\n",
    "data_samples = []\n",
    "for elem in text:\n",
    "    data_samples += elem.split() # splits on all whitespaces: add text data to corpus\n",
    "data_samples = [t.lower() for t in data_samples] # convert to lower case\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: advanced: [(u'b', 0.9996907711029053), (u's', 0.9996682405471802), (u'parkinson', 0.9996609687805176), (u'levodopa', 0.9996480941772461), (u'fluctuation', 0.9996331930160522), (u'disease', 0.9996317625045776), (u'neurodegenerative', 0.9996232986450195), (u'study', 0.9996163249015808), (u'l', 0.9996100068092346), (u'dystonia', 0.9996089935302734), (u'p', 0.99960857629776), (u'diurnal', 0.9996076822280884), (u'patients', 0.9996050596237183), (u'clinical', 0.9996035695075989), (u'the', 0.9996033906936646)]\n",
      "topic: juvenile: [(u'parkinson', 0.9997127056121826), (u'fluctuation', 0.9996970891952515), (u'p', 0.9996941089630127), (u'disease', 0.9996925592422485), (u's', 0.9996867179870605), (u'b', 0.9996837377548218), (u'diurnal', 0.9996681809425354), (u'h4', 0.9996650815010071), (u'patients', 0.9996529221534729), (u'levodopa', 0.9996474385261536), (u'study', 0.9996460676193237), (u'disorders', 0.999645471572876), (u'night', 0.9996446967124939), (u'motor', 0.9996405243873596), (u'l', 0.9996309280395508)]\n",
      "topic: dementia: [(u'disease', 0.9997852444648743), (u'b', 0.9997818470001221), (u's', 0.9997768998146057), (u'fluctuation', 0.9997720122337341), (u'parkinson', 0.9997720122337341), (u'neurodegenerative', 0.999763548374176), (u'parkinsonism', 0.9997628927230835), (u'motor', 0.9997606873512268), (u'diurnal', 0.9997591972351074), (u'p', 0.9997565746307373), (u'histamine', 0.9997561573982239), (u'levodopa', 0.9997544288635254), (u'h4', 0.9997506141662598), (u'clinical', 0.9997475147247314), (u'tmn', 0.9997465014457703)]\n",
      "topic: depression: [(u'b', 0.9993273019790649), (u'diurnal', 0.9993256330490112), (u'p', 0.9993151426315308), (u'levodopa', 0.9993122816085815), (u'h4', 0.9992951154708862), (u'disease', 0.9992854595184326), (u's', 0.9992846250534058), (u'fluctuation', 0.9992818832397461), (u'parkinson', 0.9992794990539551), (u'clinical', 0.9992777705192566), (u'expression', 0.9992762804031372), (u'neurodegenerative', 0.9992695450782776), (u'mrna', 0.9992647171020508), (u'histidine', 0.9992486238479614), (u'characteristics', 0.9992354512214661)]\n",
      "topic: pain: [(u'b', 0.9996814727783203), (u's', 0.9996798634529114), (u'fluctuation', 0.9996605515480042), (u'disease', 0.9996538162231445), (u'p', 0.9996521472930908), (u'h4', 0.999646782875061), (u'parkinson', 0.9996457099914551), (u'clinical', 0.9996446371078491), (u'diurnal', 0.9996431469917297), (u'patients', 0.9996415376663208), (u'levodopa', 0.9996413588523865), (u'neurodegenerative', 0.999628484249115), (u'fluctuations', 0.999626100063324), (u'motor', 0.9996194243431091), (u'night', 0.9996148943901062)]\n",
      "topic: night: [(u'b', 0.9998608827590942), (u'parkinson', 0.9998565912246704), (u's', 0.9998511672019958), (u'disease', 0.9998278617858887), (u'diurnal', 0.9998254179954529), (u'p', 0.9998205304145813), (u'h4', 0.9998190402984619), (u'fluctuation', 0.9998173713684082), (u'neurodegenerative', 0.9998100996017456), (u'motor', 0.9998005628585815), (u'levodopa', 0.9997994899749756), (u'clinical', 0.9997914433479309), (u'dystonia', 0.9997831583023071), (u'tmn', 0.999774694442749), (u'histamine', 0.999771237373352)]\n",
      "topic: diurnal: [(u's', 0.9999037981033325), (u'parkinson', 0.9998984336853027), (u'b', 0.9998937845230103), (u'disease', 0.9998897314071655), (u'p', 0.9998866319656372), (u'h4', 0.999886155128479), (u'fluctuation', 0.9998792409896851), (u'neurodegenerative', 0.9998675584793091), (u'histamine', 0.999863862991333), (u'clinical', 0.9998539090156555), (u'levodopa', 0.9998513460159302), (u'patients', 0.9998505711555481), (u'motor', 0.9998371005058289), (u'tmn', 0.9998318552970886), (u'expression', 0.9998304843902588)]\n"
     ]
    }
   ],
   "source": [
    "# work through topics and find similar concepts\n",
    "topic0 = [u'fluctuation', u'advanced', u'dementia', u'depression', u'dystonia', u'variability', u'gel', u'pain']\n",
    "topic1 = [u'movement', u'striatal', u'increased', u'bradykinesia', u'improvement']\n",
    "topic2 = []\n",
    "topic3 = []\n",
    "topic4 = []\n",
    "topic5 = []\n",
    "topic6 = []\n",
    "topic7 = []\n",
    "topic8 = []\n",
    "topic9 = ['parkinson', u'nlmcategory', u'conclusion', u'features']\n",
    "topicX = [u'advanced', u'juvenile', u'dementia', u'depression', u'pain', u'night', u'diurnal']\n",
    "\n",
    "\n",
    "for top in topicX:\n",
    "    #print('topic: {0}: {1}'.format(top, model.wv.most_similar([top], topn=5)))\n",
    "    print('topic: {0}: {1}'.format(top, model.wv.most_similar_cosmul([top], topn=15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "people gun state control right guns crime states law police\n",
      "Topic 1:\n",
      "time question book years did like don space answer just\n",
      "Topic 2:\n",
      "mr line rules science stephanopoulos title current define int yes\n",
      "Topic 3:\n",
      "key chip keys clipper encryption number des algorithm use bit\n",
      "Topic 4:\n",
      "edu com cs vs w7 cx mail uk 17 send\n",
      "Topic 5:\n",
      "use does window problem way used point different case value\n",
      "Topic 6:\n",
      "windows thanks know help db does dos problem like using\n",
      "Topic 7:\n",
      "bike water effect road design media dod paper like turn\n",
      "Topic 8:\n",
      "don just like think know people good ve going say\n",
      "Topic 9:\n",
      "car new price good power used air sale offer ground\n",
      "Topic 10:\n",
      "file available program edu ftp information files use image version\n",
      "Topic 11:\n",
      "ax max b8f g9v a86 145 pl 1d9 0t 34u\n",
      "Topic 12:\n",
      "government law privacy security legal encryption court fbi technology information\n",
      "Topic 13:\n",
      "card bit memory output video color data mode monitor 16\n",
      "Topic 14:\n",
      "drive scsi disk mac hard apple drives controller software port\n",
      "Topic 15:\n",
      "god jesus people believe christian bible say does life church\n",
      "Topic 16:\n",
      "year game team games season play hockey players league player\n",
      "Topic 17:\n",
      "10 00 15 25 20 11 12 14 16 13\n",
      "Topic 18:\n",
      "armenian israel armenians war people jews turkish israeli said women\n",
      "Topic 19:\n",
      "president people new said health year university school day work\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print \"Topic %d:\" % (topic_idx)\n",
    "        print \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "#tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "#tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "#tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 20\n",
    "\n",
    "# Run NMF\n",
    "#nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 10\n",
    "#display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[3, 2]\n",
      "[3, 2]\n"
     ]
    }
   ],
   "source": [
    "# forward/backward API\n",
    "import math\n",
    "\n",
    "def forward(x, y, op): # two scalars and an operation: 0=+, 1=*\n",
    "    if op == 0: # +\n",
    "        return x + y\n",
    "    else: # *\n",
    "        return x * y\n",
    "\n",
    "def backward(x, y, op, dz): # two scalars and an operation: 0=+, 1=*\n",
    "    if op == 0: # +\n",
    "        return [dz, dz]\n",
    "    else: # *\n",
    "        return [y * dz, x * dz]\n",
    "\n",
    "def numgrad(x, y, op, dz, h): # two scalars and an operation: 0=+, 1=*\n",
    "    if op == 0: # +\n",
    "        return [((x+h)+y-(x-h)+y)/(2*h), (x+(y+h)-x+(y-h))/(2*h)]\n",
    "    else: # *\n",
    "        return [((x+h)*y-(x-h)*y)/(2*h), (x*(y+h)-x*(y-h))/(2*h)]\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    print forward(2, 3, 1)\n",
    "    print backward(2, 3, 1, 1)\n",
    "    print numgrad(2, 3, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Long-term therapy of Parkinson's disease (PD) with levodopa (L-DOPA) is\n",
      "      associated with a high risk of developing motor fluctuations and dyskinesia. Deep\n",
      "      brain stimulation (DBS) in PD patients of the subthalamic nucleus can improve\n",
      "      these motor complications. Although the positive effect on motor symptoms has\n",
      "      been proven, postoperative cognitive decline has been documented. To tackle the\n",
      "      impact of PD-DBS on cognition, 18 DBS patients were compared to 25 best medically\n",
      "      treated Parkinson's patients, 24 Mild Cognitive Impairment (MCI) patients and 12 \n",
      "      healthy controls using the Neuropsychological Test Battery Vienna-long\n",
      "      (NTBV-long) for cognitive outcome 12 months after first examination. Reliable\n",
      "      change index methodology was used. Overall, there was cognitive change in\n",
      "      individual patients, but the change was very heterogeneous with gains and losses.\n",
      "      Further research is needed to identify the mechanisms that lead to improvement or\n",
      "      deterioration of cognitive functions in individual cases.\n",
      " The classic motor deficits of Parkinson's disease are caused by degeneration of\n",
      "      dopaminergic neurons in the substantia nigra pars compacta, resulting in the loss\n",
      "      of their long-distance axonal projections that modulate the striatum. Current\n",
      "      treatments only minimize the symptoms of this disconnection as there is no\n",
      "      approach capable of replacing the nigrostriatal pathway. We are applying\n",
      "      micro-tissue engineering techniques to create living, implantable constructs that\n",
      "      mimic the architecture and function of the nigrostriatal pathway. These\n",
      "      constructs consist of dopaminergic neurons with long axonal tracts encased within\n",
      "      hydrogel micro-columns. Micro-columns were seeded with dopaminergic neuronal\n",
      "      aggregates, while lumen extracellular matrix (ECM), growth factors, and end\n",
      "      targets were varied to optimize cytoarchitecture. We found a 10-fold increase in \n",
      "      axonal outgrowth from aggregates versus dissociated neurons, resulting in\n",
      "      remarkable axonal lengths of over 6 mm by 14 days and 9 mm by 28 days in vitro.\n",
      "      Axonal extension was also dependent upon lumen ECM, but did not depend on growth \n",
      "      factor enrichment or neuronal end target presence. Evoked dopamine release was\n",
      "      measured via fast scan cyclic voltammetry and synapse formation with striatal\n",
      "      neurons was observed in vitro. Constructs were microinjected to span the\n",
      "      nigrostriatal pathway in rats, revealing survival of implanted neurons while\n",
      "      maintaining their axonal projections within the micro-column. Lastly, these\n",
      "      constructs were generated with dopaminergic neurons differentiated from human\n",
      "      embryonic stem cells. This strategy may improve Parkinson's disease treatment by \n",
      "      simultaneously replacing lost dopaminergic neurons in the substantia nigra and\n",
      "      reconstructing their long-projecting axonal tracts to the striatum.\n",
      "CI  - This article is protected by copyright. All rights reserved.\n",
      " OBJECTIVES: To determine whether the drug saxagliptin, a dipeptidyl peptidase-4\n",
      "      (DPP-4) inhibitor which is utilized for the treatment of Diabetes Mellitus, has\n",
      "      neuroprotective effects in the animal model of Parkinson's disease (PD) induced\n",
      "      by 6-hydroxydopamine (6-OHDA) in rats. METHODS: Male Wistar rats (weighing\n",
      "      280-300 g) received a bilateral infusion of 6-OHDA in the substantia nigra.\n",
      "      Twenty-four hours later, they were treated with saxagliptin (1 mg/kg, p.o) once\n",
      "      daily, for 21 days. The motor function was evaluated using the open field and\n",
      "      rotarod (RT) tests. In addition, cognition was assessed with the novel object\n",
      "      recognition test (ORT). After the evaluation of the behavioural tests, the\n",
      "      animals were transcardially perfused to perform immunohistochemistry staining for\n",
      "      tyrosine hydroxylase (TH) in the substantia nigra pars compacta (SNpc). KEY\n",
      "      FINDINGS: Saxagliptin impaired the memory of animals in the sham group.\n",
      "      CONCLUSIONS: Saxagliptin treatment did not exhibit neuroprotection and it did not\n",
      "      improve the cognitive and motor deficits in the 6-OHDA model of PD.\n",
      "      Interestingly, when saxagliptin was administered to the sham animals, a cognitive\n",
      "      decline was observed. Therefore, this drug should be investigated as a possible\n",
      "      treatment for PTSD.\n",
      "CI  - (c) 2018 Royal Pharmaceutical Society.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download pubmed data\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def getres(url): # get request\n",
    "    return requests.get(url)\n",
    "\n",
    "def compile_corpus(txt): # assume medline-structure of text, extract abstracts (idenitfier AB)\n",
    "    cpos = txt.find('AB  -')\n",
    "    if cpos < 0:\n",
    "        return ''\n",
    "    cpos += 5\n",
    "    slen = len(txt)\n",
    "    rtxt = ''\n",
    "    alen = 2000\n",
    "    while cpos < slen:\n",
    "        nextpos = min(cpos+alen, max(0, txt.find('FAU -', cpos)))\n",
    "        rtxt += txt[cpos:nextpos]\n",
    "        cpos = txt.find('AB  -', nextpos+5)\n",
    "        if cpos < 0:\n",
    "            return rtxt\n",
    "        cpos += 5\n",
    "    return rtxt\n",
    "\n",
    "def save_corpus(txt): # save corpus\n",
    "    # save corpus as separate file\n",
    "    os.chdir('C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "    fout = open('bsc_corpus0.txt', 'w')\n",
    "    fout.write(txt) # requires a string\n",
    "    fout.close()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    #result = getres('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=parkinson+AND+motor&retmax=3')\n",
    "    #result = getres('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=parkinson+AND+motor&retmax=3&usehistory=y')\n",
    "    #result = getres('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/epost.fcgi?db=pubmed&id=29753604,29747823')\n",
    "    #result = getres('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id=29753604,29747823')\n",
    "    #result = getres('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29753604,29747823,29748110&rettype=abstract')\n",
    "    #result = getres('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29753604,29747823,29748110&rettype=abstract&retmode=text')\n",
    "    result = getres('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=NCID_1_11416453_130.14.22.215_9001_1526608607_1043747086_0MetA0_S_MegaStore&retmax=3&rettype=medline&retmode=text')\n",
    "    #print result.text\n",
    "    #outtext = result.text.split(\"AB  - \", 1)\n",
    "    #outt2 = outtext[1].split(\"FAU -\") # or just take 500 characters after the split?\n",
    "    #outt2 = outtext[1].splitlines()\n",
    "    outtext = compile_corpus(result.text)\n",
    "    print outtext\n",
    "    #save_corpus(outtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download pubmed data 0: search and store webenv key\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def getres(url): # get request\n",
    "    return requests.get(url)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # enter search string here, execute and copy-paste webenv key string\n",
    "    result = getres('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=parkinson+AND+motor&retmax=3&usehistory=y')\n",
    "    print result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download pubmed data 1: retrieve search results via search string and store abstracts in file\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def getres(url): # get request\n",
    "    return requests.get(url)\n",
    "\n",
    "def compile_corpus(txt): # assume medline-structure of text, extract abstracts (idenitfier AB)\n",
    "    cpos = txt.find('AB  -')\n",
    "    if cpos < 0:\n",
    "        return ''\n",
    "    cpos += 5\n",
    "    slen = len(txt)\n",
    "    rtxt = ''\n",
    "    alen = 2000\n",
    "    while cpos < slen:\n",
    "        nextpos = min(cpos+alen, max(0, txt.find('FAU -', cpos)))\n",
    "        rtxt += txt[cpos:nextpos]\n",
    "        cpos = txt.find('AB  -', nextpos+5)\n",
    "        if cpos < 0:\n",
    "            return rtxt\n",
    "        cpos += 5\n",
    "    return rtxt\n",
    "\n",
    "def save_corpus(txt): # save corpus\n",
    "    # save corpus as separate file\n",
    "    os.chdir('C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "    fout = open('bsc_corpus0.txt', 'w')\n",
    "    fout.write(txt) # requires a string\n",
    "    fout.close()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # paste webenv key here\n",
    "    result = getres('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=NCID_1_11416453_130.14.22.215_9001_1526608607_1043747086_0MetA0_S_MegaStore&retmax=3&rettype=medline&retmode=text')\n",
    "    outtext = compile_corpus(result.text)\n",
    "    print outtext\n",
    "    #save_corpus(outtext)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
