{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1548\n"
     ]
    }
   ],
   "source": [
    "# prepare text file as corpus (lower case, remove stopwords)\n",
    "import os\n",
    "import math\n",
    "import re # regex\n",
    "import numpy as np\n",
    "#import scipy.sparse as sp\n",
    "\n",
    "def read1k():\n",
    "    return f.read(1024)\n",
    "\n",
    "def process_data(chunk, text):\n",
    "    #print('processed: {0}'.format(ctr))\n",
    "    text.append(unicode(chunk, errors='ignore')) # 'utf8' codec can't decode byte 0xc3\n",
    "\n",
    "def rmsword(corpus, stopwords): # remove stopwords from corpus\n",
    "    i = 0\n",
    "    for elem in corpus:\n",
    "        for sword in stopwords:\n",
    "            if elem == sword:\n",
    "                while True:\n",
    "                    try:\n",
    "                        corpus.remove(elem) # this throws an error if elem not in corpus (might have been removed already)\n",
    "                        i += 1\n",
    "                    except:\n",
    "                        break\n",
    "    return i # returns number of stopwords removed\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # read corpus\n",
    "    os.chdir('C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "    #f = open('vcpos.txt')\n",
    "    #f = open('todinvenedig.txt')\n",
    "    #f = open('todinvenedigshrt.txt') # shorter version for tests\n",
    "    #f = open('DUO-AIBaseRaw-08Feb18.txt')\n",
    "    #f = open('AbstractsFull.txt')\n",
    "    f = open('AbstractsFull2012.txt')\n",
    "    text = []\n",
    "    for piece in iter(read1k, ''):\n",
    "        process_data(piece, text)\n",
    "    \n",
    "    corpus = []\n",
    "    for elem in text:\n",
    "        corpus += elem.split() # splits on all whitespaces\n",
    "    corpus = [t.lower() for t in corpus] # convert to lower case\n",
    "    \n",
    "    # remove special characters\n",
    "    corpus = [re.sub(r'[^\\w]', ' ', t) for t in corpus]\n",
    "    \n",
    "    # remove digits\n",
    "    corpus = [re.sub(r'\\b\\d+\\b', '', t) for t in corpus]\n",
    "    \n",
    "    #print corpus\n",
    "    \n",
    "    text = []\n",
    "    # get stopwords\n",
    "    #f = open('gerstopw0.txt') # German stopwords\n",
    "    f = open('englstopwrds.txt') # English stopwords\n",
    "    for piece in iter(read1k, ''):\n",
    "        process_data(piece, text)\n",
    "    stopwrds = []\n",
    "    for elem in text:\n",
    "        stopwrds += elem.split()\n",
    "    stopwrds = [t.lower() for t in stopwrds]\n",
    "    #print stopwrds\n",
    "    \n",
    "    #print('length of corpus: {0}'.format(len(corpus)))\n",
    "    print rmsword(corpus, stopwrds) # remove stopwords from corpus\n",
    "    #print('length of corpus: {0}'.format(len(corpus)))\n",
    "    \n",
    "    # save corpus as separate file\n",
    "    fout = open('bsc_corpus0.txt', 'w')\n",
    "    fout.write(' '.join(corpus)) # requires a string\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda2\\lib\\site-packages\\gensim-3.2.0-py2.7-win-amd64.egg\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2018-05-09 14:16:55,673 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2018-05-09 14:16:55,677 : INFO : collecting all words and their counts\n",
      "2018-05-09 14:16:55,680 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-05-09 14:16:55,683 : INFO : collected 1075 word types from a corpus of 4592 raw words and 1 sentences\n",
      "2018-05-09 14:16:55,684 : INFO : Loading a fresh vocabulary\n",
      "2018-05-09 14:16:55,687 : INFO : min_count=5 retains 190 unique words (17% of original 1075, drops 885)\n",
      "2018-05-09 14:16:55,687 : INFO : min_count=5 leaves 3366 word corpus (73% of original 4592, drops 1226)\n",
      "2018-05-09 14:16:55,690 : INFO : deleting the raw counts dictionary of 1075 items\n",
      "2018-05-09 14:16:55,691 : INFO : sample=0.001 downsamples 85 most-common words\n",
      "2018-05-09 14:16:55,694 : INFO : downsampling leaves estimated 1714 word corpus (50.9% of prior 3366)\n",
      "2018-05-09 14:16:55,694 : INFO : estimated required memory for 190 words and 150 dimensions: 323000 bytes\n",
      "2018-05-09 14:16:55,697 : INFO : resetting layer weights\n",
      "2018-05-09 14:16:55,703 : INFO : training model with 3 workers on 190 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-05-09 14:16:55,720 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-09 14:16:55,721 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-09 14:16:55,723 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-09 14:16:55,724 : INFO : training on 22960 raw words (8470 effective words) took 0.0s, 528331 effective words/s\n",
      "2018-05-09 14:16:55,726 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# import modules and set up logging\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# load up unzipped corpus from http://mattmahoney.net/dc/text8.zip\n",
    "os.chdir('C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "sentences = word2vec.Text8Corpus('bsc_corpus0.txt')\n",
    "# train the skip-gram model; default window=5\n",
    "model = word2vec.Word2Vec(sentences, size=150)\n",
    "# ... and some hours later... just as advertised...\n",
    "#print model.most_similar(positive=['a', 'b'], negative=['c'], topn=1)\n",
    " \n",
    "# pickle the entire model to disk, so we can load&resume training later\n",
    "#model.save('todvenshrt0.model')\n",
    "# store the learned weights, in a format the original C tool understands\n",
    "#model.save_word2vec_format('todvenshrt0.model.bin', binary=True)\n",
    "# or, import word weights created by the (faster) C word2vec\n",
    "# this way, you can switch between the C/Python toolkits easily\n",
    "#model = word2vec.Word2Vec.load_word2vec_format('todvenshrt0.bin', binary=True)\n",
    " \n",
    "# \"boy\" is to \"father\" as \"girl\" is to ...?\n",
    "#print model.most_similar(['a', 't0'], ['b'], topn=3)\n",
    "    \n",
    "# which word doesn't go with the others?\n",
    "#print model.doesnt_match(\"a b c\".split())\n",
    "\n",
    "# to read out vocabulary\n",
    "#model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'b', 0.9993299841880798), (u's', 0.999323844909668), (u'fluctuation', 0.9992915987968445), (u'disease', 0.999285101890564), (u'clinical', 0.9992791414260864), (u'p', 0.9992711544036865), (u'parkinson', 0.9992598295211792), (u'diurnal', 0.9992573261260986), (u'patients', 0.9992554187774658), (u'levodopa', 0.9992510080337524)]\n",
      "[(u'b', 0.9987132549285889), (u'diurnal', 0.998710572719574), (u'p', 0.9986894130706787), (u'levodopa', 0.9986794590950012), (u'disease', 0.9986448287963867), (u'clinical', 0.9986404180526733), (u'h4', 0.9986352920532227), (u's', 0.998630702495575), (u'fluctuation', 0.9986303448677063), (u'parkinson', 0.9986236691474915)]\n"
     ]
    }
   ],
   "source": [
    "print model.wv.most_similar([u'pain'], topn=10)\n",
    "print model.wv.most_similar([u'depression'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in corpus (after stopwords removed): 1051\n",
      "number of phrases: 215\n"
     ]
    }
   ],
   "source": [
    "# co-occurance test large files\n",
    "import os\n",
    "import math\n",
    "import re # regex\n",
    "import numpy as np\n",
    "#import scipy.sparse as sp\n",
    "\n",
    "def read1k():\n",
    "    return f.read(1024)\n",
    "\n",
    "def process_data(chunk, text):\n",
    "    #print('processed: {0}'.format(ctr))\n",
    "    text.append(unicode(chunk, errors='ignore')) # 'utf8' codec can't decode byte 0xc3\n",
    "    \n",
    "def valinlst(val, lst): # checks if value is in a list\n",
    "    for elem in lst:\n",
    "        if elem == val:\n",
    "            #print('valinlst elem: {0} val: {1}'.format(elem, val))\n",
    "            #re.match( val, elem, re.I)\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def findphrases(corpus, win2): # returns all phrases from corpus for given window as a list\n",
    "    i = 0\n",
    "    phrase = []\n",
    "    phrases = []\n",
    "    for elem in corpus:\n",
    "        if i > -win2-1 and i < len(corpus)-win2:\n",
    "            for j in range(i-win2,i+win2+1): # for skip-gram we would need to omit the i-value\n",
    "                phrase.append(corpus[j])\n",
    "            phrases.append(phrase)\n",
    "            phrase = []\n",
    "        #i += 1\n",
    "        i += ( 2 * win2 + 1 )\n",
    "    return phrases\n",
    "\n",
    "def rmsword(corpus, stopwords): # remove stopwords from corpus\n",
    "    i = 0\n",
    "    for elem in corpus:\n",
    "        for sword in stopwords:\n",
    "            if elem == sword:\n",
    "                while True:\n",
    "                    try:\n",
    "                        corpus.remove(elem) # this throws an error if elem not in corpus (might have been removed already)\n",
    "                        i += 1\n",
    "                    except:\n",
    "                        break\n",
    "    return i # returns number of stopwords removed\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # read corpus\n",
    "    os.chdir('C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "    #f = open('todinvenedig.txt')\n",
    "    #f = open('todinvenedigshrt.txt') # shorter version for tests\n",
    "    #f = open('vecsemtst0.txt') # test file with fixed similarities\n",
    "    f = open('bsc_corpus0.txt') # this file is already preprocessed, symbols and stopwords removed\n",
    "    \n",
    "    text = []\n",
    "    for piece in iter(read1k, ''): # read file\n",
    "        process_data(piece, text)\n",
    "    \n",
    "    corpus = []\n",
    "    for elem in text:\n",
    "        corpus += elem.split() # splits on all whitespaces: add text data to corpus\n",
    "    corpus = [t.lower() for t in corpus] # convert to lower case\n",
    "    \n",
    "    # fill words set (unique!) from corpus\n",
    "    words = set() # words as set: each entry unique\n",
    "    for elem in corpus:\n",
    "        words.add(elem)\n",
    "        \n",
    "    # todo: for very large datasets: process chunks and save to file in between ********************\n",
    "        \n",
    "    print('number of words in corpus (after stopwords removed): {0}'.format(len(words)))\n",
    "    \n",
    "    # note: +/- 1-3: syntactic, +/- 4-10: semantic context\n",
    "    phrases = findphrases(corpus, 4) # extract phrases from corpus (window +/- the given size)\n",
    "    #print phrases\n",
    "    print('number of phrases: {0}'.format(len(phrases)))\n",
    "    \n",
    "    wordcont = np.zeros((len(words),len(phrases)))\n",
    "    k = 1.5 # Laplacian smoothing\n",
    "    \n",
    "    # fill word-context (or word-phrase) matrix\n",
    "    i = 0 # word index\n",
    "    wordlist = []\n",
    "    for word in words:\n",
    "        j = 0 # phrease index\n",
    "        wordlist.append(word) # to allow for index-access later\n",
    "        for phrase in phrases:\n",
    "            if valinlst(word, phrase):\n",
    "                wordcont[i][j] += 1 + k # k for Laplacian smoothing\n",
    "                #if wordcont[i][j] >= 1.0:\n",
    "                #    print('wordcont[{0}][{1}]: {2} word: {3} phrease: {4}'.format(i,j,wordcont[i][j],word,phrase))\n",
    "            j += 1\n",
    "        i += 1\n",
    "    \n",
    "    #print sp.issparse(wordcont)\n",
    "    #print wordcont\n",
    "    #print np.nonzero(wordcont)\n",
    "    #print(\"word set: {0} word list: {1}\".format(next(iter(words)), wordlist[0]))\n",
    "    \n",
    "    # calculate pointwise mutual information\n",
    "    fijsum = 0.0\n",
    "    pistar = []\n",
    "    pstarj = []\n",
    "    for i in range(len(words)):\n",
    "        pistar.append(0.0)\n",
    "        for j in range(len(phrases)):\n",
    "            if i is 0:\n",
    "                pstarj.append(0.0)\n",
    "            fijsum += wordcont[i][j]\n",
    "            pistar[i] += wordcont[i][j]\n",
    "            pstarj[j] += wordcont[i][j]\n",
    "    \n",
    "    #print fijsum\n",
    "    #print pistar\n",
    "    #print pstarj\n",
    "    \n",
    "    ppmi = np.zeros((len(words),len(phrases))) # init ppmi matrix with 0s\n",
    "    for i in range(len(words)):\n",
    "        for j in range(len(phrases)):\n",
    "            if wordcont[i][j] > 0.0:\n",
    "                ppmi[i][j] = max(math.log((wordcont[i][j]*fijsum)/pistar[i]/pstarj[j], 2.0), 0.0)\n",
    "    \n",
    "    #print ppmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppmi mean: 0.052522468248 and stdev: 0.590914387169\n",
      "ppmi max: 8.28848161179 and min: 0.0\n",
      "wcounter - no. of ppmi above limit: 1876\n",
      "ppmi 7.70 for word: aided and phrase: [u'rapien', u'wirken', u'nms', u'auswahl', u'individuellen', u'device', u'aided', u'therapie', u'hilfreich'] idx 0 , 50\n",
      "ppmi 6.70 for word: wochen and phrase: [u'wochen', u'monate', u'beginn', u'levodopa', u'therapie', u'basierend', u'zeitliche', u'muster', u'beziehung'] idx 1 , 80\n",
      "ppmi 7.07 for word: wochen and phrase: [u'wochen', u'andauern', u'anhaltende', u'depression', u'woche', u'lnger', u'anhaltende', u'depression', u'vegetativen'] idx 1 , 203\n",
      "ppmi 7.70 for word: dynamik and phrase: [u'erklren', u'lsst', u'weitere', u'studien', u'untersuchung', u'zeitliche', u'dynamik', u'angst', u'motorischen'] idx 2 , 172\n",
      "ppmi 6.70 for word: stren and phrase: [u'parkinson', u'patienten', u'fllen', u'dyskinesien', u'schwerwiegend', u'normale', u'bewegung', u'stren', u'lebensqualitt'] idx 3 , 110\n",
      "ppmi 6.70 for word: stren and phrase: [u'aktivitten', u'stren', u'anhaltende', u'halluzinationen', u'wahnvorstellungen', u'floridale', u'psychosen', u'lage', u'sorgen'] idx 3 , 201\n",
      "ppmi 5.70 for word: rating and phrase: [u'studie', u'wurden', u'zweiundsechzig', u'pd', u'patienten', u'aufgenommen', u'einschtzungen', u'rating', u'agenturen'] idx 4 , 139\n",
      "ppmi 5.70 for word: rating and phrase: [u'lebens', u'adl', u'subscore', u'un', u'ified', u'parkinson', u's', u'disease', u'rating'] idx 4 , 143\n",
      "ppmi 5.70 for word: rating and phrase: [u'klinischen', u'bewertung', u'unterzogen', u'einschlielich', u'unified', u'parkinson', u's', u'disease', u'rating'] idx 4 , 160\n",
      "ppmi 6.07 for word: rating and phrase: [u'depression', u'rating', u'scale', u'hamd', u'hamilton', u'anxiety', u'rating', u'scale', u'hars'] idx 4 , 162\n",
      "ppmi 7.70 for word: ganz and phrase: [u'lsen', u'bentigt', u'hilfe', u'persnlichen', u'pflege', u'verlassen', u'ganz', u'allein', u'gedankenstrung'] idx 5 , 198\n",
      "ppmi 7.70 for word: gesehen and phrase: [u'verbunden', u'verschlechterung', u'pdq', u'p', u'dyskinesien', u'wurden', u'gesehen', u'globale', u'qol'] idx 6 , 102\n",
      "ppmi 7.87 for word: halluzina and phrase: [u'beibehaltung', u'einsicht', u'gelegentliche', u'hufige', u'halluzina', u'tionen', u'wahnvorstellungen', u'einsicht', u'tglichen'] idx 7 , 200\n",
      "ppmi 7.70 for word: komplikationssubtypen and phrase: [u'regressionsanalysen', u'wurden', u'durchgefhrt', u'assoziationen', u'motorischen', u'komplikationssubtypen', u'qol', u'testen', u'ergebnisse'] idx 8 , 98\n",
      "ppmi 5.70 for word: edl and phrase: [u'm', u'edl', u'zeigte', u'bessere', u'sensitivitt', u'probleme', u'identifizieren', u'tgliche', u'aktivitten'] idx 9 , 51\n",
      "ppmi 5.70 for word: edl and phrase: [u'wurde', u'm', u'edl', u'qol', u'gefunden', u'mae', u'sowohl', u'motorischen', u'nichtmotorischen'] idx 9 , 54\n",
      "ppmi 5.87 for word: edl and phrase: [u'edl', u'm', u'edl', u'erklrten', u'patienten', u'stichprobe', u'schwierigkeiten', u'skala', u'enthaltenen'] idx 9 , 56\n",
      "ppmi 5.87 for word: edl and phrase: [u'hhere', u'leistung', u'erklren', u'm', u'edl', u'probleme', u'tglichen', u'leistung', u'erfassen'] idx 9 , 61\n",
      "ppmi 7.70 for word: manahmen and phrase: [u'funktionellen', u'manahmen', u'studie', u'zeigte', u'hohe', u'korrelation', u'bewertungsskalen', u'behinderung', u'gleichzeitig'] idx 10 , 52\n",
      "ppmi 6.12 for word: mild and phrase: [u'intellektuelle', u'beeintrchtigung', u'keine', u'mild', u'konsequente', u'vergesslichkeit', u'teilerinnerung', u'ereignissen', u'schwierigkeiten'] idx 11 , 193\n"
     ]
    }
   ],
   "source": [
    "# read out ppmi-matrix and display words and context/phrases\n",
    "import numpy as np\n",
    "\n",
    "def getwordi(words, indx): # access words set\n",
    "    i = 0\n",
    "    for elem in words:\n",
    "        if i == indx:\n",
    "            return elem\n",
    "        i += 1\n",
    "\n",
    "print('ppmi mean: {0} and stdev: {1}'.format(np.mean(ppmi), np.std(ppmi)))\n",
    "print('ppmi max: {0} and min: {1}'.format(np.amax(ppmi), np.amin(ppmi)))\n",
    "\n",
    "# todo: keep only PMI-entries with values > 2 * length of context (e.g. 2 * 7 = 14)\n",
    "\n",
    "isigword = [] # index array for significant words\n",
    "isiphrs = [] # index array for significant words\n",
    "\n",
    "wcounter = 0\n",
    "for i in range(len(wordlist)):\n",
    "    for j in range(len(phrases)):\n",
    "        if ppmi[i][j] > 0.3: # note limit\n",
    "            #print('ppmi = {0} for word: {1} and phrase: {2}'.format(ppmi[i][j], wordlist[i], phrases[j]))\n",
    "            isigword.append(i)\n",
    "            isiphrs.append(j)\n",
    "            wcounter += 1\n",
    "            \n",
    "print('wcounter - no. of ppmi above limit: {0}'.format(wcounter))\n",
    "\n",
    "#for i in range(len(isigword)):\n",
    "for i in range(20):\n",
    "    print('ppmi {0:.2f} for word: {1} and phrase: {2} idx {3} , {4}'.format(ppmi[isigword[i]][isiphrs[i]], wordlist[isigword[i]], phrases[isiphrs[i]], isigword[i], isiphrs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# measure similarity between words for given context/phrases\n",
    "import numpy as np\n",
    "\n",
    "def cosine(wordv, wordw): # cosine similarity for two word vectors\n",
    "    sumv = 0.\n",
    "    sumw = 0.\n",
    "    sumvw = 0.\n",
    "    for elemv, elemw in zip(wordv, wordw):\n",
    "        #print('for: {0} {1}'.format(elemv, elemw))\n",
    "        sumv += elemv * elemv\n",
    "        sumw += elemw * elemw\n",
    "        sumvw += elemv * elemw\n",
    "    sumv = math.sqrt(sumv)\n",
    "    sumw = math.sqrt(sumw)\n",
    "    #print('{0} {1}'.format(sumv, sumw))\n",
    "    if sumv > 0. and sumw > 0.:\n",
    "        return sumvw / sumv / sumw\n",
    "    else: return -1.\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    #for i in range(len(isigword)):\n",
    "        #for j in range(i+1,len(isiphrs)):\n",
    "    for i in range(10):\n",
    "        for j in range(i+1,10):\n",
    "            thiscosine = cosine(ppmi[isigword[i]], ppmi[isigword[j]])\n",
    "            if thiscosine > 0.1 and wordlist[isigword[i]] != wordlist[isigword[j]]:\n",
    "                print('cosine {0} and {1}: {2:.3f}'.format(wordlist[isigword[i]], wordlist[isigword[j]], thiscosine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppmi_red mean: 0.049045852587 and stdev: 0.357699780101\n",
      "ppmi_red max: 7.54989504224 and min: -1.33049926166\n"
     ]
    }
   ],
   "source": [
    "# Dense vectors - applying SVD\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "W, S, C = linalg.svd(ppmi, overwrite_a=True, full_matrices=False)\n",
    "Sc = np.diag(S)\n",
    "\n",
    "#print W.shape, Sc.shape, C.shape\n",
    "#np.allclose(ppmi, np.dot(W, np.dot(Sc, C)))\n",
    "#np.dot(W, np.dot(Sc, C))\n",
    "\n",
    "# need to reduce matrices to e.g. 50\n",
    "k = 50 # number of singular values we want to keep\n",
    "Cred = C[:k, :]\n",
    "Sred = Sc[:k, :k]\n",
    "Wred = W[:, :k]\n",
    "ppmi_red = np.dot(Wred, np.dot(Sred, Cred))\n",
    "\n",
    "print('ppmi_red mean: {0} and stdev: {1}'.format(np.mean(ppmi_red), np.std(ppmi_red)))\n",
    "print('ppmi_red max: {0} and min: {1}'.format(np.amax(ppmi_red), np.amin(ppmi_red)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4309\n"
     ]
    }
   ],
   "source": [
    "# measure similarity between words for given context/phrases\n",
    "import numpy as np\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    isigword = [] # index array for significant words\n",
    "    isiphrs = [] # index array for significant words\n",
    "\n",
    "    wcounter = 0\n",
    "    for i in range(len(wordlist)):\n",
    "        for j in range(len(phrases)):\n",
    "            if ppmi_red[i][j] > 1.0: # note limit\n",
    "                isigword.append(i)\n",
    "                isiphrs.append(j)\n",
    "                wcounter += 1\n",
    "    \n",
    "    print wcounter\n",
    "    \n",
    "    #for i in range(len(isigword)):\n",
    "        #for j in range(i+1,len(isiphrs)):\n",
    "    for i in range(10):\n",
    "        for j in range(i+1,10):\n",
    "            thiscosine = cosine(ppmi_red[isigword[i]], ppmi_red[isigword[j]])\n",
    "            if thiscosine > 0.3 and wordlist[isigword[i]] != wordlist[isigword[j]]:\n",
    "                print('cosine {0} and {1}: {2:.3f}'.format(wordlist[isigword[i]], wordlist[isigword[j]], thiscosine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppmi 0.00 for word: aided and phrase: [u'auslsende', u'moment', u'therapie', u'verndern', u'oft', u'kommt', u'patient', u'ihnen', u'verndert'] idx 0 , 3\n",
      "ppmi 0.00 for word: aided and phrase: [u'cholinerge', u'pd', u'gestrt', u'fhren', u'neuropsychiatrischen', u'strungen', u'gesamtbelastung', u'nms', u'mp'] idx 0 , 30\n",
      "ppmi 0.00 for word: aided and phrase: [u'lebensqualitt', u'bei', u'prvalenz', u'nms', u'beziehung', u'lebensqualitt', u'ndert', u'gesamten', u'krankheitsverlaufs'] idx 0 , 36\n",
      "ppmi 0.00 for word: aided and phrase: [u'apd', u'patienten', u'vs', u'oraler', u'therapie', u'duodopa', u'verhilft', u'verlsslich', u'langanhaltend'] idx 0 , 40\n",
      "ppmi 7.70 for word: aided and phrase: [u'rapien', u'wirken', u'nms', u'auswahl', u'individuellen', u'device', u'aided', u'therapie', u'hilfreich'] idx 0 , 50\n",
      "ppmi 0.00 for word: aided and phrase: [u'wochen', u'monate', u'beginn', u'levodopa', u'therapie', u'basierend', u'zeitliche', u'muster', u'beziehung'] idx 0 , 80\n",
      "ppmi 0.00 for word: aided and phrase: [u'harnwegsprobleme', u'bezogen', u'pdq3', u'9', u'domains', u'gesamtlast', u'nms', u'parkinson', u'bezug'] idx 0 , 131\n",
      "ppmi 0.00 for word: wochen and phrase: [u'direkt', u'lcig', u'behandlu', u'ngsbeginn', u'sowie', u'monate', u'danach', u'ausschlielich', u'lcig'] idx 1 , 45\n",
      "ppmi 6.70 for word: wochen and phrase: [u'wochen', u'monate', u'beginn', u'levodopa', u'therapie', u'basierend', u'zeitliche', u'muster', u'beziehung'] idx 1 , 80\n",
      "ppmi 0.00 for word: wochen and phrase: [u'depression', u'rating', u'scale', u'hamd', u'hamilton', u'anxiety', u'rating', u'scale', u'hars'] idx 1 , 162\n",
      "ppmi 0.00 for word: wochen and phrase: [u'depression', u'p', u'perioden', u'traurigkeit', u'schuld', u'grer', u'normal', u'niemals', u'tage'] idx 1 , 202\n",
      "ppmi 7.07 for word: wochen and phrase: [u'wochen', u'andauern', u'anhaltende', u'depression', u'woche', u'lnger', u'anhaltende', u'depression', u'vegetativen'] idx 1 , 203\n",
      "ppmi 0.00 for word: wochen and phrase: [u'symptomen', u'schlaflosigkeit', u'anorexie', u'gewichtsverlust', u'verlust', u'interesses', u'anhaltende', u'depression', u'vegetativen'] idx 1 , 204\n",
      "ppmi 0.00 for word: dynamik and phrase: [u'grere', u'studien', u'aspekte', u'interkulturellen', u'umfeld', u'untersuchen', u'knapp', u'ziel', u'beurteilung'] idx 2 , 92\n",
      "ppmi 7.70 for word: dynamik and phrase: [u'erklren', u'lsst', u'weitere', u'studien', u'untersuchung', u'zeitliche', u'dynamik', u'angst', u'motorischen'] idx 2 , 172\n",
      "ppmi 0.00 for word: stren and phrase: [u'symptome', u'angstzustnde', u'apathie', u'kognition', u'halluzinationen', u'schlafstrungen', u'harnwegsprobleme', u'tragen', u'teil'] idx 3 , 35\n",
      "ppmi 6.70 for word: stren and phrase: [u'parkinson', u'patienten', u'fllen', u'dyskinesien', u'schwerwiegend', u'normale', u'bewegung', u'stren', u'lebensqualitt'] idx 3 , 110\n",
      "ppmi 0.00 for word: stren and phrase: [u'erregende', u'stimmung', u'apathie', u'kognitive', u'beeintrchtigungen', u'halluzinationen', u'psychosen', u'schlafstrungen', u'tagesschlfrigkeit'] idx 3 , 130\n",
      "ppmi 6.70 for word: stren and phrase: [u'aktivitten', u'stren', u'anhaltende', u'halluzinationen', u'wahnvorstellungen', u'floridale', u'psychosen', u'lage', u'sorgen'] idx 3 , 201\n",
      "ppmi 0.00 for word: rating and phrase: [u'verbindungen', u'wurden', u'klinischen', u'studien', u'vielversprechenden', u'ergebnissen', u'getestet', u'gegensatz', u'adrenerge'] idx 4 , 121\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print('ppmi {0:.2f} for word: {1} and phrase: {2} idx {3} , {4}'.format(ppmi[isigword[i]][isiphrs[i]], wordlist[isigword[i]], phrases[isiphrs[i]], isigword[i], isiphrs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 0.003s.\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 0.017s.\n",
      "Extracting tf features for LDA...\n",
      "done in 0.015s.\n",
      "\n",
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.210s.\n",
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: disease exploratory effects efficacy ejw emptying entacapone enzyme essential evaluation evolution experience expression disorder extension factors family feature features findings\n",
      "Topic #1: parkinson years exploratory effects efficacy ejw emptying entacapone enzyme essential evaluation evolution experience expression investigate extension factors family feature features\n",
      "Topic #2: h4 years experience effects efficacy ejw emptying entacapone enzyme essential evaluation evolution exploratory fmri expression extension factors family feature features\n",
      "Topic #3: patients years experience effects efficacy ejw emptying entacapone enzyme essential evaluation evolution exploratory investigate expression extension factors family feature features\n",
      "Topic #4: fluctuation years exploratory effects efficacy ejw emptying entacapone enzyme essential evaluation evolution experience expression year extension factors family feature features\n",
      "Topic #5: diurnal years expression efficacy ejw emptying entacapone enzyme essential evaluation evolution experience exploratory extension effect factors family feature features findings\n",
      "Topic #6: motor years experience effects efficacy ejw emptying entacapone enzyme essential evaluation evolution exploratory fmri expression extension factors family feature features\n",
      "Topic #7: histamine experience effect effects efficacy ejw emptying entacapone enzyme essential evaluation evolution exploratory fmri expression extension factors family feature features\n",
      "Topic #8: neurodegenerative years exploratory effects efficacy ejw emptying entacapone enzyme essential evaluation evolution experience expression fmri extension factors family feature features\n",
      "Topic #9: levodopa years exploratory effects efficacy ejw emptying entacapone enzyme essential evaluation evolution experience expression early extension factors family feature features\n",
      "\n",
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.054s.\n",
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: disease exploratory effects efficacy ejw emptying entacapone enzyme essential evaluation evolution experience expression disorder extension factors family feature features findings\n",
      "Topic #1: parkinson years exploratory effects efficacy ejw emptying entacapone enzyme essential evaluation evolution experience expression investigate extension factors family feature features\n",
      "Topic #2: h4 years experience effects efficacy ejw emptying entacapone enzyme essential evaluation evolution exploratory fmri expression extension factors family feature features\n",
      "Topic #3: patients years experience effects efficacy ejw emptying entacapone enzyme essential evaluation evolution exploratory investigate expression extension factors family feature features\n",
      "Topic #4: fluctuation years exploratory effects efficacy ejw emptying entacapone enzyme essential evaluation evolution experience expression year extension factors family feature features\n",
      "Topic #5: diurnal night hdc advanced rhythms subthalamic div juvenile lewy characteristics feature features efficacy ejw emptying entacapone enzyme essential evaluation evolution\n",
      "Topic #6: motor role long brain year therapy term rhythm tremor respect extension expression findings effect effects efficacy features ejw feature emptying\n",
      "Topic #7: histamine parkinsonism stimulation gait controls parkinsonian dystonia induced response onset rate pain data case marked effect bodies higher wake emptying\n",
      "Topic #8: neurodegenerative study fluctuations disorders nucleus decarboxylase expression histidine treatment diseases human day pd tmn h3 van ad dopa tuberomammillary early\n",
      "Topic #9: levodopa clinical mrna dementia enzyme production neuronal hd results deep carbidopa time analysis disorder sleep symptoms quantitative huntington levels infusion\n",
      "\n",
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n",
      "done in 1.012s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: h4 motor neurodegenerative decarboxylase dopa early lewy marked citation design abstract maintenance agonist gel comparison bilateral pet 18f implications trials\n",
      "Topic #1: fluctuations disorders mrna advanced controls dystonia year carbidopa quantitative limiting future dj cycle hypothalamic efficacy state depression chinese characterized stalevo\n",
      "Topic #2: diurnal treatment histamine parkinsonism nucleus enzyme juvenile day ad tuberomammillary alzheimer huntington wamelen supports key hybridization plays freezing patient temporal\n",
      "Topic #3: study expression hdc human rhythms induced versus bodies investigate markedly postmortem situ objectives abnormalities apomorphine hereditary pharmacokinetics reference dyskinesia evaluation\n",
      "Topic #4: clinical night diseases van hd h3 pain analysis levels receptor hofman conclusion tissue obtained humans shows quality someren entacapone gastric\n",
      "Topic #5: disease fluctuation levodopa parkinsonian time cognitive blood rasagiline movement measures cyclohydrolase fmri cognition gender plays respect taiwanese treatment life presence\n",
      "Topic #6: parkinson patients subthalamic deep term onset rate swaab essential rating emptying transl recessive approach findings benefit function d1 advances mood\n",
      "Topic #7: histidine neuronal tmn gait rhythm therapy effect tremor symptoms therapeutic keywords bao covering significantly health vigilance rodents drug chronic gene\n",
      "Topic #8: parkins pd long brain div response sleep non disorder studies df regulating respect clock higher wake disturbances major report scale\n",
      "Topic #9: stimulation dementia role production results characteristics dopamine data case infusion divh4 ejw ma shan daytime death class life effects model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import os\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "#from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "def read1k():\n",
    "    return f.read(1024)\n",
    "\n",
    "def process_data(chunk, text):\n",
    "    #print('processed: {0}'.format(ctr))\n",
    "    text.append(unicode(chunk, errors='ignore')) # 'utf8' codec can't decode byte 0xc3\n",
    "    \n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "#dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "#                             remove=('headers', 'footers', 'quotes'))\n",
    "#data_samples = dataset.data[:n_samples]\n",
    "#print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# read corpus\n",
    "os.chdir('C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "#f = open('todinvenedig.txt')\n",
    "#f = open('todinvenedigshrt.txt') # shorter version for tests\n",
    "#f = open('vecsemtst0.txt') # test file with fixed similarities\n",
    "f = open('bsc_corpus0.txt') # this file is already preprocessed, symbols and stopwords removed\n",
    "\n",
    "text = []\n",
    "for piece in iter(read1k, ''): # read file\n",
    "    process_data(piece, text)\n",
    "\n",
    "data_samples = []\n",
    "for elem in text:\n",
    "    data_samples += elem.split() # splits on all whitespaces: add text data to corpus\n",
    "data_samples = [t.lower() for t in data_samples] # convert to lower case\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: advanced: [(u'b', 0.9996907711029053), (u's', 0.9996682405471802), (u'parkinson', 0.9996609687805176), (u'levodopa', 0.9996480941772461), (u'fluctuation', 0.9996331930160522), (u'disease', 0.9996317625045776), (u'neurodegenerative', 0.9996232986450195), (u'study', 0.9996163249015808), (u'l', 0.9996100068092346), (u'dystonia', 0.9996089935302734), (u'p', 0.99960857629776), (u'diurnal', 0.9996076822280884), (u'patients', 0.9996050596237183), (u'clinical', 0.9996035695075989), (u'the', 0.9996033906936646)]\n",
      "topic: juvenile: [(u'parkinson', 0.9997127056121826), (u'fluctuation', 0.9996970891952515), (u'p', 0.9996941089630127), (u'disease', 0.9996925592422485), (u's', 0.9996867179870605), (u'b', 0.9996837377548218), (u'diurnal', 0.9996681809425354), (u'h4', 0.9996650815010071), (u'patients', 0.9996529221534729), (u'levodopa', 0.9996474385261536), (u'study', 0.9996460676193237), (u'disorders', 0.999645471572876), (u'night', 0.9996446967124939), (u'motor', 0.9996405243873596), (u'l', 0.9996309280395508)]\n",
      "topic: dementia: [(u'disease', 0.9997852444648743), (u'b', 0.9997818470001221), (u's', 0.9997768998146057), (u'fluctuation', 0.9997720122337341), (u'parkinson', 0.9997720122337341), (u'neurodegenerative', 0.999763548374176), (u'parkinsonism', 0.9997628927230835), (u'motor', 0.9997606873512268), (u'diurnal', 0.9997591972351074), (u'p', 0.9997565746307373), (u'histamine', 0.9997561573982239), (u'levodopa', 0.9997544288635254), (u'h4', 0.9997506141662598), (u'clinical', 0.9997475147247314), (u'tmn', 0.9997465014457703)]\n",
      "topic: depression: [(u'b', 0.9993273019790649), (u'diurnal', 0.9993256330490112), (u'p', 0.9993151426315308), (u'levodopa', 0.9993122816085815), (u'h4', 0.9992951154708862), (u'disease', 0.9992854595184326), (u's', 0.9992846250534058), (u'fluctuation', 0.9992818832397461), (u'parkinson', 0.9992794990539551), (u'clinical', 0.9992777705192566), (u'expression', 0.9992762804031372), (u'neurodegenerative', 0.9992695450782776), (u'mrna', 0.9992647171020508), (u'histidine', 0.9992486238479614), (u'characteristics', 0.9992354512214661)]\n",
      "topic: pain: [(u'b', 0.9996814727783203), (u's', 0.9996798634529114), (u'fluctuation', 0.9996605515480042), (u'disease', 0.9996538162231445), (u'p', 0.9996521472930908), (u'h4', 0.999646782875061), (u'parkinson', 0.9996457099914551), (u'clinical', 0.9996446371078491), (u'diurnal', 0.9996431469917297), (u'patients', 0.9996415376663208), (u'levodopa', 0.9996413588523865), (u'neurodegenerative', 0.999628484249115), (u'fluctuations', 0.999626100063324), (u'motor', 0.9996194243431091), (u'night', 0.9996148943901062)]\n",
      "topic: night: [(u'b', 0.9998608827590942), (u'parkinson', 0.9998565912246704), (u's', 0.9998511672019958), (u'disease', 0.9998278617858887), (u'diurnal', 0.9998254179954529), (u'p', 0.9998205304145813), (u'h4', 0.9998190402984619), (u'fluctuation', 0.9998173713684082), (u'neurodegenerative', 0.9998100996017456), (u'motor', 0.9998005628585815), (u'levodopa', 0.9997994899749756), (u'clinical', 0.9997914433479309), (u'dystonia', 0.9997831583023071), (u'tmn', 0.999774694442749), (u'histamine', 0.999771237373352)]\n",
      "topic: diurnal: [(u's', 0.9999037981033325), (u'parkinson', 0.9998984336853027), (u'b', 0.9998937845230103), (u'disease', 0.9998897314071655), (u'p', 0.9998866319656372), (u'h4', 0.999886155128479), (u'fluctuation', 0.9998792409896851), (u'neurodegenerative', 0.9998675584793091), (u'histamine', 0.999863862991333), (u'clinical', 0.9998539090156555), (u'levodopa', 0.9998513460159302), (u'patients', 0.9998505711555481), (u'motor', 0.9998371005058289), (u'tmn', 0.9998318552970886), (u'expression', 0.9998304843902588)]\n"
     ]
    }
   ],
   "source": [
    "# work through topics and find similar concepts\n",
    "topic0 = [u'fluctuation', u'advanced', u'dementia', u'depression', u'dystonia', u'variability', u'gel', u'pain']\n",
    "topic1 = [u'movement', u'striatal', u'increased', u'bradykinesia', u'improvement']\n",
    "topic2 = []\n",
    "topic3 = []\n",
    "topic4 = []\n",
    "topic5 = []\n",
    "topic6 = []\n",
    "topic7 = []\n",
    "topic8 = []\n",
    "topic9 = ['parkinson', u'nlmcategory', u'conclusion', u'features']\n",
    "topicX = [u'advanced', u'juvenile', u'dementia', u'depression', u'pain', u'night', u'diurnal']\n",
    "\n",
    "\n",
    "for top in topicX:\n",
    "    #print('topic: {0}: {1}'.format(top, model.wv.most_similar([top], topn=5)))\n",
    "    print('topic: {0}: {1}'.format(top, model.wv.most_similar_cosmul([top], topn=15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "people gun state control right guns crime states law police\n",
      "Topic 1:\n",
      "time question book years did like don space answer just\n",
      "Topic 2:\n",
      "mr line rules science stephanopoulos title current define int yes\n",
      "Topic 3:\n",
      "key chip keys clipper encryption number des algorithm use bit\n",
      "Topic 4:\n",
      "edu com cs vs w7 cx mail uk 17 send\n",
      "Topic 5:\n",
      "use does window problem way used point different case value\n",
      "Topic 6:\n",
      "windows thanks know help db does dos problem like using\n",
      "Topic 7:\n",
      "bike water effect road design media dod paper like turn\n",
      "Topic 8:\n",
      "don just like think know people good ve going say\n",
      "Topic 9:\n",
      "car new price good power used air sale offer ground\n",
      "Topic 10:\n",
      "file available program edu ftp information files use image version\n",
      "Topic 11:\n",
      "ax max b8f g9v a86 145 pl 1d9 0t 34u\n",
      "Topic 12:\n",
      "government law privacy security legal encryption court fbi technology information\n",
      "Topic 13:\n",
      "card bit memory output video color data mode monitor 16\n",
      "Topic 14:\n",
      "drive scsi disk mac hard apple drives controller software port\n",
      "Topic 15:\n",
      "god jesus people believe christian bible say does life church\n",
      "Topic 16:\n",
      "year game team games season play hockey players league player\n",
      "Topic 17:\n",
      "10 00 15 25 20 11 12 14 16 13\n",
      "Topic 18:\n",
      "armenian israel armenians war people jews turkish israeli said women\n",
      "Topic 19:\n",
      "president people new said health year university school day work\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print \"Topic %d:\" % (topic_idx)\n",
    "        print \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "#tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "#tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "#tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 20\n",
    "\n",
    "# Run NMF\n",
    "#nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 10\n",
    "#display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[3, 2]\n",
      "[3, 2]\n"
     ]
    }
   ],
   "source": [
    "# forward/backward API\n",
    "import math\n",
    "\n",
    "def forward(x, y, op): # two scalars and an operation: 0=+, 1=*\n",
    "    if op == 0: # +\n",
    "        return x + y\n",
    "    else: # *\n",
    "        return x * y\n",
    "\n",
    "def backward(x, y, op, dz): # two scalars and an operation: 0=+, 1=*\n",
    "    if op == 0: # +\n",
    "        return [dz, dz]\n",
    "    else: # *\n",
    "        return [y * dz, x * dz]\n",
    "\n",
    "def numgrad(x, y, op, dz, h): # two scalars and an operation: 0=+, 1=*\n",
    "    if op == 0: # +\n",
    "        return [((x+h)+y-(x-h)+y)/(2*h), (x+(y+h)-x+(y-h))/(2*h)]\n",
    "    else: # *\n",
    "        return [((x+h)*y-(x-h)*y)/(2*h), (x*(y+h)-x*(y-h))/(2*h)]\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    print forward(2, 3, 1)\n",
    "    print backward(2, 3, 1, 1)\n",
    "    print numgrad(2, 3, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Long-term therapy of Parkinson's disease (PD) with levodopa (L-DOPA) is\n",
      "      associated with a high risk of developing motor fluctuations and dyskinesia. Deep\n",
      "      brain stimulation (DBS) in PD patients of the subthalamic nucleus can improve\n",
      "      these motor complications. Although the positive effect on motor symptoms has\n",
      "      been proven, postoperative cognitive decline has been documented. To tackle the\n",
      "      impact of PD-DBS on cognition, 18 DBS patients were compared to 25 best medically\n",
      "      treated Parkinson's patients, 24 Mild Cognitive Impairment (MCI) patients and 12 \n",
      "      healthy controls using the Neuropsychological Test Battery Vienna-long\n",
      "      (NTBV-long) for cognitive outcome 12 months after first examination. Reliable\n",
      "      change index methodology was used. Overall, there was cognitive change in\n",
      "      individual patients, but the change was very heterogeneous with gains and losses.\n",
      "      Further research is needed to identify the mechanisms that lead to improvement or\n",
      "      deterioration of cognitive functions in individual cases.\n",
      " The classic motor deficits of Parkinson's disease are caused by degeneration of\n",
      "      dopaminergic neurons in the substantia nigra pars compacta, resulting in the loss\n",
      "      of their long-distance axonal projections that modulate the striatum. Current\n",
      "      treatments only minimize the symptoms of this disconnection as there is no\n",
      "      approach capable of replacing the nigrostriatal pathway. We are applying\n",
      "      micro-tissue engineering techniques to create living, implantable constructs that\n",
      "      mimic the architecture and function of the nigrostriatal pathway. These\n",
      "      constructs consist of dopaminergic neurons with long axonal tracts encased within\n",
      "      hydrogel micro-columns. Micro-columns were seeded with dopaminergic neuronal\n",
      "      aggregates, while lumen extracellular matrix (ECM), growth factors, and end\n",
      "      targets were varied to optimize cytoarchitecture. We found a 10-fold increase in \n",
      "      axonal outgrowth from aggregates versus dissociated neurons, resulting in\n",
      "      remarkable axonal lengths of over 6 mm by 14 days and 9 mm by 28 days in vitro.\n",
      "      Axonal extension was also dependent upon lumen ECM, but did not depend on growth \n",
      "      factor enrichment or neuronal end target presence. Evoked dopamine release was\n",
      "      measured via fast scan cyclic voltammetry and synapse formation with striatal\n",
      "      neurons was observed in vitro. Constructs were microinjected to span the\n",
      "      nigrostriatal pathway in rats, revealing survival of implanted neurons while\n",
      "      maintaining their axonal projections within the micro-column. Lastly, these\n",
      "      constructs were generated with dopaminergic neurons differentiated from human\n",
      "      embryonic stem cells. This strategy may improve Parkinson's disease treatment by \n",
      "      simultaneously replacing lost dopaminergic neurons in the substantia nigra and\n",
      "      reconstructing their long-projecting axonal tracts to the striatum.\n",
      "CI  - This article is protected by copyright. All rights reserved.\n",
      " OBJECTIVES: To determine whether the drug saxagliptin, a dipeptidyl peptidase-4\n",
      "      (DPP-4) inhibitor which is utilized for the treatment of Diabetes Mellitus, has\n",
      "      neuroprotective effects in the animal model of Parkinson's disease (PD) induced\n",
      "      by 6-hydroxydopamine (6-OHDA) in rats. METHODS: Male Wistar rats (weighing\n",
      "      280-300 g) received a bilateral infusion of 6-OHDA in the substantia nigra.\n",
      "      Twenty-four hours later, they were treated with saxagliptin (1 mg/kg, p.o) once\n",
      "      daily, for 21 days. The motor function was evaluated using the open field and\n",
      "      rotarod (RT) tests. In addition, cognition was assessed with the novel object\n",
      "      recognition test (ORT). After the evaluation of the behavioural tests, the\n",
      "      animals were transcardially perfused to perform immunohistochemistry staining for\n",
      "      tyrosine hydroxylase (TH) in the substantia nigra pars compacta (SNpc). KEY\n",
      "      FINDINGS: Saxagliptin impaired the memory of animals in the sham group.\n",
      "      CONCLUSIONS: Saxagliptin treatment did not exhibit neuroprotection and it did not\n",
      "      improve the cognitive and motor deficits in the 6-OHDA model of PD.\n",
      "      Interestingly, when saxagliptin was administered to the sham animals, a cognitive\n",
      "      decline was observed. Therefore, this drug should be investigated as a possible\n",
      "      treatment for PTSD.\n",
      "CI  - (c) 2018 Royal Pharmaceutical Society.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download pubmed data\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def getres(url): # get request\n",
    "    return requests.get(url)\n",
    "\n",
    "def compile_corpus(txt): # assume medline-structure of text, extract abstracts (idenitfier AB)\n",
    "    cpos = txt.find('AB  -')\n",
    "    if cpos < 0:\n",
    "        return ''\n",
    "    cpos += 5\n",
    "    slen = len(txt)\n",
    "    rtxt = ''\n",
    "    alen = 2000\n",
    "    while cpos < slen:\n",
    "        nextpos = min(cpos+alen, max(0, txt.find('FAU -', cpos)))\n",
    "        rtxt += txt[cpos:nextpos]\n",
    "        cpos = txt.find('AB  -', nextpos+5)\n",
    "        if cpos < 0:\n",
    "            return rtxt\n",
    "        cpos += 5\n",
    "    return rtxt\n",
    "\n",
    "def save_corpus(txt): # save corpus\n",
    "    # save corpus as separate file\n",
    "    os.chdir('C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "    fout = open('bsc_corpus0.txt', 'w')\n",
    "    fout.write(txt) # requires a string\n",
    "    fout.close()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    #result = getres('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=parkinson+AND+motor&retmax=3')\n",
    "    #result = getres('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=parkinson+AND+motor&retmax=3&usehistory=y')\n",
    "    #result = getres('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/epost.fcgi?db=pubmed&id=29753604,29747823')\n",
    "    #result = getres('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id=29753604,29747823')\n",
    "    #result = getres('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29753604,29747823,29748110&rettype=abstract')\n",
    "    #result = getres('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29753604,29747823,29748110&rettype=abstract&retmode=text')\n",
    "    result = getres('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=NCID_1_11416453_130.14.22.215_9001_1526608607_1043747086_0MetA0_S_MegaStore&retmax=3&rettype=medline&retmode=text')\n",
    "    #print result.text\n",
    "    #outtext = result.text.split(\"AB  - \", 1)\n",
    "    #outt2 = outtext[1].split(\"FAU -\") # or just take 500 characters after the split?\n",
    "    #outt2 = outtext[1].splitlines()\n",
    "    outtext = compile_corpus(result.text)\n",
    "    print outtext\n",
    "    #save_corpus(outtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download pubmed data 0: search and store webenv key\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def getres(url): # get request\n",
    "    return requests.get(url)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # enter search string here, execute and copy-paste webenv key string\n",
    "    result = getres('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=parkinson+AND+motor&retmax=3&usehistory=y')\n",
    "    print result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download pubmed data 1: retrieve search results via search string and store abstracts in file\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def getres(url): # get request\n",
    "    return requests.get(url)\n",
    "\n",
    "def compile_corpus(txt): # assume medline-structure of text, extract abstracts (idenitfier AB)\n",
    "    cpos = txt.find('AB  -')\n",
    "    if cpos < 0:\n",
    "        return ''\n",
    "    cpos += 5\n",
    "    slen = len(txt)\n",
    "    rtxt = ''\n",
    "    alen = 2000\n",
    "    while cpos < slen:\n",
    "        nextpos = min(cpos+alen, max(0, txt.find('FAU -', cpos)))\n",
    "        rtxt += txt[cpos:nextpos]\n",
    "        cpos = txt.find('AB  -', nextpos+5)\n",
    "        if cpos < 0:\n",
    "            return rtxt\n",
    "        cpos += 5\n",
    "    return rtxt\n",
    "\n",
    "def save_corpus(txt): # save corpus\n",
    "    # save corpus as separate file\n",
    "    os.chdir('C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "    fout = open('bsc_corpus0.txt', 'w')\n",
    "    fout.write(txt) # requires a string\n",
    "    fout.close()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # paste webenv key here\n",
    "    result = getres('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=1&WebEnv=NCID_1_11416453_130.14.22.215_9001_1526608607_1043747086_0MetA0_S_MegaStore&retmax=3&rettype=medline&retmode=text')\n",
    "    outtext = compile_corpus(result.text)\n",
    "    print outtext\n",
    "    #save_corpus(outtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf: enable eager execution\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'disable_eager_execution'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-58429fd22c26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable_eager_execution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'disable_eager_execution'"
     ]
    }
   ],
   "source": [
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 24056\n",
      "Most common words (+UNK) [['UNK', 1117], ('und', 789), ('die', 649), ('der', 586), ('Patrick', 412)]\n",
      "Sample data [2915, 799, 331, 924, 3619, 76, 4842, 4, 44, 22] ['\\ufeffPackis', 'Geschichte', 'Kapitel', '1', 'Schon', 'wieder', 'stolperte', 'Patrick', 'über', 'eine']\n",
      "799 Geschichte -> 331 Kapitel\n",
      "799 Geschichte -> 2915 ﻿Packis\n",
      "331 Kapitel -> 924 1\n",
      "331 Kapitel -> 799 Geschichte\n",
      "924 1 -> 331 Kapitel\n",
      "924 1 -> 3619 Schon\n",
      "3619 Schon -> 924 1\n",
      "3619 Schon -> 76 wieder\n",
      "Initialized\n",
      "Average loss at step  0 :  207.96377563476562\n",
      "Nearest to zum: Straßen, seinem, Dir,, wählte, Kohlestück, dunklen, bohrte, Ankunft,\n",
      "Nearest to dann: frei., dazwischen, verwüstet,, nächsten, Tricks,, Fortsetzung, ein.“, Es,\n",
      "Nearest to über: verkaufen, wusste,, südlichen, vorher, gezogenem, Becher., gepackt, Weise,\n",
      "Nearest to Er: Schmid, lead, zuletzt, schätze, dringen, eingelassen., soll., wegwerfende,\n",
      "Nearest to mit: halben, knirschte, Feuer., „Rank, rutter, dazu, ohnen, „Oh.“,\n",
      "Nearest to Ihr: doppelt, mehr,, gedeckt, weiteres, wenige, nahe, Heer, Gräben,\n",
      "Nearest to Sie: Bein., begrüßte, Truppe, gepresst, (welches?),, disaster:, Neue, hin,\n",
      "Nearest to dem: Buch, Augen., Eisenschienen, bestand., irgendeinen, horchte., Ufer,, Alkove,,\n",
      "Nearest to Patrick.: besonders, schwitzten, Befreiung, Hände,, describe, Untertanen, graue, Schrift,\n",
      "Nearest to Euch: Segeln, kehrte, Zugang,, confront.,, gewundene, sinken, dunkel., Göttin,\n",
      "Nearest to ich: hunderte, zuckend,, Grenze, Jasmon., gehört, Lehrer, Tür,, Übersetzung,\n",
      "Nearest to nun: verfasst,, Patrick, durchschritt, geschickt,, Weg,, Busch, Kratzer,, angegriffen.“,\n",
      "Nearest to durch: Augenwinkel, Bewegungen, Raschte, fadenscheinig., Turm., Teil, ließen, silbrig,\n",
      "Nearest to waren: leicht,, Karte., Reaktion, höher, succeed, Köpfe, Ob, •,\n",
      "Nearest to ihr: irdene, wandelnden, Diebesgilde,, Füßen, Ausrüstung,, Fensterluke, gemustert, Vertrauen,\n",
      "Nearest to auch: Gaststube, Gasse, Szenen:, prüfte., vier, Schießscharten., Derden, metallenen,\n",
      "Average loss at step  2000 :  23.906010870695113\n",
      "Average loss at step  4000 :  4.309587460517883\n",
      "Average loss at step  6000 :  3.70804302752018\n",
      "Average loss at step  8000 :  3.399901805281639\n",
      "Average loss at step  10000 :  3.2135682064890863\n",
      "Nearest to zum: Straßen, kümmert, nochmals, Dir,, wählte, fail, decken, Decke,\n",
      "Nearest to dann: dazwischen, frei., Tricks,, Es, ein.“, Norden, Oberarm., Fortsetzung,\n",
      "Nearest to über: vorher, südlichen, verkaufen, „Eins,, gezogenem, verlangt, wusste,, langen,\n",
      "Nearest to Er: zuletzt, schreckte, lead, soll., eingelassen., angenähten, Trapsen, Reichtum,\n",
      "Nearest to mit: halben, knirschte, ohnen, rutter, Hand, schmutziges, „Rank, Feld,\n",
      "Nearest to Ihr: mehr,, „Hatte, doppelt, natürliche, weiteres, Heer, Umhänge, werden.“,\n",
      "Nearest to Sie: Bein., begrüßte, hin, schönen, Wir, halbe, disaster:, meinem,\n",
      "Nearest to dem: Alkove,, Buch, den, Putzenscheiben, horchte., einem, bestand., Eisenschienen,\n",
      "Nearest to Patrick.: Jakob., Marlein., griff, schwitzten, anstellen, sie., Aurelia., er.,\n",
      "Nearest to Euch: Segeln, Zugang,, sinken, nie, „Jeder, confront.,, Samurai,, dunkel.,\n",
      "Nearest to ich: hunderte, gestürzt, gehört, Lehrer, gemeinsam.“, Tisch., Grenze, zuckend,,\n",
      "Nearest to nun: Weg,, verfasst,, groß, durchschritt, Ihr,, Steinplatten, be, eingenommen,,\n",
      "Nearest to durch: Augenwinkel, wollen., Raschte, fadenscheinig., heller,, sanft, Marktplatz., Zug,\n",
      "Nearest to waren: leicht,, stirbt, Köpfe, entrollte, succeed, •, Ob, Diebe,,\n",
      "Nearest to ihr: zusteht.“, irdene, wandelnden, seltenen, geprägt,, Fensterluke, handelt, something,\n",
      "Nearest to auch: beseitigt, Szenen:, abgekühlte, Derden, Gaststube, climax/resolution, spritzte, Burghof,\n",
      "Average loss at step  12000 :  3.0704868201613427\n",
      "Average loss at step  14000 :  2.960234669685364\n",
      "Average loss at step  16000 :  2.867578670799732\n",
      "Average loss at step  18000 :  2.791891873002052\n",
      "Average loss at step  20000 :  2.726184146165848\n",
      "Nearest to zum: kümmert, Straßen, nochmals, decken, commit;, Am, fail, Marlein.,\n",
      "Nearest to dann: dazwischen, Norden, frei., meckern, Oberarm., Tricks,, Seeufer, treu,\n",
      "Nearest to über: südlichen, vorher, „Eins,, verkaufen, Risse,, commit;, Zimmertür, verlangt,\n",
      "Nearest to Er: zuletzt, soll., schreckte, Erling, lead, angenähten, Reichtum, Kopfes,\n",
      "Nearest to mit: halben, Hand, ohnen, rutter, knirschte, schmutziges, Feld, „Rank,\n",
      "Nearest to Ihr: „Hatte, mehr,, natürliche, weiteres, Umhänge, Breite, Stadt:, aufging.,\n",
      "Nearest to Sie: Bein., begrüßte, halbe, Wir, schricht, disaster:, (welches?),, hin,\n",
      "Nearest to dem: Buch, Alkove,, horchte., Putzenscheiben, den, einem, bestand., fanden,\n",
      "Nearest to Patrick.: Jakob., Marlein., griff, gebissen, Aurelia., anstellen, Wagenrädern, sprach,\n",
      "Nearest to Euch: Segeln, Zugang,, gebrauchen., sinken, nie, „Jeder, Kunring,, Samurai,,\n",
      "Nearest to ich: hunderte, gestürzt, gemeinsam.“, lichter, gehört, Lehrer, übereinander, herbei,\n",
      "Nearest to nun: Weg,, durchschritt, verfasst,, Fenster., Ihr,, dachten, groß, Patrick?“,\n",
      "Nearest to durch: Augenwinkel, wollen., fadenscheinig., Raschte, Neuankömmlinge, Gänge, heller,, entglitt,\n",
      "Nearest to waren: leicht,, entrollte, Diebe,, Köpfe, Könige, succeed, stirbt, Karren,\n",
      "Nearest to ihr: zusteht.“, irdene, seltenen, wandelnden, keiner, sichern,, Zehn,, Fensterluke,\n",
      "Nearest to auch: beseitigt, climax/resolution, spritzte, „Jan-Gelling,, Szenen:, Ullins,, Gaststube, Burghof,\n",
      "Average loss at step  22000 :  2.6764073932766914\n",
      "Average loss at step  24000 :  2.6416501811146738\n",
      "Average loss at step  26000 :  2.6065912687778474\n",
      "Average loss at step  28000 :  2.579874454140663\n",
      "Average loss at step  30000 :  2.563206072628498\n",
      "Nearest to zum: Straßen, kümmert, nochmals, decken, commit;, fail, Am, behielt,\n",
      "Nearest to dann: dazwischen, Norden, meckern, Seeufer, Oberarm., treu, Tricks,, frei.,\n",
      "Nearest to über: vorher, südlichen, Risse,, commit;, Zimmertür, „Eins,, Schwung, verkaufen,\n",
      "Nearest to Er: soll., zuletzt, Erling, schreckte, Reichtum, umgeschlagen.“, lethargisch, lead,\n",
      "Nearest to mit: halben, Hand, rutter, schmutziges, ohnen, Feld, Zugbrücke,, Einfluss,\n",
      "Nearest to Ihr: „Hatte, natürliche, mehr,, (Hügelgräber),, Bogenschützen,, Stadt:, Häuserwände, weiteres,\n",
      "Nearest to Sie: Bein., schricht, halbe, begrüßte, (welches?),, Truppe, disaster:, Gestalt,\n",
      "Nearest to dem: horchte., Alkove,, Buch, Putzenscheiben, den, einem, bestand., fanden,\n",
      "Nearest to Patrick.: Jakob., Marlein., gebissen, Aurelia., Wagenrädern, sprach, anstellen, fernhält,\n",
      "Nearest to Euch: Segeln, gebrauchen., Zugang,, nie, „Jeder, Kunring,, sinken, Samurai,,\n",
      "Nearest to ich: hunderte, gestürzt, lichter, gemeinsam.“, gehört, Lehrer, Steine,, herbei,\n",
      "Nearest to nun: Fenster., Weg,, dachten, durchschritt, Ihr,, verfasst,, Patrick?“, groß,\n",
      "Nearest to durch: Augenwinkel, wollen., Neuankömmlinge, fadenscheinig., geschlossenen, entglitt, ambition?, Gänge,\n",
      "Nearest to waren: leicht,, entrollte, Diebe,, Könige, bitten, Karren, begannen, Steinplatte,\n",
      "Nearest to ihr: zusteht.“, irdene, sichern,, seltenen, keiner, Zehn,, sein, Ihm,\n",
      "Nearest to auch: beseitigt, climax/resolution, spritzte, „Jan-Gelling,, Ullins,, Szenen:, Burghof, strich,\n",
      "Average loss at step  32000 :  2.5422803918123247\n",
      "Average loss at step  34000 :  2.535452584952116\n",
      "Average loss at step  36000 :  2.5269587590992453\n",
      "Average loss at step  38000 :  2.5104886524677275\n",
      "Average loss at step  40000 :  2.5074108837246896\n",
      "Nearest to zum: Straßen, kümmert, nochmals, decken, commit;, behielt, fail, Am,\n",
      "Nearest to dann: Norden, dazwischen, Seeufer, meckern, Oberarm., treu, frei., ausnehmen,,\n",
      "Nearest to über: Risse,, vorher, commit;, Zimmertür, Schwung, südlichen, „Eins,, Rotwein,\n",
      "Nearest to Er: soll., Erling, zuletzt, schreckte, lethargisch, umgeschlagen.“, Dann, Reichtum,\n",
      "Nearest to mit: halben, schmutziges, rutter, Hand, ohnen, Zugbrücke,, geduckt, Feld,\n",
      "Nearest to Ihr: „Hatte, natürliche, mehr,, (Hügelgräber),, Häuserwände, Mädchenhand, aufging., sah,,\n",
      "Nearest to Sie: Bein., schricht, halbe, Truppe, begrüßte, (welches?),, Gestalt, disaster:,\n",
      "Nearest to dem: horchte., Alkove,, Buch, Putzenscheiben, den, einem, bestand., Großmutter,\n",
      "Nearest to Patrick.: Jakob., Marlein., gebissen, Wagenrädern, Aurelia., sprach, dauern?, Jetzt,,\n",
      "Nearest to Euch: Segeln, gebrauchen., nie, Kunring,, „Jeder, Zugang,, Samurai,, sinken,\n",
      "Nearest to ich: lichter, hunderte, gestürzt, gemeinsam.“, gehört, Lehrer, gehalten, herbei,\n",
      "Nearest to nun: Weg,, Fenster., dachten, durchschritt, Ihr,, verfasst,, Patrick?“, be,\n",
      "Nearest to durch: Augenwinkel, wollen., Neuankömmlinge, entglitt, geschlossenen, Gänge, ambition?, Bergen.,\n",
      "Nearest to waren: leicht,, entrollte, Könige, Karren, begannen, Diebe,, bitten, Steinplatte,\n",
      "Nearest to ihr: zusteht.“, irdene, sichern,, keiner, seltenen, sein, Zehn,, Ihm,\n",
      "Nearest to auch: beseitigt, climax/resolution, Ullins,, spritzte, „Jan-Gelling,, Szenen:, strich, Lederhandschuh.,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  42000 :  2.501821564435959\n",
      "Average loss at step  44000 :  2.4955537236332894\n",
      "Average loss at step  46000 :  2.4893244272768498\n",
      "Average loss at step  48000 :  2.4869545041024685\n",
      "Average loss at step  50000 :  2.4812588822245596\n",
      "Nearest to zum: Straßen, kümmert, nochmals, decken, commit;, behielt, Am, langsamer,,\n",
      "Nearest to dann: Norden, dazwischen, Seeufer, ausnehmen,, treu, Tricks,, meckern, Oberarm.,\n",
      "Nearest to über: Zimmertür, vorher, Risse,, commit;, Schwung, „Eins,, Rotwein, südlichen,\n",
      "Nearest to Er: soll., Erling, zuletzt, umgeschlagen.“, Reichtum, lethargisch, Dann, angenähten,\n",
      "Nearest to mit: halben, schmutziges, Hand, rutter, Zugbrücke,, ohnen, geduckt, Einfluss,\n",
      "Nearest to Ihr: „Hatte, natürliche, mehr,, (Hügelgräber),, Häuserwände, sah,, danke, aufging.,\n",
      "Nearest to Sie: Bein., schricht, Truppe, halbe, begrüßte, (welches?),, disaster:, Dornenästen,\n",
      "Nearest to dem: horchte., Alkove,, Buch, Putzenscheiben, den, Großmutter, einem, bestand.,\n",
      "Nearest to Patrick.: Jakob., Marlein., Aurelia., Wagenrädern, gebissen, sprach, dauern?, Jetzt,,\n",
      "Nearest to Euch: Segeln, gebrauchen., Kunring,, nie, „Jeder, Zugang,, Samurai,, negativen,\n",
      "Nearest to ich: lichter, gestürzt, hunderte, gemeinsam.“, gehalten, da, Lehrer, herbei,\n",
      "Nearest to nun: Weg,, Fenster., dachten, durchschritt, Ihr,, verfasst,, Eisentüre., Patrick?“,\n",
      "Nearest to durch: Augenwinkel, wollen., Neuankömmlinge, entglitt, geschlossenen, Gänge, ambition?, Bergen.,\n",
      "Nearest to waren: leicht,, entrollte, Karren, begannen, Diebe,, Könige, bitten, Steinplatte,\n",
      "Nearest to ihr: zusteht.“, irdene, sichern,, keiner, seltenen, Zehn,, Ihm, sein,\n",
      "Nearest to auch: beseitigt, climax/resolution, Ullins,, spritzte, „Jan-Gelling,, Lederhandschuh., Szenen:, strich,\n",
      "Average loss at step  52000 :  2.477461604297161\n",
      "Average loss at step  54000 :  2.4765237315893174\n",
      "Average loss at step  56000 :  2.47410362303257\n",
      "Average loss at step  58000 :  2.472877077639103\n",
      "Average loss at step  60000 :  2.469208761334419\n",
      "Nearest to zum: kümmert, Straßen, nochmals, decken, behielt, commit;, langsamer,, Am,\n",
      "Nearest to dann: Norden, dazwischen, Seeufer, ausnehmen,, Tricks,, treu, frei., meckern,\n",
      "Nearest to über: Zimmertür, commit;, vorher, Risse,, Schwung, Rotwein, „Eins,, südlichen,\n",
      "Nearest to Er: Erling, soll., zuletzt, umgeschlagen.“, lethargisch, Reichtum, angenähten, Dann,\n",
      "Nearest to mit: halben, Hand, schmutziges, rutter, Zugbrücke,, Einfluss, ohnen, Bollte,\n",
      "Nearest to Ihr: „Hatte, natürliche, mehr,, (Hügelgräber),, sah,, Häuserwände, danke, aufging.,\n",
      "Nearest to Sie: Bein., schricht, Truppe, halbe, begrüßte, (welches?),, disaster:, Dornenästen,\n",
      "Nearest to dem: horchte., Alkove,, Buch, Putzenscheiben, den, Großmutter, seinem, einem,\n",
      "Nearest to Patrick.: Jakob., Aurelia., sprach, Wagenrädern, Marlein., gebissen, dauern?, Jetzt,,\n",
      "Nearest to Euch: Segeln, gebrauchen., Kunring,, nie, „Jeder, negativen, Samurai,, schaukelte,\n",
      "Nearest to ich: lichter, gestürzt, gemeinsam.“, hunderte, da, Treppen, Lehrer, gehalten,\n",
      "Nearest to nun: dachten, Fenster., Weg,, verfasst,, Ihr,, durchschritt, Eisentüre., töten?,\n",
      "Nearest to durch: Augenwinkel, wollen., Neuankömmlinge, entglitt, geschlossenen, Gänge, Bergen., ambition?,\n",
      "Nearest to waren: leicht,, Könige, Diebe,, Karren, begannen, entrollte, bitten, Steinplatte,\n",
      "Nearest to ihr: zusteht.“, sichern,, irdene, keiner, seltenen, Anteil, sein, Ihm,\n",
      "Nearest to auch: beseitigt, climax/resolution, Ullins,, spritzte, Lederhandschuh., „Jan-Gelling,, Göttin,, Szenen:,\n",
      "Average loss at step  62000 :  2.468343488276005\n",
      "Average loss at step  64000 :  2.4608599570989607\n",
      "Average loss at step  66000 :  2.461812419861555\n",
      "Average loss at step  68000 :  2.457398420244455\n",
      "Average loss at step  70000 :  2.456371725618839\n",
      "Nearest to zum: Straßen, kümmert, nochmals, decken, behielt, commit;, langsamer,, überrascht,\n",
      "Nearest to dann: Norden, dazwischen, ausnehmen,, Tricks,, Seeufer, meckern, Oberarm., treu,\n",
      "Nearest to über: Zimmertür, vorher, Risse,, commit;, Schwung, Rotwein, „Eins,, südlichen,\n",
      "Nearest to Er: Erling, soll., zuletzt, umgeschlagen.“, Reichtum, lethargisch, lead, durchqueren,\n",
      "Nearest to mit: halben, schmutziges, rutter, Hand, Bollte, Zugbrücke,, geduckt, Einfluss,\n",
      "Nearest to Ihr: „Hatte, mehr,, natürliche, (Hügelgräber),, sah,, Häuserwände, aufging., Schwänze,\n",
      "Nearest to Sie: Bein., schricht, Truppe, (welches?),, hin, halbe, begrüßte, Neue,\n",
      "Nearest to dem: horchte., Alkove,, den, Putzenscheiben, Buch, Großmutter, seinem, einem,\n",
      "Nearest to Patrick.: Jakob., sprach, Aurelia., Marlein., Wagenrädern, dauern?, gebissen, Jetzt,,\n",
      "Nearest to Euch: Segeln, gebrauchen., Kunring,, nie, Samurai,, „Jeder, schaukelte, negativen,\n",
      "Nearest to ich: lichter, gestürzt, gemeinsam.“, hunderte, da, herbei, gehalten, Treppen,\n",
      "Nearest to nun: dachten, Fenster., Weg,, Ihr,, verfasst,, durchschritt, Eisentüre., Patrick?“,\n",
      "Nearest to durch: Augenwinkel, wollen., Neuankömmlinge, entglitt, geschlossenen, Gänge, Bergen., ambition?,\n",
      "Nearest to waren: leicht,, begannen, Karren, Diebe,, Könige, entrollte, junge, Schlecht,\n",
      "Nearest to ihr: zusteht.“, sichern,, keiner, irdene, seltenen, Anteil, Hände,, Ihm,\n",
      "Nearest to auch: beseitigt, climax/resolution, Ullins,, spritzte, Lederhandschuh., „Jan-Gelling,, Göttin,, Szenen:,\n",
      "Average loss at step  72000 :  2.458331431865692\n",
      "Average loss at step  74000 :  2.4588146788179874\n",
      "Average loss at step  76000 :  2.452831050336361\n",
      "Average loss at step  78000 :  2.4528014072179793\n",
      "Average loss at step  80000 :  2.4490309393405916\n",
      "Nearest to zum: kümmert, Straßen, nochmals, decken, behielt, commit;, langsamer,, überrascht,\n",
      "Nearest to dann: Norden, ausnehmen,, Tricks,, Seeufer, dazwischen, Oberarm., treu, schwebte.,\n",
      "Nearest to über: Zimmertür, Schwung, Risse,, vorher, commit;, Rotwein, durchsuchte, „Eins,,\n",
      "Nearest to Er: Erling, soll., zuletzt, Reichtum, lethargisch, durchqueren, umgeschlagen.“, Lust,,\n",
      "Nearest to mit: halben, schmutziges, rutter, Hand, Zugbrücke,, Bollte, Einfluss, ohnen,\n",
      "Nearest to Ihr: „Hatte, mehr,, natürliche, (Hügelgräber),, sah,, suche., Häuserwände, aufging.,\n",
      "Nearest to Sie: Bein., schricht, Truppe, halbe, Neue, (welches?),, hin, begrüßte,\n",
      "Nearest to dem: horchte., Alkove,, Buch, Putzenscheiben, den, Großmutter, seinem, Ungetüm,\n",
      "Nearest to Patrick.: Jakob., Aurelia., Wagenrädern, sprach, gebissen, Marlein., dauern?, Jetzt,,\n",
      "Nearest to Euch: Segeln, gebrauchen., nie, Kunring,, schaukelte, Samurai,, negativen, „Jeder,\n",
      "Nearest to ich: lichter, gestürzt, gemeinsam.“, da, Lehrer, herbei, gehalten, gehört,\n",
      "Nearest to nun: dachten, Fenster., Weg,, Ihr,, verfasst,, Eisentüre., durchschritt, trinkt,\n",
      "Nearest to durch: Augenwinkel, wollen., Neuankömmlinge, entglitt, geschlossenen, Gänge, Bergen., ambition?,\n",
      "Nearest to waren: leicht,, begannen, Karren, entrollte, Könige, Diebe,, junge, Schlecht,\n",
      "Nearest to ihr: zusteht.“, sichern,, keiner, irdene, seltenen, Anteil, Hände,, Ihm,\n",
      "Nearest to auch: climax/resolution, beseitigt, Ullins,, Lederhandschuh., spritzte, „Jan-Gelling,, Göttin,, ganz,\n",
      "Average loss at step  82000 :  2.450908053994179\n",
      "Average loss at step  84000 :  2.446658759713173\n",
      "Average loss at step  86000 :  2.44762835675478\n",
      "Average loss at step  88000 :  2.4417664320170878\n",
      "Average loss at step  90000 :  2.447688959747553\n",
      "Nearest to zum: Straßen, kümmert, decken, nochmals, behielt, commit;, langsamer,, überrascht,\n",
      "Nearest to dann: Norden, ausnehmen,, Seeufer, Tricks,, dazwischen, Oberarm., schwebte., treu,\n",
      "Nearest to über: Zimmertür, Schwung, commit;, Risse,, vorher, Rotwein, durchsuchte, fertigte,\n",
      "Nearest to Er: Erling, soll., zuletzt, Reichtum, umgeschlagen.“, lethargisch, durchqueren, Lust,,\n",
      "Nearest to mit: halben, schmutziges, rutter, Bollte, Einfluss, drängten, geduckt, Zugbrücke,,\n",
      "Nearest to Ihr: „Hatte, natürliche, mehr,, (Hügelgräber),, Schwänze, sah,, suche., Häuserwände,\n",
      "Nearest to Sie: Bein., schricht, Truppe, halbe, Neue, (welches?),, geheimnisvolle, hin,\n",
      "Nearest to dem: horchte., Alkove,, Buch, den, Großmutter, Putzenscheiben, seinem, einem,\n",
      "Nearest to Patrick.: Jakob., Aurelia., sprach, Wagenrädern, gebissen, dauern?, schlecht.“, Marlein.,\n",
      "Nearest to Euch: Segeln, gebrauchen., nie, schaukelte, negativen, Kunring,, Samurai,, „Jeder,\n",
      "Nearest to ich: lichter, gestürzt, gemeinsam.“, herbei, Lehrer, da, Treppen, gehalten,\n",
      "Nearest to nun: dachten, Fenster., Weg,, Ihr,, verfasst,, Eisentüre., durchschritt, laut,\n",
      "Nearest to durch: Augenwinkel, wollen., Neuankömmlinge, entglitt, geschlossenen, Bergen., Gänge, ambition?,\n",
      "Nearest to waren: leicht,, Karren, begannen, Könige, Diebe,, entrollte, Schlecht, Steinplatte,\n",
      "Nearest to ihr: zusteht.“, sichern,, keiner, Anteil, irdene, seltenen, sein, Ihm,\n",
      "Nearest to auch: climax/resolution, beseitigt, Ullins,, spritzte, Lederhandschuh., „Jan-Gelling,, Göttin,, nördliche,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  92000 :  2.4458310819268227\n",
      "Average loss at step  94000 :  2.4433801993131636\n",
      "Average loss at step  96000 :  2.4421831654310227\n",
      "Average loss at step  98000 :  2.4452894805669785\n",
      "Average loss at step  100000 :  2.4348521998226644\n",
      "Nearest to zum: Straßen, kümmert, decken, behielt, nochmals, commit;, langsamer,, überrascht,\n",
      "Nearest to dann: Norden, ausnehmen,, Tricks,, Seeufer, Zugpferden,, treu, dazwischen, Oberarm.,\n",
      "Nearest to über: Zimmertür, Schwung, Risse,, vorher, commit;, ging., durchsuchte, fertigte,\n",
      "Nearest to Er: Erling, soll., Reichtum, umgeschlagen.“, zuletzt, durchqueren, Lust,, lethargisch,\n",
      "Nearest to mit: schmutziges, halben, Bollte, rutter, Zugbrücke,, Einfluss, Hand, geduckt,\n",
      "Nearest to Ihr: „Hatte, natürliche, mehr,, suche., (Hügelgräber),, sah,, Häuserwände, Schwänze,\n",
      "Nearest to Sie: Bein., schricht, Truppe, geheimnisvolle, hin, halbe, Neue, erkundete,\n",
      "Nearest to dem: horchte., Alkove,, Buch, Großmutter, seinem, Putzenscheiben, den, Ungetüm,\n",
      "Nearest to Patrick.: Jakob., Aurelia., sprach, Wagenrädern, gebissen, dauern?, schlecht.“, Marlein.,\n",
      "Nearest to Euch: Segeln, gebrauchen., nie, Kunring,, schaukelte, negativen, Samurai,, „Jeder,\n",
      "Nearest to ich: lichter, gestürzt, gemeinsam.“, Lehrer, da, gehalten, herbei, Treppen,\n",
      "Nearest to nun: dachten, Fenster., Ihr,, Weg,, Eisentüre., verfasst,, durchschritt, Patrick?“,\n",
      "Nearest to durch: Augenwinkel, wollen., Neuankömmlinge, entglitt, Gänge, geschlossenen, Bergen., ambition?,\n",
      "Nearest to waren: leicht,, junge, Diebe,, Karren, begannen, Steinplatte, entrollte, Schlecht,\n",
      "Nearest to ihr: zusteht.“, sichern,, keiner, Anteil, irdene, seltenen, Hände,, Ihm,\n",
      "Nearest to auch: climax/resolution, beseitigt, spritzte, Ullins,, Göttin,, „Jan-Gelling,, Lederhandschuh., nördliche,\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\ufeff' in position 0: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-ddf214bf21fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    314\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/metadata.tsv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m       \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreverse_dictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m   \u001b[1;31m# Save the model for checkpoints.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\envs\\tensorflow\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\ufeff' in position 0: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Basic word2vec example.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "# Give a folder path as an argument with '--log_dir' to save\n",
    "# TensorBoard summaries. Default is a log folder in current directory.\n",
    "current_path = os.path.dirname(os.path.realpath(sys.argv[0]))\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--log_dir',\n",
    "    type=str,\n",
    "    default=os.path.join(current_path, 'log'),\n",
    "    help='The log directory for TensorBoard summaries.')\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "# Create the directory for TensorBoard variables if there is not.\n",
    "if not os.path.exists(FLAGS.log_dir):\n",
    "  os.makedirs(FLAGS.log_dir)\n",
    "\n",
    "# Step 1: Download the data.\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "# pylint: disable=redefined-outer-name\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  local_filename = os.path.join(gettempdir(), filename)\n",
    "  if not os.path.exists(local_filename):\n",
    "    local_filename, _ = urllib.request.urlretrieve(url + filename,\n",
    "                                                   local_filename)\n",
    "  statinfo = os.stat(local_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception('Failed to verify ' + local_filename +\n",
    "                    '. Can you get to it with a browser?')\n",
    "  return local_filename\n",
    "\n",
    "#filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    #with zipfile.ZipFile(filename) as f:\n",
    "    #data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    #data = tf.compat.as_str(f.read('BS-Packi-12Feb18.txt')).split()\n",
    "    data = f.read().split()\n",
    "    return data\n",
    "\n",
    "# BSC insert read file here\n",
    "def read1k():\n",
    "    return f.read(1024)\n",
    "\n",
    "def process_data(chunk, text):\n",
    "    #print('processed: {0}'.format(ctr))\n",
    "    text.append(str(chunk, errors='ignore')) # 'utf8' codec can't decode byte 0xc3, Python 3 renamed the unicode type to str, the old str type has been replaced by bytes\n",
    "\n",
    "os.chdir(r'C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "f = open('BS-Packi-12Feb18.txt', encoding=\"utf8\")\n",
    "#text = []\n",
    "#for piece in iter(read1k, ''):\n",
    "    #process_data(piece, text)\n",
    "\n",
    "#vocabulary = read_data(filename)\n",
    "vocabulary = read_data(filename)\n",
    "print('Data size', len(vocabulary))\n",
    "\n",
    "# Step 2: Build the dictionary and replace rare words with UNK token.\n",
    "vocabulary_size = 5000 # was 50000, causes key error\n",
    "\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "  \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    index = dictionary.get(word, 0)\n",
    "    if index == 0:  # dictionary['UNK']\n",
    "      unk_count += 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "  return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "\n",
    "# Filling 4 global variables:\n",
    "# data - list of codes (integers from 0 to vocabulary_size-1).\n",
    "#   This is the original text but words are replaced by their codes\n",
    "# count - map of words(strings) to count of occurrences\n",
    "# dictionary - map of words(strings) to their codes(integers)\n",
    "# reverse_dictionary - maps codes(integers) to words(strings)\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(\n",
    "    vocabulary, vocabulary_size)\n",
    "del vocabulary  # Hint to reduce memory.\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "\n",
    "data_index = 0\n",
    "\n",
    "\n",
    "# Step 3: Function to generate a training batch for the skip-gram model.\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)  # pylint: disable=redefined-builtin\n",
    "  if data_index + span > len(data):\n",
    "    data_index = 0\n",
    "  buffer.extend(data[data_index:data_index + span])\n",
    "  data_index += span\n",
    "  for i in range(batch_size // num_skips):\n",
    "    context_words = [w for w in range(span) if w != skip_window]\n",
    "    words_to_use = random.sample(context_words, num_skips)\n",
    "    for j, context_word in enumerate(words_to_use):\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "    if data_index == len(data):\n",
    "      buffer.extend(data[0:span])\n",
    "      data_index = span\n",
    "    else:\n",
    "      buffer.append(data[data_index])\n",
    "      data_index += 1\n",
    "  # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "  data_index = (data_index + len(data) - span) % len(data)\n",
    "  return batch, labels\n",
    "\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "  print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0],\n",
    "        reverse_dictionary[labels[i, 0]])\n",
    "\n",
    "# Step 4: Build and train a skip-gram model.\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1  # How many words to consider left and right.\n",
    "num_skips = 2  # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64  # Number of negative examples to sample.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. These 3 variables are used only for\n",
    "# displaying model accuracy, they don't affect calculation.\n",
    "valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  with tf.name_scope('inputs'):\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "  # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "  with tf.device('/cpu:0'):\n",
    "    # Look up embeddings for inputs.\n",
    "    with tf.name_scope('embeddings'):\n",
    "      embeddings = tf.Variable(\n",
    "          tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "      embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    with tf.name_scope('weights'):\n",
    "      nce_weights = tf.Variable(\n",
    "          tf.truncated_normal(\n",
    "              [vocabulary_size, embedding_size],\n",
    "              stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    with tf.name_scope('biases'):\n",
    "      nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Compute the average NCE loss for the batch.\n",
    "  # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "  # time we evaluate the loss.\n",
    "  # Explanation of the meaning of NCE loss:\n",
    "  #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "  with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(\n",
    "            weights=nce_weights,\n",
    "            biases=nce_biases,\n",
    "            labels=train_labels,\n",
    "            inputs=embed,\n",
    "            num_sampled=num_sampled,\n",
    "            num_classes=vocabulary_size))\n",
    "\n",
    "  # Add the loss value as a scalar to summary.\n",
    "  tf.summary.scalar('loss', loss)\n",
    "\n",
    "  # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "  with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "  # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n",
    "                                            valid_dataset)\n",
    "  similarity = tf.matmul(\n",
    "      valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "  # Merge all summaries.\n",
    "  merged = tf.summary.merge_all()\n",
    "\n",
    "  # Add variable initializer.\n",
    "  init = tf.global_variables_initializer()\n",
    "\n",
    "  # Create a saver.\n",
    "  saver = tf.train.Saver()\n",
    "\n",
    "# Step 5: Begin training.\n",
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # Open a writer to write summaries.\n",
    "  writer = tf.summary.FileWriter(FLAGS.log_dir, session.graph)\n",
    "\n",
    "  # We must initialize all variables before we use them.\n",
    "  init.run()\n",
    "  print('Initialized')\n",
    "\n",
    "  average_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch(batch_size, num_skips,\n",
    "                                                skip_window)\n",
    "    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "    # Define metadata variable.\n",
    "    run_metadata = tf.RunMetadata()\n",
    "\n",
    "    # We perform one update step by evaluating the optimizer op (including it\n",
    "    # in the list of returned values for session.run()\n",
    "    # Also, evaluate the merged op to get all summaries from the returned \"summary\" variable.\n",
    "    # Feed metadata variable to session for visualizing the graph in TensorBoard.\n",
    "    _, summary, loss_val = session.run(\n",
    "        [optimizer, merged, loss],\n",
    "        feed_dict=feed_dict,\n",
    "        run_metadata=run_metadata)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    # Add returned summaries to writer in each step.\n",
    "    writer.add_summary(summary, step)\n",
    "    # Add metadata to visualize the graph for the last run.\n",
    "    if step == (num_steps - 1):\n",
    "      writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss /= 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print('Average loss at step ', step, ': ', average_loss)\n",
    "      average_loss = 0\n",
    "\n",
    "    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in xrange(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8  # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "        log_str = 'Nearest to %s:' % valid_word\n",
    "        for k in xrange(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log_str = '%s %s,' % (log_str, close_word)\n",
    "        print(log_str)\n",
    "  final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "  # Write corresponding labels for the embeddings.\n",
    "  with open(FLAGS.log_dir + '/metadata.tsv', 'w') as f:\n",
    "    for i in xrange(vocabulary_size):\n",
    "      f.write(reverse_dictionary[i] + '\\n')\n",
    "\n",
    "  # Save the model for checkpoints.\n",
    "  saver.save(session, os.path.join(FLAGS.log_dir, 'model.ckpt'))\n",
    "\n",
    "  # Create a configuration for visualizing embeddings with the labels in TensorBoard.\n",
    "  config = projector.ProjectorConfig()\n",
    "  embedding_conf = config.embeddings.add()\n",
    "  embedding_conf.tensor_name = embeddings.name\n",
    "  embedding_conf.metadata_path = os.path.join(FLAGS.log_dir, 'metadata.tsv')\n",
    "  projector.visualize_embeddings(writer, config)\n",
    "\n",
    "writer.close()\n",
    "\n",
    "# Step 6: Visualize the embeddings.\n",
    "\n",
    "\n",
    "# pylint: disable=missing-docstring\n",
    "# Function to draw visualization of distance between embeddings.\n",
    "def plot_with_labels(low_dim_embs, labels, filename):\n",
    "  assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "  plt.figure(figsize=(18, 18))  # in inches\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy=(x, y),\n",
    "        xytext=(5, 2),\n",
    "        textcoords='offset points',\n",
    "        ha='right',\n",
    "        va='bottom')\n",
    "\n",
    "  plt.savefig(filename)\n",
    "\n",
    "\n",
    "try:\n",
    "  # pylint: disable=g-import-not-at-top\n",
    "  from sklearn.manifold import TSNE\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  tsne = TSNE(\n",
    "      perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "  plot_only = 500\n",
    "  low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "  labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "  plot_with_labels(low_dim_embs, labels, os.path.join(gettempdir(), 'tsne.png'))\n",
    "\n",
    "except ImportError as ex:\n",
    "  print('Please install sklearn, matplotlib, and scipy to show embeddings.')\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('03', 0.6798624856996945), ('11', 0.6155051569724165), ('kinder', 0.5713145965480997), ('tn', 0.5678680778967423), ('hs', 0.5448660710326015), ('hs', 0.5416014438956638), ('remission_t2t', 0.5335713278578653), ('stempel', 0.5223053902021395), ('cost', 0.5215182936343506), ('krankenschwestern', 0.5143415927602053), ('eims', 0.5127097335639982), ('hs', 0.5108868109728149), ('pflegern', 0.5049962770473483), ('astrid', 0.48067007897623854), ('mri', 0.4781193040153952), ('rkisch', 0.4756856095717724), ('2018', 0.4740490841198874), ('remsima', 0.4715152201279725), ('ga_affiliate', 0.46902719639440293), ('zugewiesenen', 0.4653826877301448), ('daten', 0.46194202856014616), ('esprit', 0.46194202856014616), ('hs', 0.45905941492013985), ('03', 0.4586976005767887), ('humirabewilligung', 0.4575211045932022), ('niedergel', 0.45696806359051007), ('gelenksultraschallkurs', 0.452527433963335), ('cicha', 0.45004373264925485), ('vorstellung', 0.445079946045802), ('cco', 0.4440374739900925), ('puchner', 0.441514114052536), ('07', 0.43544887347149297), ('zugewiesenen', 0.43490753883168987), ('db', 0.432407733049092), ('wahlarztordi', 0.42766772649001755), ('dgim', 0.42742944592359067), ('wochen', 0.4252916779546518), ('migschitz', 0.4221753056581818), ('teilnehmer', 0.42186292078022364), ('wunschkongress', 0.42010384152311947), ('fall', 0.41983637357599485), ('hamburg', 0.41963116789123794), ('elektronisches', 0.4184696259128027), ('letztem', 0.4177114413179611), ('kollegin', 0.41654633045207834), ('ev', 0.4142701429134893), ('rheumasprechtag', 0.4118041880380506), ('07', 0.4115659615972897), ('eahp', 0.4107429782447632), ('04', 0.4096010241001752)]\n"
     ]
    }
   ],
   "source": [
    "# read text file and compute tf-idf\n",
    "import os\n",
    "import math\n",
    "import re # regex\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def read1k():\n",
    "    return f.read(1024)\n",
    "\n",
    "def process_data(chunk, text):\n",
    "    #print('processed: {0}'.format(ctr))\n",
    "    #text.append(unicode(chunk, errors='ignore')) # 'utf8' codec can't decode byte 0xc3\n",
    "    text.append(chunk) # 'utf8' codec can't decode byte 0xc3 - python 3: unicode to str\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # read corpus\n",
    "    os.chdir(r'C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "    f = open('vcpos0.txt', encoding=\"utf8\")\n",
    "    text = []\n",
    "    for piece in iter(read1k, ''):\n",
    "        process_data(piece, text)\n",
    "    \n",
    "    corpus = []\n",
    "    for elem in text:\n",
    "        #corpus += elem.split() # split at whitespaces\n",
    "        corpus += elem.splitlines() # split at newline\n",
    "    corpus = [t.lower() for t in corpus] # convert to lower case\n",
    "    \n",
    "    # remove special characters\n",
    "    corpus = [re.sub(r'[^\\w]', ' ', t) for t in corpus]\n",
    "    # remove digits\n",
    "    corpus = [re.sub(r'\\b\\d+\\b', '', t) for t in corpus]\n",
    "    \n",
    "    #corpus = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "    #corpus = [\"Der schnelle braune Fuchs springt über.\"]\n",
    "    #print(corpus)\n",
    "    \n",
    "    # Term Frequency: This summarizes how often a given word appears within a document.\n",
    "    # Inverse Document Frequency: This downscales words that appear a lot across documents.\n",
    "    #text = [\"The quick brown fox jumped over the lazy dog.\",\"The dog.\",\"The fox\"]\n",
    "    # create the transform\n",
    "    vectorizer = TfidfVectorizer(smooth_idf=True,\n",
    "                             ngram_range = (1,1))\n",
    "    # tokenize and build vocabulary\n",
    "    #vectorizer.fit(corpus)\n",
    "    # summarize\n",
    "    #print(vectorizer.vocabulary_)\n",
    "    #print(vectorizer.idf_) # idf=inverse document frequencies: the most frequent word \"the\" is assigned the lowest frequency, 1\n",
    "    # encode document (as a sparse matrix, scores are normalized)\n",
    "    #vector = vectorizer.transform(corpus)\n",
    "    tfidf_matrix =  vectorizer.fit_transform(text)\n",
    "    #tfidf_matrix = vectorizer.transform(corpus)\n",
    "    # summarize encoded vector\n",
    "    #print(vector.shape) # shape = [n_samples, n_features], will be (3,8) here\n",
    "    #print(vector.toarray())\n",
    "    #print(vectorizer.get_feature_names())\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    wordscorelst = []\n",
    "    #doc = 0\n",
    "    for doc in range(tfidf_matrix.shape[0]):\n",
    "        feature_index = tfidf_matrix[doc,:].nonzero()[1]\n",
    "        #print(tfidf_matrix.nonzero())\n",
    "        #print(tfidf_matrix.shape)\n",
    "        #print(tfidf_matrix[1:10])\n",
    "        #print(type(tfidf_matrix))\n",
    "        #print(tfidf_matrix.toarray())\n",
    "        #print(tfidf_matrix[doc,:])\n",
    "        #print(feature_index)\n",
    "        tfidf_scores = zip(feature_index, [tfidf_matrix[doc, x] for x in feature_index]) # zip: x = [1, 2], y = [4, 5] zipped: [(1, 4), (2, 5)]\n",
    "        #print([tfidf_matrix[doc, x] for x in feature_index])\n",
    "        #print(doc)\n",
    "        for w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:\n",
    "            #print(w, s)\n",
    "            wordscorelst.append((w,s))\n",
    "    wordscorelst.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    print(wordscorelst[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfkernel",
   "language": "python",
   "name": "tfkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
