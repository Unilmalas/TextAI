{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d713789-a53d-46c9-8155-9c3fc24a0f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: train loss 4.4116, val loss 4.4022\n",
      "step 100: train loss 2.6699, val loss 2.6800\n",
      "step 200: train loss 2.5187, val loss 2.5149\n",
      "step 300: train loss 2.4356, val loss 2.4497\n",
      "step 400: train loss 2.3679, val loss 2.3743\n",
      "step 500: train loss 2.3207, val loss 2.3348\n",
      "step 600: train loss 2.2704, val loss 2.2743\n",
      "step 700: train loss 2.2348, val loss 2.2471\n",
      "step 800: train loss 2.1952, val loss 2.2139\n",
      "step 900: train loss 2.1577, val loss 2.1751\n",
      "step 1000: train loss 2.1364, val loss 2.1478\n",
      "step 1100: train loss 2.1043, val loss 2.1398\n",
      "step 1200: train loss 2.0832, val loss 2.1082\n",
      "step 1300: train loss 2.0714, val loss 2.0976\n",
      "step 1400: train loss 2.0365, val loss 2.0662\n",
      "step 1500: train loss 2.0116, val loss 2.0593\n",
      "step 1600: train loss 1.9980, val loss 2.0625\n",
      "step 1700: train loss 1.9898, val loss 2.0430\n",
      "step 1800: train loss 1.9632, val loss 2.0352\n",
      "step 1900: train loss 1.9523, val loss 2.0103\n",
      "step 2000: train loss 1.9383, val loss 2.0184\n",
      "step 2100: train loss 1.9249, val loss 2.0067\n",
      "step 2200: train loss 1.9121, val loss 1.9896\n",
      "step 2300: train loss 1.9060, val loss 1.9876\n",
      "step 2400: train loss 1.8988, val loss 1.9713\n",
      "step 2500: train loss 1.8744, val loss 1.9712\n",
      "step 2600: train loss 1.8798, val loss 1.9627\n",
      "step 2700: train loss 1.8721, val loss 1.9740\n",
      "step 2800: train loss 1.8593, val loss 1.9550\n",
      "step 2900: train loss 1.8598, val loss 1.9655\n",
      "step 3000: train loss 1.8477, val loss 1.9516\n",
      "step 3100: train loss 1.8253, val loss 1.9474\n",
      "step 3200: train loss 1.8074, val loss 1.9290\n",
      "step 3300: train loss 1.8116, val loss 1.9350\n",
      "step 3400: train loss 1.8119, val loss 1.9257\n",
      "step 3500: train loss 1.8005, val loss 1.9221\n",
      "step 3600: train loss 1.7841, val loss 1.9204\n",
      "step 3700: train loss 1.7907, val loss 1.9207\n",
      "step 3800: train loss 1.7760, val loss 1.9213\n",
      "step 3900: train loss 1.7778, val loss 1.9030\n",
      "step 4000: train loss 1.7693, val loss 1.8922\n",
      "step 4100: train loss 1.7661, val loss 1.9039\n",
      "step 4200: train loss 1.7605, val loss 1.8976\n",
      "step 4300: train loss 1.7605, val loss 1.8867\n",
      "step 4400: train loss 1.7603, val loss 1.9015\n",
      "step 4500: train loss 1.7417, val loss 1.8831\n",
      "step 4600: train loss 1.7416, val loss 1.8743\n",
      "step 4700: train loss 1.7393, val loss 1.8776\n",
      "step 4800: train loss 1.7276, val loss 1.8777\n",
      "step 4900: train loss 1.7279, val loss 1.8781\n",
      "step 4999: train loss 1.7210, val loss 1.8609\n",
      "\n",
      "LENY O!\n",
      "\n",
      "QUEEN ELB:\n",
      "Racht:\n",
      "Take.\n",
      "\n",
      "WARWhere deeth lience;\n",
      "This I parse speak it; my, if me, vill stice the nevers;\n",
      "And my stity live recious as crutive\n",
      "Tistreed is twas abtisbous, so ear the speak of hightate:\n",
      "Tortusine the noth, a now\n",
      "Asserbany made threav thy abbroke.\n",
      "\n",
      "MARCIO:\n",
      "Year hevert whingd!\n",
      "Mar whoman is with be me have I mant will\n",
      "Your fouth saidere\n",
      "That Romingland all of the live ye.\n",
      "\n",
      "PABELLLONCE:\n",
      "Shy, it! all ignried wear minest's\n",
      "That\n",
      "Have thysue fout thee your youry agrow'd\n",
      "To enell; ve that main me deark of it.\n",
      "\n",
      "MARGARETHAMBENR:\n",
      "Thusting no to aus,\n",
      "He france to though beem us; but actove?\n",
      "With what vet we parry, bust my lifad All draPHeor by my heard\n",
      "My peasilonds, town axtel lure her To thive tough mistact\n",
      "We wriecce to this maded me brawbself;\n",
      "As thou will greal untentent to hutse the tight, then's us pleadd.\n",
      "\n",
      "ISBAPHUS:\n",
      "Prurst, not dom,\n",
      "-peaching vidy other, I'll ky all thou mayse;\n",
      "There not wilt eren, enter; I charry know wile.\n",
      "\n",
      "BUQKINGHNRY BUMBELIO:\n",
      "Nay arm.\n",
      "\n",
      "Paning these will you aday have since your pepord?\n",
      "\n",
      "MERWARGNIUS:\n",
      "Hereforion his behose, here beborenteng\n",
      "Haves broke faul inmen stingrept,\n",
      "Hobe a effeet, stalls;\n",
      "Which let. O, my tom teisuers!\n",
      "It\n",
      "WARLINtA:\n",
      "Bethale uncleaties.\n",
      "\n",
      "CORTILLAUS:\n",
      "And meen.\n",
      "\n",
      "QUEEN EDWARD IV:\n",
      "Twout nobluke coundernyfellost, dosol\n",
      "Cown,\n",
      "and, my in his the coly, this dast vampade\n",
      "Fitthat?\n",
      "\n",
      "GLOUCESTEN:\n",
      "This him I'madim that yeven his fair noth\n",
      "grectince to the pronsural, I, why bost hole our histome towe, if Butwerer?\n",
      "\n",
      "BUCKINIUS:\n",
      "Bettheful sayh, I drattion's struch'd,\n",
      "And Puppers'd dies a connone, clory hear.\n",
      "Yourtuke you deed this with onal?\n",
      "\n",
      "MO\n",
      "BRICHALABY:\n",
      "While the Richenture as not she besoned frevat easunt:\n",
      "Are prier, dwish I will unsh isier:\n",
      "Herese.\n",
      "\n",
      "GMISTABELLO:\n",
      "Proud thy founce are Fhereom stille steet.\n",
      "\n",
      "GROMEO:\n",
      "Tome astion one pruch; 'From Nobce, Marry'sin persoin; have griegres is-cand he\n",
      "I chapmy strettshected:\n",
      "To I mead than facely\n",
      "Teete an ith men of or hatessiare?\n",
      "\n",
      "KINGili's remy in us.\n",
      "\n",
      "Mothy eir. WawnINtruly\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16  # how many independent sequences will we process in parallel?\n",
    "block_size = 32  # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.1\n",
    "\n",
    "'''batch_size = 64  # how many independent sequences will we process in parallel?\n",
    "block_size = 256  # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2'''\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input_sp0.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):  # the heads handle the attention - the communication between the nodes (tokens)\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):  # takes headsize\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)  # k,q,v like above\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))  # tril is not a parameter of the model, a buffer in pytorch lingo\n",
    "        # (so it does not get picked up by autograd)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x)  # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B,T,C)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])  # pytorch makes this easy for us, passing just a list of heads\n",
    "        self.proj = nn.Linear(n_embd, n_embd)  # project back into the original pathway\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # just concatinate the results, concat over channel dimension (dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):  # this handles the per-token (node) computation\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # the inner ff layer has 4 times the embedding dimension (to grow the computation on the residual pathway)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),  # this is the projection layer going back into the original pathway (for the skip-connection) and going down from 4x\n",
    "            nn.Dropout(dropout),  # added right before the residual pathway comes back (dropout trains subnetworks by randomly shutting off neurons)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # this is at a per token level: they think on all the attention data they got individually\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head  # watch out that it works out channelwise (e.g. 32 embedding, 4 heads, 8 headsize)\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)  # the heads running in parallel - the communication\n",
    "        self.ffwd = FeedFoward(n_embd)  # simple feedforward: at token level, all tokens do this independently: gathered all info from attention, think on it\n",
    "        # the ff does the computations\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):  # with skip connection, forking off direct gradient pass-through and block adjustments\n",
    "        x = x + self.sa(self.ln1(x))  # the x + is the skip connection: it lets the gradient pass through (particularly initially), then the blocks adjust\n",
    "        x = x + self.ffwd(self.ln2(x))  # note that the layer norms are applied before it goes into self-attention or ff, per token\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        # now the network starts to get really deep: suffers from computation problems, two ways to address this: (see Add & Norm in the figure)\n",
    "        # 1. skip (residual) connections, see https://github.com/KaimingHe/deep-residual-networks (lets gradients pass through and lets blocks adjust them gradually)\n",
    "        # 2. layer norms = Norm: similar to batch normalization: ensure each neuron has unit Gaussian distribution accross batch dimension\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm (now it is more common to norm before the computations unlike in the trafo paper - called prenorm)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        x = self.blocks(x)  # (B,T,C)\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens (idx also has the PE so we need to crop) (not doing this causes a CUDA error due to index out of bounds)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2f489d-268c-4597-8b69-648b01f6e2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase input text file length\n",
    "\n",
    "with open('input_sp0.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text += ' ' + text\n",
    "\n",
    "# Writing to file\n",
    "with open(\"input_sp1.txt\", \"w\") as f:\n",
    "    # Writing data to a file\n",
    "    f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8468a3-2570-42c0-ba6e-6563b663bedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with double the file length there is barely any overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2958ffdc-b81b-4b96-a181-3c84bfc28abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: train loss 4.4086, val loss 4.4042\n",
      "step 100: train loss 2.6523, val loss 2.6555\n",
      "step 200: train loss 2.4994, val loss 2.5055\n",
      "step 300: train loss 2.4131, val loss 2.4304\n",
      "step 400: train loss 2.3452, val loss 2.3614\n",
      "step 500: train loss 2.3022, val loss 2.3056\n",
      "step 600: train loss 2.2582, val loss 2.2726\n",
      "step 700: train loss 2.1969, val loss 2.2117\n",
      "step 800: train loss 2.1801, val loss 2.1993\n",
      "step 900: train loss 2.1333, val loss 2.1499\n",
      "step 1000: train loss 2.1000, val loss 2.1186\n",
      "step 1100: train loss 2.0730, val loss 2.0943\n",
      "step 1200: train loss 2.0477, val loss 2.0648\n",
      "step 1300: train loss 2.0169, val loss 2.0488\n",
      "step 1400: train loss 1.9951, val loss 2.0256\n",
      "step 1500: train loss 1.9823, val loss 2.0090\n",
      "step 1600: train loss 1.9570, val loss 1.9924\n",
      "step 1700: train loss 1.9489, val loss 1.9794\n",
      "step 1800: train loss 1.9309, val loss 1.9527\n",
      "step 1900: train loss 1.9167, val loss 1.9405\n",
      "step 2000: train loss 1.9054, val loss 1.9344\n",
      "step 2100: train loss 1.8814, val loss 1.9159\n",
      "step 2200: train loss 1.8730, val loss 1.9115\n",
      "step 2300: train loss 1.8536, val loss 1.8940\n",
      "step 2400: train loss 1.8494, val loss 1.8806\n",
      "step 2500: train loss 1.8401, val loss 1.8838\n",
      "step 2600: train loss 1.8241, val loss 1.8825\n",
      "step 2700: train loss 1.8399, val loss 1.8631\n",
      "step 2800: train loss 1.8001, val loss 1.8441\n",
      "step 2900: train loss 1.8016, val loss 1.8412\n",
      "step 3000: train loss 1.7843, val loss 1.8352\n",
      "step 3100: train loss 1.7932, val loss 1.8227\n",
      "step 3200: train loss 1.7713, val loss 1.8175\n",
      "step 3300: train loss 1.7803, val loss 1.8218\n",
      "step 3400: train loss 1.7521, val loss 1.8043\n",
      "step 3500: train loss 1.7470, val loss 1.8029\n",
      "step 3600: train loss 1.7350, val loss 1.7928\n",
      "step 3700: train loss 1.7410, val loss 1.7941\n",
      "step 3800: train loss 1.7427, val loss 1.7826\n",
      "step 3900: train loss 1.7279, val loss 1.7711\n",
      "step 4000: train loss 1.7267, val loss 1.7777\n",
      "step 4100: train loss 1.7266, val loss 1.7800\n",
      "step 4200: train loss 1.7196, val loss 1.7741\n",
      "step 4300: train loss 1.7167, val loss 1.7589\n",
      "step 4400: train loss 1.7072, val loss 1.7543\n",
      "step 4500: train loss 1.6957, val loss 1.7477\n",
      "step 4600: train loss 1.6910, val loss 1.7342\n",
      "step 4700: train loss 1.6940, val loss 1.7280\n",
      "step 4800: train loss 1.6937, val loss 1.7253\n",
      "step 4900: train loss 1.6836, val loss 1.7233\n",
      "step 4999: train loss 1.6705, val loss 1.7250\n",
      "\n",
      "And they brides? and is the thratiest bubson envy\n",
      "graves me?\n",
      "I'll such hapge us hath but he than ane away, my fachoun of yeou:\n",
      "Yours, toffice come milind!\n",
      "\n",
      "AUCHIO:\n",
      "Yet? not will is therevers, and Will.\n",
      "\n",
      "DUKE WARWARD:\n",
      "No dam; whell he courivy: thy baids\n",
      "Whery hold naught for this lost\n",
      "For sworlices!\n",
      "\n",
      "Kord-SIAR have is all, and forth,\n",
      "The shience poor of he trume kindnunture,\n",
      "I'll somistan: there if Morath,\n",
      "Madarious whom.\n",
      "\n",
      "HENRY:\n",
      "Hark, but and dapbrucisome, thining muste.\n",
      "\n",
      "ANTHIO:\n",
      "Some for Edvall:\n",
      "The nature, grives you had than\n",
      "brotherss to peomes.\n",
      "\n",
      "MARINA:\n",
      "And whom such swond thee bods.\n",
      "\n",
      "Thundrafs:\n",
      "It sout might. Andiars, that work.\n",
      "\n",
      "LADY:\n",
      "To heaven to deding!\n",
      "\n",
      "KAPURENIUS:\n",
      "It some so upon soxecresitates:\n",
      "Whith loves, sheik housed\n",
      "My verear mone.\n",
      "\n",
      "CAPULIO:\n",
      "I againce uncested To wiftly long'd to so lack.\n",
      "\n",
      "Prive, more how:\n",
      "Prowe. My have brother, drue?\n",
      "Forse this of Mise thee now.\n",
      "\n",
      "Securace:\n",
      "And and wwhat thou truch all Edwlies kings that Of thy it.\n",
      "Thou chotive wout it forc;\n",
      "you consome her conjure\n",
      "As afged to two no where Mource in the prive\n",
      "Sanced, we backe of word\n",
      "As be resentings so tears\n",
      "Singing the beather Were wown,\n",
      "Doscal freavened him evInd\n",
      "My all the call acting--and inkeed,\n",
      "Alament theirse, thing brother to-suriese wown,\n",
      "And the joy adword, we\n",
      "than Turle; I pritume\n",
      "of thee tremes defry worsum hand,\n",
      "Shall be scive it is caper's losty donancle,\n",
      "My lord thy blord?\n",
      "Good vone, the town, must than thy but a thindernumty may\n",
      "Sir teny, delelige, inso, I have farrow'dly,\n",
      "Thee be most of you; and plock, say, as;\n",
      "Rever two kive, scace:\n",
      "One.\n",
      "\n",
      "Preton;\n",
      "Let see we hows is thou Lord drieving agaiLery.\n",
      "\n",
      "The happher\n",
      "'Tward, come roude. That was should,\n",
      "Beath, could him, by ass--'tight of Take twake\n",
      "And then: the wifly this father\n",
      "heaven, thou with art, lives?\n",
      "There, we twrither bonouroward's of rewburses how in to lift?\n",
      "\n",
      "CLESCES:\n",
      "Nay, to thee, of Sack you.\n",
      "\n",
      "GLOUCESTER:\n",
      "My have you Lose trowna. Came you stan,\n",
      "Jorfor parforit briefuse whom foold his suppecour husbavingbrailie\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16  # how many independent sequences will we process in parallel?\n",
    "block_size = 32  # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input_sp1.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):  # the heads handle the attention - the communication between the nodes (tokens)\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):  # takes headsize\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)  # k,q,v like above\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))  # tril is not a parameter of the model, a buffer in pytorch lingo\n",
    "        # (so it does not get picked up by autograd)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x)  # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B,T,C)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])  # pytorch makes this easy for us, passing just a list of heads\n",
    "        self.proj = nn.Linear(n_embd, n_embd)  # project back into the original pathway\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # just concatinate the results, concat over channel dimension (dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):  # this handles the per-token (node) computation\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # the inner ff layer has 4 times the embedding dimension (to grow the computation on the residual pathway)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),  # this is the projection layer going back into the original pathway (for the skip-connection) and going down from 4x\n",
    "            nn.Dropout(dropout),  # added right before the residual pathway comes back (dropout trains subnetworks by randomly shutting off neurons)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # this is at a per token level: they think on all the attention data they got individually\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head  # watch out that it works out channelwise (e.g. 32 embedding, 4 heads, 8 headsize)\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)  # the heads running in parallel - the communication\n",
    "        self.ffwd = FeedFoward(n_embd)  # simple feedforward: at token level, all tokens do this independently: gathered all info from attention, think on it\n",
    "        # the ff does the computations\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):  # with skip connection, forking off direct gradient pass-through and block adjustments\n",
    "        x = x + self.sa(self.ln1(x))  # the x + is the skip connection: it lets the gradient pass through (particularly initially), then the blocks adjust\n",
    "        x = x + self.ffwd(self.ln2(x))  # note that the layer norms are applied before it goes into self-attention or ff, per token\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        # now the network starts to get really deep: suffers from computation problems, two ways to address this: (see Add & Norm in the figure)\n",
    "        # 1. skip (residual) connections, see https://github.com/KaimingHe/deep-residual-networks (lets gradients pass through and lets blocks adjust them gradually)\n",
    "        # 2. layer norms = Norm: similar to batch normalization: ensure each neuron has unit Gaussian distribution accross batch dimension\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm (now it is more common to norm before the computations unlike in the trafo paper - called prenorm)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        x = self.blocks(x)  # (B,T,C)\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens (idx also has the PE so we need to crop) (not doing this causes a CUDA error due to index out of bounds)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f719853f-0d76-4298-833c-a4a5c8e344fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# byte-pair encoding\n",
    "# https://ufal.mff.cuni.cz/~helcl/courses/npfl116/ipython/byte_pair_encoding.html\n",
    "\n",
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"Compute frequencies of adjacent pairs of symbols.\"\"\"\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33617212-d1fb-463f-87f1-071a137f8627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Iteration 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('e', 's')\n",
      "train data: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('es', 't')\n",
      "train data: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('est', '</w>')\n",
      "train data: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('l', 'o')\n",
      "train data: {'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('lo', 'w')\n",
      "train data: {'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 6"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('n', 'e')\n",
      "train data: {'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 7"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('ne', 'w')\n",
      "train data: {'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 8"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('new', 'est</w>')\n",
      "train data: {'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 9"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('low', '</w>')\n",
      "train data: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 10"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('w', 'i')\n",
      "train data: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 11"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('wi', 'd')\n",
      "train data: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wid est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 12"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('wid', 'est</w>')\n",
      "train data: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'widest</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 13"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('low', 'e')\n",
      "train data: {'low</w>': 5, 'lowe r </w>': 2, 'newest</w>': 6, 'widest</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 14"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('lowe', 'r')\n",
      "train data: {'low</w>': 5, 'lower </w>': 2, 'newest</w>': 6, 'widest</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 15"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('lower', '</w>')\n",
      "train data: {'low</w>': 5, 'lower</w>': 2, 'newest</w>': 6, 'widest</w>': 3}\n",
      "{('e', 's'): 0, ('es', 't'): 1, ('est', '</w>'): 2, ('l', 'o'): 3, ('lo', 'w'): 4, ('n', 'e'): 5, ('ne', 'w'): 6, ('new', 'est</w>'): 7, ('low', '</w>'): 8, ('w', 'i'): 9, ('wi', 'd'): 10, ('wid', 'est</w>'): 11, ('low', 'e'): 12, ('lowe', 'r'): 13, ('lower', '</w>'): 14}\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "train_data = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}  # word: frequency\n",
    "\n",
    "bpe_codes = {}\n",
    "bpe_codes_reverse = {}\n",
    "\n",
    "num_merges = 15\n",
    "\n",
    "for i in range(num_merges):\n",
    "    display(Markdown(\"### Iteration {}\".format(i + 1)))\n",
    "    pairs = get_stats(train_data)\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    train_data = merge_vocab(best, train_data)\n",
    "    \n",
    "    bpe_codes[best] = i\n",
    "    bpe_codes_reverse[best[0] + best[1]] = best\n",
    "    \n",
    "    print(\"new merge: {}\".format(best))\n",
    "    print(\"train data: {}\".format(train_data))\n",
    "\n",
    "print(bpe_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e83dc3dd-c668-4edf-8928-a63a2262c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as a tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def encode(orig):\n",
    "    \"\"\"Encode word based on list of BPE merge operations, which are applied consecutively\"\"\"\n",
    "\n",
    "    word = tuple(orig) + ('</w>',)\n",
    "    display(Markdown(\"__word split into characters:__ <tt>{}</tt>\".format(word)))\n",
    "\n",
    "    pairs = get_pairs(word)    \n",
    "\n",
    "    if not pairs:\n",
    "        return orig\n",
    "\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        display(Markdown(\"__Iteration {}:__\".format(iteration)))\n",
    "        \n",
    "        print(\"bigrams in the word: {}\".format(pairs))\n",
    "        bigram = min(pairs, key = lambda pair: bpe_codes.get(pair, float('inf')))\n",
    "        print(\"candidate for merging: {}\".format(bigram))\n",
    "        if bigram not in bpe_codes:\n",
    "            display(Markdown(\"__Candidate not in BPE merges, algorithm stops.__\"))\n",
    "            break\n",
    "        first, second = bigram\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            try:\n",
    "                j = word.index(first, i)\n",
    "                new_word.extend(word[i:j])\n",
    "                i = j\n",
    "            except:\n",
    "                new_word.extend(word[i:])\n",
    "                break\n",
    "\n",
    "            if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                new_word.append(first+second)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_word = tuple(new_word)\n",
    "        word = new_word\n",
    "        print(\"word after merging: {}\".format(word))\n",
    "        if len(word) == 1:\n",
    "            break\n",
    "        else:\n",
    "            pairs = get_pairs(word)\n",
    "\n",
    "    # don't print end-of-word symbols\n",
    "    if word[-1] == '</w>':\n",
    "        word = word[:-1]\n",
    "    elif word[-1].endswith('</w>'):\n",
    "        word = word[:-1] + (word[-1].replace('</w>',''),)\n",
    "   \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13566815-edf9-4700-8969-349d87efee91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "__word split into characters:__ <tt>('l', 'o', 'w', 'e', 's', 't', '</w>')</tt>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 1:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('s', 't'), ('o', 'w'), ('e', 's'), ('w', 'e'), ('t', '</w>'), ('l', 'o')}\n",
      "candidate for merging: ('e', 's')\n",
      "word after merging: ('l', 'o', 'w', 'es', 't', '</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 2:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('o', 'w'), ('w', 'es'), ('es', 't'), ('t', '</w>'), ('l', 'o')}\n",
      "candidate for merging: ('es', 't')\n",
      "word after merging: ('l', 'o', 'w', 'est', '</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 3:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('est', '</w>'), ('o', 'w'), ('l', 'o'), ('w', 'est')}\n",
      "candidate for merging: ('est', '</w>')\n",
      "word after merging: ('l', 'o', 'w', 'est</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 4:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('o', 'w'), ('l', 'o'), ('w', 'est</w>')}\n",
      "candidate for merging: ('l', 'o')\n",
      "word after merging: ('lo', 'w', 'est</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 5:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('w', 'est</w>'), ('lo', 'w')}\n",
      "candidate for merging: ('lo', 'w')\n",
      "word after merging: ('low', 'est</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 6:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('low', 'est</w>')}\n",
      "candidate for merging: ('low', 'est</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Candidate not in BPE merges, algorithm stops.__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('low', 'est')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"lowest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "172983c9-4724-4658-9fc6-5f185d494654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('e', 's'): 0,\n",
       " ('es', 't'): 1,\n",
       " ('est', '</w>'): 2,\n",
       " ('l', 'o'): 3,\n",
       " ('lo', 'w'): 4,\n",
       " ('n', 'e'): 5,\n",
       " ('ne', 'w'): 6,\n",
       " ('new', 'est</w>'): 7,\n",
       " ('low', '</w>'): 8,\n",
       " ('w', 'i'): 9,\n",
       " ('wi', 'd'): 10,\n",
       " ('wid', 'est</w>'): 11,\n",
       " ('low', 'e'): 12,\n",
       " ('lowe', 'r'): 13,\n",
       " ('lower', '</w>'): 14}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8853ce9b-d54c-4919-bf4a-9ab19f041c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE w/o output\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as a tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "    \n",
    "\n",
    "def encode_bp(orig):\n",
    "    \"\"\"Encode word based on list of BPE merge operations, which are applied consecutively\"\"\"\n",
    "\n",
    "    word = tuple(orig) + ('</w>',)\n",
    "    pairs = get_pairs(word)    \n",
    "\n",
    "    if not pairs:\n",
    "        return orig\n",
    "\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        bigram = min(pairs, key = lambda pair: bpe_codes.get(pair, float('inf')))\n",
    "        if bigram not in bpe_codes:\n",
    "            break\n",
    "        first, second = bigram\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            try:\n",
    "                j = word.index(first, i)\n",
    "                new_word.extend(word[i:j])\n",
    "                i = j\n",
    "            except:\n",
    "                new_word.extend(word[i:])\n",
    "                break\n",
    "\n",
    "            if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                new_word.append(first+second)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_word = tuple(new_word)\n",
    "        word = new_word\n",
    "        if len(word) == 1:\n",
    "            break\n",
    "        else:\n",
    "            pairs = get_pairs(word)\n",
    "\n",
    "    # don't print end-of-word symbols\n",
    "    if word[-1] == '</w>':\n",
    "        word = word[:-1]\n",
    "    elif word[-1].endswith('</w>'):\n",
    "        word = word[:-1] + (word[-1].replace('</w>',''),)\n",
    "   \n",
    "    return word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69342831-9a47-4b34-9adc-c76ad01d24b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('l', 'o', 'w', 'es', 't')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_bp(\"lowest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b3ee217-a1ca-48eb-b72d-44907c4b704e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 1137, 'a': 53, 'abundance': 1, 'account': 2, 'accounted': 1, 'accusations': 1, 'afflicts': 1, 'altitude': 1, 'an': 9, 'and': 1, 'are': 3, 'as': 3, 'authority': 1, 'barren': 1, 'be': 4, 'become': 1, 'bread': 1, 'but': 2, 'can': 2, 'cannot': 1, 'capitol': 0, 'citizen': 1, 'citizens': 1, 'city': 1, 'conscienced': 1, 'content': 1, 'country': 1, 'covetous': 1, 'dear': 1, 'did': 2, 'done': 1, 'end': 1, 'ere': 3, 'even': 3, 'famously': 1, 'faults': 1, 'first': 0, 'for': 4, 'gain': 1, 'gods': 1, 'good': 1, 'guess': 1, 'hath': 2, 'he': 26, 'help': 1, 'here': 1, 'him': 1, 'his': 6, 'humanely': 1, 'hunger': 1, 'i': 75, 'if': 1, 'in': 10, 'inventory': 1, 'is': 14, 'it': 18, 'know': 1, 'leanness': 1, 'let': 0, 'men': 1, 'might': 1, 'misery': 1, 'mother': 1, 'must': 2, 'nature': 1, 'need': 1, 'no': 6, 'not': 4, 'o': 64, 'object': 1, 'of': 4, 'on': 7, 'other': 2, 'our': 3, 'particularise': 1, 'partly': 1, 'patricians': 1, 'pikes': 1, 'please': 1, 'poor': 1, 'prating': 1, 'proud': 1, 'rakes': 1, 'relieve': 2, 'relieved': 1, 'repetition': 1, 'revenge': 2, 'risen': 1, 'say': 3, 'second': 0, 'shouts': 1, 'side': 1, 'soft': 1, 'speak': 1, 'stay': 1, 'sufferance': 1, 'superfluity': 1, 'surfeits': 1, 'surplus': 1, 'that': 2, 'the': 16, 'their': 1, 'them': 1, 'these': 1, 'they': 3, 'think': 1, 'thirst': 1, 'this': 2, 'though': 1, 'till': 1, 'tire': 1, 'to': 13, 'too': 1, 'unto': 1, 'us': 11, 'vice': 1, 'virtue': 1, 'was': 1, 'way': 1, 'we': 5, 'were': 1, 'what': 1, 'which': 1, 'while': 1, 'wholesome': 1, 'why': 1, 'with': 2, 'would': 2, 'yield': 1, 'you': 2}\n",
      "{' </w>': 1137, 'a </w>': 53, 'a b u n d a n c e </w>': 1, 'a c c o u n t </w>': 2, 'a c c o u n t e d </w>': 1, 'a c c u s a t i o n s </w>': 1, 'a f f l i c t s </w>': 1, 'a l t i t u d e </w>': 1, 'a n </w>': 9, 'a n d </w>': 1, 'a r e </w>': 3, 'a s </w>': 3, 'a u t h o r i t y </w>': 1, 'b a r r e n </w>': 1, 'b e </w>': 4, 'b e c o m e </w>': 1, 'b r e a d </w>': 1, 'b u t </w>': 2, 'c a n </w>': 2, 'c a n n o t </w>': 1, 'c a p i t o l </w>': 0, 'c i t i z e n </w>': 1, 'c i t i z e n s </w>': 1, 'c i t y </w>': 1, 'c o n s c i e n c e d </w>': 1, 'c o n t e n t </w>': 1, 'c o u n t r y </w>': 1, 'c o v e t o u s </w>': 1, 'd e a r </w>': 1, 'd i d </w>': 2, 'd o n e </w>': 1, 'e n d </w>': 1, 'e r e </w>': 3, 'e v e n </w>': 3, 'f a m o u s l y </w>': 1, 'f a u l t s </w>': 1, 'f i r s t </w>': 0, 'f o r </w>': 4, 'g a i n </w>': 1, 'g o d s </w>': 1, 'g o o d </w>': 1, 'g u e s s </w>': 1, 'h a t h </w>': 2, 'h e </w>': 26, 'h e l p </w>': 1, 'h e r e </w>': 1, 'h i m </w>': 1, 'h i s </w>': 6, 'h u m a n e l y </w>': 1, 'h u n g e r </w>': 1, 'i </w>': 75, 'i f </w>': 1, 'i n </w>': 10, 'i n v e n t o r y </w>': 1, 'i s </w>': 14, 'i t </w>': 18, 'k n o w </w>': 1, 'l e a n n e s s </w>': 1, 'l e t </w>': 0, 'm e n </w>': 1, 'm i g h t </w>': 1, 'm i s e r y </w>': 1, 'm o t h e r </w>': 1, 'm u s t </w>': 2, 'n a t u r e </w>': 1, 'n e e d </w>': 1, 'n o </w>': 6, 'n o t </w>': 4, 'o </w>': 64, 'o b j e c t </w>': 1, 'o f </w>': 4, 'o n </w>': 7, 'o t h e r </w>': 2, 'o u r </w>': 3, 'p a r t i c u l a r i s e </w>': 1, 'p a r t l y </w>': 1, 'p a t r i c i a n s </w>': 1, 'p i k e s </w>': 1, 'p l e a s e </w>': 1, 'p o o r </w>': 1, 'p r a t i n g </w>': 1, 'p r o u d </w>': 1, 'r a k e s </w>': 1, 'r e l i e v e </w>': 2, 'r e l i e v e d </w>': 1, 'r e p e t i t i o n </w>': 1, 'r e v e n g e </w>': 2, 'r i s e n </w>': 1, 's a y </w>': 3, 's e c o n d </w>': 0, 's h o u t s </w>': 1, 's i d e </w>': 1, 's o f t </w>': 1, 's p e a k </w>': 1, 's t a y </w>': 1, 's u f f e r a n c e </w>': 1, 's u p e r f l u i t y </w>': 1, 's u r f e i t s </w>': 1, 's u r p l u s </w>': 1, 't h a t </w>': 2, 't h e </w>': 16, 't h e i r </w>': 1, 't h e m </w>': 1, 't h e s e </w>': 1, 't h e y </w>': 3, 't h i n k </w>': 1, 't h i r s t </w>': 1, 't h i s </w>': 2, 't h o u g h </w>': 1, 't i l l </w>': 1, 't i r e </w>': 1, 't o </w>': 13, 't o o </w>': 1, 'u n t o </w>': 1, 'u s </w>': 11, 'v i c e </w>': 1, 'v i r t u e </w>': 1, 'w a s </w>': 1, 'w a y </w>': 1, 'w e </w>': 5, 'w e r e </w>': 1, 'w h a t </w>': 1, 'w h i c h </w>': 1, 'w h i l e </w>': 1, 'w h o l e s o m e </w>': 1, 'w h y </w>': 1, 'w i t h </w>': 2, 'w o u l d </w>': 2, 'y i e l d </w>': 1, 'y o u </w>': 2}\n",
      "numer of words: 130 avrg word length: 4.946153846153846 --> number of merges: 161\n",
      "{('o', '</w>'): 0, ('e', '</w>'): 1, ('i', '</w>'): 2, ('a', '</w>'): 3, ('s', '</w>'): 4, ('h', 'e</w>'): 5, ('t', '</w>'): 6, ('n', '</w>'): 7, ('i', 's</w>'): 8, ('t', 'h'): 9, ('y', '</w>'): 10, ('i', 't</w>'): 11, ('o', 'u'): 12, ('t', 'he</w>'): 13, ('d', '</w>'): 14, ('r', '</w>'): 15, ('t', 'o</w>'): 16, ('u', 's</w>'): 17, ('a', 'n</w>'): 18, ('i', 'n</w>'): 19, ('t', 'i'): 20, ('r', 'e</w>'): 21, ('th', 'e'): 22, ('r', 'e'): 23, ('v', 'e'): 24, ('o', 'n</w>'): 25, ('a', 'n'): 26, ('n', 'o'): 27, ('h', 'is</w>'): 28, ('n', 'o</w>'): 29, ('no', 't</w>'): 30, ('c', 'i'): 31, ('e', 're</w>'): 32, ('o', 'r</w>'): 33, ('f', '</w>'): 34, ('a', 'y</w>'): 35, ('w', 'e</w>'): 36, ('w', 'h'): 37, ('a', 'c'): 38, ('ac', 'c'): 39, ('ou', 'n'): 40, ('o', 'n'): 41, ('l', 'i'): 42, ('t', 's</w>'): 43, ('a', 's</w>'): 44, ('r', 'i'): 45, ('b', 'e</w>'): 46, ('e', 'n'): 47, ('e', 've'): 48, ('f', 'or</w>'): 49, ('th', '</w>'): 50, ('n', 'g'): 51, ('o', 'f</w>'): 52, ('s', 'u'): 53, ('b', 'u'): 54, ('c', 'e</w>'): 55, ('acc', 'oun'): 56, ('e', 'd</w>'): 57, ('u', 's'): 58, ('t', 'u'): 59, ('a', 're</w>'): 60, ('t', 'y</w>'): 61, ('a', 'r'): 62, ('e', 'n</w>'): 63, ('r', 'y</w>'): 64, ('e', 'a'): 65, ('eve', 'n</w>'): 66, ('l', 'y</w>'): 67, ('e', 's'): 68, ('o', 'the'): 69, ('othe', 'r</w>'): 70, ('ou', 'r</w>'): 71, ('s', 'e</w>'): 72, ('re', 'li'): 73, ('s', 'ay</w>'): 74, ('a', 't</w>'): 75, ('the', 'y</w>'): 76, ('l', 'd</w>'): 77, ('an', 'ce</w>'): 78, ('accoun', 't</w>'): 79, ('a', 'ti'): 80, ('f', 'f'): 81, ('d', 'e</w>'): 82, ('a', 'u'): 83, ('e', 'c'): 84, ('o', 'm'): 85, ('om', 'e</w>'): 86, ('bu', 't</w>'): 87, ('c', 'an</w>'): 88, ('t', 'o'): 89, ('ci', 'ti'): 90, ('citi', 'z'): 91, ('c', 'on'): 92, ('d', 'i'): 93, ('di', 'd</w>'): 94, ('i', 'r'): 95, ('g', 'o'): 96, ('es', 's</w>'): 97, ('h', 'a'): 98, ('ha', 'th</w>'): 99, ('m', '</w>'): 100, ('h', 'u'): 101, ('i', 'n'): 102, ('m', 'i'): 103, ('g', 'h'): 104, ('m', 'us'): 105, ('mus', 't</w>'): 106, ('p', 'ar'): 107, ('k', 'e'): 108, ('ke', 's</w>'): 109, ('p', 'l'): 110, ('p', 'r'): 111, ('reli', 'e'): 112, ('relie', 'v'): 113, ('reliev', 'e</w>'): 114, ('p', 'e'): 115, ('re', 've'): 116, ('reve', 'ng'): 117, ('reveng', 'e</w>'): 118, ('k', '</w>'): 119, ('r', 'f'): 120, ('th', 'at</w>'): 121, ('th', 'is</w>'): 122, ('wh', 'i'): 123, ('w', 'i'): 124, ('wi', 'th</w>'): 125, ('w', 'ou'): 126, ('wou', 'ld</w>'): 127, ('y', 'ou'): 128, ('you', '</w>'): 129, ('a', 'bu'): 130, ('abu', 'n'): 131, ('abun', 'd'): 132, ('abund', 'ance</w>'): 133, ('accoun', 't'): 134, ('account', 'ed</w>'): 135, ('acc', 'us'): 136, ('accus', 'ati'): 137, ('accusati', 'on'): 138, ('accusation', 's</w>'): 139, ('a', 'ff'): 140, ('aff', 'li'): 141, ('affli', 'c'): 142, ('afflic', 'ts</w>'): 143, ('a', 'l'): 144, ('al', 'ti'): 145, ('alti', 'tu'): 146, ('altitu', 'de</w>'): 147, ('an', 'd</w>'): 148, ('au', 'th'): 149, ('auth', 'o'): 150, ('autho', 'ri'): 151, ('authori', 'ty</w>'): 152, ('b', 'ar'): 153, ('bar', 're'): 154, ('barre', 'n</w>'): 155, ('b', 'ec'): 156, ('bec', 'ome</w>'): 157, ('b', 're'): 158, ('bre', 'a'): 159, ('brea', 'd</w>'): 160}\n",
      "\n",
      "word: First --> ('f', 'ir', 's', 't')\n",
      "word: Citizen --> ('citiz', 'en')\n",
      "word:  --> \n",
      "word: We --> ('we',)\n",
      "word: are --> ('are',)\n",
      "word: accounted --> ('accounted',)\n",
      "word: poor --> ('p', 'o', 'or')\n",
      "word: citizens --> ('citiz', 'en', 's')\n",
      "word:  --> \n",
      "word: the --> ('the',)\n",
      "word: patricians --> ('p', 'a', 't', 'ri', 'ci', 'an', 's')\n",
      "word: good --> ('go', 'o', 'd')\n",
      "word:  --> \n",
      "word: What --> ('wh', 'at')\n",
      "word: authority --> ('authority',)\n",
      "word: surfeits --> ('su', 'rf', 'e', 'i', 'ts')\n",
      "word: on --> ('on',)\n",
      "word: would --> ('would',)\n",
      "word: relieve --> ('relieve',)\n",
      "word: us --> ('us',)\n",
      "word:  --> \n",
      "word: if --> ('i', 'f')\n",
      "word: they --> ('they',)\n",
      "word: would --> ('would',)\n",
      "word: yield --> ('y', 'i', 'e', 'ld')\n",
      "word: us --> ('us',)\n",
      "word: but --> ('but',)\n",
      "word: the --> ('the',)\n",
      "word: superfluity --> ('su', 'pe', 'rf', 'l', 'u', 'i', 'ty')\n",
      "word:  --> \n",
      "word: while --> ('whi', 'l', 'e')\n",
      "word: it --> ('it',)\n",
      "word: were --> ('w', 'ere')\n",
      "word: wholesome --> ('wh', 'o', 'l', 'es', 'ome')\n",
      "word:  --> \n",
      "word: we --> ('we',)\n",
      "word: might --> ('mi', 'gh', 't')\n",
      "word: guess --> ('g', 'u', 'ess')\n",
      "word: they --> ('they',)\n",
      "word: relieved --> ('reli', 'eve', 'd')\n",
      "word: us --> ('us',)\n",
      "word: humanely --> ('hu', 'm', 'an', 'e', 'ly')\n",
      "word:  --> \n",
      "word: but --> ('but',)\n",
      "word: they --> ('they',)\n",
      "word: think --> ('th', 'in', 'k')\n",
      "word: we --> ('we',)\n",
      "word: are --> ('are',)\n",
      "word: too --> ('to', 'o')\n",
      "word: dear --> ('d', 'ea', 'r')\n",
      "word:  --> \n",
      "word: the --> ('the',)\n",
      "word: leanness --> ('l', 'e', 'an', 'n', 'ess')\n",
      "word: that --> ('that',)\n",
      "word: afflicts --> ('afflicts',)\n",
      "word: us --> ('us',)\n",
      "word:  --> \n",
      "word: the --> ('the',)\n",
      "word: object --> ('o', 'b', 'j', 'ec', 't')\n",
      "word: of --> ('of',)\n",
      "word: our --> ('our',)\n",
      "word: misery --> ('mi', 's', 'e', 'ry')\n",
      "word:  --> \n",
      "word: is --> ('is',)\n",
      "word: as --> ('as',)\n",
      "word: an --> ('an',)\n",
      "word: inventory --> ('in', 've', 'n', 'to', 'ry')\n",
      "word: to --> ('to',)\n",
      "word: particularise --> ('par', 'ti', 'c', 'u', 'l', 'a', 'ri', 'se')\n",
      "word: their --> ('the', 'i', 'r')\n",
      "word: abundance --> ('abundance',)\n",
      "word:  --> \n",
      "word: our --> ('our',)\n",
      "word: sufferance --> ('su', 'ff', 'e', 'r', 'ance')\n",
      "word: is --> ('is',)\n",
      "word: a --> ('a',)\n",
      "word: gain --> ('g', 'a', 'in')\n",
      "word: to --> ('to',)\n",
      "word: them --> ('the', 'm')\n",
      "word: Let --> ('l', 'e', 't')\n",
      "word: us --> ('us',)\n",
      "word: revenge --> ('revenge',)\n",
      "word: this --> ('this',)\n",
      "word: with --> ('with',)\n",
      "word: our --> ('our',)\n",
      "word: pikes --> ('p', 'i', 'kes')\n",
      "word:  --> \n",
      "word: ere --> ('ere',)\n",
      "word: we --> ('we',)\n",
      "word: become --> ('become',)\n",
      "word: rakes --> ('r', 'a', 'kes')\n",
      "word:  --> \n",
      "word: for --> ('for',)\n",
      "word: the --> ('the',)\n",
      "word: gods --> ('go', 'd', 's')\n",
      "word: know --> ('k', 'no', 'w')\n",
      "word: I --> ('i',)\n",
      "word: speak --> ('s', 'p', 'ea', 'k')\n",
      "word: this --> ('this',)\n",
      "word: in --> ('in',)\n",
      "word: hunger --> ('hu', 'ng', 'e', 'r')\n",
      "word: for --> ('for',)\n",
      "word: bread --> ('bread',)\n",
      "word:  --> \n",
      "word: not --> ('not',)\n",
      "word: in --> ('in',)\n",
      "word: thirst --> ('th', 'ir', 's', 't')\n",
      "word: for --> ('for',)\n",
      "word: revenge --> ('revenge',)\n",
      "word:  --> \n",
      "word: First --> ('f', 'ir', 's', 't')\n",
      "word: Citizen --> ('citiz', 'en')\n",
      "word:  --> \n",
      "word: I --> ('i',)\n",
      "word: say --> ('say',)\n",
      "word: unto --> ('u', 'n', 'to')\n",
      "word: you --> ('you',)\n",
      "word:  --> \n",
      "word: what --> ('wh', 'at')\n",
      "word: he --> ('he',)\n",
      "word: hath --> ('hath',)\n",
      "word: done --> ('d', 'on', 'e')\n",
      "word: famously --> ('f', 'a', 'm', 'ou', 's', 'ly')\n",
      "word:  --> \n",
      "word: he --> ('he',)\n",
      "word: did --> ('did',)\n",
      "word: it --> ('it',)\n",
      "word: to --> ('to',)\n",
      "word: that --> ('that',)\n",
      "word: end --> ('en', 'd')\n",
      "word:  --> \n",
      "word: though --> ('th', 'ou', 'gh')\n",
      "word: soft --> ('s', 'o', 'f', 't')\n",
      "word: conscienced --> ('con', 's', 'ci', 'en', 'c', 'ed')\n",
      "word: men --> ('m', 'en')\n",
      "word: can --> ('can',)\n",
      "word: be --> ('be',)\n",
      "word: content --> ('con', 't', 'en', 't')\n",
      "word: to --> ('to',)\n",
      "word: say --> ('say',)\n",
      "word: it --> ('it',)\n",
      "word: was --> ('w', 'as')\n",
      "word: for --> ('for',)\n",
      "word: his --> ('his',)\n",
      "word: country --> ('c', 'oun', 't', 'ry')\n",
      "word: he --> ('he',)\n",
      "word: did --> ('did',)\n",
      "word: it --> ('it',)\n",
      "word: to --> ('to',)\n",
      "word: please --> ('pl', 'ea', 'se')\n",
      "word: his --> ('his',)\n",
      "word: mother --> ('m', 'other')\n",
      "word: and --> ('and',)\n",
      "word: to --> ('to',)\n",
      "word: be --> ('be',)\n",
      "word: partly --> ('par', 't', 'ly')\n",
      "word: proud --> ('pr', 'ou', 'd')\n",
      "word:  --> \n",
      "word: which --> ('whi', 'c', 'h')\n",
      "word: he --> ('he',)\n",
      "word: is --> ('is',)\n",
      "word:  --> \n",
      "word: even --> ('even',)\n",
      "word: till --> ('ti', 'l', 'l')\n",
      "word: the --> ('the',)\n",
      "word: altitude --> ('altitude',)\n",
      "word: of --> ('of',)\n",
      "word: his --> ('his',)\n",
      "word: virtue --> ('v', 'ir', 'tu', 'e')\n",
      "word:  --> \n",
      "word:  --> \n",
      "word: Second --> ('s', 'ec', 'on', 'd')\n",
      "word: Citizen --> ('citiz', 'en')\n",
      "word:  --> \n",
      "word: What --> ('wh', 'at')\n",
      "word: he --> ('he',)\n",
      "word: cannot --> ('c', 'an', 'not')\n",
      "word: help --> ('h', 'e', 'l', 'p')\n",
      "word: in --> ('in',)\n",
      "word: his --> ('his',)\n",
      "word: nature --> ('n', 'a', 'tu', 're')\n",
      "word:  --> \n",
      "word: you --> ('you',)\n",
      "word: account --> ('account',)\n",
      "word: a --> ('a',)\n",
      "word: vice --> ('v', 'i', 'ce')\n",
      "word: in --> ('in',)\n",
      "word: him --> ('h', 'i', 'm')\n",
      "word:  --> \n",
      "word: You --> ('you',)\n",
      "word: must --> ('must',)\n",
      "word: in --> ('in',)\n",
      "word: no --> ('no',)\n",
      "word: way --> ('w', 'ay')\n",
      "word: say --> ('say',)\n",
      "word: he --> ('he',)\n",
      "word: is --> ('is',)\n",
      "word: covetous --> ('c', 'o', 've', 't', 'ou', 's')\n",
      "word:  --> \n",
      "word:  --> \n",
      "word: First --> ('f', 'ir', 's', 't')\n",
      "word: Citizen --> ('citiz', 'en')\n",
      "word:  --> \n",
      "word: If --> ('i', 'f')\n",
      "word: I --> ('i',)\n",
      "word: must --> ('must',)\n",
      "word: not --> ('not',)\n",
      "word:  --> \n",
      "word: I --> ('i',)\n",
      "word: need --> ('n', 'e', 'ed')\n",
      "word: not --> ('not',)\n",
      "word: be --> ('be',)\n",
      "word: barren --> ('barren',)\n",
      "word: of --> ('of',)\n",
      "word: accusations --> ('accusations',)\n",
      "word:  --> \n",
      "word: he --> ('he',)\n",
      "word: hath --> ('hath',)\n",
      "word: faults --> ('f', 'au', 'l', 'ts')\n",
      "word:  --> \n",
      "word: with --> ('with',)\n",
      "word: surplus --> ('su', 'r', 'pl', 'us')\n",
      "word:  --> \n",
      "word: to --> ('to',)\n",
      "word: tire --> ('ti', 're')\n",
      "word: in --> ('in',)\n",
      "word: repetition --> ('re', 'pe', 'ti', 'ti', 'on')\n",
      "word:  --> \n",
      "word: What --> ('wh', 'at')\n",
      "word: shouts --> ('s', 'h', 'ou', 'ts')\n",
      "word: are --> ('are',)\n",
      "word: these --> ('the', 'se')\n",
      "word:  --> \n",
      "word: The --> ('the',)\n",
      "word: other --> ('other',)\n",
      "word: side --> ('s', 'i', 'de')\n",
      "word: o --> ('o',)\n",
      "word:  --> \n",
      "word: the --> ('the',)\n",
      "word: city --> ('ci', 'ty')\n",
      "word: is --> ('is',)\n",
      "word: risen --> ('ri', 's', 'en')\n",
      "word:  --> \n",
      "word: why --> ('wh', 'y')\n",
      "word: stay --> ('s', 't', 'ay')\n",
      "word: we --> ('we',)\n",
      "word: prating --> ('pr', 'ati', 'ng')\n",
      "word: here --> ('h', 'ere')\n",
      "word:  --> \n",
      "word: to --> ('to',)\n",
      "word: the --> ('the',)\n",
      "word: Capitol --> ('c', 'a', 'p', 'i', 'to', 'l')\n",
      "word:  --> \n",
      "1.683794466403162\n"
     ]
    }
   ],
   "source": [
    "# train BPE with our input text\n",
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"Compute frequencies of adjacent pairs of symbols.\"\"\"\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "with open('inp_testbpe0.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = re.sub(r'[^\\w]', ' ', text)  # remove all symbols\n",
    "\n",
    "text = text.replace('\\n', ' ')\n",
    "words = sorted(list(set(text.lower().split(' '))))\n",
    "wordfreq = {w: text.count(w) for w in words}\n",
    "#print(words)\n",
    "print(wordfreq)\n",
    "\n",
    "#print({(' '.join([el for el in v.lower()]) + ' </w>'):idx for idx,v in enumerate(text.split(' '))})\n",
    "\n",
    "#train_data = {(' '.join([el for el in v.lower()]) + ' </w>'):idx for idx,v in enumerate(text.split(' '))}\n",
    "#train_data = {(' '.join([el for el in v.lower()]) + ' </w>'):idx for idx,v in enumerate(words)}\n",
    "#train_data = {(v + '</w>'):idx for idx,v in enumerate(words)}\n",
    "#train_data = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
    "#train_data = wordfreq\n",
    "train_data = {(' '.join([el for el in key]) + ' </w>'):val for key,val in wordfreq.items()}\n",
    "print(train_data)\n",
    "#print(get_stats(train_data))\n",
    "\n",
    "bpe_codes = {}\n",
    "bpe_codes_reverse = {}\n",
    "\n",
    "#num_merges = length of word l, length of subword n: l! / (n!(l-n)!) --> sum for n=1 to l = 2**l - 1 + number of words\n",
    "# average word length\n",
    "avrgwordlen = sum([len(w) for w in words]) / len(words)\n",
    "num_merges = 2**int(avrgwordlen+1) - 1 + len(words)\n",
    "# simpler: number of words * 2 seems to work just as well\n",
    "#num_merges = 2*len(words)\n",
    "print(f\"numer of words: {len(words)} avrg word length: {avrgwordlen} --> number of merges: {num_merges}\")\n",
    "\n",
    "for i in range(num_merges):\n",
    "    #print(\"### Iteration {}\".format(i + 1))\n",
    "    pairs = get_stats(train_data)\n",
    "    best = max(pairs, key = pairs.get)\n",
    "    train_data = merge_vocab(best, train_data)\n",
    "    \n",
    "    bpe_codes[best] = i\n",
    "    bpe_codes_reverse[best[0] + best[1]] = best\n",
    "    \n",
    "    #print(\"new merge: {}\".format(best))\n",
    "    #print(\"train data: {}\".format(train_data))\n",
    "\n",
    "print(bpe_codes)\n",
    "print()\n",
    "\n",
    "#for word in words:\n",
    "encodedlen = 0\n",
    "for word in text.split(' '):\n",
    "    wordencoded = encode_bp(word.lower())\n",
    "    print(f\"word: {word} --> {wordencoded}\")\n",
    "    encodedlen += len(wordencoded)\n",
    "\n",
    "print(encodedlen / len(text.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9db44815-3caa-4cd3-85a5-34329494087b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "__word split into characters:__ <tt>('m', 'i', 'g', 'r', 'a', 't', 'e', '</w>')</tt>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 1:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('i', 'g'), ('t', 'e'), ('m', 'i'), ('e', '</w>'), ('a', 't'), ('g', 'r'), ('r', 'a')}\n",
      "candidate for merging: ('e', '</w>')\n",
      "word after merging: ('m', 'i', 'g', 'r', 'a', 't', 'e</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 2:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('i', 'g'), ('m', 'i'), ('a', 't'), ('g', 'r'), ('r', 'a'), ('t', 'e</w>')}\n",
      "candidate for merging: ('m', 'i')\n",
      "word after merging: ('mi', 'g', 'r', 'a', 't', 'e</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 3:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('mi', 'g'), ('a', 't'), ('g', 'r'), ('r', 'a'), ('t', 'e</w>')}\n",
      "candidate for merging: ('mi', 'g')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Candidate not in BPE merges, algorithm stops.__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('mi', 'g', 'r', 'a', 't', 'e')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode('migrate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14625333-b9ba-4629-a2a0-a57653395fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to integrate BPE into the LM?\n",
    "\n",
    "\n",
    "\n",
    "# next steps:\n",
    "# - implement positional encoding via sin/cos\n",
    "# - test iterative transformer (i.e. iterative self-attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef07cc5a-cf66-4387-a2f1-516d42c82b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test number of merges \n",
    "import re, collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"Compute frequencies of adjacent pairs of symbols.\"\"\"\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "with open('inp_testbpe0.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = re.sub(r'[^\\w]', ' ', text)  # remove all symbols\n",
    "\n",
    "text = text.replace('\\n', ' ')\n",
    "words = sorted(list(set(text.lower().split(' '))))\n",
    "wordfreq = {w: text.count(w) for w in words}\n",
    "\n",
    "\n",
    "y = []\n",
    "\n",
    "for num_merges in range(len(words)//2, 2*len(words), 2):\n",
    "    \n",
    "    train_data = {(' '.join([el for el in key]) + ' </w>'):val for key,val in wordfreq.items()}\n",
    "    \n",
    "    bpe_codes = {}\n",
    "    bpe_codes_reverse = {}\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        #print(\"### Iteration {}\".format(i + 1))\n",
    "        pairs = get_stats(train_data)\n",
    "        best = max(pairs, key = pairs.get)\n",
    "        train_data = merge_vocab(best, train_data)\n",
    "        \n",
    "        bpe_codes[best] = i\n",
    "        bpe_codes_reverse[best[0] + best[1]] = best\n",
    "        \n",
    "        #print(\"new merge: {}\".format(best))\n",
    "        #print(\"train data: {}\".format(train_data))\n",
    "    \n",
    "    #print(bpe_codes)\n",
    "    #print()\n",
    "    \n",
    "    #for word in words:\n",
    "    encodedlen = 0\n",
    "    for word in text.split(' '):\n",
    "        wordencoded = encode_bp(word.lower())\n",
    "        #print(f\"word: {word} --> {wordencoded}\")\n",
    "        encodedlen += len(wordencoded)\n",
    "    \n",
    "    #print(encodedlen / len(text.split(' ')))\n",
    "    y.append(encodedlen / len(text.split(' ')))\n",
    "\n",
    "\n",
    "x = np.arange(len(y))\n",
    "plt.bar(x, height=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1cab4a2-9e3e-40b6-9f8c-bf0ac5af2f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numer of words: 130 avrg word length: 4.946153846153846 --> number of merges: 161\n",
      "\n",
      "{'de': 0, 'ly': 1, 'for': 2, 'at': 3, 'th': 4, 'that': 5, 'authority': 6, 'abundance': 7, 'mi': 8, 'the': 9, 'say': 10, 'be': 11, 'you': 12, 'kes': 13, 'us': 14, 'a': 15, 'ty': 16, 'revenge': 17, 'ng': 18, 'account': 19, 'c': 20, 'become': 21, 'u': 22, 'ere': 23, 'ti': 24, 'b': 25, 'afflicts': 26, 'are': 27, 'whi': 28, 'k': 29, 'no': 30, 'oun': 31, 'au': 32, 'm': 33, 'our': 34, 'accusations': 35, 'go': 36, 'e': 37, 'o': 38, 'ea': 39, 'in': 40, 'ts': 41, 'as': 42, 'gh': 43, 'must': 44, 'p': 45, 'an': 46, 'n': 47, 'it': 48, 'did': 49, 'w': 50, 'hath': 51, 'h': 52, 'v': 53, 'pe': 54, 'ay': 55, 'citiz': 56, 't': 57, 'ce': 58, 'pl': 59, 'ati': 60, 'relieve': 61, 'other': 62, 'even': 63, 'barren': 64, 'l': 65, 's': 66, 'g': 67, 'su': 68, 'ance': 69, 'par': 70, 'this': 71, 'we': 72, 'rf': 73, 'eve': 74, 'of': 75, 'with': 76, 'ir': 77, 'ff': 78, 'wh': 79, 'se': 80, 'on': 81, 'or': 82, 'and': 83, 'ome': 84, 'he': 85, 'they': 86, 'en': 87, 'ld': 88, 'hu': 89, 'con': 90, 'y': 91, 'reli': 92, 'ed': 93, 've': 94, 'to': 95, 'accounted': 96, 'ou': 97, 'r': 98, 'i': 99, 'would': 100, 'not': 101, 'ci': 102, 're': 103, 'pr': 104, 'ry': 105, 'is': 106, 'his': 107, 'es': 108, 'but': 109, 'j': 110, 'bread': 111, 'can': 112, 'f': 113, 'altitude': 114, 'd': 115, 'ec': 116, 'tu': 117, 'ri': 118, 'ess': 119, '</w>': inf}\n",
      "[113, 77, 66, 57, inf, 56, 87, inf, inf, 72, inf, 27, inf, 96, inf, 45, 38, 82, inf, 56, 87, 66, inf, inf, 9, inf, 45, 15, 57, 118, 102, 46, 66, inf, 36, 38, 115, inf, inf, 79, 3, inf, 6, inf, 68, 73, 37, 99, 41, inf, 81, inf, 100, inf, 61, inf, 14, inf, inf, 99, 113, inf, 86, inf, 100, inf, 91, 99, 37, 88, inf, 14, inf, 109, inf, 9, inf, 68, 54, 73, 65, 22, 99, 16, inf, inf, 28, 65, 37, inf, 48, inf, 50, 23, inf, 79, 38, 65, 108, 84, inf, inf, 72, inf, 8, 43, 57, inf, 67, 22, 119, inf, 86, inf, 92, 74, 115, inf, 14, inf, 89, 33, 46, 37, 1, inf, inf, 109, inf, 86, inf, 4, 40, 29, inf, 72, inf, 27, inf, 95, 38, inf, 115, 39, 98, inf, inf, 9, inf, 65, 37, 46, 47, 119, inf, 5, inf, 26, inf, 14, inf, inf, 9, inf, 38, 25, 110, 116, 57, inf, 75, inf, 34, inf, 8, 66, 37, 105, inf, inf, 106, inf, 42, inf, 46, inf, 40, 94, 47, 95, 105, inf, 95, inf, 70, 24, 20, 22, 65, 15, 118, 80, inf, 9, 99, 98, inf, 7, inf, inf, 34, inf, 68, 78, 37, 98, 69, inf, 106, inf, 15, inf, 67, 15, 40, inf, 95, inf, 9, 33, inf, 65, 37, 57, inf, 14, inf, 17, inf, 71, inf, 76, inf, 34, inf, 45, 99, 13, inf, inf, 23, inf, 72, inf, 21, inf, 98, 15, 13, inf, inf, 2, inf, 9, inf, 36, 115, 66, inf, 29, 30, 50, inf, 99, inf, 66, 45, 39, 29, inf, 71, inf, 40, inf, 89, 18, 37, 98, inf, 2, inf, 111, inf, inf, 101, inf, 40, inf, 4, 77, 66, 57, inf, 2, inf, 17, inf, inf, 113, 77, 66, 57, inf, 56, 87, inf, inf, 99, inf, 10, inf, 22, 47, 95, inf, 12, inf, inf, 79, 3, inf, 85, inf, 51, inf, 115, 81, 37, inf, 113, 15, 33, 97, 66, 1, inf, inf, 85, inf, 49, inf, 48, inf, 95, inf, 5, inf, 87, 115, inf, inf, 4, 97, 43, inf, 66, 38, 113, 57, inf, 90, 66, 102, 87, 20, 93, inf, 33, 87, inf, 112, inf, 11, inf, 90, 57, 87, 57, inf, 95, inf, 10, inf, 48, inf, 50, 42, inf, 2, inf, 107, inf, 20, 31, 57, 105, inf, 85, inf, 49, inf, 48, inf, 95, inf, 59, 39, 80, inf, 107, inf, 33, 62, inf, 83, inf, 95, inf, 11, inf, 70, 57, 1, inf, 104, 97, 115, inf, inf, 28, 20, 52, inf, 85, inf, 106, inf, inf, 63, inf, 24, 65, 65, inf, 9, inf, 114, inf, 75, inf, 107, inf, 53, 77, 117, 37, inf, inf, inf, 66, 116, 81, 115, inf, 56, 87, inf, inf, 79, 3, inf, 85, inf, 20, 46, 101, inf, 52, 37, 65, 45, inf, 40, inf, 107, inf, 47, 15, 117, 103, inf, inf, 12, inf, 19, inf, 15, inf, 53, 99, 58, inf, 40, inf, 52, 99, 33, inf, inf, 12, inf, 44, inf, 40, inf, 30, inf, 50, 55, inf, 10, inf, 85, inf, 106, inf, 20, 38, 94, 57, 97, 66, inf, inf, inf, 113, 77, 66, 57, inf, 56, 87, inf, inf, 99, 113, inf, 99, inf, 44, inf, 101, inf, inf, 99, inf, 47, 37, 93, inf, 101, inf, 11, inf, 64, inf, 75, inf, 35, inf, inf, 85, inf, 51, inf, 113, 32, 65, 41, inf, inf, 76, inf, 68, 98, 59, 14, inf, inf, 95, inf, 24, 103, inf, 40, inf, 103, 54, 24, 24, 81, inf, inf, 79, 3, inf, 66, 52, 97, 41, inf, 27, inf, 9, 80, inf, inf, 9, inf, 62, inf, 66, 99, 0, inf, 38, inf, inf, 9, inf, 102, 16, inf, 106, inf, 118, 66, 87, inf, inf, 79, 91, inf, 66, 57, 55, inf, 72, inf, 104, 60, 18, inf, 52, 23, inf, inf, 95, inf, 9, inf, 20, 15, 45, 99, 95, 65, inf, inf]\n"
     ]
    }
   ],
   "source": [
    "# train BPE and build symbolic embedding - test\n",
    "import re, collections\n",
    "import math\n",
    "\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"Compute frequencies of adjacent pairs of symbols.\"\"\"\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "with open('inp_testbpe0.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = re.sub(r'[^\\w]', ' ', text)  # remove all symbols\n",
    "\n",
    "text = text.replace('\\n', ' ')\n",
    "words = sorted(list(set(text.lower().split(' '))))\n",
    "wordfreq = {w: text.count(w) for w in words}\n",
    "#print(words)\n",
    "#print(wordfreq)\n",
    "\n",
    "#print({(' '.join([el for el in v.lower()]) + ' </w>'):idx for idx,v in enumerate(text.split(' '))})\n",
    "\n",
    "#train_data = {(' '.join([el for el in v.lower()]) + ' </w>'):idx for idx,v in enumerate(text.split(' '))}\n",
    "#train_data = {(' '.join([el for el in v.lower()]) + ' </w>'):idx for idx,v in enumerate(words)}\n",
    "#train_data = {(v + '</w>'):idx for idx,v in enumerate(words)}\n",
    "#train_data = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
    "#train_data = wordfreq\n",
    "train_data = {(' '.join([el for el in key]) + ' </w>'):val for key,val in wordfreq.items()}\n",
    "#print(train_data)\n",
    "#print(get_stats(train_data))\n",
    "\n",
    "bpe_codes = {}\n",
    "bpe_codes_reverse = {}\n",
    "\n",
    "#num_merges = length of word l, length of subword n: l! / (n!(l-n)!) --> sum for n=1 to l = 2**l - 1 + number of words\n",
    "# average word length\n",
    "avrgwordlen = sum([len(w) for w in words]) / len(words)\n",
    "num_merges = 2**int(avrgwordlen+1) - 1 + len(words)\n",
    "# simpler: number of words * 2 seems to work just as well\n",
    "#num_merges = 2*len(words)\n",
    "print(f\"numer of words: {len(words)} avrg word length: {avrgwordlen} --> number of merges: {num_merges}\")\n",
    "print()\n",
    "\n",
    "for i in range(num_merges):\n",
    "    #print(\"### Iteration {}\".format(i + 1))\n",
    "    pairs = get_stats(train_data)\n",
    "    best = max(pairs, key = pairs.get)\n",
    "    train_data = merge_vocab(best, train_data)\n",
    "    \n",
    "    bpe_codes[best] = i\n",
    "    bpe_codes_reverse[best[0] + best[1]] = best\n",
    "    \n",
    "    #print(\"new merge: {}\".format(best))\n",
    "    #print(\"train data: {}\".format(train_data))\n",
    "\n",
    "#print(bpe_codes)\n",
    "#print()\n",
    "\n",
    "bpesymbols = set()\n",
    "textencoded = []\n",
    "\n",
    "for word in text.split(' '):\n",
    "    wordencoded = encode_bp(word.lower())\n",
    "    for s in wordencoded:\n",
    "        textencoded.append(s)\n",
    "    textencoded.append('</w>')\n",
    "    #print(f\"word: {word} --> {wordencoded}\")\n",
    "    for w in wordencoded:\n",
    "        bpesymbols.add(w)\n",
    "\n",
    "#bpesymbols.add('</w>')\n",
    "\n",
    "bpetoi = { symb:i for i, symb in enumerate(bpesymbols) }\n",
    "bpetoi['</w>'] = math.inf\n",
    "print(bpetoi)\n",
    "\n",
    "textbpesymb = [bpetoi[s] for s in textencoded]\n",
    "print(textbpesymb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3b411ed0-896e-4ffa-b35b-5cc6b1632dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.25 0.5  0.75]\n",
      "[0.   0.25 0.5  0.75 1.   1.25 1.5  1.75]\n",
      "[0.   0.25 0.5  0.75 0.   0.25 0.5  0.75]\n",
      "[[0.         0.         0.        ]\n",
      " [0.25       0.2        0.14285714]\n",
      " [0.5        0.4        0.28571429]\n",
      " [0.75       0.6        0.42857143]\n",
      " [0.         0.8        0.57142857]\n",
      " [0.25       0.         0.71428571]\n",
      " [0.5        0.2        0.85714286]\n",
      " [0.75       0.4        0.        ]]\n",
      "\n",
      "0.3505826054802293\n",
      "0.7892453124759793\n",
      "\n",
      "sin: [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.00000000e+00  9.51056516e-01  7.81831482e-01]\n",
      " [ 1.22464680e-16  5.87785252e-01  9.74927912e-01]\n",
      " [-1.00000000e+00 -5.87785252e-01  4.33883739e-01]\n",
      " [ 0.00000000e+00 -9.51056516e-01 -4.33883739e-01]\n",
      " [ 1.00000000e+00  0.00000000e+00 -9.74927912e-01]\n",
      " [ 1.22464680e-16  9.51056516e-01 -7.81831482e-01]\n",
      " [-1.00000000e+00  5.87785252e-01  0.00000000e+00]]\n",
      "cos: [[ 1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      " [ 6.12323400e-17  3.09016994e-01  6.23489802e-01]\n",
      " [-1.00000000e+00 -8.09016994e-01 -2.22520934e-01]\n",
      " [-1.83697020e-16 -8.09016994e-01 -9.00968868e-01]\n",
      " [ 1.00000000e+00  3.09016994e-01 -9.00968868e-01]\n",
      " [ 6.12323400e-17  1.00000000e+00 -2.22520934e-01]\n",
      " [-1.00000000e+00  3.09016994e-01  6.23489802e-01]\n",
      " [-1.83697020e-16 -8.09016994e-01  1.00000000e+00]]\n",
      "\n",
      "[[ 0.00000000e+00  1.00000000e+00  1.22464680e-16 -1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  1.22464680e-16 -1.00000000e+00]\n",
      " [ 1.00000000e+00  6.12323400e-17 -1.00000000e+00 -1.83697020e-16\n",
      "   1.00000000e+00  6.12323400e-17 -1.00000000e+00 -1.83697020e-16]\n",
      " [ 0.00000000e+00  9.51056516e-01  5.87785252e-01 -5.87785252e-01\n",
      "  -9.51056516e-01  0.00000000e+00  9.51056516e-01  5.87785252e-01]\n",
      " [ 1.00000000e+00  3.09016994e-01 -8.09016994e-01 -8.09016994e-01\n",
      "   3.09016994e-01  1.00000000e+00  3.09016994e-01 -8.09016994e-01]\n",
      " [ 0.00000000e+00  7.81831482e-01  9.74927912e-01  4.33883739e-01\n",
      "  -4.33883739e-01 -9.74927912e-01 -7.81831482e-01  0.00000000e+00]\n",
      " [ 1.00000000e+00  6.23489802e-01 -2.22520934e-01 -9.00968868e-01\n",
      "  -9.00968868e-01 -2.22520934e-01  6.23489802e-01  1.00000000e+00]]\n",
      "\n",
      "2.0334665985780633\n",
      "2.0334665985780633\n"
     ]
    }
   ],
   "source": [
    "# positional encoding\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "positions = np.arange(4)\n",
    "print(positions/4)  # unique values\n",
    "positions = np.arange(8)\n",
    "print(positions/4)  # unique but > 1\n",
    "\n",
    "positions = np.arange(8)\n",
    "print(np.remainder(positions, 4)/4)  # <1 but not unique\n",
    "\n",
    "positions = np.arange(8)\n",
    "positionvectors = np.ones(8) * np.array([np.remainder(positions, 4)/4, np.remainder(positions, 5)/5, np.remainder(positions, 7)/7])\n",
    "print(positionvectors.T)\n",
    "print()\n",
    "\n",
    "print(np.linalg.norm(positionvectors.T[3] - positionvectors.T[2]))  # distances not equal\n",
    "print(np.linalg.norm(positionvectors.T[4] - positionvectors.T[3]))\n",
    "\n",
    "#print(positionvectors.T * np.deg2rad(360))  # angles\n",
    "print()\n",
    "\n",
    "print(f\"sin: {np.sin(positionvectors.T * np.deg2rad(360))}\")  # xy coordinates\n",
    "print(f\"cos: {np.cos(positionvectors.T * np.deg2rad(360))}\")\n",
    "print()\n",
    "\n",
    "positionvectorssin = np.array([[np.sin(v[i]) for v in positionvectors.T * np.deg2rad(360)] for i in range(3)])\n",
    "positionvectorscos = np.array([[np.cos(v[i]) for v in positionvectors.T * np.deg2rad(360)] for i in range(3)])\n",
    "\n",
    "row_a, col_a = np.shape(positionvectorssin)\n",
    "positionvectorsxy = np.ravel([positionvectorssin, positionvectorscos], order=\"F\")\n",
    "#positionvectorsxy = np.reshape(positionvectorsxy, (2*row_a, col_a))\n",
    "positionvectorsxy = np.array([positionvectorsxy.tolist()[i:i+2*row_a] for i in range(0, 2*row_a*col_a, 2*row_a)])\n",
    "print(positionvectorsxy.T)\n",
    "print()\n",
    "\n",
    "print(np.linalg.norm(positionvectorsxy.T[0:, 3] - positionvectorsxy.T[0:, 2]))  # distances are now equal\n",
    "print(np.linalg.norm(positionvectorsxy.T[0:, 4] - positionvectorsxy.T[0:, 3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b2bf717a-b486-4145-96f4-f4a481d5641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://helsinki-nlp.github.io/shroom/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36536b0-35e9-4adf-a4de-094c145b2fda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
