{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d713789-a53d-46c9-8155-9c3fc24a0f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: train loss 4.4116, val loss 4.4022\n",
      "step 100: train loss 2.6568, val loss 2.6670\n",
      "step 200: train loss 2.5090, val loss 2.5058\n",
      "step 300: train loss 2.4198, val loss 2.4339\n",
      "step 400: train loss 2.3502, val loss 2.3563\n",
      "step 500: train loss 2.2965, val loss 2.3128\n",
      "step 600: train loss 2.2411, val loss 2.2498\n",
      "step 700: train loss 2.2062, val loss 2.2199\n",
      "step 800: train loss 2.1640, val loss 2.1870\n",
      "step 900: train loss 2.1244, val loss 2.1507\n",
      "step 1000: train loss 2.1035, val loss 2.1302\n",
      "step 1100: train loss 2.0693, val loss 2.1174\n",
      "step 1200: train loss 2.0380, val loss 2.0798\n",
      "step 1300: train loss 2.0248, val loss 2.0646\n",
      "step 1400: train loss 1.9916, val loss 2.0361\n",
      "step 1500: train loss 1.9699, val loss 2.0306\n",
      "step 1600: train loss 1.9635, val loss 2.0488\n",
      "step 1700: train loss 1.9406, val loss 2.0144\n",
      "step 1800: train loss 1.9078, val loss 1.9939\n",
      "step 1900: train loss 1.9095, val loss 1.9888\n",
      "step 2000: train loss 1.8847, val loss 1.9950\n",
      "step 2100: train loss 1.8714, val loss 1.9759\n",
      "step 2200: train loss 1.8579, val loss 1.9605\n",
      "step 2300: train loss 1.8540, val loss 1.9516\n",
      "step 2400: train loss 1.8410, val loss 1.9418\n",
      "step 2500: train loss 1.8177, val loss 1.9443\n",
      "step 2600: train loss 1.8259, val loss 1.9359\n",
      "step 2700: train loss 1.8132, val loss 1.9357\n",
      "step 2800: train loss 1.8037, val loss 1.9219\n",
      "step 2900: train loss 1.8030, val loss 1.9301\n",
      "step 3000: train loss 1.7930, val loss 1.9194\n",
      "step 3100: train loss 1.7710, val loss 1.9166\n",
      "step 3200: train loss 1.7540, val loss 1.9130\n",
      "step 3300: train loss 1.7554, val loss 1.9039\n",
      "step 3400: train loss 1.7551, val loss 1.8949\n",
      "step 3500: train loss 1.7398, val loss 1.8943\n",
      "step 3600: train loss 1.7241, val loss 1.8854\n",
      "step 3700: train loss 1.7318, val loss 1.8874\n",
      "step 3800: train loss 1.7186, val loss 1.8896\n",
      "step 3900: train loss 1.7206, val loss 1.8704\n",
      "step 4000: train loss 1.7137, val loss 1.8573\n",
      "step 4100: train loss 1.7128, val loss 1.8719\n",
      "step 4200: train loss 1.7032, val loss 1.8615\n",
      "step 4300: train loss 1.6964, val loss 1.8436\n",
      "step 4400: train loss 1.7071, val loss 1.8653\n",
      "step 4500: train loss 1.6918, val loss 1.8558\n",
      "step 4600: train loss 1.6876, val loss 1.8351\n",
      "step 4700: train loss 1.6859, val loss 1.8476\n",
      "step 4800: train loss 1.6641, val loss 1.8409\n",
      "step 4900: train loss 1.6675, val loss 1.8348\n",
      "step 4999: train loss 1.6624, val loss 1.8222\n",
      "\n",
      "And they bride with to loves that set bube to take Our my call ands:\n",
      "Whith fult him to barte?\n",
      "\n",
      "DUKE, MENENCE:\n",
      "What, art zorn heart, Let fuit heart my would\n",
      "As egriant:\n",
      "You, will is there by waste\n",
      "Wiltness is wan thel lind that.\n",
      "His my now by atcher is plaw yet:\n",
      "Mistrence open, and whom\n",
      "that dematherly aling to dry I brow'll my\n",
      "Murning in him:\n",
      "And-litgre-borrute kingn,\n",
      "Turt first; if his shat thy flear thee firsh?\n",
      "\n",
      "KING HENRY VI:\n",
      "Hastied is wards be his great huis courtear tear repts I\n",
      "arm for his neet. Ponsmagumt of come, my mest a cempreame?\n",
      "\n",
      "NORFOLK:\n",
      "Whis and whom the head not us bod,\n",
      "Thou confessyy stiltchs lome\n",
      "This son to evers.\n",
      "\n",
      "LUCIO:\n",
      "Abenerly to-madins!\n",
      "\n",
      "KING HENMIONRY HARD III:\n",
      "Have the eRpeakes: there loves\n",
      "Tunniough, if I hine. Pomfare, throbed what\n",
      "which uncertute of vollumann's, do so lace.\n",
      "\n",
      "PUTHIO:\n",
      "Firshould,\n",
      "Come in him the not.\n",
      "\n",
      "POLIXENEN:\n",
      "How, too, is of no not the summon approant. whose no crave soleed Blowly made that O\n",
      "For father watch the\n",
      "diout in not Gleen well. Yet runt, I boy sofle, I hursen as entren the vie,\n",
      "The notitanous, withalte or word.\n",
      "Me, maste mines and enterying.\n",
      "\n",
      "GRUMERIZAPULET:\n",
      "Yet whide,\n",
      "And where\n",
      "He bence oIn percale the call as ince any in Becain,\n",
      "Thou tell true exre breamn'd my have eat toge\n",
      "Betta detion for Hencher morral brack!\n",
      "And forther worjess the vown;\n",
      "Mad there this ands, all this clady,\n",
      "Albety dong hold my lied the beguts\n",
      "with vonsure that thou the baste hold;\n",
      "I they none to me, meet, speak.\n",
      "\n",
      "LUCHIUS:\n",
      "While these art woll, we\n",
      "ell smen of my soul be\n",
      "that Lord, as If have to kaviless cornful.\n",
      "\n",
      "POLANNA:\n",
      "Have us\n",
      "Out, strar\n",
      "En EDWARD IV:\n",
      "Who dest thy patried prince of Worscoman wound arm that sure give affive of mhild by as is Pressents\n",
      "He have blay not, there wife\n",
      "Oncer with I heav toge of the hamt,\n",
      "Threo?\n",
      "There courter thee tonguius, give our will the peepose: life?\n",
      "\n",
      "COMINIUS:\n",
      "You do the art; whom more, now would promb,\n",
      "They and sentry talbatte you stan,\n",
      "Thre Riparford: I will down's fortunds, such I he have will,\n",
      "\n",
      "Marr\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16  # how many independent sequences will we process in parallel?\n",
    "block_size = 32  # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input_sp0.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):  # the heads handle the attention - the communication between the nodes (tokens)\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):  # takes headsize\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)  # k,q,v like above\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))  # tril is not a parameter of the model, a buffer in pytorch lingo\n",
    "        # (so it does not get picked up by autograd)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x)  # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B,T,C)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])  # pytorch makes this easy for us, passing just a list of heads\n",
    "        self.proj = nn.Linear(n_embd, n_embd)  # project back into the original pathway\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # just concatinate the results, concat over channel dimension (dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):  # this handles the per-token (node) computation\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # the inner ff layer has 4 times the embedding dimension (to grow the computation on the residual pathway)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),  # this is the projection layer going back into the original pathway (for the skip-connection) and going down from 4x\n",
    "            nn.Dropout(dropout),  # added right before the residual pathway comes back (dropout trains subnetworks by randomly shutting off neurons)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # this is at a per token level: they think on all the attention data they got individually\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head  # watch out that it works out channelwise (e.g. 32 embedding, 4 heads, 8 headsize)\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)  # the heads running in parallel - the communication\n",
    "        self.ffwd = FeedFoward(n_embd)  # simple feedforward: at token level, all tokens do this independently: gathered all info from attention, think on it\n",
    "        # the ff does the computations\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):  # with skip connection, forking off direct gradient pass-through and block adjustments\n",
    "        x = x + self.sa(self.ln1(x))  # the x + is the skip connection: it lets the gradient pass through (particularly initially), then the blocks adjust\n",
    "        x = x + self.ffwd(self.ln2(x))  # note that the layer norms are applied before it goes into self-attention or ff, per token\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        # now the network starts to get really deep: suffers from computation problems, two ways to address this: (see Add & Norm in the figure)\n",
    "        # 1. skip (residual) connections, see https://github.com/KaimingHe/deep-residual-networks (lets gradients pass through and lets blocks adjust them gradually)\n",
    "        # 2. layer norms = Norm: similar to batch normalization: ensure each neuron has unit Gaussian distribution accross batch dimension\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm (now it is more common to norm before the computations unlike in the trafo paper - called prenorm)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        x = self.blocks(x)  # (B,T,C)\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens (idx also has the PE so we need to crop) (not doing this causes a CUDA error due to index out of bounds)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2f489d-268c-4597-8b69-648b01f6e2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase input text file length\n",
    "\n",
    "with open('input_sp0.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text += ' ' + text\n",
    "\n",
    "# Writing to file\n",
    "with open(\"input_sp1.txt\", \"w\") as f:\n",
    "    # Writing data to a file\n",
    "    f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8468a3-2570-42c0-ba6e-6563b663bedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with double the file length there is barely any overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2958ffdc-b81b-4b96-a181-3c84bfc28abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: train loss 4.4086, val loss 4.4042\n",
      "step 100: train loss 2.6523, val loss 2.6555\n",
      "step 200: train loss 2.4994, val loss 2.5055\n",
      "step 300: train loss 2.4131, val loss 2.4304\n",
      "step 400: train loss 2.3452, val loss 2.3614\n",
      "step 500: train loss 2.3022, val loss 2.3056\n",
      "step 600: train loss 2.2582, val loss 2.2726\n",
      "step 700: train loss 2.1969, val loss 2.2117\n",
      "step 800: train loss 2.1801, val loss 2.1993\n",
      "step 900: train loss 2.1333, val loss 2.1499\n",
      "step 1000: train loss 2.1000, val loss 2.1186\n",
      "step 1100: train loss 2.0730, val loss 2.0943\n",
      "step 1200: train loss 2.0477, val loss 2.0648\n",
      "step 1300: train loss 2.0169, val loss 2.0488\n",
      "step 1400: train loss 1.9951, val loss 2.0256\n",
      "step 1500: train loss 1.9823, val loss 2.0090\n",
      "step 1600: train loss 1.9570, val loss 1.9924\n",
      "step 1700: train loss 1.9489, val loss 1.9794\n",
      "step 1800: train loss 1.9309, val loss 1.9527\n",
      "step 1900: train loss 1.9167, val loss 1.9405\n",
      "step 2000: train loss 1.9054, val loss 1.9344\n",
      "step 2100: train loss 1.8814, val loss 1.9159\n",
      "step 2200: train loss 1.8730, val loss 1.9115\n",
      "step 2300: train loss 1.8536, val loss 1.8940\n",
      "step 2400: train loss 1.8494, val loss 1.8806\n",
      "step 2500: train loss 1.8401, val loss 1.8838\n",
      "step 2600: train loss 1.8241, val loss 1.8825\n",
      "step 2700: train loss 1.8399, val loss 1.8631\n",
      "step 2800: train loss 1.8001, val loss 1.8441\n",
      "step 2900: train loss 1.8016, val loss 1.8412\n",
      "step 3000: train loss 1.7843, val loss 1.8352\n",
      "step 3100: train loss 1.7932, val loss 1.8227\n",
      "step 3200: train loss 1.7713, val loss 1.8175\n",
      "step 3300: train loss 1.7803, val loss 1.8218\n",
      "step 3400: train loss 1.7521, val loss 1.8043\n",
      "step 3500: train loss 1.7470, val loss 1.8029\n",
      "step 3600: train loss 1.7350, val loss 1.7928\n",
      "step 3700: train loss 1.7410, val loss 1.7941\n",
      "step 3800: train loss 1.7427, val loss 1.7826\n",
      "step 3900: train loss 1.7279, val loss 1.7711\n",
      "step 4000: train loss 1.7267, val loss 1.7777\n",
      "step 4100: train loss 1.7266, val loss 1.7800\n",
      "step 4200: train loss 1.7196, val loss 1.7741\n",
      "step 4300: train loss 1.7167, val loss 1.7589\n",
      "step 4400: train loss 1.7072, val loss 1.7543\n",
      "step 4500: train loss 1.6957, val loss 1.7477\n",
      "step 4600: train loss 1.6910, val loss 1.7342\n",
      "step 4700: train loss 1.6940, val loss 1.7280\n",
      "step 4800: train loss 1.6937, val loss 1.7253\n",
      "step 4900: train loss 1.6836, val loss 1.7233\n",
      "step 4999: train loss 1.6705, val loss 1.7250\n",
      "\n",
      "And they brides? and is the thratiest bubson envy\n",
      "graves me?\n",
      "I'll such hapge us hath but he than ane away, my fachoun of yeou:\n",
      "Yours, toffice come milind!\n",
      "\n",
      "AUCHIO:\n",
      "Yet? not will is therevers, and Will.\n",
      "\n",
      "DUKE WARWARD:\n",
      "No dam; whell he courivy: thy baids\n",
      "Whery hold naught for this lost\n",
      "For sworlices!\n",
      "\n",
      "Kord-SIAR have is all, and forth,\n",
      "The shience poor of he trume kindnunture,\n",
      "I'll somistan: there if Morath,\n",
      "Madarious whom.\n",
      "\n",
      "HENRY:\n",
      "Hark, but and dapbrucisome, thining muste.\n",
      "\n",
      "ANTHIO:\n",
      "Some for Edvall:\n",
      "The nature, grives you had than\n",
      "brotherss to peomes.\n",
      "\n",
      "MARINA:\n",
      "And whom such swond thee bods.\n",
      "\n",
      "Thundrafs:\n",
      "It sout might. Andiars, that work.\n",
      "\n",
      "LADY:\n",
      "To heaven to deding!\n",
      "\n",
      "KAPURENIUS:\n",
      "It some so upon soxecresitates:\n",
      "Whith loves, sheik housed\n",
      "My verear mone.\n",
      "\n",
      "CAPULIO:\n",
      "I againce uncested To wiftly long'd to so lack.\n",
      "\n",
      "Prive, more how:\n",
      "Prowe. My have brother, drue?\n",
      "Forse this of Mise thee now.\n",
      "\n",
      "Securace:\n",
      "And and wwhat thou truch all Edwlies kings that Of thy it.\n",
      "Thou chotive wout it forc;\n",
      "you consome her conjure\n",
      "As afged to two no where Mource in the prive\n",
      "Sanced, we backe of word\n",
      "As be resentings so tears\n",
      "Singing the beather Were wown,\n",
      "Doscal freavened him evInd\n",
      "My all the call acting--and inkeed,\n",
      "Alament theirse, thing brother to-suriese wown,\n",
      "And the joy adword, we\n",
      "than Turle; I pritume\n",
      "of thee tremes defry worsum hand,\n",
      "Shall be scive it is caper's losty donancle,\n",
      "My lord thy blord?\n",
      "Good vone, the town, must than thy but a thindernumty may\n",
      "Sir teny, delelige, inso, I have farrow'dly,\n",
      "Thee be most of you; and plock, say, as;\n",
      "Rever two kive, scace:\n",
      "One.\n",
      "\n",
      "Preton;\n",
      "Let see we hows is thou Lord drieving agaiLery.\n",
      "\n",
      "The happher\n",
      "'Tward, come roude. That was should,\n",
      "Beath, could him, by ass--'tight of Take twake\n",
      "And then: the wifly this father\n",
      "heaven, thou with art, lives?\n",
      "There, we twrither bonouroward's of rewburses how in to lift?\n",
      "\n",
      "CLESCES:\n",
      "Nay, to thee, of Sack you.\n",
      "\n",
      "GLOUCESTER:\n",
      "My have you Lose trowna. Came you stan,\n",
      "Jorfor parforit briefuse whom foold his suppecour husbavingbrailie\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16  # how many independent sequences will we process in parallel?\n",
    "block_size = 32  # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input_sp1.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):  # the heads handle the attention - the communication between the nodes (tokens)\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):  # takes headsize\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)  # k,q,v like above\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))  # tril is not a parameter of the model, a buffer in pytorch lingo\n",
    "        # (so it does not get picked up by autograd)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x)  # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B,T,C)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])  # pytorch makes this easy for us, passing just a list of heads\n",
    "        self.proj = nn.Linear(n_embd, n_embd)  # project back into the original pathway\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # just concatinate the results, concat over channel dimension (dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):  # this handles the per-token (node) computation\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # the inner ff layer has 4 times the embedding dimension (to grow the computation on the residual pathway)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),  # this is the projection layer going back into the original pathway (for the skip-connection) and going down from 4x\n",
    "            nn.Dropout(dropout),  # added right before the residual pathway comes back (dropout trains subnetworks by randomly shutting off neurons)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # this is at a per token level: they think on all the attention data they got individually\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head  # watch out that it works out channelwise (e.g. 32 embedding, 4 heads, 8 headsize)\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)  # the heads running in parallel - the communication\n",
    "        self.ffwd = FeedFoward(n_embd)  # simple feedforward: at token level, all tokens do this independently: gathered all info from attention, think on it\n",
    "        # the ff does the computations\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):  # with skip connection, forking off direct gradient pass-through and block adjustments\n",
    "        x = x + self.sa(self.ln1(x))  # the x + is the skip connection: it lets the gradient pass through (particularly initially), then the blocks adjust\n",
    "        x = x + self.ffwd(self.ln2(x))  # note that the layer norms are applied before it goes into self-attention or ff, per token\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        # now the network starts to get really deep: suffers from computation problems, two ways to address this: (see Add & Norm in the figure)\n",
    "        # 1. skip (residual) connections, see https://github.com/KaimingHe/deep-residual-networks (lets gradients pass through and lets blocks adjust them gradually)\n",
    "        # 2. layer norms = Norm: similar to batch normalization: ensure each neuron has unit Gaussian distribution accross batch dimension\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm (now it is more common to norm before the computations unlike in the trafo paper - called prenorm)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        x = self.blocks(x)  # (B,T,C)\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens (idx also has the PE so we need to crop) (not doing this causes a CUDA error due to index out of bounds)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f719853f-0d76-4298-833c-a4a5c8e344fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# byte-pair encoding\n",
    "# https://ufal.mff.cuni.cz/~helcl/courses/npfl116/ipython/byte_pair_encoding.html\n",
    "\n",
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"Compute frequencies of adjacent pairs of symbols.\"\"\"\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33617212-d1fb-463f-87f1-071a137f8627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Iteration 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('e', 's')\n",
      "train data: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('es', 't')\n",
      "train data: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('est', '</w>')\n",
      "train data: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('l', 'o')\n",
      "train data: {'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('lo', 'w')\n",
      "train data: {'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 6"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('n', 'e')\n",
      "train data: {'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 7"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('ne', 'w')\n",
      "train data: {'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 8"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('new', 'est</w>')\n",
      "train data: {'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 9"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('low', '</w>')\n",
      "train data: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 10"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('w', 'i')\n",
      "train data: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "train_data = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
    "\n",
    "bpe_codes = {}\n",
    "bpe_codes_reverse = {}\n",
    "\n",
    "num_merges = 10\n",
    "\n",
    "for i in range(num_merges):\n",
    "    display(Markdown(\"### Iteration {}\".format(i + 1)))\n",
    "    pairs = get_stats(train_data)\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    train_data = merge_vocab(best, train_data)\n",
    "    \n",
    "    bpe_codes[best] = i\n",
    "    bpe_codes_reverse[best[0] + best[1]] = best\n",
    "    \n",
    "    print(\"new merge: {}\".format(best))\n",
    "    print(\"train data: {}\".format(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e83dc3dd-c668-4edf-8928-a63a2262c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as a tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def encode(orig):\n",
    "    \"\"\"Encode word based on list of BPE merge operations, which are applied consecutively\"\"\"\n",
    "\n",
    "    word = tuple(orig) + ('</w>',)\n",
    "    display(Markdown(\"__word split into characters:__ <tt>{}</tt>\".format(word)))\n",
    "\n",
    "    pairs = get_pairs(word)    \n",
    "\n",
    "    if not pairs:\n",
    "        return orig\n",
    "\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        display(Markdown(\"__Iteration {}:__\".format(iteration)))\n",
    "        \n",
    "        print(\"bigrams in the word: {}\".format(pairs))\n",
    "        bigram = min(pairs, key = lambda pair: bpe_codes.get(pair, float('inf')))\n",
    "        print(\"candidate for merging: {}\".format(bigram))\n",
    "        if bigram not in bpe_codes:\n",
    "            display(Markdown(\"__Candidate not in BPE merges, algorithm stops.__\"))\n",
    "            break\n",
    "        first, second = bigram\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            try:\n",
    "                j = word.index(first, i)\n",
    "                new_word.extend(word[i:j])\n",
    "                i = j\n",
    "            except:\n",
    "                new_word.extend(word[i:])\n",
    "                break\n",
    "\n",
    "            if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                new_word.append(first+second)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_word = tuple(new_word)\n",
    "        word = new_word\n",
    "        print(\"word after merging: {}\".format(word))\n",
    "        if len(word) == 1:\n",
    "            break\n",
    "        else:\n",
    "            pairs = get_pairs(word)\n",
    "\n",
    "    # don't print end-of-word symbols\n",
    "    if word[-1] == '</w>':\n",
    "        word = word[:-1]\n",
    "    elif word[-1].endswith('</w>'):\n",
    "        word = word[:-1] + (word[-1].replace('</w>',''),)\n",
    "   \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13566815-edf9-4700-8969-349d87efee91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "__word split into characters:__ <tt>('l', 'o', 'w', 'e', 's', 't', '</w>')</tt>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 1:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('l', 'o'), ('o', 'w'), ('w', 'e'), ('t', '</w>'), ('e', 's'), ('s', 't')}\n",
      "candidate for merging: ('e', 's')\n",
      "word after merging: ('l', 'o', 'w', 'es', 't', '</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 2:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('l', 'o'), ('w', 'es'), ('o', 'w'), ('es', 't'), ('t', '</w>')}\n",
      "candidate for merging: ('es', 't')\n",
      "word after merging: ('l', 'o', 'w', 'est', '</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 3:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('l', 'o'), ('o', 'w'), ('w', 'est'), ('est', '</w>')}\n",
      "candidate for merging: ('est', '</w>')\n",
      "word after merging: ('l', 'o', 'w', 'est</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 4:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('l', 'o'), ('o', 'w'), ('w', 'est</w>')}\n",
      "candidate for merging: ('l', 'o')\n",
      "word after merging: ('lo', 'w', 'est</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 5:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('lo', 'w'), ('w', 'est</w>')}\n",
      "candidate for merging: ('lo', 'w')\n",
      "word after merging: ('low', 'est</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 6:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('low', 'est</w>')}\n",
      "candidate for merging: ('low', 'est</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Candidate not in BPE merges, algorithm stops.__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('low', 'est')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"lowest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8853ce9b-d54c-4919-bf4a-9ab19f041c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE w/o output\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as a tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "    \n",
    "\n",
    "def encode_bp(orig):\n",
    "    \"\"\"Encode word based on list of BPE merge operations, which are applied consecutively\"\"\"\n",
    "\n",
    "    word = tuple(orig) + ('</w>',)\n",
    "    pairs = get_pairs(word)    \n",
    "\n",
    "    if not pairs:\n",
    "        return orig\n",
    "\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        bigram = min(pairs, key = lambda pair: bpe_codes.get(pair, float('inf')))\n",
    "        if bigram not in bpe_codes:\n",
    "            break\n",
    "        first, second = bigram\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            try:\n",
    "                j = word.index(first, i)\n",
    "                new_word.extend(word[i:j])\n",
    "                i = j\n",
    "            except:\n",
    "                new_word.extend(word[i:])\n",
    "                break\n",
    "\n",
    "            if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                new_word.append(first+second)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_word = tuple(new_word)\n",
    "        word = new_word\n",
    "        if len(word) == 1:\n",
    "            break\n",
    "        else:\n",
    "            pairs = get_pairs(word)\n",
    "\n",
    "    # don't print end-of-word symbols\n",
    "    if word[-1] == '</w>':\n",
    "        word = word[:-1]\n",
    "    elif word[-1].endswith('</w>'):\n",
    "        word = word[:-1] + (word[-1].replace('</w>',''),)\n",
    "   \n",
    "    return word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69342831-9a47-4b34-9adc-c76ad01d24b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('low', 'est')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_bp(\"lowest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0b3ee217-a1ca-48eb-b72d-44907c4b704e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('i', '</w>'): 0, ('e', '</w>'): 1, ('a', '</w>'): 2, ('t', 'h'): 3, ('s', '</w>'): 4, ('t', '</w>'): 5, ('n', '</w>'): 6, ('th', 'e</w>'): 7, ('r', '</w>'): 8, ('i', 's</w>'): 9, ('r', 'e'): 10, (',', '</w>'): 11, ('i', 't</w>'): 12, ('o', 'u'): 13, ('a', 'n</w>'): 14, ('e', 'r'): 15, ('i', 'n</w>'): 16, ('t', 'o'): 17, ('a', 'n'): 18, ('d', '</w>'): 19, ('a', 'r'): 20, ('y', '</w>'): 21, ('e', 'n'): 22, ('e', 's'): 23, ('th', 'e'): 24, ('u', 's</w>'): 25, ('l', 'i'): 26, ('i', 't'): 27, ('o', 'r</w>'): 28, ('v', 'en'): 29, ('to', '</w>'): 30, ('w', 'e</w>'): 31, ('b', 'u'): 32, (':', '</w>'): 33, ('er', 'e</w>'): 34, ('f', 'or</w>'): 35, ('ou', 'r</w>'): 36, ('re', 'li'): 37, ('reli', 'e'): 38, ('relie', 'v'): 39, ('re', 'ven'): 40, ('reven', 'g'): 41, ('s', 'u'): 42, ('the', 'y</w>'): 43, ('l', 'd</w>'): 44, ('an', 'c'): 45, (';', '</w>'): 46, ('e', 'd</w>'): 47, ('f', 'f'): 48, ('ar', 'e</w>'): 49, ('e', 'c'): 50, ('o', 'm'): 51, ('bu', 't</w>'): 52, ('s', ',</w>'): 53, ('g', 'o'): 54, ('.', '</w>'): 55, ('es', 's</w>'): 56, ('h', 'u'): 57, ('f', '</w>'): 58, ('i', 'n'): 59, ('n', 'o'): 60, ('m', 'i'): 61, ('y', ',</w>'): 62, ('i', 'c'): 63, ('k', 'es'): 64, ('reliev', 'e</w>'): 65, ('reveng', 'e</w>'): 66, ('k', '</w>'): 67, ('th', 'is</w>'): 68, ('w', 'h'): 69, ('w', 'ou'): 70, ('wou', 'ld</w>'): 71, ('a', 'bu'): 72, ('abu', 'n'): 73, ('abun', 'd'): 74, ('abund', 'anc'): 75, ('abundanc', 'e'): 76, ('abundance', ';</w>'): 77, ('a', 'c'): 78, ('ac', 'c'): 79, ('acc', 'ou'): 80, ('accou', 'n'): 81, ('accoun', 't'): 82, ('account', 'ed</w>'): 83, ('a', 'ff'): 84, ('aff', 'li'): 85, ('affli', 'c'): 86, ('afflic', 't'): 87, ('afflict', 's</w>'): 88, ('a', 's</w>'): 89, ('a', 'u'): 90, ('au', 'th'): 91, ('auth', 'o'): 92, ('autho', 'r'): 93, ('author', 'it'): 94, ('authorit', 'y</w>'): 95, ('b', 'ec'): 96, ('bec', 'om'): 97, ('becom', 'e</w>'): 98, ('b', 're'): 99, ('bre', 'a'): 100, ('brea', 'd'): 101, ('bread', ',</w>'): 102, ('c', 'it'): 103, ('cit', 'i'): 104, ('citi', 'z'): 105, ('citiz', 'en'): 106, ('citizen', 's,</w>'): 107, ('d', 'e'): 108, ('de', 'ar'): 109, ('dear', ':</w>'): 110, ('i', 'r'): 111, ('ir', 's'): 112, ('irs', 't</w>'): 113, ('g', 'a'): 114, ('ga', 'in</w>'): 115, ('go', 'd'): 116, ('god', 's</w>'): 117, ('go', 'o'): 118, ('goo', 'd'): 119, ('good', '.</w>'): 120, ('g', 'u'): 121, ('gu', 'ess</w>'): 122, ('hu', 'm'): 123, ('hum', 'an'): 124, ('human', 'e'): 125, ('humane', 'l'): 126, ('humanel', 'y'): 127, ('humanely', ';</w>'): 128, ('hu', 'n'): 129, ('hun', 'g'): 130, ('hung', 'e'): 131, ('hunge', 'r</w>'): 132, ('i', 'f</w>'): 133, ('in', 'ven'): 134, ('inven', 'to'): 135, ('invento', 'r'): 136, ('inventor', 'y</w>'): 137, ('k', 'no'): 138, ('kno', 'w'): 139, ('know', '</w>'): 140, ('l', 'e'): 141, ('le', 'an'): 142, ('lean', 'n'): 143, ('leann', 'ess</w>'): 144, ('mi', 'g'): 145, ('mig', 'h'): 146, ('migh', 't</w>'): 147, ('mi', 's'): 148, ('mis', 'er'): 149, ('miser', 'y,</w>'): 150, ('no', 't</w>'): 151, ('o', 'b'): 152, ('ob', 'j'): 153, ('obj', 'ec'): 154, ('objec', 't</w>'): 155, ('o', 'f</w>'): 156, ('o', 'n</w>'): 157, ('p', 'ar'): 158, ('par', 't'): 159, ('part', 'ic'): 160, ('partic', 'u'): 161, ('particu', 'l'): 162, ('particul', 'ar'): 163, ('particular', 'i'): 164, ('particulari', 's'): 165, ('particularis', 'e</w>'): 166, ('p', 'a'): 167, ('pa', 't'): 168, ('pat', 'r'): 169, ('patr', 'ic'): 170, ('patric', 'i'): 171, ('patrici', 'an'): 172, ('patrician', 's</w>'): 173, ('p', 'i'): 174, ('pi', 'kes'): 175, ('pikes', ',</w>'): 176, ('p', 'o'): 177, ('po', 'or</w>'): 178, ('r', 'a'): 179, ('ra', 'kes'): 180, ('rakes', ':</w>'): 181, ('reliev', 'ed</w>'): 182, ('reveng', 'e'): 183, ('revenge', '.</w>'): 184, ('s', 'p'): 185, ('sp', 'e'): 186, ('spe', 'a'): 187, ('spea', 'k</w>'): 188, ('su', 'ff'): 189, ('suff', 'er'): 190, ('suffer', 'anc'): 191, ('sufferanc', 'e</w>'): 192, ('su', 'p'): 193, ('sup', 'er'): 194, ('super', 'f'): 195, ('superf', 'l'): 196, ('superfl', 'u'): 197, ('superflu', 'it'): 198, ('superfluit', 'y,</w>'): 199, ('su', 'r'): 200, ('sur', 'f'): 201, ('surf', 'e'): 202, ('surfe', 'it'): 203, ('surfeit', 's</w>'): 204, ('th', 'a'): 205, ('tha', 't</w>'): 206, ('the', 'i'): 207, ('thei', 'r</w>'): 208, ('the', 'm'): 209, ('them', '</w>'): 210, ('th', 'in'): 211, ('thin', 'k</w>'): 212, ('th', 'irst</w>'): 213, ('to', 'o'): 214, ('too', '</w>'): 215, ('u', 's,</w>'): 216, ('u', 's'): 217, ('us', ':</w>'): 218, ('w', 'ere</w>'): 219, ('wh', 'i'): 220, ('whi', 'l'): 221, ('whil', 'e</w>'): 222, ('wh', 'o'): 223, ('who', 'l'): 224, ('whol', 'es'): 225, ('wholes', 'om'): 226, ('wholesom', 'e'): 227, ('wholesome', ',</w>'): 228, ('w', 'i'): 229}\n",
      "\n",
      "('a',)\n",
      "('abundance;',)\n",
      "('accounted',)\n",
      "('afflicts',)\n",
      "('an',)\n",
      "('are',)\n",
      "('as',)\n",
      "('authority',)\n",
      "('become',)\n",
      "('bread,',)\n",
      "('but',)\n",
      "('citizen', ':')\n",
      "('citizens,',)\n",
      "('dear:',)\n",
      "('ere',)\n",
      "('f', 'irst')\n",
      "('for',)\n",
      "('gain',)\n",
      "('gods',)\n",
      "('good.',)\n",
      "('guess',)\n",
      "('humanely;',)\n",
      "('hunger',)\n",
      "('i',)\n",
      "('if',)\n",
      "('in',)\n",
      "('inventory',)\n",
      "('is',)\n",
      "('it',)\n",
      "('know',)\n",
      "('leanness',)\n",
      "('le', 't')\n",
      "('might',)\n",
      "('misery,',)\n",
      "('not',)\n",
      "('object',)\n",
      "('of',)\n",
      "('on',)\n",
      "('our',)\n",
      "('particularise',)\n",
      "('patricians',)\n",
      "('pikes,',)\n",
      "('poor',)\n",
      "('rakes:',)\n",
      "('relieve',)\n",
      "('relieved',)\n",
      "('revenge',)\n",
      "('revenge.',)\n",
      "('speak',)\n",
      "('sufferance',)\n",
      "('superfluity,',)\n",
      "('surfeits',)\n",
      "('that',)\n",
      "('the',)\n",
      "('their',)\n",
      "('them',)\n",
      "('they',)\n",
      "('think',)\n",
      "('thirst',)\n",
      "('this',)\n",
      "('to',)\n",
      "('too',)\n",
      "('us',)\n",
      "('us,',)\n",
      "('us:',)\n",
      "('we',)\n",
      "('were',)\n",
      "('wh', 'a', 't')\n",
      "('while',)\n",
      "('wholesome,',)\n",
      "('wi', 'th')\n",
      "('would',)\n",
      "('y', 'i', 'e', 'ld')\n"
     ]
    }
   ],
   "source": [
    "# train BPE with our input text\n",
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"Compute frequencies of adjacent pairs of symbols.\"\"\"\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "with open('inp_testbpe0.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text.replace('\\n', ' ')\n",
    "words = sorted(list(set(text.lower().split(' '))))\n",
    "wordfreq = {w: text.count(w) for w in words}\n",
    "#print(words)\n",
    "#print(wordfreq)\n",
    "\n",
    "#print({(' '.join([el for el in v.lower()]) + ' </w>'):idx for idx,v in enumerate(text.split(' '))})\n",
    "\n",
    "#train_data = {(' '.join([el for el in v.lower()]) + ' </w>'):idx for idx,v in enumerate(text.split(' '))}\n",
    "#train_data = {(' '.join([el for el in v.lower()]) + ' </w>'):idx for idx,v in enumerate(words)}\n",
    "#train_data = {(v + '</w>'):idx for idx,v in enumerate(words)}\n",
    "#train_data = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
    "#train_data = wordfreq\n",
    "train_data = {(' '.join([el for el in key]) + ' </w>'):val for key,val in wordfreq.items()}\n",
    "#print(train_data)\n",
    "#print(get_stats(train_data))\n",
    "\n",
    "bpe_codes = {}\n",
    "bpe_codes_reverse = {}\n",
    "\n",
    "num_merges = 230\n",
    "\n",
    "for i in range(num_merges):\n",
    "    #print(\"### Iteration {}\".format(i + 1))\n",
    "    pairs = get_stats(train_data)\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    train_data = merge_vocab(best, train_data)\n",
    "    \n",
    "    bpe_codes[best] = i\n",
    "    bpe_codes_reverse[best[0] + best[1]] = best\n",
    "    \n",
    "    #print(\"new merge: {}\".format(best))\n",
    "    #print(\"train data: {}\".format(train_data))\n",
    "\n",
    "print(bpe_codes)\n",
    "print()\n",
    "\n",
    "for word in words:\n",
    "    print(encode_bp(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1169af-b7ad-4937-83f3-976f432ccb44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
