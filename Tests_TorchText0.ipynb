{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from http://www.statmt.org/europarl/\n",
    "\n",
    "europarl_en = open(r'D:\\BernieData\\ML4D_copy\\data/europarl-v7.fr-en.en', encoding='utf-8').read().split('\\n')\n",
    "europarl_fr = open(r'D:\\BernieData\\ML4D_copy\\data/europarl-v7.fr-en.fr', encoding='utf-8').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text using Torchtext and Spacy together\n",
    "import spacy\n",
    "import torchtext\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "\n",
    "#en = spacy.load('en')\n",
    "#fr = spacy.load('fr')\n",
    "en = spacy.load('en_core_web_sm')\n",
    "fr = spacy.load('fr_core_news_sm')\n",
    "\n",
    "def tokenize_en(sentence):\n",
    "    return [tok.text for tok in en.tokenizer(sentence)]\n",
    "def tokenize_fr(sentence):\n",
    "    return [tok.text for tok in fr.tokenizer(sentence)]\n",
    "\n",
    "\n",
    "EN_TEXT = Field(tokenize=tokenize_en)\n",
    "FR_TEXT = Field(tokenize=tokenize_fr, init_token = \"<sos>\", eos_token = \"<eos>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn our data into an appropriate CSV file\n",
    "import pandas as pd\n",
    "\n",
    "raw_data = {'English' : [line for line in europarl_en], 'French': [line for line in europarl_fr]}\n",
    "df = pd.DataFrame(raw_data, columns=[\"English\", \"French\"])\n",
    "\n",
    "# remove very long sentences and sentences where translations are\n",
    "# not of roughly equal length\n",
    "df['eng_len'] = df['English'].str.count(' ')\n",
    "df['fr_len'] = df['French'].str.count(' ')\n",
    "df = df.query('fr_len < 80 & eng_len < 80')\n",
    "df = df.query('fr_len < eng_len * 1.5 & fr_len * 1.5 > eng_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to create a validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create train and validation set\n",
    "train, val = train_test_split(df, test_size=0.1)\n",
    "train.to_csv(\"train.csv\", index=False)\n",
    "val.to_csv(\"val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the magic TabularDataset.splits then returns a train and validation dataset with\n",
    "# the respective data loaded into them, processed(/tokenized) according to the fields we\n",
    "# defined earlier.\n",
    "\n",
    "# associate the text in the 'English' column with the EN_TEXT field,\n",
    "# and 'French' with FR_TEXT\n",
    "data_fields = [('English', EN_TEXT), ('French', FR_TEXT)]\n",
    "train,val = TabularDataset.splits(path='./', train='train.csv', validation='val.csv', format='csv', fields=data_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index all the tokens\n",
    "\n",
    "FR_TEXT.build_vocab(train, val)\n",
    "EN_TEXT.build_vocab(train, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "print(EN_TEXT.vocab.stoi['the'])\n",
    "print(EN_TEXT.vocab.itos[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[20984,  5961,    27,  ...,    68,  8574,    50],\n",
      "        [   86,  7561,    32,  ...,   328,   393,    30],\n",
      "        [   14,   168,    28,  ...,     3,    30,     6],\n",
      "        ...,\n",
      "        [    1,     1,     1,  ...,     1,     1,     1],\n",
      "        [    1,     1,     1,  ...,     1,     1,     1],\n",
      "        [    1,     1,     1,  ...,     1,     1,     1]])\n"
     ]
    }
   ],
   "source": [
    "#example input: print(EN_TEXT.vocab.stoi['the'])\n",
    "#example_output: 11\n",
    "#example input: print(EN_TEXT.vocab.itos[11])\n",
    "#example_output: 'the'\n",
    "    \n",
    "train_iter = BucketIterator(train, batch_size=20, sort_key=lambda x: len(x.French), shuffle=True)\n",
    "batch = next(iter(train_iter))\n",
    "print(batch.English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacking TorchText\n",
    "# code from http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "# read text after for description of what it does\n",
    "from torchtext import data\n",
    "global max_src_in_batch, max_tgt_in_batch\n",
    "\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch, len(new.English))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch, len(new.French) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)\n",
    "\n",
    "class MyIterator(data.Iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(sorted(p, key=self.sort_key), self.batch_size, self.batch_size_fn)\n",
    "                for b in random_shuffler(list(p_batch)):\n",
    "                    yield b  \n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size, self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more efficient iterator\n",
    "import torch\n",
    "torch.device = 'cuda:0'\n",
    "train_iter = MyIterator(train, batch_size=1300, repeat=False, sort_key= lambda x: (len(x.English), len(x.French)), batch_size_fn=batch_size_fn, train=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2622,    27,    43,  ...,  5175, 18817,    43],\n",
      "        [  522,  1060,     8,  ...,     3,     6,   880],\n",
      "        [59264,     5,    15,  ...,   938,  1460,     2],\n",
      "        ...,\n",
      "        [    6, 15133,     5,  ...,    19,    49,  1575],\n",
      "        [29028, 16011,  1772,  ...,    55,   370,  3917],\n",
      "        [    4,     4,     4,  ...,     4,     4,     4]])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter)) # sentences vertically (columns), each index represents a token (word)\n",
    "print(batch.English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its\n",
      "Article\n",
      "I-41\n",
      ",\n",
      "paragraph\n",
      "3\n",
      "states\n",
      "that\n",
      "‘\n",
      "the\n",
      "Member\n",
      "States\n",
      "are\n",
      "obliged\n",
      "to\n",
      "progressively\n",
      "improve\n",
      "their\n",
      "military\n",
      "capacities\n",
      "’\n",
      ",\n",
      "which\n",
      "means\n",
      "that\n",
      "the\n",
      "EU\n",
      "’s\n",
      "Member\n",
      "States\n",
      "are\n",
      "obliged\n",
      "to\n",
      "rearm\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in batch.English[:,0]:\n",
    "    print(EN_TEXT.vocab.itos[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
