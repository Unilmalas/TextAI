{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'brown': 0, u'lazy': 4, u'jumped': 3, u'over': 5, u'fox': 2, u'dog': 1, u'quick': 6, u'the': 7}\n",
      "(1, 8)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_) # vocabulary: which word is represented by which number? -> id of word\n",
    "# encode document\n",
    "vector = vectorizer.transform(text) # create encoded vector: the index is the id of the word, only \"the\" (id=index=7) appears twice\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'brown': 0, u'lazy': 4, u'jumped': 3, u'over': 5, u'fox': 2, u'dog': 1, u'quick': 6, u'the': 7}\n",
      "[ 1.69314718  1.28768207  1.28768207  1.69314718  1.69314718  1.69314718\n",
      "  1.69314718  1.        ]\n",
      "(3, 8)\n",
      "[[ 0.36388646  0.27674503  0.27674503  0.36388646  0.36388646  0.36388646\n",
      "   0.36388646  0.42983441]\n",
      " [ 0.          0.78980693  0.          0.          0.          0.          0.\n",
      "   0.61335554]\n",
      " [ 0.          0.          0.78980693  0.          0.          0.          0.\n",
      "   0.61335554]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "# Term Frequency: This summarizes how often a given word appears within a document.\n",
    "# Inverse Document Frequency: This downscales words that appear a lot across documents.\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\"The dog.\",\"The fox\"]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocabulary\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_) # idf=inverse document frequencies: the most frequent word \"the\" is assigned the lowest frequency, 1\n",
    "# encode document (as a sparse matrix, scores are normalized)\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape) # shape = [n_samples, n_features], will be (3,8) here\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\"The dog.\",\"The fox\"]\n",
    "# create the transform, tokenize and build vocab\n",
    "cvectorizer = CountVectorizer().fit(text)\n",
    "cvector = cvectorizer.transform(text) # create encoded vector: the index is the id of the word, only \"the\" (id=index=7) appears twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# list of text documents\n",
    "# Term Frequency: This summarizes how often a given word appears within a document.\n",
    "# Inverse Document Frequency: This downscales words that appear a lot across documents.\n",
    "# create the transform, tokenize and build vocabulary\n",
    "tvectorizer = TfidfTransformer(use_idf=False).fit(cvector)\n",
    "tvector = tvectorizer.transform(cvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NB learning\n",
    "from sklearn.naive_bayes import MultinomialNB # naive Bayes classification\n",
    "import numpy as np\n",
    "clf = MultinomialNB().fit(tvector, np.array([0,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2]\n"
     ]
    }
   ],
   "source": [
    "# check results of training\n",
    "docs_new = [\"lazy dog\", \"dog\",\"fox\"]\n",
    "X_new_counts = cvectorizer.transform(docs_new)\n",
    "X_new_tf = tvectorizer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tf)\n",
    "\n",
    "print(predicted)\n",
    "\n",
    "#for doc, category in zip(docs_new, predicted):\n",
    "#    print('%r => %s' % (doc, np.array([0,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "[[ 0.          0.          0.          0.          0.          0.33333333\n",
      "   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n",
      "   0.          0.          0.         -0.33333333  0.          0.\n",
      "  -0.66666667  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = HashingVectorizer(n_features=20) # one way hash of words to convert them to integers\n",
    "# encode document (downside is that the hash is a one-way function so there is no way to convert the encoding back to a word)\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "[7, 4, 11, 11, 14, 26, 22, 14, 17, 11, 3]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "h\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding of 'hello world'\n",
    "from numpy import argmax\n",
    "# define input string\n",
    "data = 'hello world'\n",
    "print(data)\n",
    "# define universe of possible input values\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz '\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet)) # seasons = ['Spring',... -> [(0, 'Spring'),...\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# integer encode input data\n",
    "integer_encoded = [char_to_int[char] for char in data]\n",
    "print(integer_encoded)\n",
    "# one hot encode\n",
    "onehot_encoded = list()\n",
    "for value in integer_encoded:\n",
    "\tletter = [0 for _ in range(len(alphabet))]\n",
    "\tletter[value] = 1\n",
    "\tonehot_encoded.append(letter)\n",
    "print(onehot_encoded)\n",
    "# invert encoding\n",
    "inverted = int_to_char[argmax(onehot_encoded[0])]\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<type 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "wdict = {'a': 0, 'b': 1, 'c': 2, 'd': 3}\n",
    "dictarr = np.asarray(wdict.values()).reshape(-1, 1)\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(dictarr)\n",
    "enc.transform([[2]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vectorize text with skip-grams in scikit-learn by passing the skip gram tokens as the vocabulary\n",
    "# to CountVectorizer will not work -> example vectorizer that produces 1-skip-2-grams\n",
    "from toolz import itertoolz, compose\n",
    "from toolz.curried import map as cmap, sliding_window, pluck\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# pluck: plucking “fields” from an iterable of values e.g. pluck(objects, 'age') -> [30, 56, 56]\n",
    "#        or list(pluck([0, 1], [[1, 2, 3], [4, 5, 7]])) -> [(1, 2), (4, 5)]\n",
    "# sliding_window creates a sliding window: list(sliding_window(2, [1, 2, 3, 4])) -> [(1, 2), (2, 3), (3, 4)]\n",
    "# map: apply function to every item of iterable and return a list of the results\n",
    "# curried form of map: map(func,[[1,2],[3,4]]) can be written as map(func)([[1,2],[3,4]])\n",
    "\n",
    "class SkipGramVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):    \n",
    "        preprocess = self.build_preprocessor() # Return a function to preprocess the text before tokenization\n",
    "        stop_words = self.get_stop_words() # Build or fetch the effective stop words (words that are filtered out) list\n",
    "        tokenize = self.build_tokenizer() # Return a function that splits a string into a sequence of tokens\n",
    "        return lambda doc: self._word_skip_grams( # lambda-functions: anonymous functions not bound to a name\n",
    "                compose(tokenize, preprocess, self.decode)(doc), # compose: ompose functions to operate in series\n",
    "                stop_words)\n",
    "\n",
    "    def _word_skip_grams(self, tokens, stop_words=None):\n",
    "        # handle stop words\n",
    "        if stop_words is not None:\n",
    "            tokens = [w for w in tokens if w not in stop_words]\n",
    "        print list(sliding_window(3, tokens))\n",
    "        return compose(cmap(' '.join), pluck([0, 2]), sliding_window(3))(tokens) # str.join(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'the', u'rain', u'in'), (u'rain', u'in', u'spain'), (u'in', u'spain', u'falls'), (u'spain', u'falls', u'mainly'), (u'falls', u'mainly', u'on'), (u'mainly', u'on', u'the'), (u'on', u'the', u'plain')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'falls on',\n",
       " u'in falls',\n",
       " u'mainly the',\n",
       " u'on plain',\n",
       " u'rain spain',\n",
       " u'spain mainly',\n",
       " u'the in']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['the rain in Spain falls mainly on the plain']\n",
    "\n",
    "vect = SkipGramVectorizer()\n",
    "vect.fit(text) # Learn a vocabulary dictionary of all tokens in the raw documents\n",
    "vect.get_feature_names() # Array mapping from feature integer indices to feature name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 5, 6]\n",
      "a#-#b#-#c\n"
     ]
    }
   ],
   "source": [
    "tokens = [2,3,4,5,6]\n",
    "stop_words = [0,3]\n",
    "print [w for w in tokens if w not in stop_words]\n",
    "separator = \"#-#\"\n",
    "sequence = (\"a\", \"b\", \"c\")\n",
    "print separator.join(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vectorize text with skip-grams in scikit-learn by passing the skip gram tokens as the vocabulary\n",
    "# to CountVectorizer will not work -> example vectorizer that produces 1-skip-nsize-grams\n",
    "from toolz import itertoolz, compose\n",
    "from toolz.curried import map as cmap, sliding_window, pluck\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# pluck: plucking “fields” from an iterable of values e.g. pluck(objects, 'age') -> [30, 56, 56]\n",
    "#        or list(pluck([0, 1], [[1, 2, 3], [4, 5, 7]])) -> [(1, 2), (4, 5)]\n",
    "# sliding_window creates a sliding window: list(sliding_window(2, [1, 2, 3, 4])) -> [(1, 2), (2, 3), (3, 4)]\n",
    "# map: apply function to every item of iterable and return a list of the results\n",
    "# curried form of map: map(func,[[1,2],[3,4]]) can be written as map(func)([[1,2],[3,4]])\n",
    "\n",
    "nsize = 5\n",
    "\n",
    "class SkipGramVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        preprocess = self.build_preprocessor() # Return a function to preprocess the text before tokenization\n",
    "        stop_words = self.get_stop_words() # Build or fetch the effective stop words (words that are filtered out) list\n",
    "        tokenize = self.build_tokenizer() # Return a function that splits a string into a sequence of tokens\n",
    "        return lambda doc: self._word_skip_grams( # lambda-functions: anonymous functions not bound to a name\n",
    "                compose(tokenize, preprocess, self.decode)(doc), # compose: ompose functions to operate in series\n",
    "                stop_words)\n",
    "\n",
    "    def _word_skip_grams(self, tokens, stop_words=None):\n",
    "        # handle stop words\n",
    "        if stop_words is not None:\n",
    "            tokens = [w for w in tokens if w not in stop_words]\n",
    "        # print list(compose(cmap(' '.join), pluck(range(0,nsize)), sliding_window(nsize))(tokens))\n",
    "        return compose(cmap(' '.join), pluck(range(0,nsize)), sliding_window(nsize))(tokens) # str.join(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'falls mainly on the plain',\n",
       " u'hardly any rain stains the',\n",
       " u'in spain falls mainly on',\n",
       " u'mainly on the plain with',\n",
       " u'on the plain with pain',\n",
       " u'pain where hardly any rain',\n",
       " u'plain with pain where hardly',\n",
       " u'rain in spain falls mainly',\n",
       " u'spain falls mainly on the',\n",
       " u'the plain with pain where',\n",
       " u'the rain in spain falls',\n",
       " u'where hardly any rain stains',\n",
       " u'with pain where hardly any']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['the rain in Spain falls mainly on the plain with pain where hardly any rain stains the']\n",
    "\n",
    "vect = SkipGramVectorizer()\n",
    "vect.fit(text) # Learn a vocabulary dictionary of all tokens in the raw documents\n",
    "vect.get_feature_names() # Array mapping from feature integer indices to feature name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'on': 6, 'pain': 10, 'stains': 15, 'plain': 8, 'mainly': 5, 'rain': 1, 'falls': 4, 'where': 11, 'hardly': 12, 'in': 2, 'the': 0, 'with': 9, 'any': 13, 'spain': 3}\n"
     ]
    }
   ],
   "source": [
    "# build a dictonary from a text as input to a one-hot encoder\n",
    "# the number of unique words == the vocabulary == dimension V\n",
    "from toolz import itertoolz\n",
    "\n",
    "text = 'the rain in Spain falls mainly on the plain with pain where hardly any rain stains the'\n",
    "textlist = text.split(\" \")\n",
    "myDict = {}\n",
    "i = 0\n",
    "for word in textlist:\n",
    "    # print word, i\n",
    "    newEntry = {word.lower(): i}\n",
    "    if not myDict.has_key(word):\n",
    "        myDict.update(newEntry)\n",
    "    i = i + 1\n",
    "\n",
    "print myDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encode the dictionary\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "dictarr = np.asarray(myDict.values()).reshape(-1, 1)\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(dictarr)\n",
    "enc.transform([[8]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'on': 6, u'the': 0, u'plain': 8, u'mainly': 5, u'falls': 4}, {u'stains': 15, u'the': 0, u'hardly': 12, u'any': 13, u'rain': 1}, {u'on': 6, u'falls': 4, u'mainly': 5, u'spain': 3, u'in': 2}, {u'with': 9, u'on': 6, u'the': 0, u'mainly': 5, u'plain': 8}, {u'on': 6, u'the': 0, u'with': 9, u'pain': 10, u'plain': 8}, {u'where': 11, u'pain': 10, u'any': 13, u'rain': 1, u'hardly': 12}, {u'plain': 8, u'with': 9, u'where': 11, u'hardly': 12, u'pain': 10}, {u'falls': 4, u'mainly': 5, u'spain': 3, u'rain': 1, u'in': 2}, {u'on': 6, u'the': 0, u'mainly': 5, u'spain': 3, u'falls': 4}, {u'plain': 8, u'the': 0, u'with': 9, u'where': 11, u'pain': 10}, {u'the': 0, u'falls': 4, u'spain': 3, u'rain': 1, u'in': 2}, {u'stains': 15, u'hardly': 12, u'where': 11, u'any': 13, u'rain': 1}, {u'any': 13, u'pain': 10, u'with': 9, u'where': 11, u'hardly': 12}]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode the 1-skip-ngrams from SkipGramVectorizer: map words from vocabulary (dict myDict)\n",
    "\n",
    "from toolz import itertoolz\n",
    "\n",
    "text = 'the rain in Spain falls mainly on the plain with pain where hardly any rain stains the'\n",
    "\n",
    "ovecm = []\n",
    "for ovec in vect.get_feature_names():\n",
    "    # print ovec\n",
    "    ovecd = {}\n",
    "    for oword in ovec.split(\" \"):\n",
    "        # print (oword, myDict[oword])\n",
    "        ovecd[oword] = myDict[oword]\n",
    "    ovecm.append(ovecd)\n",
    "\n",
    "print ovecm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  0.  0.]]\n",
      "[[ 1.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.]]\n",
      "[[ 1.  0.  0.  0.  0.]]\n",
      "[[ 1.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.]]\n",
      "[[ 1.  0.  0.  0.  0.]]\n",
      "[[ 1.  0.  0.  0.  0.]]\n",
      "[[ 1.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# now one-hot encode the words mapped from vocabulary\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "for wdict in ovecm:\n",
    "    # print np.asarray(wdict.values()).reshape(-1, 1)\n",
    "    dictarr = np.asarray(wdict.values()).reshape(-1, 1) # extract values from dict (.values), as array and transform (reshape)\n",
    "    enc = OneHotEncoder() # Encode categorical integer features using a one-hot aka one-of-K scheme\n",
    "    # The output will be a sparse matrix where each column corresponds to one possible value of one feature\n",
    "    enc.fit(dictarr) # Fit OneHotEncoder to dictarr\n",
    "    print enc.transform([[0]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02  0.14  0.02  0.02  0.01  0.01  0.02  0.01]]\n"
     ]
    }
   ],
   "source": [
    "# skip-gram learning example\n",
    "# model structure\n",
    "# xk -> wi -> ht -> wo -> yk : tc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "wi = np.array([[-0.094, -0.44, 0.31], [-0.491, -0.23, 0.065], [0.07, 0.17, -0.36], [0.1, 0.46, 0.08], [-0.23, -0.15, -0.04], [0.41, -0.19, -0.44], [0.18, 0.09, 0.28], [-0.05, 0.49, 0.26]])\n",
    "wo = np.array([[0.02, 0.48, 0.43, 0.37, -0.36, -0.12, 0.27, -0.35], [-0.37, 0.42, -0.26, -0.15, 0.03, 0.35, -0.14, 0.13], [0.42, 0.36, 0.47, -0.02, -0.42, -0.44, 0.27, -0.45]])\n",
    "xk = np.array([[0, 1, 0, 0, 0, 0, 0, 0]])\n",
    "ht = np.dot(xk, wi)\n",
    "u0 = np.dot(ht, wo)\n",
    "\n",
    "#print u0\n",
    "yk = np.exp(u0) / np.dot(np.exp(u0), np.exp(u0).transpose())\n",
    "#print yk\n",
    "\n",
    "# backpropagation (following Xin Rong's paper here)\n",
    "tc = np.array([[0, 1, 0, 0, 0, 0, 0, 0]]) # truth\n",
    "ej = yk - tc # error\n",
    "etha = 0.01\n",
    "\n",
    "# Update equation for hidden→output weights\n",
    "wo = wo - etha * (np.transpose(ht) * ej)\n",
    "# Update equation for input→hidden weights\n",
    "ehi = np.dot(ej, np.transpose(wo)) # dE/dhi\n",
    "wi = wi - etha * ehi\n",
    "\n",
    "# next cycles\n",
    "i=0\n",
    "while i<200:\n",
    "    ht = np.dot(xk, wi)\n",
    "    u0 = np.dot(ht, wo)\n",
    "    yk = np.exp(u0) / np.dot(np.exp(u0), np.exp(u0).transpose())\n",
    "    ej = yk - tc # error\n",
    "    #print ej\n",
    "    #ei = np.sum(ej) sum has to be over context, here C=1\n",
    "    wo = wo - etha * (np.transpose(ht) * ej)\n",
    "    ehi = np.dot(ej, np.transpose(wo)) # dE/dhi\n",
    "    wi = wi - etha * ehi\n",
    "    i += 1\n",
    "\n",
    "#print '%.2f' % yk\n",
    "np.set_printoptions(precision=2)\n",
    "print yk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.104993585404\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def sig(x, ds):\n",
    "    if ds:\n",
    "        return sig(x, False) * (1.0 - sig(x, False))\n",
    "    return 1.0 / (1.0 + math.exp(-x))\n",
    "\n",
    "print sig(2.0, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02  0.2   0.02  0.01  0.01  0.01  0.02  0.01]]\n"
     ]
    }
   ],
   "source": [
    "# negative sampling weight updates (based on above example)\n",
    "# model structure\n",
    "# xk -> wi -> ht -> wo -> yk : tc\n",
    "import numpy as np\n",
    "\n",
    "def sig(x, ds):\n",
    "    if ds:\n",
    "        return sig(x, False) * (1.0 - sig(x, False))\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "# negative sample: all the words that train to 0: P(wi)=f(wi^3/4)/sum(f(wi)^3/4)\n",
    "# for now manual\n",
    "wneg = [0, 0, 0, 0, 1, 1, 1, 1] # truth is the 2nd word, 4 words in the negative sample\n",
    "\n",
    "# vocabulary of V=8, N=3 hidden nodes: wi(VxN), wo(NxV)\n",
    "wi = np.array([[-0.094, -0.44, 0.31], [-0.491, -0.23, 0.065], [0.07, 0.17, -0.36], [0.1, 0.46, 0.08], [-0.23, -0.15, -0.04], [0.41, -0.19, -0.44], [0.18, 0.09, 0.28], [-0.05, 0.49, 0.26]])\n",
    "wo = np.array([[0.02, 0.48, 0.43, 0.37, -0.36, -0.12, 0.27, -0.35], [-0.37, 0.42, -0.26, -0.15, 0.03, 0.35, -0.14, 0.13], [0.42, 0.36, 0.47, -0.02, -0.42, -0.44, 0.27, -0.45]])\n",
    "xk = np.array([[0, 1, 0, 0, 0, 0, 0, 0]])\n",
    "ht = np.dot(xk, wi)\n",
    "u0 = np.dot(ht, wo)\n",
    "\n",
    "yk = np.exp(u0) / np.dot(np.exp(u0), np.exp(u0).transpose())\n",
    "\n",
    "# backpropagation (following Xin Rong's paper here)\n",
    "tc = np.array([[0, 1, 0, 0, 0, 0, 0, 0]]) # truth\n",
    "ej = yk - tc # error\n",
    "etha = 0.01\n",
    "\n",
    "# \"label\" of the word: tj=1 if wj in positive sample, t=0 otherwise\n",
    "tj = [0, 1, 0, 0, 0, 0, 0, 0] # this is equal to truth tc\n",
    "\n",
    "# Update equation for hidden→output weights\n",
    "# todo here: only for wo and wneg -> need to restrict\n",
    "wo = wo - etha * (sig(wo * np.transpose(ht), False) - tj) * np.transpose(ht)\n",
    "# Update equation for input→hidden weights\n",
    "ehi = (sig(wo * np.transpose(ht), False) - tj) * wo\n",
    "wi = wi - etha * np.transpose(ehi)\n",
    "\n",
    "#print yk\n",
    "\n",
    "# next cycles\n",
    "i=0\n",
    "while i<400:\n",
    "    ht = np.dot(xk, wi)\n",
    "    u0 = np.dot(ht, wo)\n",
    "    yk = np.exp(u0) / np.dot(np.exp(u0), np.exp(u0).transpose())\n",
    "    ej = yk - tc # error\n",
    "    wo = wo - etha * (sig(wo * np.transpose(ht), False) - tj) * np.transpose(ht)\n",
    "    ehi = (sig(wo * np.transpose(ht), False) - tj) * wo\n",
    "    wi = wi - etha * np.transpose(ehi)\n",
    "    i += 1\n",
    "\n",
    "print yk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never: 0.776\n",
      "not: 0.684\n",
      "others: 0.684\n",
      "some: 0.776\n",
      "test: 0.776\n",
      "text: 0.684\n",
      "the: 0.842\n",
      "this: 0.776\n",
      "was: 0.684\n"
     ]
    }
   ],
   "source": [
    "# word training set threshold\n",
    "import math\n",
    "\n",
    "# Run-length encoding of a list\n",
    "def rlencode(lst):\n",
    "    lstlst = []\n",
    "    sublst = []\n",
    "    previtem = lst[0]\n",
    "    for item in lst:\n",
    "        if item != previtem:\n",
    "            lstlst.append(sublst)\n",
    "            sublst = []\n",
    "        sublst.append(item)\n",
    "        previtem = item\n",
    "    lstlst.append(sublst)\n",
    "    lst = []    \n",
    "    for item in lstlst:\n",
    "        lst.append([len(item), item[0]])\n",
    "    return lst\n",
    "\n",
    "# threshold probability\n",
    "def thresprob(f, t):\n",
    "    if f > 0.0:\n",
    "        return 1.0 - math.sqrt(t / f)\n",
    "    else:\n",
    "        reurn -1.0\n",
    "\n",
    "text = 'this test text the the was not this test some others some the never the never'\n",
    "textlist = text.split(\" \")\n",
    "textlist.sort()\n",
    "wordfreqs = rlencode(textlist)\n",
    "for elem in wordfreqs:\n",
    "    print('{0}: {1:.3f}'.format(elem[1], thresprob(elem[0], 0.1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 7116\n",
      "[ 5.44265126  5.44265126  5.44265126 ...,  5.44265126  5.44265126\n",
      "  5.44265126]\n",
      "(169, 7116)\n",
      "[[ 0.12712286  0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "5.23157147097\n",
      "0.414852073285\n"
     ]
    }
   ],
   "source": [
    "# read a large text file, line by line and build vocabulary\n",
    "import os\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import operator # to sort dict\n",
    "\n",
    "def read1k():\n",
    "    return f.read(1024)\n",
    "\n",
    "def process_data(chunk, text):\n",
    "    #print('processed: {0}'.format(ctr))\n",
    "    text.append(unicode(chunk, errors='ignore')) # 'utf8' codec can't decode byte 0xc3\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    # read corpus\n",
    "    os.chdir('C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "    f = open('todinvenedig.txt')\n",
    "    text = []\n",
    "    for piece in iter(read1k, ''):\n",
    "        process_data(piece, text)\n",
    "    \n",
    "    # get stopwords\n",
    "    f = open('gerstopw0.txt')\n",
    "    stptext = []\n",
    "    for piece in iter(read1k, ''):\n",
    "        process_data(piece, stptext)\n",
    "    stopwrds = []\n",
    "    for elem in stptext:\n",
    "        stopwrds += elem.split('\\n')\n",
    "    #print stopwrds\n",
    "    \n",
    "    # create the transform\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', stop_words=stopwrds)\n",
    "    # tokenize and build vocabulary\n",
    "    vectorizer.fit(text)\n",
    "    # summarize\n",
    "    #print(vectorizer.vocabulary_)\n",
    "    print('size of vocabulary: {0}'.format(len(vectorizer.vocabulary_)))\n",
    "    print(vectorizer.idf_) # idf=inverse document frequencies: the most frequent word \"the\" is assigned the lowest frequency, 1\n",
    "    # encode document (as a sparse matrix, scores are normalized)\n",
    "    vector = vectorizer.transform(text)\n",
    "    # summarize encoded vector\n",
    "    print(vector.shape) # shape = [n_samples, n_features]\n",
    "    print(vector.toarray())\n",
    "    \n",
    "    \n",
    "    # print the idf for the vocabulary, sort dict: sorted_x = sorted(x.items(), key=operator.itemgetter(1))\n",
    "    idf = vectorizer.idf_\n",
    "    featureidfs = sorted(dict(zip(vectorizer.get_feature_names(), idf)).items(), key=operator.itemgetter(1))\n",
    "    favrg = 0.0\n",
    "    for f in featureidfs:\n",
    "        favrg += f[1]\n",
    "    #print featureidfs\n",
    "    favrg /= float(len(featureidfs))\n",
    "    print favrg\n",
    "    fstdev = 0.0\n",
    "    for f in featureidfs:\n",
    "        fstdev += (f[1] - favrg) * (f[1] - favrg)\n",
    "    fstdev = math.sqrt(fstdev / float(len(featureidfs) - 1))\n",
    "    print fstdev\n",
    "    \n",
    "    # delete all entries from featureidfs that have very low scores (2 std dev)\n",
    "    #flimit = favrg - 2.0 * fstdev\n",
    "    #for f in featureidfs:\n",
    "        #if f[1] < flimit:\n",
    "            #print f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'aber', u'alle', u'allem', u'allen', u'aller', u'alles', u'als', u'also', u'am', u'an', u'ander', u'andere', u'anderem', u'anderen', u'anderer', u'anderes', u'anderm', u'andern', u'anderr', u'anders', u'auch', u'auf', u'aus', u'bei', u'bin', u'bis', u'bist', u'da', u'damit', u'dann', u'der', u'den', u'des', u'dem', u'die', u'das', u'da\\ufffd\\ufffd', u'dass', u'derselbe', u'derselben', u'denselben', u'desselben', u'demselben', u'dieselbe', u'dieselben', u'dasselbe', u'dazu', u'dein', u'deine', u'deinem', u'deinen', u'deiner', u'deines', u'denn', u'derer', u'dessen', u'dich', u'dir', u'du', u'dies', u'diese', u'diesem', u'diesen', u'dieser', u'dieses', u'doch', u'dort', u'durch', u'ein', u'eine', u'einem', u'einen', u'einer', u'eines', u'einig', u'einige', u'einigem', u'einigen', u'einiger', u'einiges', u'einmal', u'er', u'ihn', u'ihm', u'es', u'etwas', u'euer', u'eure', u'eurem', u'euren', u'eurer', u'eures', u'f\\ufffd\\ufffdr', u'gegen', u'gewesen', u'hab', u'habe', u'haben', u'hat', u'hatte', u'hatten', u'hier', u'hin', u'hinter', u'ich', u'mich', u'mir', u'ihr', u'ihre', u'ihrem', u'ihren', u'ihrer', u'ihres', u'euch', u'im', u'in', u'indem', u'ins', u'ist', u'jede', u'jedem', u'jeden', u'jeder', u'jedes', u'jene', u'jenem', u'jenen', u'jener', u'jenes', u'jetzt', u'kann', u'kein', u'keine', u'keinem', u'keinen', u'keiner', u'keines', u'k\\ufffd\\ufffdnnen', u'k\\ufffd\\ufffdnnte', u'machen', u'man', u'manche', u'manchem', u'manchen', u'mancher', u'manches', u'mein', u'meine', u'meinem', u'meinen', u'meiner', u'meines', u'mit', u'muss', u'musste', u'nach', u'nicht', u'nichts', u'noch', u'nun', u'nur', u'ob', u'oder', u'ohne', u'sehr', u'sei', u'sein', u'seine', u'seinem', u'seinen', u'seiner', u'seines', u'selbst', u'sich', u'sie', u'ihne'], [u'n', u'sind', u'so', u'solche', u'solchem', u'solchen', u'solcher', u'solches', u'soll', u'sollte', u'sondern', u'sonst', u'\\ufffd\\ufffdber', u'um', u'und', u'uns', u'unse', u'unsem', u'unsen', u'unser', u'unses', u'unter', u'viel', u'vom', u'von', u'vor', u'w\\ufffd\\ufffdhrend', u'war', u'waren', u'warst', u'was', u'weg', u'weil', u'weiter', u'welche', u'welchem', u'welchen', u'welcher', u'welches', u'wenn', u'werde', u'werden', u'wie', u'wieder', u'will', u'wir', u'wird', u'wirst', u'wo', u'wobei', u'wodurch', u'wollen', u'wollte', u'womit', u'w\\ufffd\\ufffdrde', u'w\\ufffd\\ufffdrden', u'zu', u'zum', u'zur', u'zwar', u'zwischen']]\n"
     ]
    }
   ],
   "source": [
    "# test reading stopword file\n",
    "import os\n",
    "\n",
    "def read1l():\n",
    "    return f.read(1024)\n",
    "\n",
    "def process_data(chunk, text):\n",
    "    text.append(unicode(chunk, errors='replace')) # 'utf8' codec can't decode byte 0xc3\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    os.chdir('C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "    f = open('gerstopw0.txt')\n",
    "    text = []\n",
    "    \n",
    "    for piece in iter(read1k, ''):\n",
    "        process_data(piece, text)\n",
    "    \n",
    "    #print text\n",
    "    \n",
    "    stopwrds = []\n",
    "    \n",
    "    for elem in text:\n",
    "        stopwrds.append(elem.split('\\n'))\n",
    "    \n",
    "    print stopwrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.494480916111\n"
     ]
    }
   ],
   "source": [
    "# size of vocabulary about 7000: test init weight matrices\n",
    "import numpy as np\n",
    "\n",
    "wi = np.random.rand(7000,3) - 0.5\n",
    "print wi[2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_c: 0.115\n",
      "c_g: 1.115\n",
      "d_e: 0.478\n",
      "b_h: 0.700\n",
      "a_d: 0.308\n",
      "a_b: -0.469\n",
      "d_b: -0.107\n",
      "e_g: 0.700\n",
      "b_a: -0.469\n",
      "e_b: -0.300\n",
      "a_h: 1.115\n",
      "b_d: -0.107\n",
      "b_e: -0.300\n",
      "c_e: 0.700\n",
      "g_h: 1.700\n",
      "b_g: 0.115\n",
      "d_g: 0.893\n",
      "g_a: 0.531\n",
      "e_h: 1.285\n",
      "c_d: 0.893\n",
      "e_a: 0.115\n",
      "b_b: -0.885\n",
      "a_c: 0.531\n",
      "d_a: 0.308\n",
      "a_e: 0.115\n",
      "c_b: 0.115\n"
     ]
    }
   ],
   "source": [
    "# Pointwise Mutual Information(PMI)\n",
    "from collections import Counter # implements specialized container datatypes\n",
    "from math import log\n",
    "\n",
    "def gen_bigrams(data, window_size=5):\n",
    "    for idx in range(len(data)):\n",
    "        window = data[idx: idx + window_size]\n",
    "        if len(window) < 2:\n",
    "            break\n",
    "        w = window[0]\n",
    "        for next_word in window[1:]:\n",
    "            yield (w, next_word) # like return but returns a generator (a one-time iterator)\n",
    "            \n",
    "\n",
    "def construct_vocab(data):\n",
    "    vocab = Counter()\n",
    "    for (w1, w2) in gen_bigrams(data, window_size=5): # count 1gram & 2gram\n",
    "        vocab.update([w1, w2, (w1, w2)])\n",
    "    return vocab\n",
    "        \n",
    "\n",
    "def calc_pmi(vocab):\n",
    "    det = sum(vocab.values())\n",
    "    for (w1, w2) in filter(lambda el: isinstance(el, tuple), vocab):\n",
    "        p_a, p_b = float(vocab[w1]), float(vocab[w2])\n",
    "        p_ab = float(vocab[(w1, w2)])\n",
    "        \n",
    "        yield (w1, w2, log((det * p_ab) / (p_a * p_b), 2))\n",
    "    \n",
    "\n",
    "corpus = [\"a\", \"b\", \"c\", \"d\", \"e\", \"b\", \"g\", \"a\", \"h\"]\n",
    "vocab = construct_vocab(corpus)\n",
    "\n",
    "#print vocab\n",
    "\n",
    "#for i in gen_bigrams(corpus):\n",
    "    #print i\n",
    "\n",
    "for (w1, w2, pmi) in calc_pmi(vocab):\n",
    "    print(\"{}_{}: {:.3f}\".format(w1, w2, pmi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "xxxxxxxxxxxxxxxxxxxxxx\n",
      "0\n",
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "a = [('a',1), ('b',2)]\n",
    "for k in a:\n",
    "    # v is the list of grades for student k\n",
    "    #avgDict[k] = sum(v)/ float(len(v))\n",
    "    print k[1]\n",
    "print 'xxxxxxxxxxxxxxxxxxxxxx'\n",
    "mygenerator = (x*x for x in range(3))\n",
    "for i in mygenerator:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['g', 'h', 'a', 'b', 'c'], ['h', 'a', 'b', 'c', 'd'], ['a', 'b', 'c', 'd', 'e'], ['b', 'c', 'd', 'e', 'f'], ['c', 'd', 'e', 'f', 'g'], ['d', 'e', 'f', 'g', 'h']]\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# phrase extraction\n",
    "\n",
    "def valinlst(lst, val):\n",
    "    for elem in lst:\n",
    "        if elem == val:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "a = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
    "win2 = 2\n",
    "i = 0\n",
    "phrase = []\n",
    "phrases = []\n",
    "for elem in a:\n",
    "    if i > -win2-1 and i < len(a)-win2:\n",
    "        for j in range(i-win2,i+win2+1):\n",
    "            phrase.append(a[j])\n",
    "        phrases.append(phrase)\n",
    "        phrase = []\n",
    "    i += 1\n",
    "print phrases\n",
    "# PMI = log(p(phrase)/tt p(w)) w from phrase\n",
    "for elem in phrases:\n",
    "    print valinlst(elem, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          2.30742853  0.          2.30742853]\n",
      " [ 0.          0.          2.30742853  0.          2.30742853]\n",
      " [ 1.69647082  0.          0.          0.          0.        ]\n",
      " [ 0.          0.56192226  0.          0.46325333  0.        ]]\n",
      "[[ 0.          0.          0.74144607  0.          0.74144607]\n",
      " [ 0.          0.          0.74144607  0.          0.74144607]\n",
      " [ 0.80856027  0.          0.          0.          0.        ]\n",
      " [ 0.          0.88346508  0.          0.2757825   0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log2\n",
      "  del sys.path[0]\n",
      "D:\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: divide by zero encountered in log2\n"
     ]
    }
   ],
   "source": [
    "# co-occurance matrix and PMI (example from vector semantics Stanford NLP)\n",
    "import numpy as np\n",
    "\n",
    "cooccf = np.matrix([[0., 0., .05, 0., .05], [0., 0., .05, 0., .05], [0.11, 0.05, 0., 0.05, 0.], [0.05, 0.32, 0., 0.21, 0.]])\n",
    "\n",
    "lplcsm = 0.0 # Laplace smoothing between 0.1 to 3.0\n",
    "\n",
    "pistar = (np.sum(cooccf, axis=1) + lplcsm) / (np.sum(cooccf) + lplcsm) # words\n",
    "#print pistar\n",
    "pstarj = (np.sum(cooccf, axis=0) + lplcsm) / (np.sum(cooccf) + lplcsm) # contexts\n",
    "#print pstarj\n",
    "pij = cooccf / np.sum(cooccf)\n",
    "ppmi = np.maximum(np.log2(pij / pistar / pstarj), 0.)\n",
    "\n",
    "print ppmi\n",
    "\n",
    "alpha = 0.75 # Levy et al. (2015)\n",
    "palpha = np.power(np.count_nonzero(cooccf, axis=0) + lplcsm, alpha) / np.power(np.count_nonzero(cooccf) + lplcsm, alpha)\n",
    "ppmia = np.maximum(np.log2(pij / pistar / palpha), 0.)\n",
    "\n",
    "print ppmia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1090\n",
      "number of words in corpus (after stopwords removed): 1199\n",
      "number of phrases: 452\n"
     ]
    }
   ],
   "source": [
    "# co-occurance test large files\n",
    "import os\n",
    "import math\n",
    "import re # regex\n",
    "import numpy as np\n",
    "#import scipy.sparse as sp\n",
    "\n",
    "def read1k():\n",
    "    return f.read(1024)\n",
    "\n",
    "def process_data(chunk, text):\n",
    "    #print('processed: {0}'.format(ctr))\n",
    "    text.append(unicode(chunk, errors='ignore')) # 'utf8' codec can't decode byte 0xc3\n",
    "    \n",
    "def valinlst(val, lst): # checks if value is in a list\n",
    "    for elem in lst:\n",
    "        if elem == val:\n",
    "            #print('valinlst elem: {0} val: {1}'.format(elem, val))\n",
    "            #re.match( val, elem, re.I)\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def findphrases(corpus, win2): # returns all phrases from corpus for given window as a list\n",
    "    i = 0\n",
    "    phrase = []\n",
    "    phrases = []\n",
    "    for elem in corpus:\n",
    "        if i > -win2-1 and i < len(corpus)-win2:\n",
    "            for j in range(i-win2,i+win2+1): # for skip-gram we would need to omit the i-value\n",
    "                phrase.append(corpus[j])\n",
    "            phrases.append(phrase)\n",
    "            phrase = []\n",
    "        #i += 1\n",
    "        i += ( 2 * win2 + 1 )\n",
    "    return phrases\n",
    "\n",
    "def rmsword(corpus, stopwords): # remove stopwords from corpus\n",
    "    i = 0\n",
    "    for elem in corpus:\n",
    "        for sword in stopwords:\n",
    "            if elem == sword:\n",
    "                while True:\n",
    "                    try:\n",
    "                        corpus.remove(elem) # this throws an error if elem not in corpus (might have been removed already)\n",
    "                        i += 1\n",
    "                    except:\n",
    "                        break\n",
    "    return i # returns number of stopwords removed\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # read corpus\n",
    "    os.chdir('C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "    #f = open('todinvenedig.txt')\n",
    "    f = open('todinvenedigshrt.txt') # shorter version for tests\n",
    "    text = []\n",
    "    for piece in iter(read1k, ''):\n",
    "        process_data(piece, text)\n",
    "    \n",
    "    corpus = []\n",
    "    for elem in text:\n",
    "        corpus += elem.split() # splits on all whitespaces\n",
    "    corpus = [t.lower() for t in corpus] # convert to lower case\n",
    "    \n",
    "    # remove special characters\n",
    "    corpus = [re.sub(r'[^\\w]', ' ', t) for t in corpus]\n",
    "    \n",
    "    #print corpus\n",
    "    \n",
    "    text = []\n",
    "    # get stopwords\n",
    "    f = open('gerstopw0.txt')\n",
    "    for piece in iter(read1k, ''):\n",
    "        process_data(piece, text)\n",
    "    stopwrds = []\n",
    "    for elem in text:\n",
    "        stopwrds += elem.split()\n",
    "    stopwrds = [t.lower() for t in stopwrds]\n",
    "    #print stopwrds\n",
    "    \n",
    "    #print('length of corpus: {0}'.format(len(corpus)))\n",
    "    print rmsword(corpus, stopwrds) # remove stopwords from corpus\n",
    "    #print('length of corpus: {0}'.format(len(corpus)))\n",
    "    \n",
    "    # fill words set (unique!) from corpus\n",
    "    words = set() # words as set: each entry unique\n",
    "    for elem in corpus:\n",
    "        words.add(elem)\n",
    "        \n",
    "    print('number of words in corpus (after stopwords removed): {0}'.format(len(words)))\n",
    "    \n",
    "    phrases = findphrases(corpus, 1) # extract phrases from corpus (window +/- the given size)\n",
    "    #print phrases\n",
    "    print('number of phrases: {0}'.format(len(phrases)))\n",
    "    \n",
    "    wordcont = np.zeros((len(words),len(phrases)))\n",
    "    k = 1.5 # Laplacian smoothing\n",
    "    \n",
    "    # fill word-context (or word-phrase) matrix\n",
    "    i = 0 # word index\n",
    "    wordlist = []\n",
    "    for word in words:\n",
    "        j = 0 # phrease index\n",
    "        wordlist.append(word) # to allow for index-access later\n",
    "        for phrase in phrases:\n",
    "            if valinlst(word, phrase):\n",
    "                wordcont[i][j] += 1 + k # k for Laplacian smoothing\n",
    "                #if wordcont[i][j] >= 1.0:\n",
    "                #    print('wordcont[{0}][{1}]: {2} word: {3} phrease: {4}'.format(i,j,wordcont[i][j],word,phrase))\n",
    "            j += 1\n",
    "        i += 1\n",
    "    \n",
    "    #print sp.issparse(wordcont)\n",
    "    #print wordcont\n",
    "    #print np.nonzero(wordcont)\n",
    "    #print(\"word set: {0} word list: {1}\".format(next(iter(words)), wordlist[0]))\n",
    "    \n",
    "    # calculate pointwise mutual information\n",
    "    fijsum = 0.0\n",
    "    pistar = []\n",
    "    pstarj = []\n",
    "    for i in range(len(words)):\n",
    "        pistar.append(0.0)\n",
    "        for j in range(len(phrases)):\n",
    "            if i is 0:\n",
    "                pstarj.append(0.0)\n",
    "            fijsum += wordcont[i][j]\n",
    "            pistar[i] += wordcont[i][j]\n",
    "            pstarj[j] += wordcont[i][j]\n",
    "    \n",
    "    #print fijsum\n",
    "    #print pistar\n",
    "    #print pstarj\n",
    "    \n",
    "    ppmi = np.zeros((len(words),len(phrases)))\n",
    "    for i in range(len(words)):\n",
    "        for j in range(len(phrases)):\n",
    "            if wordcont[i][j] > 0.0:\n",
    "                ppmi[i][j] = max(math.log((wordcont[i][j]*fijsum)/pistar[i]/pstarj[j], 2.0), 0.0)\n",
    "    \n",
    "    #print ppmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppmi mean: 0.0213573785218 and stdev: 0.427847960743\n",
      "ppmi max: 9.40301202357 and min: 0.0\n",
      "wcounter - no. of ppmi above limit: 1246\n",
      "ppmi 8.81804952285 for word: phosphoreszierenden and phrase: [u'phosphoreszierenden', u'lichter', u'tigers'] idx 0 , 206\n",
      "ppmi 8.81804952285 for word: wochen and phrase: [u'drei ', u'vier', u'wochen'] idx 1 , 329\n",
      "ppmi 8.81804952285 for word: geboren and phrase: [u'berufen ', u'eigentlich', u'geboren'] idx 2 , 429\n",
      "ppmi 8.81804952285 for word: oberhalb and phrase: [u'zurckkehrend ', u'portikus ', u'oberhalb'] idx 3 , 78\n",
      "ppmi 8.81804952285 for word: publikums and phrase: [u'glauben', u'breiten', u'publikums'] idx 4 , 409\n",
      "ppmi 7.81804952285 for word: ganz and phrase: [u'bemerkte ', u'ganz', u'gewhnliche'] idx 5 , 81\n",
      "ppmi 7.81804952285 for word: ganz and phrase: [u'ganz', u'berraschend', u'bewut '] idx 5 , 155\n",
      "ppmi 8.81804952285 for word: landschaft  and phrase: [u'ungeheuere', u'landschaft ', u'tropisches'] idx 6 , 172\n",
      "ppmi 8.81804952285 for word: beobachter and phrase: [u'beobachter', u'gesellschaft ', u'sehen'] idx 7 , 418\n",
      "ppmi 8.81804952285 for word: straffes  and phrase: [u'knigs ', u'staates ', u'straffes '] idx 8 , 372\n",
      "ppmi 8.81804952285 for word: gehalten  and phrase: [u'gehalten ', u'wahrhaft', u'gro '] idx 9 , 447\n",
      "ppmi 8.81804952285 for word: stegreifdasein  and phrase: [u'einschaltung', u'not ', u'stegreifdasein '] idx 10 , 321\n",
      "ppmi 8.81804952285 for word: kleinen and phrase: [u'kleinen', u'hause', u'magd '] idx 11 , 315\n",
      "ppmi 8.81804952285 for word: standort and phrase: [u'standort', u'eindruck', u'bei  hatte'] idx 12 , 123\n",
      "ppmi 8.81804952285 for word: schatten and phrase: [u'menschenschicksal', u'schatten', u'idee'] idx 13 , 349\n",
      "ppmi 8.81804952285 for word: bajuwarischen and phrase: [u'bajuwarischen', u'schlages ', u'wenigstens'] idx 14 , 96\n",
      "ppmi 7.81804952285 for word: boden  and phrase: [u'rcken', u'blick', u'boden '] idx 15 , 161\n",
      "ppmi 7.81804952285 for word: boden  and phrase: [u'boden ', u'wasser', u'senkten '] idx 15 , 187\n",
      "ppmi 8.81804952285 for word: gang and phrase: [u'berhrt ', u'abwandte', u'gang'] idx 16 , 145\n",
      "ppmi 8.81804952285 for word: neigung and phrase: [u'betrachtet ', u'sinn', u'neigung'] idx 17 , 217\n"
     ]
    }
   ],
   "source": [
    "# read out ppmi-matrix and display words and context/phrases\n",
    "import numpy as np\n",
    "\n",
    "def getwordi(words, indx): # access words set\n",
    "    i = 0\n",
    "    for elem in words:\n",
    "        if i == indx:\n",
    "            return elem\n",
    "        i += 1\n",
    "\n",
    "print('ppmi mean: {0} and stdev: {1}'.format(np.mean(ppmi), np.std(ppmi)))\n",
    "print('ppmi max: {0} and min: {1}'.format(np.amax(ppmi), np.amin(ppmi)))\n",
    "\n",
    "isigword = [] # index array for significant words\n",
    "isiphrs = [] # index array for significant words\n",
    "\n",
    "wcounter = 0\n",
    "for i in range(len(wordlist)):\n",
    "    for j in range(len(phrases)):\n",
    "        if ppmi[i][j] > 7.5: # note limit\n",
    "            #print('ppmi = {0} for word: {1} and phrase: {2}'.format(ppmi[i][j], wordlist[i], phrases[j]))\n",
    "            isigword.append(i)\n",
    "            isiphrs.append(j)\n",
    "            wcounter += 1\n",
    "            \n",
    "print('wcounter - no. of ppmi above limit: {0}'.format(wcounter))\n",
    "\n",
    "#for i in range(len(isigword)):\n",
    "for i in range(20):\n",
    "    print('ppmi {0} for word: {1} and phrase: {2} idx {3} , {4}'.format(ppmi[isigword[i]][isiphrs[i]], wordlist[isigword[i]], phrases[isiphrs[i]], isigword[i], isiphrs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine phosphoreszierenden and phosphoreszierenden: 1.0\n",
      "cosine wochen and wochen: 1.0\n",
      "cosine geboren and geboren: 1.0\n",
      "cosine oberhalb and oberhalb: 1.0\n",
      "cosine publikums and publikums: 1.0\n",
      "cosine ganz and ganz: 1.0\n",
      "cosine ganz and ganz: 1.0\n",
      "cosine ganz and ganz: 1.0\n",
      "cosine ganz and ganz: 1.0\n",
      "cosine landschaft  and landschaft : 1.0\n",
      "cosine beobachter and beobachter: 1.0\n",
      "cosine straffes  and straffes : 1.0\n"
     ]
    }
   ],
   "source": [
    "# measure similarity between words for given context/phrases\n",
    "import numpy as np\n",
    "\n",
    "def cosine(wordv, wordw): # cosine similarity for two word vectors\n",
    "    sumv = 0.\n",
    "    sumw = 0.\n",
    "    sumvw = 0.\n",
    "    for elemv, elemw in zip(wordv, wordw):\n",
    "        #print('for: {0} {1}'.format(elemv, elemw))\n",
    "        sumv += elemv * elemv\n",
    "        sumw += elemw * elemw\n",
    "        sumvw += elemv * elemw\n",
    "    sumv = math.sqrt(sumv)\n",
    "    sumw = math.sqrt(sumw)\n",
    "    #print('{0} {1}'.format(sumv, sumw))\n",
    "    if sumv > 0. and sumw > 0.:\n",
    "        return sumvw / sumv / sumw\n",
    "    else: return -1.\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    #for i in range(len(isigword)):\n",
    "        #for j in range(len(isiphrs)):\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            thiscosine = cosine(ppmi[isigword[i]], ppmi[isigword[j]])\n",
    "            if thiscosine > 0.:\n",
    "                print('cosine {0} and {1}: {2}'.format(wordlist[isigword[i]], wordlist[isigword[j]], thiscosine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
