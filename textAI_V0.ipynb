{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'brown': 0, u'lazy': 4, u'jumped': 3, u'over': 5, u'fox': 2, u'dog': 1, u'quick': 6, u'the': 7}\n",
      "(1, 8)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_) # vocabulary: which word is represented by which number? -> id of word\n",
    "# encode document\n",
    "vector = vectorizer.transform(text) # create encoded vector: the index is the id of the word, only \"the\" (id=index=7) appears twice\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'brown': 0, u'lazy': 4, u'jumped': 3, u'over': 5, u'fox': 2, u'dog': 1, u'quick': 6, u'the': 7}\n",
      "[ 1.69314718  1.28768207  1.28768207  1.69314718  1.69314718  1.69314718\n",
      "  1.69314718  1.        ]\n",
      "(3, 8)\n",
      "[[ 0.36388646  0.27674503  0.27674503  0.36388646  0.36388646  0.36388646\n",
      "   0.36388646  0.42983441]\n",
      " [ 0.          0.78980693  0.          0.          0.          0.          0.\n",
      "   0.61335554]\n",
      " [ 0.          0.          0.78980693  0.          0.          0.          0.\n",
      "   0.61335554]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "# Term Frequency: This summarizes how often a given word appears within a document.\n",
    "# Inverse Document Frequency: This downscales words that appear a lot across documents.\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\"The dog.\",\"The fox\"]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocabulary\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_) # idf=inverse document frequencies: the most frequent word \"the\" is assigned the lowest frequency, 1\n",
    "# encode document (as a sparse matrix, scores are normalized)\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape) # shape = [n_samples, n_features], will be (3,8) here\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\"The dog.\",\"The fox\"]\n",
    "# create the transform, tokenize and build vocab\n",
    "cvectorizer = CountVectorizer().fit(text)\n",
    "cvector = cvectorizer.transform(text) # create encoded vector: the index is the id of the word, only \"the\" (id=index=7) appears twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# list of text documents\n",
    "# Term Frequency: This summarizes how often a given word appears within a document.\n",
    "# Inverse Document Frequency: This downscales words that appear a lot across documents.\n",
    "# create the transform, tokenize and build vocabulary\n",
    "tvectorizer = TfidfTransformer(use_idf=False).fit(cvector)\n",
    "tvector = tvectorizer.transform(cvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NB learning\n",
    "from sklearn.naive_bayes import MultinomialNB # naive Bayes classification\n",
    "import numpy as np\n",
    "clf = MultinomialNB().fit(tvector, np.array([0,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2]\n"
     ]
    }
   ],
   "source": [
    "# check results of training\n",
    "docs_new = [\"lazy dog\", \"dog\",\"fox\"]\n",
    "X_new_counts = cvectorizer.transform(docs_new)\n",
    "X_new_tf = tvectorizer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tf)\n",
    "\n",
    "print(predicted)\n",
    "\n",
    "#for doc, category in zip(docs_new, predicted):\n",
    "#    print('%r => %s' % (doc, np.array([0,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "[[ 0.          0.          0.          0.          0.          0.33333333\n",
      "   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n",
      "   0.          0.          0.         -0.33333333  0.          0.\n",
      "  -0.66666667  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = HashingVectorizer(n_features=20) # one way hash of words to convert them to integers\n",
    "# encode document (downside is that the hash is a one-way function so there is no way to convert the encoding back to a word)\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "[7, 4, 11, 11, 14, 26, 22, 14, 17, 11, 3]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "h\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding of 'hello world'\n",
    "from numpy import argmax\n",
    "# define input string\n",
    "data = 'hello world'\n",
    "print(data)\n",
    "# define universe of possible input values\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz '\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet)) # seasons = ['Spring',... -> [(0, 'Spring'),...\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# integer encode input data\n",
    "integer_encoded = [char_to_int[char] for char in data]\n",
    "print(integer_encoded)\n",
    "# one hot encode\n",
    "onehot_encoded = list()\n",
    "for value in integer_encoded:\n",
    "\tletter = [0 for _ in range(len(alphabet))]\n",
    "\tletter[value] = 1\n",
    "\tonehot_encoded.append(letter)\n",
    "print(onehot_encoded)\n",
    "# invert encoding\n",
    "inverted = int_to_char[argmax(onehot_encoded[0])]\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<type 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "wdict = {'a': 0, 'b': 1, 'c': 2, 'd': 3}\n",
    "dictarr = np.asarray(wdict.values()).reshape(-1, 1)\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(dictarr)\n",
    "enc.transform([[2]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vectorize text with skip-grams in scikit-learn by passing the skip gram tokens as the vocabulary\n",
    "# to CountVectorizer will not work -> example vectorizer that produces 1-skip-2-grams\n",
    "from toolz import itertoolz, compose\n",
    "from toolz.curried import map as cmap, sliding_window, pluck\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# pluck: plucking “fields” from an iterable of values e.g. pluck(objects, 'age') -> [30, 56, 56]\n",
    "#        or list(pluck([0, 1], [[1, 2, 3], [4, 5, 7]])) -> [(1, 2), (4, 5)]\n",
    "# sliding_window creates a sliding window: list(sliding_window(2, [1, 2, 3, 4])) -> [(1, 2), (2, 3), (3, 4)]\n",
    "# map: apply function to every item of iterable and return a list of the results\n",
    "# curried form of map: map(func,[[1,2],[3,4]]) can be written as map(func)([[1,2],[3,4]])\n",
    "\n",
    "class SkipGramVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):    \n",
    "        preprocess = self.build_preprocessor() # Return a function to preprocess the text before tokenization\n",
    "        stop_words = self.get_stop_words() # Build or fetch the effective stop words (words that are filtered out) list\n",
    "        tokenize = self.build_tokenizer() # Return a function that splits a string into a sequence of tokens\n",
    "        return lambda doc: self._word_skip_grams( # lambda-functions: anonymous functions not bound to a name\n",
    "                compose(tokenize, preprocess, self.decode)(doc), # compose: ompose functions to operate in series\n",
    "                stop_words)\n",
    "\n",
    "    def _word_skip_grams(self, tokens, stop_words=None):\n",
    "        # handle stop words\n",
    "        if stop_words is not None:\n",
    "            tokens = [w for w in tokens if w not in stop_words]\n",
    "        print list(sliding_window(3, tokens))\n",
    "        return compose(cmap(' '.join), pluck([0, 2]), sliding_window(3))(tokens) # str.join(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'the', u'rain', u'in'), (u'rain', u'in', u'spain'), (u'in', u'spain', u'falls'), (u'spain', u'falls', u'mainly'), (u'falls', u'mainly', u'on'), (u'mainly', u'on', u'the'), (u'on', u'the', u'plain')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'falls on',\n",
       " u'in falls',\n",
       " u'mainly the',\n",
       " u'on plain',\n",
       " u'rain spain',\n",
       " u'spain mainly',\n",
       " u'the in']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['the rain in Spain falls mainly on the plain']\n",
    "\n",
    "vect = SkipGramVectorizer()\n",
    "vect.fit(text) # Learn a vocabulary dictionary of all tokens in the raw documents\n",
    "vect.get_feature_names() # Array mapping from feature integer indices to feature name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 5, 6]\n",
      "a#-#b#-#c\n"
     ]
    }
   ],
   "source": [
    "tokens = [2,3,4,5,6]\n",
    "stop_words = [0,3]\n",
    "print [w for w in tokens if w not in stop_words]\n",
    "separator = \"#-#\"\n",
    "sequence = (\"a\", \"b\", \"c\")\n",
    "print separator.join(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vectorize text with skip-grams in scikit-learn by passing the skip gram tokens as the vocabulary\n",
    "# to CountVectorizer will not work -> example vectorizer that produces 1-skip-nsize-grams\n",
    "from toolz import itertoolz, compose\n",
    "from toolz.curried import map as cmap, sliding_window, pluck\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# pluck: plucking “fields” from an iterable of values e.g. pluck(objects, 'age') -> [30, 56, 56]\n",
    "#        or list(pluck([0, 1], [[1, 2, 3], [4, 5, 7]])) -> [(1, 2), (4, 5)]\n",
    "# sliding_window creates a sliding window: list(sliding_window(2, [1, 2, 3, 4])) -> [(1, 2), (2, 3), (3, 4)]\n",
    "# map: apply function to every item of iterable and return a list of the results\n",
    "# curried form of map: map(func,[[1,2],[3,4]]) can be written as map(func)([[1,2],[3,4]])\n",
    "\n",
    "nsize = 5\n",
    "\n",
    "class SkipGramVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        preprocess = self.build_preprocessor() # Return a function to preprocess the text before tokenization\n",
    "        stop_words = self.get_stop_words() # Build or fetch the effective stop words (words that are filtered out) list\n",
    "        tokenize = self.build_tokenizer() # Return a function that splits a string into a sequence of tokens\n",
    "        return lambda doc: self._word_skip_grams( # lambda-functions: anonymous functions not bound to a name\n",
    "                compose(tokenize, preprocess, self.decode)(doc), # compose: ompose functions to operate in series\n",
    "                stop_words)\n",
    "\n",
    "    def _word_skip_grams(self, tokens, stop_words=None):\n",
    "        # handle stop words\n",
    "        if stop_words is not None:\n",
    "            tokens = [w for w in tokens if w not in stop_words]\n",
    "        # print list(compose(cmap(' '.join), pluck(range(0,nsize)), sliding_window(nsize))(tokens))\n",
    "        return compose(cmap(' '.join), pluck(range(0,nsize)), sliding_window(nsize))(tokens) # str.join(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'falls mainly on the plain',\n",
       " u'hardly any rain stains the',\n",
       " u'in spain falls mainly on',\n",
       " u'mainly on the plain with',\n",
       " u'on the plain with pain',\n",
       " u'pain where hardly any rain',\n",
       " u'plain with pain where hardly',\n",
       " u'rain in spain falls mainly',\n",
       " u'spain falls mainly on the',\n",
       " u'the plain with pain where',\n",
       " u'the rain in spain falls',\n",
       " u'where hardly any rain stains',\n",
       " u'with pain where hardly any']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['the rain in Spain falls mainly on the plain with pain where hardly any rain stains the']\n",
    "\n",
    "vect = SkipGramVectorizer()\n",
    "vect.fit(text) # Learn a vocabulary dictionary of all tokens in the raw documents\n",
    "vect.get_feature_names() # Array mapping from feature integer indices to feature name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'on': 6, 'pain': 10, 'stains': 15, 'plain': 8, 'mainly': 5, 'rain': 1, 'falls': 4, 'where': 11, 'hardly': 12, 'in': 2, 'the': 0, 'with': 9, 'any': 13, 'spain': 3}\n"
     ]
    }
   ],
   "source": [
    "# build a dictonary from a text as input to a one-hot encoder\n",
    "# the number of unique words == the vocabulary == dimension V\n",
    "from toolz import itertoolz\n",
    "\n",
    "text = 'the rain in Spain falls mainly on the plain with pain where hardly any rain stains the'\n",
    "textlist = text.split(\" \")\n",
    "myDict = {}\n",
    "i = 0\n",
    "for word in textlist:\n",
    "    # print word, i\n",
    "    newEntry = {word.lower(): i}\n",
    "    if not myDict.has_key(word):\n",
    "        myDict.update(newEntry)\n",
    "    i = i + 1\n",
    "\n",
    "print myDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encode the dictionary\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "dictarr = np.asarray(myDict.values()).reshape(-1, 1)\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(dictarr)\n",
    "enc.transform([[8]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'on': 6, u'the': 0, u'plain': 8, u'mainly': 5, u'falls': 4}, {u'stains': 15, u'the': 0, u'hardly': 12, u'any': 13, u'rain': 1}, {u'on': 6, u'falls': 4, u'mainly': 5, u'spain': 3, u'in': 2}, {u'with': 9, u'on': 6, u'the': 0, u'mainly': 5, u'plain': 8}, {u'on': 6, u'the': 0, u'with': 9, u'pain': 10, u'plain': 8}, {u'where': 11, u'pain': 10, u'any': 13, u'rain': 1, u'hardly': 12}, {u'plain': 8, u'with': 9, u'where': 11, u'hardly': 12, u'pain': 10}, {u'falls': 4, u'mainly': 5, u'spain': 3, u'rain': 1, u'in': 2}, {u'on': 6, u'the': 0, u'mainly': 5, u'spain': 3, u'falls': 4}, {u'plain': 8, u'the': 0, u'with': 9, u'where': 11, u'pain': 10}, {u'the': 0, u'falls': 4, u'spain': 3, u'rain': 1, u'in': 2}, {u'stains': 15, u'hardly': 12, u'where': 11, u'any': 13, u'rain': 1}, {u'any': 13, u'pain': 10, u'with': 9, u'where': 11, u'hardly': 12}]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode the 1-skip-ngrams from SkipGramVectorizer: map words from vocabulary (dict myDict)\n",
    "\n",
    "from toolz import itertoolz\n",
    "\n",
    "text = 'the rain in Spain falls mainly on the plain with pain where hardly any rain stains the'\n",
    "\n",
    "ovecm = []\n",
    "for ovec in vect.get_feature_names():\n",
    "    # print ovec\n",
    "    ovecd = {}\n",
    "    for oword in ovec.split(\" \"):\n",
    "        # print (oword, myDict[oword])\n",
    "        ovecd[oword] = myDict[oword]\n",
    "    ovecm.append(ovecd)\n",
    "\n",
    "print ovecm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  0.  0.]]\n",
      "[[ 1.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.]]\n",
      "[[ 1.  0.  0.  0.  0.]]\n",
      "[[ 1.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.]]\n",
      "[[ 1.  0.  0.  0.  0.]]\n",
      "[[ 1.  0.  0.  0.  0.]]\n",
      "[[ 1.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# now one-hot encode the words mapped from vocabulary\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "for wdict in ovecm:\n",
    "    # print np.asarray(wdict.values()).reshape(-1, 1)\n",
    "    dictarr = np.asarray(wdict.values()).reshape(-1, 1) # extract values from dict (.values), as array and transform (reshape)\n",
    "    enc = OneHotEncoder() # Encode categorical integer features using a one-hot aka one-of-K scheme\n",
    "    # The output will be a sparse matrix where each column corresponds to one possible value of one feature\n",
    "    enc.fit(dictarr) # Fit OneHotEncoder to dictarr\n",
    "    print enc.transform([[0]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02  0.14  0.02  0.02  0.01  0.01  0.02  0.01]]\n"
     ]
    }
   ],
   "source": [
    "# skip-gram learning example\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "wi = np.array([[-0.094, -0.44, 0.31], [-0.491, -0.23, 0.065], [0.07, 0.17, -0.36], [0.1, 0.46, 0.08], [-0.23, -0.15, -0.04], [0.41, -0.19, -0.44], [0.18, 0.09, 0.28], [-0.05, 0.49, 0.26]])\n",
    "wo = np.array([[0.02, 0.48, 0.43, 0.37, -0.36, -0.12, 0.27, -0.35], [-0.37, 0.42, -0.26, -0.15, 0.03, 0.35, -0.14, 0.13], [0.42, 0.36, 0.47, -0.02, -0.42, -0.44, 0.27, -0.45]])\n",
    "xk = np.array([[0, 1, 0, 0, 0, 0, 0, 0]])\n",
    "ht = np.dot(xk, wi)\n",
    "u0 = np.dot(ht, wo)\n",
    "\n",
    "#print u0\n",
    "yk = np.exp(u0) / np.dot(np.exp(u0), np.exp(u0).transpose())\n",
    "#print yk\n",
    "\n",
    "# backpropagation (following Xin Rong's paper here)\n",
    "tc = np.array([[0, 1, 0, 0, 0, 0, 0, 0]]) # truth\n",
    "ej = yk - tc # error\n",
    "etha = 0.01\n",
    "\n",
    "# Update equation for hidden→output weights\n",
    "wo = wo - etha * (np.transpose(ht) * ej)\n",
    "# Update equation for input→hidden weights\n",
    "ehi = np.dot(ej, np.transpose(wo)) # dE/dhi\n",
    "wi = wi - etha * ehi\n",
    "\n",
    "# next cycles\n",
    "i=0\n",
    "while i<200:\n",
    "    ht = np.dot(xk, wi)\n",
    "    u0 = np.dot(ht, wo)\n",
    "    yk = np.exp(u0) / np.dot(np.exp(u0), np.exp(u0).transpose())\n",
    "    ej = yk - tc # error\n",
    "    #print ej\n",
    "    #ei = np.sum(ej) sum has to be over context, here C=1\n",
    "    wo = wo - etha * (np.transpose(ht) * ej)\n",
    "    ehi = np.dot(ej, np.transpose(wo)) # dE/dhi\n",
    "    wi = wi - etha * ehi\n",
    "    i += 1\n",
    "\n",
    "#print '%.2f' % yk\n",
    "np.set_printoptions(precision=2)\n",
    "print yk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
