{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'brown': 0, u'lazy': 4, u'jumped': 3, u'over': 5, u'fox': 2, u'dog': 1, u'quick': 6, u'the': 7}\n",
      "(1, 8)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_) # vocabulary: which word is represented by which number? -> id of word\n",
    "# encode document\n",
    "vector = vectorizer.transform(text) # create encoded vector: the index is the id of the word, only \"the\" (id=index=7) appears twice\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'brown': 0, u'lazy': 4, u'jumped': 3, u'over': 5, u'fox': 2, u'dog': 1, u'quick': 6, u'the': 7}\n",
      "[ 1.69314718  1.28768207  1.28768207  1.69314718  1.69314718  1.69314718\n",
      "  1.69314718  1.        ]\n",
      "(3, 8)\n",
      "[[ 0.36388646  0.27674503  0.27674503  0.36388646  0.36388646  0.36388646\n",
      "   0.36388646  0.42983441]\n",
      " [ 0.          0.78980693  0.          0.          0.          0.          0.\n",
      "   0.61335554]\n",
      " [ 0.          0.          0.78980693  0.          0.          0.          0.\n",
      "   0.61335554]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "# Term Frequency: This summarizes how often a given word appears within a document.\n",
    "# Inverse Document Frequency: This downscales words that appear a lot across documents.\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\"The dog.\",\"The fox\"]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocabulary\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_) # idf=inverse document frequencies: the most frequent word \"the\" is assigned the lowest frequency, 1\n",
    "# encode document (as a sparse matrix, scores are normalized)\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape) # shape = [n_samples, n_features], will be (3,8) here\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\"The dog.\",\"The fox\"]\n",
    "# create the transform, tokenize and build vocab\n",
    "cvectorizer = CountVectorizer().fit(text)\n",
    "cvector = cvectorizer.transform(text) # create encoded vector: the index is the id of the word, only \"the\" (id=index=7) appears twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# list of text documents\n",
    "# Term Frequency: This summarizes how often a given word appears within a document.\n",
    "# Inverse Document Frequency: This downscales words that appear a lot across documents.\n",
    "# create the transform, tokenize and build vocabulary\n",
    "tvectorizer = TfidfTransformer(use_idf=False).fit(cvector)\n",
    "tvector = tvectorizer.transform(cvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NB learning\n",
    "from sklearn.naive_bayes import MultinomialNB # naive Bayes classification\n",
    "import numpy as np\n",
    "clf = MultinomialNB().fit(tvector, np.array([0,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2]\n"
     ]
    }
   ],
   "source": [
    "# check results of training\n",
    "docs_new = [\"lazy dog\", \"dog\",\"fox\"]\n",
    "X_new_counts = cvectorizer.transform(docs_new)\n",
    "X_new_tf = tvectorizer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tf)\n",
    "\n",
    "print(predicted)\n",
    "\n",
    "#for doc, category in zip(docs_new, predicted):\n",
    "#    print('%r => %s' % (doc, np.array([0,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "[[ 0.          0.          0.          0.          0.          0.33333333\n",
      "   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n",
      "   0.          0.          0.         -0.33333333  0.          0.\n",
      "  -0.66666667  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = HashingVectorizer(n_features=20) # one way hash of words to convert them to integers\n",
    "# encode document (downside is that the hash is a one-way function so there is no way to convert the encoding back to a word)\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "[7, 4, 11, 11, 14, 26, 22, 14, 17, 11, 3]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "h\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding of 'hello world'\n",
    "from numpy import argmax\n",
    "# define input string\n",
    "data = 'hello world'\n",
    "print(data)\n",
    "# define universe of possible input values\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz '\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet)) # seasons = ['Spring',... -> [(0, 'Spring'),...\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# integer encode input data\n",
    "integer_encoded = [char_to_int[char] for char in data]\n",
    "print(integer_encoded)\n",
    "# one hot encode\n",
    "onehot_encoded = list()\n",
    "for value in integer_encoded:\n",
    "\tletter = [0 for _ in range(len(alphabet))]\n",
    "\tletter[value] = 1\n",
    "\tonehot_encoded.append(letter)\n",
    "print(onehot_encoded)\n",
    "# invert encoding\n",
    "inverted = int_to_char[argmax(onehot_encoded[0])]\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<type 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "wdict = {'a': 0, 'b': 1, 'c': 2, 'd': 3}\n",
    "dictarr = np.asarray(wdict.values()).reshape(-1, 1)\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(dictarr)\n",
    "enc.transform([[2]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vectorize text with skip-grams in scikit-learn by passing the skip gram tokens as the vocabulary\n",
    "# to CountVectorizer will not work -> example vectorizer that produces 1-skip-2-grams\n",
    "from toolz import itertoolz, compose\n",
    "from toolz.curried import map as cmap, sliding_window, pluck\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# pluck: plucking “fields” from an iterable of values e.g. pluck(objects, 'age') -> [30, 56, 56]\n",
    "#        or list(pluck([0, 1], [[1, 2, 3], [4, 5, 7]])) -> [(1, 2), (4, 5)]\n",
    "# sliding_window creates a sliding window: list(sliding_window(2, [1, 2, 3, 4])) -> [(1, 2), (2, 3), (3, 4)]\n",
    "# map: apply function to every item of iterable and return a list of the results\n",
    "# curried form of map: map(func,[[1,2],[3,4]]) can be written as map(func)([[1,2],[3,4]])\n",
    "\n",
    "class SkipGramVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):    \n",
    "        preprocess = self.build_preprocessor() # Return a function to preprocess the text before tokenization\n",
    "        stop_words = self.get_stop_words() # Build or fetch the effective stop words (words that are filtered out) list\n",
    "        tokenize = self.build_tokenizer() # Return a function that splits a string into a sequence of tokens\n",
    "        return lambda doc: self._word_skip_grams( # lambda-functions: anonymous functions not bound to a name\n",
    "                compose(tokenize, preprocess, self.decode)(doc), # compose: ompose functions to operate in series\n",
    "                stop_words)\n",
    "\n",
    "    def _word_skip_grams(self, tokens, stop_words=None):\n",
    "        # handle stop words\n",
    "        if stop_words is not None:\n",
    "            tokens = [w for w in tokens if w not in stop_words]\n",
    "        print list(sliding_window(3, tokens))\n",
    "        return compose(cmap(' '.join), pluck([0, 2]), sliding_window(3))(tokens) # str.join(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'the', u'rain', u'in'), (u'rain', u'in', u'spain'), (u'in', u'spain', u'falls'), (u'spain', u'falls', u'mainly'), (u'falls', u'mainly', u'on'), (u'mainly', u'on', u'the'), (u'on', u'the', u'plain')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'falls on',\n",
       " u'in falls',\n",
       " u'mainly the',\n",
       " u'on plain',\n",
       " u'rain spain',\n",
       " u'spain mainly',\n",
       " u'the in']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['the rain in Spain falls mainly on the plain']\n",
    "\n",
    "vect = SkipGramVectorizer()\n",
    "vect.fit(text) # Learn a vocabulary dictionary of all tokens in the raw documents\n",
    "vect.get_feature_names() # Array mapping from feature integer indices to feature name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 5, 6]\n",
      "a#-#b#-#c\n"
     ]
    }
   ],
   "source": [
    "tokens = [2,3,4,5,6]\n",
    "stop_words = [0,3]\n",
    "print [w for w in tokens if w not in stop_words]\n",
    "separator = \"#-#\"\n",
    "sequence = (\"a\", \"b\", \"c\")\n",
    "print separator.join(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vectorize text with skip-grams in scikit-learn by passing the skip gram tokens as the vocabulary\n",
    "# to CountVectorizer will not work -> example vectorizer that produces 1-skip-nsize-grams\n",
    "from toolz import itertoolz, compose\n",
    "from toolz.curried import map as cmap, sliding_window, pluck\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# pluck: plucking “fields” from an iterable of values e.g. pluck(objects, 'age') -> [30, 56, 56]\n",
    "#        or list(pluck([0, 1], [[1, 2, 3], [4, 5, 7]])) -> [(1, 2), (4, 5)]\n",
    "# sliding_window creates a sliding window: list(sliding_window(2, [1, 2, 3, 4])) -> [(1, 2), (2, 3), (3, 4)]\n",
    "# map: apply function to every item of iterable and return a list of the results\n",
    "# curried form of map: map(func,[[1,2],[3,4]]) can be written as map(func)([[1,2],[3,4]])\n",
    "\n",
    "nsize = 5\n",
    "\n",
    "class SkipGramVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        preprocess = self.build_preprocessor() # Return a function to preprocess the text before tokenization\n",
    "        stop_words = self.get_stop_words() # Build or fetch the effective stop words (words that are filtered out) list\n",
    "        tokenize = self.build_tokenizer() # Return a function that splits a string into a sequence of tokens\n",
    "        return lambda doc: self._word_skip_grams( # lambda-functions: anonymous functions not bound to a name\n",
    "                compose(tokenize, preprocess, self.decode)(doc), # compose: ompose functions to operate in series\n",
    "                stop_words)\n",
    "\n",
    "    def _word_skip_grams(self, tokens, stop_words=None):\n",
    "        # handle stop words\n",
    "        if stop_words is not None:\n",
    "            tokens = [w for w in tokens if w not in stop_words]\n",
    "        # print list(compose(cmap(' '.join), pluck(range(0,nsize)), sliding_window(nsize))(tokens))\n",
    "        return compose(cmap(' '.join), pluck(range(0,nsize)), sliding_window(nsize))(tokens) # str.join(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'falls mainly on the plain',\n",
       " u'hardly any rain stains the',\n",
       " u'in spain falls mainly on',\n",
       " u'mainly on the plain with',\n",
       " u'on the plain with pain',\n",
       " u'pain where hardly any rain',\n",
       " u'plain with pain where hardly',\n",
       " u'rain in spain falls mainly',\n",
       " u'spain falls mainly on the',\n",
       " u'the plain with pain where',\n",
       " u'the rain in spain falls',\n",
       " u'where hardly any rain stains',\n",
       " u'with pain where hardly any']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['the rain in Spain falls mainly on the plain with pain where hardly any rain stains the']\n",
    "\n",
    "vect = SkipGramVectorizer()\n",
    "vect.fit(text) # Learn a vocabulary dictionary of all tokens in the raw documents\n",
    "vect.get_feature_names() # Array mapping from feature integer indices to feature name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'on': 6, 'pain': 10, 'stains': 15, 'plain': 8, 'mainly': 5, 'rain': 1, 'falls': 4, 'where': 11, 'hardly': 12, 'in': 2, 'the': 0, 'with': 9, 'any': 13, 'spain': 3}\n"
     ]
    }
   ],
   "source": [
    "# build a dictonary from a text as input to a one-hot encoder\n",
    "# the number of unique words == the vocabulary == dimension V\n",
    "from toolz import itertoolz\n",
    "\n",
    "text = 'the rain in Spain falls mainly on the plain with pain where hardly any rain stains the'\n",
    "textlist = text.split(\" \")\n",
    "myDict = {}\n",
    "i = 0\n",
    "for word in textlist:\n",
    "    # print word, i\n",
    "    newEntry = {word.lower(): i}\n",
    "    if not myDict.has_key(word):\n",
    "        myDict.update(newEntry)\n",
    "    i = i + 1\n",
    "\n",
    "print myDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encode the dictionary\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "dictarr = np.asarray(myDict.values()).reshape(-1, 1)\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(dictarr)\n",
    "enc.transform([[8]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'on': 6, u'the': 0, u'plain': 8, u'mainly': 5, u'falls': 4}, {u'stains': 15, u'the': 0, u'hardly': 12, u'any': 13, u'rain': 1}, {u'on': 6, u'falls': 4, u'mainly': 5, u'spain': 3, u'in': 2}, {u'with': 9, u'on': 6, u'the': 0, u'mainly': 5, u'plain': 8}, {u'on': 6, u'the': 0, u'with': 9, u'pain': 10, u'plain': 8}, {u'where': 11, u'pain': 10, u'any': 13, u'rain': 1, u'hardly': 12}, {u'plain': 8, u'with': 9, u'where': 11, u'hardly': 12, u'pain': 10}, {u'falls': 4, u'mainly': 5, u'spain': 3, u'rain': 1, u'in': 2}, {u'on': 6, u'the': 0, u'mainly': 5, u'spain': 3, u'falls': 4}, {u'plain': 8, u'the': 0, u'with': 9, u'where': 11, u'pain': 10}, {u'the': 0, u'falls': 4, u'spain': 3, u'rain': 1, u'in': 2}, {u'stains': 15, u'hardly': 12, u'where': 11, u'any': 13, u'rain': 1}, {u'any': 13, u'pain': 10, u'with': 9, u'where': 11, u'hardly': 12}]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode the 1-skip-ngrams from SkipGramVectorizer: map words from vocabulary (dict myDict)\n",
    "\n",
    "from toolz import itertoolz\n",
    "\n",
    "text = 'the rain in Spain falls mainly on the plain with pain where hardly any rain stains the'\n",
    "\n",
    "ovecm = []\n",
    "for ovec in vect.get_feature_names():\n",
    "    # print ovec\n",
    "    ovecd = {}\n",
    "    for oword in ovec.split(\" \"):\n",
    "        # print (oword, myDict[oword])\n",
    "        ovecd[oword] = myDict[oword]\n",
    "    ovecm.append(ovecd)\n",
    "\n",
    "print ovecm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  0.  0.]]\n",
      "[[ 1.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.]]\n",
      "[[ 1.  0.  0.  0.  0.]]\n",
      "[[ 1.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.]]\n",
      "[[ 1.  0.  0.  0.  0.]]\n",
      "[[ 1.  0.  0.  0.  0.]]\n",
      "[[ 1.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# now one-hot encode the words mapped from vocabulary\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "for wdict in ovecm:\n",
    "    # print np.asarray(wdict.values()).reshape(-1, 1)\n",
    "    dictarr = np.asarray(wdict.values()).reshape(-1, 1) # extract values from dict (.values), as array and transform (reshape)\n",
    "    enc = OneHotEncoder() # Encode categorical integer features using a one-hot aka one-of-K scheme\n",
    "    # The output will be a sparse matrix where each column corresponds to one possible value of one feature\n",
    "    enc.fit(dictarr) # Fit OneHotEncoder to dictarr\n",
    "    print enc.transform([[0]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02  0.14  0.02  0.02  0.01  0.01  0.02  0.01]]\n"
     ]
    }
   ],
   "source": [
    "# skip-gram learning example\n",
    "# model structure\n",
    "# xk -> wi -> ht -> wo -> yk : tc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "wi = np.array([[-0.094, -0.44, 0.31], [-0.491, -0.23, 0.065], [0.07, 0.17, -0.36], [0.1, 0.46, 0.08], [-0.23, -0.15, -0.04], [0.41, -0.19, -0.44], [0.18, 0.09, 0.28], [-0.05, 0.49, 0.26]])\n",
    "wo = np.array([[0.02, 0.48, 0.43, 0.37, -0.36, -0.12, 0.27, -0.35], [-0.37, 0.42, -0.26, -0.15, 0.03, 0.35, -0.14, 0.13], [0.42, 0.36, 0.47, -0.02, -0.42, -0.44, 0.27, -0.45]])\n",
    "xk = np.array([[0, 1, 0, 0, 0, 0, 0, 0]])\n",
    "ht = np.dot(xk, wi)\n",
    "u0 = np.dot(ht, wo)\n",
    "\n",
    "#print u0\n",
    "yk = np.exp(u0) / np.dot(np.exp(u0), np.exp(u0).transpose())\n",
    "#print yk\n",
    "\n",
    "# backpropagation (following Xin Rong's paper here)\n",
    "tc = np.array([[0, 1, 0, 0, 0, 0, 0, 0]]) # truth\n",
    "ej = yk - tc # error\n",
    "etha = 0.01\n",
    "\n",
    "# Update equation for hidden→output weights\n",
    "wo = wo - etha * (np.transpose(ht) * ej)\n",
    "# Update equation for input→hidden weights\n",
    "ehi = np.dot(ej, np.transpose(wo)) # dE/dhi\n",
    "wi = wi - etha * ehi\n",
    "\n",
    "# next cycles\n",
    "i=0\n",
    "while i<200:\n",
    "    ht = np.dot(xk, wi)\n",
    "    u0 = np.dot(ht, wo)\n",
    "    yk = np.exp(u0) / np.dot(np.exp(u0), np.exp(u0).transpose())\n",
    "    ej = yk - tc # error\n",
    "    #print ej\n",
    "    #ei = np.sum(ej) sum has to be over context, here C=1\n",
    "    wo = wo - etha * (np.transpose(ht) * ej)\n",
    "    ehi = np.dot(ej, np.transpose(wo)) # dE/dhi\n",
    "    wi = wi - etha * ehi\n",
    "    i += 1\n",
    "\n",
    "#print '%.2f' % yk\n",
    "np.set_printoptions(precision=2)\n",
    "print yk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.104993585404\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def sig(x, ds):\n",
    "    if ds:\n",
    "        return sig(x, False) * (1.0 - sig(x, False))\n",
    "    return 1.0 / (1.0 + math.exp(-x))\n",
    "\n",
    "print sig(2.0, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02  0.2   0.02  0.01  0.01  0.01  0.02  0.01]]\n"
     ]
    }
   ],
   "source": [
    "# negative sampling weight updates (based on above example)\n",
    "# model structure\n",
    "# xk -> wi -> ht -> wo -> yk : tc\n",
    "import numpy as np\n",
    "\n",
    "def sig(x, ds):\n",
    "    if ds:\n",
    "        return sig(x, False) * (1.0 - sig(x, False))\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "# negative sample: all the words that train to 0: P(wi)=f(wi^3/4)/sum(f(wi)^3/4)\n",
    "# for now manual\n",
    "wneg = [0, 0, 0, 0, 1, 1, 1, 1] # truth is the 2nd word, 4 words in the negative sample\n",
    "\n",
    "# vocabulary of V=8, N=3 hidden nodes: wi(VxN), wo(NxV)\n",
    "wi = np.array([[-0.094, -0.44, 0.31], [-0.491, -0.23, 0.065], [0.07, 0.17, -0.36], [0.1, 0.46, 0.08], [-0.23, -0.15, -0.04], [0.41, -0.19, -0.44], [0.18, 0.09, 0.28], [-0.05, 0.49, 0.26]])\n",
    "wo = np.array([[0.02, 0.48, 0.43, 0.37, -0.36, -0.12, 0.27, -0.35], [-0.37, 0.42, -0.26, -0.15, 0.03, 0.35, -0.14, 0.13], [0.42, 0.36, 0.47, -0.02, -0.42, -0.44, 0.27, -0.45]])\n",
    "xk = np.array([[0, 1, 0, 0, 0, 0, 0, 0]])\n",
    "ht = np.dot(xk, wi)\n",
    "u0 = np.dot(ht, wo)\n",
    "\n",
    "yk = np.exp(u0) / np.dot(np.exp(u0), np.exp(u0).transpose())\n",
    "\n",
    "# backpropagation (following Xin Rong's paper here)\n",
    "tc = np.array([[0, 1, 0, 0, 0, 0, 0, 0]]) # truth\n",
    "ej = yk - tc # error\n",
    "etha = 0.01\n",
    "\n",
    "# \"label\" of the word: tj=1 if wj in positive sample, t=0 otherwise\n",
    "tj = [0, 1, 0, 0, 0, 0, 0, 0] # this is equal to truth tc\n",
    "\n",
    "# Update equation for hidden→output weights\n",
    "# todo here: only for wo and wneg -> need to restrict\n",
    "wo = wo - etha * (sig(wo * np.transpose(ht), False) - tj) * np.transpose(ht)\n",
    "# Update equation for input→hidden weights\n",
    "ehi = (sig(wo * np.transpose(ht), False) - tj) * wo\n",
    "wi = wi - etha * np.transpose(ehi)\n",
    "\n",
    "#print yk\n",
    "\n",
    "# next cycles\n",
    "i=0\n",
    "while i<400:\n",
    "    ht = np.dot(xk, wi)\n",
    "    u0 = np.dot(ht, wo)\n",
    "    yk = np.exp(u0) / np.dot(np.exp(u0), np.exp(u0).transpose())\n",
    "    ej = yk - tc # error\n",
    "    wo = wo - etha * (sig(wo * np.transpose(ht), False) - tj) * np.transpose(ht)\n",
    "    ehi = (sig(wo * np.transpose(ht), False) - tj) * wo\n",
    "    wi = wi - etha * np.transpose(ehi)\n",
    "    i += 1\n",
    "\n",
    "print yk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never: 0.776\n",
      "not: 0.684\n",
      "others: 0.684\n",
      "some: 0.776\n",
      "test: 0.776\n",
      "text: 0.684\n",
      "the: 0.842\n",
      "this: 0.776\n",
      "was: 0.684\n"
     ]
    }
   ],
   "source": [
    "# word training set threshold\n",
    "import math\n",
    "\n",
    "# Run-length encoding of a list\n",
    "def rlencode(lst):\n",
    "    lstlst = []\n",
    "    sublst = []\n",
    "    previtem = lst[0]\n",
    "    for item in lst:\n",
    "        if item != previtem:\n",
    "            lstlst.append(sublst)\n",
    "            sublst = []\n",
    "        sublst.append(item)\n",
    "        previtem = item\n",
    "    lstlst.append(sublst)\n",
    "    lst = []    \n",
    "    for item in lstlst:\n",
    "        lst.append([len(item), item[0]])\n",
    "    return lst\n",
    "\n",
    "# threshold probability\n",
    "def thresprob(f, t):\n",
    "    if f > 0.0:\n",
    "        return 1.0 - math.sqrt(t / f)\n",
    "    else:\n",
    "        reurn -1.0\n",
    "\n",
    "text = 'this test text the the was not this test some others some the never the never'\n",
    "textlist = text.split(\" \")\n",
    "textlist.sort()\n",
    "wordfreqs = rlencode(textlist)\n",
    "for elem in wordfreqs:\n",
    "    print('{0}: {1:.3f}'.format(elem[1], thresprob(elem[0], 0.1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 7116\n",
      "[ 5.44265126  5.44265126  5.44265126 ...,  5.44265126  5.44265126\n",
      "  5.44265126]\n",
      "(169, 7116)\n",
      "[[ 0.12712286  0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "5.23157147097\n",
      "0.414852073285\n"
     ]
    }
   ],
   "source": [
    "# read a large text file, line by line and build vocabulary\n",
    "import os\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import operator # to sort dict\n",
    "\n",
    "def read1k():\n",
    "    return f.read(1024)\n",
    "\n",
    "def process_data(chunk, text):\n",
    "    #print('processed: {0}'.format(ctr))\n",
    "    text.append(unicode(chunk, errors='ignore')) # 'utf8' codec can't decode byte 0xc3\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    # read corpus\n",
    "    os.chdir('C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "    f = open('todinvenedig.txt')\n",
    "    text = []\n",
    "    for piece in iter(read1k, ''):\n",
    "        process_data(piece, text)\n",
    "    \n",
    "    # get stopwords\n",
    "    f = open('gerstopw0.txt')\n",
    "    stptext = []\n",
    "    for piece in iter(read1k, ''):\n",
    "        process_data(piece, stptext)\n",
    "    stopwrds = []\n",
    "    for elem in stptext:\n",
    "        stopwrds += elem.split('\\n')\n",
    "    #print stopwrds\n",
    "    \n",
    "    # create the transform\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', stop_words=stopwrds)\n",
    "    # tokenize and build vocabulary\n",
    "    vectorizer.fit(text)\n",
    "    # summarize\n",
    "    #print(vectorizer.vocabulary_)\n",
    "    print('size of vocabulary: {0}'.format(len(vectorizer.vocabulary_)))\n",
    "    print(vectorizer.idf_) # idf=inverse document frequencies: the most frequent word \"the\" is assigned the lowest frequency, 1\n",
    "    # encode document (as a sparse matrix, scores are normalized)\n",
    "    vector = vectorizer.transform(text)\n",
    "    # summarize encoded vector\n",
    "    print(vector.shape) # shape = [n_samples, n_features]\n",
    "    print(vector.toarray())\n",
    "    \n",
    "    \n",
    "    # print the idf for the vocabulary, sort dict: sorted_x = sorted(x.items(), key=operator.itemgetter(1))\n",
    "    idf = vectorizer.idf_\n",
    "    featureidfs = sorted(dict(zip(vectorizer.get_feature_names(), idf)).items(), key=operator.itemgetter(1))\n",
    "    favrg = 0.0\n",
    "    for f in featureidfs:\n",
    "        favrg += f[1]\n",
    "    #print featureidfs\n",
    "    favrg /= float(len(featureidfs))\n",
    "    print favrg\n",
    "    fstdev = 0.0\n",
    "    for f in featureidfs:\n",
    "        fstdev += (f[1] - favrg) * (f[1] - favrg)\n",
    "    fstdev = math.sqrt(fstdev / float(len(featureidfs) - 1))\n",
    "    print fstdev\n",
    "    \n",
    "    # delete all entries from featureidfs that have very low scores (2 std dev)\n",
    "    #flimit = favrg - 2.0 * fstdev\n",
    "    #for f in featureidfs:\n",
    "        #if f[1] < flimit:\n",
    "            #print f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'aber', u'alle', u'allem', u'allen', u'aller', u'alles', u'als', u'also', u'am', u'an', u'ander', u'andere', u'anderem', u'anderen', u'anderer', u'anderes', u'anderm', u'andern', u'anderr', u'anders', u'auch', u'auf', u'aus', u'bei', u'bin', u'bis', u'bist', u'da', u'damit', u'dann', u'der', u'den', u'des', u'dem', u'die', u'das', u'da\\ufffd\\ufffd', u'dass', u'derselbe', u'derselben', u'denselben', u'desselben', u'demselben', u'dieselbe', u'dieselben', u'dasselbe', u'dazu', u'dein', u'deine', u'deinem', u'deinen', u'deiner', u'deines', u'denn', u'derer', u'dessen', u'dich', u'dir', u'du', u'dies', u'diese', u'diesem', u'diesen', u'dieser', u'dieses', u'doch', u'dort', u'durch', u'ein', u'eine', u'einem', u'einen', u'einer', u'eines', u'einig', u'einige', u'einigem', u'einigen', u'einiger', u'einiges', u'einmal', u'er', u'ihn', u'ihm', u'es', u'etwas', u'euer', u'eure', u'eurem', u'euren', u'eurer', u'eures', u'f\\ufffd\\ufffdr', u'gegen', u'gewesen', u'hab', u'habe', u'haben', u'hat', u'hatte', u'hatten', u'hier', u'hin', u'hinter', u'ich', u'mich', u'mir', u'ihr', u'ihre', u'ihrem', u'ihren', u'ihrer', u'ihres', u'euch', u'im', u'in', u'indem', u'ins', u'ist', u'jede', u'jedem', u'jeden', u'jeder', u'jedes', u'jene', u'jenem', u'jenen', u'jener', u'jenes', u'jetzt', u'kann', u'kein', u'keine', u'keinem', u'keinen', u'keiner', u'keines', u'k\\ufffd\\ufffdnnen', u'k\\ufffd\\ufffdnnte', u'machen', u'man', u'manche', u'manchem', u'manchen', u'mancher', u'manches', u'mein', u'meine', u'meinem', u'meinen', u'meiner', u'meines', u'mit', u'muss', u'musste', u'nach', u'nicht', u'nichts', u'noch', u'nun', u'nur', u'ob', u'oder', u'ohne', u'sehr', u'sei', u'sein', u'seine', u'seinem', u'seinen', u'seiner', u'seines', u'selbst', u'sich', u'sie', u'ihne'], [u'n', u'sind', u'so', u'solche', u'solchem', u'solchen', u'solcher', u'solches', u'soll', u'sollte', u'sondern', u'sonst', u'\\ufffd\\ufffdber', u'um', u'und', u'uns', u'unse', u'unsem', u'unsen', u'unser', u'unses', u'unter', u'viel', u'vom', u'von', u'vor', u'w\\ufffd\\ufffdhrend', u'war', u'waren', u'warst', u'was', u'weg', u'weil', u'weiter', u'welche', u'welchem', u'welchen', u'welcher', u'welches', u'wenn', u'werde', u'werden', u'wie', u'wieder', u'will', u'wir', u'wird', u'wirst', u'wo', u'wobei', u'wodurch', u'wollen', u'wollte', u'womit', u'w\\ufffd\\ufffdrde', u'w\\ufffd\\ufffdrden', u'zu', u'zum', u'zur', u'zwar', u'zwischen']]\n"
     ]
    }
   ],
   "source": [
    "# test reading stopword file\n",
    "import os\n",
    "\n",
    "def read1l():\n",
    "    return f.read(1024)\n",
    "\n",
    "def process_data(chunk, text):\n",
    "    text.append(unicode(chunk, errors='replace')) # 'utf8' codec can't decode byte 0xc3\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    os.chdir('C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "    f = open('gerstopw0.txt')\n",
    "    text = []\n",
    "    \n",
    "    for piece in iter(read1k, ''):\n",
    "        process_data(piece, text)\n",
    "    \n",
    "    #print text\n",
    "    \n",
    "    stopwrds = []\n",
    "    \n",
    "    for elem in text:\n",
    "        stopwrds.append(elem.split('\\n'))\n",
    "    \n",
    "    print stopwrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.494480916111\n"
     ]
    }
   ],
   "source": [
    "# size of vocabulary about 7000: test init weight matrices\n",
    "import numpy as np\n",
    "\n",
    "wi = np.random.rand(7000,3) - 0.5\n",
    "print wi[2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_c: 0.115\n",
      "c_g: 1.115\n",
      "d_e: 0.478\n",
      "b_h: 0.700\n",
      "a_d: 0.308\n",
      "a_b: -0.469\n",
      "d_b: -0.107\n",
      "e_g: 0.700\n",
      "b_a: -0.469\n",
      "e_b: -0.300\n",
      "a_h: 1.115\n",
      "b_d: -0.107\n",
      "b_e: -0.300\n",
      "c_e: 0.700\n",
      "g_h: 1.700\n",
      "b_g: 0.115\n",
      "d_g: 0.893\n",
      "g_a: 0.531\n",
      "e_h: 1.285\n",
      "c_d: 0.893\n",
      "e_a: 0.115\n",
      "b_b: -0.885\n",
      "a_c: 0.531\n",
      "d_a: 0.308\n",
      "a_e: 0.115\n",
      "c_b: 0.115\n"
     ]
    }
   ],
   "source": [
    "# Pointwise Mutual Information(PMI)\n",
    "from collections import Counter # implements specialized container datatypes\n",
    "from math import log\n",
    "\n",
    "def gen_bigrams(data, window_size=5):\n",
    "    for idx in range(len(data)):\n",
    "        window = data[idx: idx + window_size]\n",
    "        if len(window) < 2:\n",
    "            break\n",
    "        w = window[0]\n",
    "        for next_word in window[1:]:\n",
    "            yield (w, next_word) # like return but returns a generator (a one-time iterator)\n",
    "            \n",
    "\n",
    "def construct_vocab(data):\n",
    "    vocab = Counter()\n",
    "    for (w1, w2) in gen_bigrams(data, window_size=5): # count 1gram & 2gram\n",
    "        vocab.update([w1, w2, (w1, w2)])\n",
    "    return vocab\n",
    "        \n",
    "\n",
    "def calc_pmi(vocab):\n",
    "    det = sum(vocab.values())\n",
    "    for (w1, w2) in filter(lambda el: isinstance(el, tuple), vocab):\n",
    "        p_a, p_b = float(vocab[w1]), float(vocab[w2])\n",
    "        p_ab = float(vocab[(w1, w2)])\n",
    "        \n",
    "        yield (w1, w2, log((det * p_ab) / (p_a * p_b), 2))\n",
    "    \n",
    "\n",
    "corpus = [\"a\", \"b\", \"c\", \"d\", \"e\", \"b\", \"g\", \"a\", \"h\"]\n",
    "vocab = construct_vocab(corpus)\n",
    "\n",
    "#print vocab\n",
    "\n",
    "#for i in gen_bigrams(corpus):\n",
    "    #print i\n",
    "\n",
    "for (w1, w2, pmi) in calc_pmi(vocab):\n",
    "    print(\"{}_{}: {:.3f}\".format(w1, w2, pmi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "xxxxxxxxxxxxxxxxxxxxxx\n",
      "0\n",
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "a = [('a',1), ('b',2)]\n",
    "for k in a:\n",
    "    # v is the list of grades for student k\n",
    "    #avgDict[k] = sum(v)/ float(len(v))\n",
    "    print k[1]\n",
    "print 'xxxxxxxxxxxxxxxxxxxxxx'\n",
    "mygenerator = (x*x for x in range(3))\n",
    "for i in mygenerator:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['g', 'h', 'a', 'b', 'c'], ['h', 'a', 'b', 'c', 'd'], ['a', 'b', 'c', 'd', 'e'], ['b', 'c', 'd', 'e', 'f'], ['c', 'd', 'e', 'f', 'g'], ['d', 'e', 'f', 'g', 'h']]\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# phrase extraction\n",
    "\n",
    "def valinlst(lst, val):\n",
    "    for elem in lst:\n",
    "        if elem == val:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "a = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
    "win2 = 2\n",
    "i = 0\n",
    "phrase = []\n",
    "phrases = []\n",
    "for elem in a:\n",
    "    if i > -win2-1 and i < len(a)-win2:\n",
    "        for j in range(i-win2,i+win2+1):\n",
    "            phrase.append(a[j])\n",
    "        phrases.append(phrase)\n",
    "        phrase = []\n",
    "    i += 1\n",
    "print phrases\n",
    "# PMI = log(p(phrase)/tt p(w)) w from phrase\n",
    "for elem in phrases:\n",
    "    print valinlst(elem, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          2.30742853  0.          2.30742853]\n",
      " [ 0.          0.          2.30742853  0.          2.30742853]\n",
      " [ 1.69647082  0.          0.          0.          0.        ]\n",
      " [ 0.          0.56192226  0.          0.46325333  0.        ]]\n",
      "[[ 0.          0.          0.74144607  0.          0.74144607]\n",
      " [ 0.          0.          0.74144607  0.          0.74144607]\n",
      " [ 0.80856027  0.          0.          0.          0.        ]\n",
      " [ 0.          0.88346508  0.          0.2757825   0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log2\n",
      "  del sys.path[0]\n",
      "D:\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: divide by zero encountered in log2\n"
     ]
    }
   ],
   "source": [
    "# co-occurance matrix and PMI (example from vector semantics Stanford NLP)\n",
    "import numpy as np\n",
    "\n",
    "cooccf = np.matrix([[0., 0., .05, 0., .05], [0., 0., .05, 0., .05], [0.11, 0.05, 0., 0.05, 0.], [0.05, 0.32, 0., 0.21, 0.]])\n",
    "\n",
    "lplcsm = 0.0 # Laplace smoothing between 0.1 to 3.0\n",
    "\n",
    "pistar = (np.sum(cooccf, axis=1) + lplcsm) / (np.sum(cooccf) + lplcsm) # words\n",
    "#print pistar\n",
    "pstarj = (np.sum(cooccf, axis=0) + lplcsm) / (np.sum(cooccf) + lplcsm) # contexts\n",
    "#print pstarj\n",
    "pij = cooccf / np.sum(cooccf)\n",
    "ppmi = np.maximum(np.log2(pij / pistar / pstarj), 0.)\n",
    "\n",
    "print ppmi\n",
    "\n",
    "alpha = 0.75 # Levy et al. (2015)\n",
    "palpha = np.power(np.count_nonzero(cooccf, axis=0) + lplcsm, alpha) / np.power(np.count_nonzero(cooccf) + lplcsm, alpha)\n",
    "ppmia = np.maximum(np.log2(pij / pistar / palpha), 0.)\n",
    "\n",
    "print ppmia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "number of words in corpus (after stopwords removed): 10\n",
      "number of phrases: 70\n"
     ]
    }
   ],
   "source": [
    "# co-occurance test large files\n",
    "import os\n",
    "import math\n",
    "import re # regex\n",
    "import numpy as np\n",
    "#import scipy.sparse as sp\n",
    "\n",
    "def read1k():\n",
    "    return f.read(1024)\n",
    "\n",
    "def process_data(chunk, text):\n",
    "    #print('processed: {0}'.format(ctr))\n",
    "    text.append(unicode(chunk, errors='ignore')) # 'utf8' codec can't decode byte 0xc3\n",
    "    \n",
    "def valinlst(val, lst): # checks if value is in a list\n",
    "    for elem in lst:\n",
    "        if elem == val:\n",
    "            #print('valinlst elem: {0} val: {1}'.format(elem, val))\n",
    "            #re.match( val, elem, re.I)\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def findphrases(corpus, win2): # returns all phrases from corpus for given window as a list\n",
    "    i = 0\n",
    "    phrase = []\n",
    "    phrases = []\n",
    "    for elem in corpus:\n",
    "        if i > -win2-1 and i < len(corpus)-win2:\n",
    "            for j in range(i-win2,i+win2+1): # for skip-gram we would need to omit the i-value\n",
    "                phrase.append(corpus[j])\n",
    "            phrases.append(phrase)\n",
    "            phrase = []\n",
    "        #i += 1\n",
    "        i += ( 2 * win2 + 1 )\n",
    "    return phrases\n",
    "\n",
    "def rmsword(corpus, stopwords): # remove stopwords from corpus\n",
    "    i = 0\n",
    "    for elem in corpus:\n",
    "        for sword in stopwords:\n",
    "            if elem == sword:\n",
    "                while True:\n",
    "                    try:\n",
    "                        corpus.remove(elem) # this throws an error if elem not in corpus (might have been removed already)\n",
    "                        i += 1\n",
    "                    except:\n",
    "                        break\n",
    "    return i # returns number of stopwords removed\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # read corpus\n",
    "    os.chdir('C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "    #f = open('todinvenedig.txt')\n",
    "    #f = open('todinvenedigshrt.txt') # shorter version for tests\n",
    "    f = open('vecsemtst0.txt') # test file with fixed similarities\n",
    "    text = []\n",
    "    for piece in iter(read1k, ''):\n",
    "        process_data(piece, text)\n",
    "    \n",
    "    corpus = []\n",
    "    for elem in text:\n",
    "        corpus += elem.split() # splits on all whitespaces\n",
    "    corpus = [t.lower() for t in corpus] # convert to lower case\n",
    "    \n",
    "    # remove special characters\n",
    "    corpus = [re.sub(r'[^\\w]', ' ', t) for t in corpus]\n",
    "    \n",
    "    #print corpus\n",
    "    \n",
    "    text = []\n",
    "    # get stopwords\n",
    "    f = open('gerstopw0.txt')\n",
    "    for piece in iter(read1k, ''):\n",
    "        process_data(piece, text)\n",
    "    stopwrds = []\n",
    "    for elem in text:\n",
    "        stopwrds += elem.split()\n",
    "    stopwrds = [t.lower() for t in stopwrds]\n",
    "    #print stopwrds\n",
    "    \n",
    "    #print('length of corpus: {0}'.format(len(corpus)))\n",
    "    print rmsword(corpus, stopwrds) # remove stopwords from corpus\n",
    "    #print('length of corpus: {0}'.format(len(corpus)))\n",
    "    \n",
    "    # fill words set (unique!) from corpus\n",
    "    words = set() # words as set: each entry unique\n",
    "    for elem in corpus:\n",
    "        words.add(elem)\n",
    "        \n",
    "    print('number of words in corpus (after stopwords removed): {0}'.format(len(words)))\n",
    "    \n",
    "    phrases = findphrases(corpus, 1) # extract phrases from corpus (window +/- the given size)\n",
    "    #print phrases\n",
    "    print('number of phrases: {0}'.format(len(phrases)))\n",
    "    \n",
    "    wordcont = np.zeros((len(words),len(phrases)))\n",
    "    k = 1.5 # Laplacian smoothing\n",
    "    \n",
    "    # fill word-context (or word-phrase) matrix\n",
    "    i = 0 # word index\n",
    "    wordlist = []\n",
    "    for word in words:\n",
    "        j = 0 # phrease index\n",
    "        wordlist.append(word) # to allow for index-access later\n",
    "        for phrase in phrases:\n",
    "            if valinlst(word, phrase):\n",
    "                wordcont[i][j] += 1 + k # k for Laplacian smoothing\n",
    "                #if wordcont[i][j] >= 1.0:\n",
    "                #    print('wordcont[{0}][{1}]: {2} word: {3} phrease: {4}'.format(i,j,wordcont[i][j],word,phrase))\n",
    "            j += 1\n",
    "        i += 1\n",
    "    \n",
    "    #print sp.issparse(wordcont)\n",
    "    #print wordcont\n",
    "    #print np.nonzero(wordcont)\n",
    "    #print(\"word set: {0} word list: {1}\".format(next(iter(words)), wordlist[0]))\n",
    "    \n",
    "    # calculate pointwise mutual information\n",
    "    fijsum = 0.0\n",
    "    pistar = []\n",
    "    pstarj = []\n",
    "    for i in range(len(words)):\n",
    "        pistar.append(0.0)\n",
    "        for j in range(len(phrases)):\n",
    "            if i is 0:\n",
    "                pstarj.append(0.0)\n",
    "            fijsum += wordcont[i][j]\n",
    "            pistar[i] += wordcont[i][j]\n",
    "            pstarj[j] += wordcont[i][j]\n",
    "    \n",
    "    #print fijsum\n",
    "    #print pistar\n",
    "    #print pstarj\n",
    "    \n",
    "    ppmi = np.zeros((len(words),len(phrases)))\n",
    "    for i in range(len(words)):\n",
    "        for j in range(len(phrases)):\n",
    "            if wordcont[i][j] > 0.0:\n",
    "                ppmi[i][j] = max(math.log((wordcont[i][j]*fijsum)/pistar[i]/pstarj[j], 2.0), 0.0)\n",
    "    \n",
    "    #print ppmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppmi mean: 0.39103800535 and stdev: 0.856071298175\n",
      "ppmi max: 5.533978572 and min: 0.0\n",
      "wcounter - no. of ppmi above limit: 139\n",
      "ppmi 0.63 for word: fool and phrase: [u'fool', u'like', u'night'] idx 0 , 1\n",
      "ppmi 1.21 for word: fool and phrase: [u'fool', u'like', u'fool'] idx 0 , 30\n",
      "ppmi 1.21 for word: fool and phrase: [u'like', u'fool', u'like'] idx 0 , 31\n",
      "ppmi 1.21 for word: fool and phrase: [u'fool', u'like', u'fool'] idx 0 , 32\n",
      "ppmi 1.21 for word: fool and phrase: [u'like', u'fool', u'like'] idx 0 , 33\n",
      "ppmi 1.21 for word: fool and phrase: [u'fool', u'like', u'fool'] idx 0 , 34\n",
      "ppmi 1.21 for word: fool and phrase: [u'like', u'fool', u'like'] idx 0 , 35\n",
      "ppmi 1.21 for word: fool and phrase: [u'fool', u'like', u'fool'] idx 0 , 36\n",
      "ppmi 1.21 for word: fool and phrase: [u'like', u'fool', u'like'] idx 0 , 37\n",
      "ppmi 1.21 for word: fool and phrase: [u'fool', u'like', u'fool'] idx 0 , 38\n",
      "ppmi 1.21 for word: fool and phrase: [u'like', u'fool', u'like'] idx 0 , 39\n",
      "ppmi 1.21 for word: fool and phrase: [u'fool', u'like', u'fool'] idx 0 , 40\n",
      "ppmi 0.63 for word: fool and phrase: [u'dummy', u'fool', u'henry'] idx 0 , 43\n",
      "ppmi 1.21 for word: fool and phrase: [u'fool', u'henry', u'fool'] idx 0 , 44\n",
      "ppmi 1.21 for word: fool and phrase: [u'henry', u'fool', u'henry'] idx 0 , 45\n",
      "ppmi 1.21 for word: fool and phrase: [u'fool', u'henry', u'fool'] idx 0 , 46\n",
      "ppmi 1.21 for word: fool and phrase: [u'henry', u'fool', u'henry'] idx 0 , 47\n",
      "ppmi 1.21 for word: fool and phrase: [u'fool', u'henry', u'fool'] idx 0 , 48\n",
      "ppmi 1.21 for word: fool and phrase: [u'henry', u'fool', u'henry'] idx 0 , 49\n",
      "ppmi 1.21 for word: fool and phrase: [u'fool', u'henry', u'fool'] idx 0 , 50\n"
     ]
    }
   ],
   "source": [
    "# read out ppmi-matrix and display words and context/phrases\n",
    "import numpy as np\n",
    "\n",
    "def getwordi(words, indx): # access words set\n",
    "    i = 0\n",
    "    for elem in words:\n",
    "        if i == indx:\n",
    "            return elem\n",
    "        i += 1\n",
    "\n",
    "print('ppmi mean: {0} and stdev: {1}'.format(np.mean(ppmi), np.std(ppmi)))\n",
    "print('ppmi max: {0} and min: {1}'.format(np.amax(ppmi), np.amin(ppmi)))\n",
    "\n",
    "isigword = [] # index array for significant words\n",
    "isiphrs = [] # index array for significant words\n",
    "\n",
    "wcounter = 0\n",
    "for i in range(len(wordlist)):\n",
    "    for j in range(len(phrases)):\n",
    "        if ppmi[i][j] > 0.3: # note limit\n",
    "            #print('ppmi = {0} for word: {1} and phrase: {2}'.format(ppmi[i][j], wordlist[i], phrases[j]))\n",
    "            isigword.append(i)\n",
    "            isiphrs.append(j)\n",
    "            wcounter += 1\n",
    "            \n",
    "print('wcounter - no. of ppmi above limit: {0}'.format(wcounter))\n",
    "\n",
    "#for i in range(len(isigword)):\n",
    "for i in range(20):\n",
    "    print('ppmi {0:.2f} for word: {1} and phrase: {2} idx {3} , {4}'.format(ppmi[isigword[i]][isiphrs[i]], wordlist[isigword[i]], phrases[isiphrs[i]], isigword[i], isiphrs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine dummy and caesar: 0.209\n",
      "cosine ol and henry: 0.142\n",
      "cosine ol and henry: 0.142\n",
      "cosine ol and henry: 0.142\n",
      "cosine ol and henry: 0.142\n",
      "cosine ol and henry: 0.142\n",
      "cosine ol and henry: 0.142\n",
      "cosine ol and henry: 0.142\n",
      "cosine ol and henry: 0.142\n",
      "cosine ol and henry: 0.142\n",
      "cosine ol and henry: 0.142\n",
      "cosine ol and henry: 0.142\n",
      "cosine ol and henry: 0.142\n",
      "cosine ol and henry: 0.142\n",
      "cosine ol and henry: 0.142\n",
      "cosine ol and henry: 0.142\n",
      "cosine ol and henry: 0.142\n",
      "cosine ol and henry: 0.142\n",
      "cosine ol and henry: 0.142\n",
      "cosine ol and henry: 0.142\n",
      "cosine ol and henry: 0.142\n",
      "cosine ol and henry: 0.142\n",
      "cosine ol and henry: 0.142\n",
      "cosine ol and henry: 0.142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and fo: 0.142\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and fo: 0.142\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and fo: 0.142\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and fo: 0.142\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and fo: 0.142\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and fo: 0.142\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and fo: 0.142\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and fo: 0.142\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and fo: 0.142\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and fo: 0.142\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and fo: 0.142\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and fo: 0.142\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and fo: 0.142\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and fo: 0.142\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and fo: 0.142\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and fo: 0.142\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and fo: 0.142\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and fo: 0.142\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and fo: 0.142\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and fo: 0.142\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and fo: 0.142\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and fo: 0.142\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and soldier: 0.168\n",
      "cosine henry and fo: 0.142\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and night: 0.235\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n"
     ]
    }
   ],
   "source": [
    "# measure similarity between words for given context/phrases\n",
    "import numpy as np\n",
    "\n",
    "def cosine(wordv, wordw): # cosine similarity for two word vectors\n",
    "    sumv = 0.\n",
    "    sumw = 0.\n",
    "    sumvw = 0.\n",
    "    for elemv, elemw in zip(wordv, wordw):\n",
    "        #print('for: {0} {1}'.format(elemv, elemw))\n",
    "        sumv += elemv * elemv\n",
    "        sumw += elemw * elemw\n",
    "        sumvw += elemv * elemw\n",
    "    sumv = math.sqrt(sumv)\n",
    "    sumw = math.sqrt(sumw)\n",
    "    #print('{0} {1}'.format(sumv, sumw))\n",
    "    if sumv > 0. and sumw > 0.:\n",
    "        return sumvw / sumv / sumw\n",
    "    else: return -1.\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    #for i in range(len(isigword)):\n",
    "        #for j in range(i+1,len(isiphrs)):\n",
    "    for i in range(10):\n",
    "        for j in range(i+1,10):\n",
    "            thiscosine = cosine(ppmi[isigword[i]], ppmi[isigword[j]])\n",
    "            if thiscosine > 0.1 and wordlist[isigword[i]] != wordlist[isigword[j]]:\n",
    "                print('cosine {0} and {1}: {2:.3f}'.format(wordlist[isigword[i]], wordlist[isigword[j]], thiscosine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppmi_red mean: 0.39103800535 and stdev: 0.856071298175\n",
      "ppmi_red max: 5.533978572 and min: -3.94305785178e-15\n"
     ]
    }
   ],
   "source": [
    "# Dense vectors - applying SVD\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "W, S, C = linalg.svd(ppmi, overwrite_a=True, full_matrices=False)\n",
    "Sc = np.diag(S)\n",
    "\n",
    "#print W.shape, Sc.shape, C.shape\n",
    "#np.allclose(ppmi, np.dot(W, np.dot(Sc, C)))\n",
    "#np.dot(W, np.dot(Sc, C))\n",
    "\n",
    "# need to reduce matrices to e.g. 50\n",
    "k = 50 # number of singular values we want to keep\n",
    "Cred = C[:k, :]\n",
    "Sred = Sc[:k, :k]\n",
    "Wred = W[:, :k]\n",
    "ppmi_red = np.dot(Wred, np.dot(Sred, Cred))\n",
    "\n",
    "print('ppmi_red mean: {0} and stdev: {1}'.format(np.mean(ppmi_red), np.std(ppmi_red)))\n",
    "print('ppmi_red max: {0} and min: {1}'.format(np.amax(ppmi_red), np.amin(ppmi_red)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and like: 0.617\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n",
      "cosine fool and henry: 0.687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine soldier and night: 0.750\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n",
      "cosine caesar and battle: 0.622\n"
     ]
    }
   ],
   "source": [
    "# measure similarity between words for given context/phrases\n",
    "import numpy as np\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    isigword = [] # index array for significant words\n",
    "    isiphrs = [] # index array for significant words\n",
    "\n",
    "    wcounter = 0\n",
    "    for i in range(len(wordlist)):\n",
    "        for j in range(len(phrases)):\n",
    "            if ppmi_red[i][j] > 1.0: # note limit\n",
    "                isigword.append(i)\n",
    "                isiphrs.append(j)\n",
    "                wcounter += 1\n",
    "    \n",
    "    print wcounter\n",
    "    \n",
    "    #for i in range(len(isigword)):\n",
    "        #for j in range(i+1,len(isiphrs)):\n",
    "    for i in range(10):\n",
    "        for j in range(i+1,10):\n",
    "            thiscosine = cosine(ppmi_red[isigword[i]], ppmi_red[isigword[j]])\n",
    "            if thiscosine > 0.3and wordlist[isigword[i]] != wordlist[isigword[j]]:\n",
    "                print('cosine {0} and {1}: {2:.3f}'.format(wordlist[isigword[i]], wordlist[isigword[j]], thiscosine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yk for a is 0.1962\n",
      "yk for t0 is 0.1036\n",
      "yk for b is 2.7945\n",
      "yk for t1 is 0.0951\n",
      "yk for c is 0.1983\n",
      "yk for t2 is 1.5698\n"
     ]
    }
   ],
   "source": [
    "# regular weight updates\n",
    "# model structure\n",
    "# xk -> wi -> ht -> wo -> yk : tc\n",
    "import os\n",
    "import math\n",
    "import re # regex\n",
    "import numpy as np\n",
    "from toolz import itertoolz\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def read1k():\n",
    "    return f.read(1024)\n",
    "\n",
    "def process_data(chunk, text):\n",
    "    #print('processed: {0}'.format(ctr))\n",
    "    text.append(unicode(chunk, errors='ignore')) # 'utf8' codec can't decode byte 0xc3\n",
    "    \n",
    "def onehotvec(key, vec, dim): # returns a one-hot vector of dimension dim, all 0 but for 1 at vec\n",
    "    retvec = []\n",
    "    retvec.append(key)\n",
    "    for i in range(dim):\n",
    "        if i == vec:\n",
    "            retvec.append(1)\n",
    "        else:\n",
    "            retvec.append(0)\n",
    "    return retvec\n",
    "\n",
    "def onehotenc(dic): # one-hot encodes a dictionary\n",
    "    ohotret = []\n",
    "    dlen = len(dic)\n",
    "    i=0\n",
    "    for key in dic:\n",
    "        #ohotret.append(onehotvec(dic[key], dlen)) # does not work as dict keys are not guaranteed to be continuous\n",
    "        ohotret.append(onehotvec(dic[key], i, dlen))\n",
    "        i += 1\n",
    "    return ohotret\n",
    "\n",
    "def get1hot(dic, dichot, word): # get a 1-hot encoded vector for word from vocabulary dic and its 1-hot version dichot\n",
    "    keyval = dic[word]\n",
    "    for dicvec in dichot:\n",
    "        if dicvec[0] == keyval:\n",
    "            return dicvec[1:] # return all but the first entry as vector\n",
    "    return []\n",
    "\n",
    "def skipgram(corpus, window): # returns skip-grams for given window size from corpus (center word included)\n",
    "    if window > 5:\n",
    "        return []\n",
    "    cpanel = []\n",
    "    for i in range(window+1,len(corpus)-window+1):\n",
    "        skipg = []\n",
    "        for j in range(i-window-1,i+window):\n",
    "            skipg.append(corpus[j])\n",
    "        cpanel.append(skipg)\n",
    "    return cpanel\n",
    "\n",
    "def sig(x, ds): # sigmoid\n",
    "    if ds:\n",
    "        return sig(x, False) * (1.0 - sig(x, False))\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def relu(x, ds): # linear rectifier (approx. version)\n",
    "    if ds:\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    return np.log(1.0 + np.exp(x))\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # read corpus from file\n",
    "    os.chdir('C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "    f = open('vecsemtst0.txt') # test file with fixed similarities\n",
    "    #f = open('todinvenedigshrt.txt')\n",
    "    text = []\n",
    "    for piece in iter(read1k, ''):\n",
    "        process_data(piece, text)\n",
    "    \n",
    "    corpus = []\n",
    "    for elem in text:\n",
    "        corpus += elem.split() # splits on all whitespaces\n",
    "    corpus = [t.lower() for t in corpus] # convert to lower case\n",
    "    \n",
    "    # remove special characters\n",
    "    corpus = [re.sub(r'[^\\w]', ' ', t) for t in corpus]\n",
    "\n",
    "    # build dictionary\n",
    "    myDict = {}\n",
    "    i = 0\n",
    "    wordlist = []\n",
    "    for word in corpus:\n",
    "        # print word, i\n",
    "        newEntry = {word.lower(): i}\n",
    "        if not myDict.has_key(word):\n",
    "            myDict.update(newEntry)\n",
    "            wordlist.append(word)\n",
    "        i = i + 1\n",
    "\n",
    "    #print myDict\n",
    "\n",
    "    # one-hot encode the dictionary\n",
    "    #dictarr = np.asarray(myDict.values()).reshape(-1, 1)\n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit(dictarr)\n",
    "    #enc.transform([[8]]).toarray()\n",
    "    myDic1hot = onehotenc(myDict)\n",
    "    \n",
    "    # simple example to illustrate learning: window size = 3\n",
    "    # vocabulary [A,B,C,D,E,F] represented at indices [0,1,2,3,4,5]\n",
    "    # two contexts: C0: [A,B,C] and C1: [D,E,F]\n",
    "    # input word wi = B\n",
    "    # for C0: [0,1,0,0,0,0] -> truth [1,0,0,0,0,0] and [0,0,1,0,0,0] (wi=B is in context, A and C are related to B)\n",
    "    # for C1: [0,1,0,0,0,0] -> truth [0,0,0,0,0,0] and [0,0,0,0,0,0] (wi=B is not in context, D and F are unrelated to B)\n",
    "    # input word wi = E\n",
    "    # for C1: [0,0,0,0,1,0] -> truth [0,0,0,1,0,0] and [0,0,0,0,0,1] (wi=E is in context, D and F are related to E)\n",
    "\n",
    "    # negative sample: all the words that train to 0: P(wi)=f(wi^3/4)/sum(f(wi)^3/4)\n",
    "    \n",
    "    # build all possible contexts\n",
    "    winsize = 1\n",
    "    contexts = skipgram(corpus, winsize)\n",
    "    \n",
    "    V = len(myDict)\n",
    "    N = 4\n",
    "    # initialize weight matrices (random -0.5 to 0.5)\n",
    "    # vocabulary of V=8, N=3 hidden nodes: wi(VxN), wo(NxV)\n",
    "    wi = np.random.rand(V,N) - np.full((V,N), 0.5) # inputs to hidden\n",
    "    wo = np.random.rand(N,V) - np.full((N,V), 0.5) # hidden to outputs\n",
    "    \n",
    "    #print('wi before: {0}'.format(wi))\n",
    "\n",
    "    for context in contexts: # loop over context\n",
    "        inpword = context[winsize] # middle word of current context\n",
    "        # access vocabulary dictionary an get 1-hot encoded vector\n",
    "        inpvec = get1hot(myDict, myDic1hot, inpword)\n",
    "        xk = np.array(inpvec) # input vector for training\n",
    "        #print('for input {0} and context word {1} in context {2}'.format(inpword, cword, context))\n",
    "        # for non-central context words run the training (word-vector entry for context word = 1)\n",
    "        ht = np.dot(xk, wi)\n",
    "        u0 = np.dot(ht, wo)\n",
    "\n",
    "        yk = np.exp(u0 + 1.0) / ( np.dot(np.exp(u0), np.exp(u0).transpose()) + 1.0) # added + 1.0 to prevent overflow\n",
    "\n",
    "        # backpropagation (following Xin Rong's paper here)       \n",
    "        ej = np.zeros(V)\n",
    "        i = 0\n",
    "        for cword in context: # loop over current context, cword is the truth\n",
    "            # access vocabulary dictionary an get 1-hot encoded vector\n",
    "            #if i != winsize: # input word not in context (or is it ???)\n",
    "            cwordvec = get1hot(myDict, myDic1hot, cword)\n",
    "            ej += yk - cwordvec # error summed over context\n",
    "            i += 1\n",
    "        #print('ej {0}'.format(ej))\n",
    "        etha = 0.03\n",
    "\n",
    "        # Update equation for hidden→output weights\n",
    "        wo -= etha * (np.transpose([ht]) * ej)\n",
    "        # Update equation for input→hidden weights\n",
    "        ehi = np.dot(ej, np.transpose(wo)) # dE/dhi\n",
    "        wi -= etha * ehi\n",
    "\n",
    "    #print('wi after: {0}'.format(wi))\n",
    "    \n",
    "    # check results\n",
    "    #print myDict\n",
    "    #print('cwordvec for: {0} is: {1}'.format('erkennen',get1hot(myDict, myDic1hot, 'erkennen')))\n",
    "    cwordvec = get1hot(myDict, myDic1hot, 't2')\n",
    "    ht = np.dot(xk, wi)\n",
    "    u0 = np.dot(ht, wo)\n",
    "    yk = np.exp(u0) / np.dot(np.exp(u0), np.exp(u0).transpose())\n",
    "    i = 0\n",
    "    for elem in yk:\n",
    "        if elem > 0.0:\n",
    "            print('yk for {0} is {1:.4f}'.format(wordlist[i],elem))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yk for a is 20.62\n",
      "yk for t0 is 26.15\n",
      "yk for b is 46.24\n",
      "yk for t1 is 8.20\n",
      "yk for c is 11.25\n",
      "yk for t2 is 10.69\n"
     ]
    }
   ],
   "source": [
    "# negative sampling weight updates\n",
    "# model structure\n",
    "# xk -> wi -> ht -> wo -> yk : tc\n",
    "import os\n",
    "import math\n",
    "import re # regex\n",
    "import numpy as np\n",
    "from toolz import itertoolz\n",
    "from random import randint\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def read1k():\n",
    "    return f.read(1024)\n",
    "\n",
    "def process_data(chunk, text):\n",
    "    #print('processed: {0}'.format(ctr))\n",
    "    text.append(unicode(chunk, errors='ignore')) # 'utf8' codec can't decode byte 0xc3\n",
    "    \n",
    "def onehotvec(key, vec, dim): # returns a one-hot vector of dimension dim, all 0 but for 1 at vec\n",
    "    retvec = []\n",
    "    retvec.append(key)\n",
    "    for i in range(dim):\n",
    "        if i == vec:\n",
    "            retvec.append(1)\n",
    "        else:\n",
    "            retvec.append(0)\n",
    "    return retvec\n",
    "\n",
    "def onehotenc(dic): # one-hot encodes a dictionary\n",
    "    ohotret = []\n",
    "    dlen = len(dic)\n",
    "    i=0\n",
    "    for key in dic:\n",
    "        #ohotret.append(onehotvec(dic[key], dlen)) # does not work as dict keys are not guaranteed to be continuous\n",
    "        ohotret.append(onehotvec(dic[key], i, dlen))\n",
    "        i += 1\n",
    "    return ohotret\n",
    "\n",
    "def get1hot(dic, dichot, word): # get a 1-hot encoded vector for word from vocabulary dic and its 1-hot version dichot\n",
    "    keyval = dic[word]\n",
    "    for dicvec in dichot:\n",
    "        if dicvec[0] == keyval:\n",
    "            return dicvec[1:] # return all but the first entry as vector\n",
    "    return np.zeros(len(dic))\n",
    "\n",
    "def skipgram(corpus, window): # returns skip-grams for given window size from corpus (center word included)\n",
    "    if window > 5:\n",
    "        return []\n",
    "    cpanel = []\n",
    "    for i in range(window+1,len(corpus)-window+1):\n",
    "        skipg = []\n",
    "        for j in range(i-window-1,i+window):\n",
    "            skipg.append(corpus[j])\n",
    "        cpanel.append(skipg)\n",
    "    return cpanel\n",
    "\n",
    "def noncword(w, cword, contexts): # return a random word from contexts not equal to the current word cword\n",
    "    rndctx = contexts[randint(0, len(contexts)-1)]\n",
    "    while cword == rndctx[w]:\n",
    "         rndctx = contexts[randint(0, len(contexts)-1)]\n",
    "    return rndctx[w+1]\n",
    "\n",
    "def sig(x, ds): # sigmoid\n",
    "    if ds:\n",
    "        return sig(x, False) * (1.0 - sig(x, False))\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def relu(x, ds): # linear rectifier (approx. version)\n",
    "    if ds:\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    return np.log(1.0 + np.exp(x))\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # read corpus from file\n",
    "    os.chdir('C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "    f = open('vecsemtst0.txt') # test file with fixed similarities\n",
    "    #f = open('todinvenedigshrt.txt')\n",
    "    text = []\n",
    "    for piece in iter(read1k, ''):\n",
    "        process_data(piece, text)\n",
    "    \n",
    "    corpus = []\n",
    "    for elem in text:\n",
    "        corpus += elem.split() # splits on all whitespaces\n",
    "    corpus = [t.lower() for t in corpus] # convert to lower case\n",
    "    \n",
    "    # remove special characters\n",
    "    corpus = [re.sub(r'[^\\w]', ' ', t) for t in corpus]\n",
    "\n",
    "    # build dictionary\n",
    "    myDict = {}\n",
    "    i = 0\n",
    "    wordlist = []\n",
    "    for word in corpus:\n",
    "        # print word, i\n",
    "        newEntry = {word.lower(): i}\n",
    "        if not myDict.has_key(word):\n",
    "            myDict.update(newEntry)\n",
    "            wordlist.append(word)\n",
    "        i = i + 1\n",
    "\n",
    "    #print myDict\n",
    "\n",
    "    # one-hot encode the dictionary\n",
    "    #dictarr = np.asarray(myDict.values()).reshape(-1, 1)\n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit(dictarr)\n",
    "    #enc.transform([[8]]).toarray()\n",
    "    myDic1hot = onehotenc(myDict)\n",
    "    \n",
    "    # simple example to illustrate learning: window size = 3\n",
    "    # vocabulary [A,B,C,D,E,F] represented at indices [0,1,2,3,4,5]\n",
    "    # two contexts: C0: [A,B,C] and C1: [D,E,F]\n",
    "    # input word wi = B\n",
    "    # for C0: [0,1,0,0,0,0] -> truth [1,0,0,0,0,0] and [0,0,1,0,0,0] (wi=B is in context, A and C are related to B)\n",
    "    # for C1: [0,1,0,0,0,0] -> truth [0,0,0,0,0,0] and [0,0,0,0,0,0] (wi=B is not in context, D and F are unrelated to B)\n",
    "    # input word wi = E\n",
    "    # for C1: [0,0,0,0,1,0] -> truth [0,0,0,1,0,0] and [0,0,0,0,0,1] (wi=E is in context, D and F are related to E)\n",
    "\n",
    "    # negative sample: all the words that train to 0: P(wi)=f(wi^3/4)/sum(f(wi)^3/4)\n",
    "    \n",
    "    # build all possible contexts\n",
    "    winsize = 1\n",
    "    contexts = skipgram(corpus, winsize)\n",
    "    \n",
    "    V = len(myDict)\n",
    "    N = 5\n",
    "    # initialize weight matrices (random -0.5 to 0.5)\n",
    "    # vocabulary of V=8, N=3 hidden nodes: wi(VxN), wo(NxV)\n",
    "    wi = np.random.rand(V,N) - np.full((V,N), 0.5) # inputs to hidden\n",
    "    wo = np.random.rand(N,V) - np.full((N,V), 0.5) # hidden to outputs\n",
    "    \n",
    "    #print('wi before: {0}'.format(wi))\n",
    "    \n",
    "    for context in contexts: # loop over contexts, each context a collection of words\n",
    "        \n",
    "        # for negative samples need to add negative words not in context\n",
    "        j=0\n",
    "        while j<5: # 3 iterations, 1 positive sampe, 4 negative samples\n",
    "            if j == 0:\n",
    "                inpword = context[winsize] # middle word of current context\n",
    "            else:\n",
    "                inpword = noncword(winsize, context[winsize], contexts) # a word not in the current context (neg. sample)\n",
    "            # access vocabulary dictionary an get 1-hot encoded vector\n",
    "            inpvec = get1hot(myDict, myDic1hot, inpword)\n",
    "            if j == 0:\n",
    "                truth = inpvec\n",
    "            else:\n",
    "                truth = np.zeros(len(myDict))\n",
    "            xk = np.array(inpvec) # input vector for training\n",
    "            # for non-central context words run the training (word-vector entry for context word = 1)\n",
    "\n",
    "            ht = np.dot(xk, wi)\n",
    "            #u0 = np.dot(ht, wo)\n",
    "\n",
    "            #yk = np.exp(u0) / np.dot(np.exp(u0), np.exp(u0).transpose())\n",
    "\n",
    "            # backpropagation (following Xin Rong's paper here)      \n",
    "            #ej = np.zeros(V)\n",
    "            #for cword in context: # loop over current context, cword is the truth\n",
    "                # access vocabulary dictionary an get 1-hot encoded vector\n",
    "                #cwordvec = get1hot(myDict, myDic1hot, cword)\n",
    "                #ej += yk - cwordvec # error summed over context\n",
    "            etha = 0.025\n",
    "\n",
    "            # \"label\" of the word: tj=1 if wj in positive sample, t=0 otherwise\n",
    "            tj = truth # this is equal to truth tc\n",
    "\n",
    "            # Update equation for hidden→output weights\n",
    "            # todo here: only for wo and wneg -> need to restrict\n",
    "            #print np.multiply((sig(np.dot(np.transpose(wo), ht), False) - tj), ht[:, np.newaxis])\n",
    "            wo = wo - etha * np.multiply((sig(np.dot(np.transpose(wo), ht), False) - tj), ht[:, np.newaxis])\n",
    "            # Update equation for input→hidden weights\n",
    "            ehi = (sig(np.dot(np.transpose(wo), ht), False) - tj) * wo\n",
    "            wi = wi - etha * np.transpose(ehi)\n",
    "\n",
    "            j += 1\n",
    "\n",
    "    #print('wi after: {0}'.format(wi))\n",
    "    \n",
    "    # check results\n",
    "    #print myDict\n",
    "    #print('cwordvec for: {0} is: {1}'.format('erkennen',get1hot(myDict, myDic1hot, 'erkennen')))\n",
    "    cwordvec = get1hot(myDict, myDic1hot, 't2')\n",
    "    ht = np.dot(xk, wi)\n",
    "    u0 = np.dot(ht, wo)\n",
    "    yk = np.exp(u0) / np.dot(np.exp(u0), np.exp(u0).transpose())\n",
    "    i = 0\n",
    "    for elem in yk:\n",
    "        if elem > 0.:\n",
    "            print('yk for {0} is {1:.2f}'.format(wordlist[i],elem))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myresults for a is 1.00\n",
      "myresults for t0 is 0.00\n",
      "myresults for b is 0.00\n",
      "myresults for t1 is 0.00\n",
      "myresults for c is 0.00\n",
      "myresults for t2 is 0.00\n"
     ]
    }
   ],
   "source": [
    "# scikit learn weight updates\n",
    "# model structure\n",
    "# xk -> wi -> ht -> wo -> yk : tc\n",
    "import os\n",
    "import math\n",
    "import re # regex\n",
    "import numpy as np\n",
    "from toolz import itertoolz\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def read1k():\n",
    "    return f.read(1024)\n",
    "\n",
    "def process_data(chunk, text):\n",
    "    #print('processed: {0}'.format(ctr))\n",
    "    text.append(unicode(chunk, errors='ignore')) # 'utf8' codec can't decode byte 0xc3\n",
    "    \n",
    "def onehotvec(key, vec, dim): # returns a one-hot vector of dimension dim, all 0 but for 1 at vec\n",
    "    retvec = []\n",
    "    retvec.append(key)\n",
    "    for i in range(dim):\n",
    "        if i == vec:\n",
    "            retvec.append(1)\n",
    "        else:\n",
    "            retvec.append(0)\n",
    "    return retvec\n",
    "\n",
    "def onehotenc(dic): # one-hot encodes a dictionary\n",
    "    ohotret = []\n",
    "    dlen = len(dic)\n",
    "    i=0\n",
    "    for key in dic:\n",
    "        #ohotret.append(onehotvec(dic[key], dlen)) # does not work as dict keys are not guaranteed to be continuous\n",
    "        ohotret.append(onehotvec(dic[key], i, dlen))\n",
    "        i += 1\n",
    "    return ohotret\n",
    "\n",
    "def get1hot(dic, dichot, word): # get a 1-hot encoded vector for word from vocabulary dic and its 1-hot version dichot\n",
    "    keyval = dic[word]\n",
    "    for dicvec in dichot:\n",
    "        if dicvec[0] == keyval:\n",
    "            return dicvec[1:] # return all but the first entry as vector\n",
    "    return []\n",
    "\n",
    "def skipgram(corpus, window): # returns skip-grams for given window size from corpus (center word included)\n",
    "    if window > 5:\n",
    "        return []\n",
    "    cpanel = []\n",
    "    for i in range(window+1,len(corpus)-window+1):\n",
    "        skipg = []\n",
    "        for j in range(i-window-1,i+window):\n",
    "            skipg.append(corpus[j])\n",
    "        cpanel.append(skipg)\n",
    "    return cpanel\n",
    "\n",
    "def sig(x, ds): # sigmoid\n",
    "    if ds:\n",
    "        return sig(x, False) * (1.0 - sig(x, False))\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def relu(x, ds): # linear rectifier (approx. version)\n",
    "    if ds:\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    return np.log(1.0 + np.exp(x))\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # read corpus from file\n",
    "    os.chdir('C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "    f = open('vecsemtst0.txt') # test file with fixed similarities\n",
    "    #f = open('todinvenedigshrt.txt')\n",
    "    text = []\n",
    "    for piece in iter(read1k, ''):\n",
    "        process_data(piece, text)\n",
    "    \n",
    "    corpus = []\n",
    "    for elem in text:\n",
    "        corpus += elem.split() # splits on all whitespaces\n",
    "    corpus = [t.lower() for t in corpus] # convert to lower case\n",
    "    \n",
    "    # remove special characters\n",
    "    corpus = [re.sub(r'[^\\w]', ' ', t) for t in corpus]\n",
    "\n",
    "    # build dictionary\n",
    "    myDict = {}\n",
    "    i = 0\n",
    "    wordlist = []\n",
    "    for word in corpus:\n",
    "        # print word, i\n",
    "        newEntry = {word.lower(): i}\n",
    "        if not myDict.has_key(word):\n",
    "            myDict.update(newEntry)\n",
    "            wordlist.append(word)\n",
    "        i = i + 1\n",
    "\n",
    "    #print myDict\n",
    "\n",
    "    # one-hot encode the dictionary\n",
    "    #dictarr = np.asarray(myDict.values()).reshape(-1, 1)\n",
    "    #enc = OneHotEncoder()\n",
    "    #enc.fit(dictarr)\n",
    "    #enc.transform([[8]]).toarray()\n",
    "    myDic1hot = onehotenc(myDict)\n",
    "    \n",
    "    # build all possible contexts\n",
    "    winsize = 1\n",
    "    contexts = skipgram(corpus, winsize)\n",
    "    \n",
    "    V = len(myDict)\n",
    "    N = 5\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for context in contexts: # loop over context\n",
    "        inpword = context[winsize] # middle word of current context\n",
    "        # access vocabulary dictionary an get 1-hot encoded vector\n",
    "        inpvec = get1hot(myDict, myDic1hot, inpword)\n",
    "        \n",
    "        i = 0\n",
    "        for cword in context: # loop over current context, cword is the truth\n",
    "            # access vocabulary dictionary an get 1-hot encoded vector\n",
    "            if i != winsize: # input word not in context\n",
    "                cwordvec = get1hot(myDict, myDic1hot, cword)\n",
    "                X.append(cwordvec) # target\n",
    "                y.append(inpvec)\n",
    "            i += 1\n",
    "        \n",
    "    # train model\n",
    "    #print('shape X: {0} shape y: {1}'.format(len(X), len(y)))\n",
    "    clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(N,), random_state=1)\n",
    "    \n",
    "    #X = np.array(X).reshape((len(X), 1))\n",
    "    clf.fit(X, y)\n",
    "    # check results\n",
    "    cwordvec = get1hot(myDict, myDic1hot, 't0')\n",
    "    myresults = clf.predict([cwordvec])\n",
    "    i = 0\n",
    "    for elem in myresults[0]:\n",
    "        print('myresults for {0} is {1:.2f}'.format(wordlist[i], elem))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda2\\lib\\site-packages\\gensim-3.2.0-py2.7-win-amd64.egg\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2018-01-07 12:07:40,540 : INFO : collecting all words and their counts\n",
      "2018-01-07 12:07:40,543 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-01-07 12:07:40,545 : INFO : collected 6 word types from a corpus of 232 raw words and 1 sentences\n",
      "2018-01-07 12:07:40,546 : INFO : Loading a fresh vocabulary\n",
      "2018-01-07 12:07:40,549 : INFO : min_count=5 retains 6 unique words (100% of original 6, drops 0)\n",
      "2018-01-07 12:07:40,549 : INFO : min_count=5 leaves 232 word corpus (100% of original 232, drops 0)\n",
      "2018-01-07 12:07:40,552 : INFO : deleting the raw counts dictionary of 6 items\n",
      "2018-01-07 12:07:40,553 : INFO : sample=0.001 downsamples 6 most-common words\n",
      "2018-01-07 12:07:40,555 : INFO : downsampling leaves estimated 18 word corpus (8.0% of prior 232)\n",
      "2018-01-07 12:07:40,558 : INFO : estimated required memory for 6 words and 200 dimensions: 12600 bytes\n",
      "2018-01-07 12:07:40,559 : INFO : resetting layer weights\n",
      "2018-01-07 12:07:40,562 : INFO : training model with 3 workers on 6 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-01-07 12:07:40,566 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-07 12:07:40,569 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-07 12:07:40,569 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-07 12:07:40,572 : INFO : training on 1160 raw words (87 effective words) took 0.0s, 13581 effective words/s\n",
      "2018-01-07 12:07:40,572 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "D:\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "2018-01-07 12:07:40,575 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'b', 0.2965014576911926), (u'c', 0.25921857357025146), (u't2', 0.2152470052242279)]\n"
     ]
    }
   ],
   "source": [
    "# import modules and set up logging\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# load up unzipped corpus from http://mattmahoney.net/dc/text8.zip\n",
    "os.chdir('C:\\Users\\Bernie\\Documents\\ML4D\\lrgtxt0')\n",
    "sentences = word2vec.Text8Corpus('vecsemtst0.txt')\n",
    "# train the skip-gram model; default window=5\n",
    "model = word2vec.Word2Vec(sentences, size=200)\n",
    "# ... and some hours later... just as advertised...\n",
    "#print model.most_similar(positive=['a', 'b'], negative=['c'], topn=1)\n",
    " \n",
    "# pickle the entire model to disk, so we can load&resume training later\n",
    "#model.save('todvenshrt0.model')\n",
    "# store the learned weights, in a format the original C tool understands\n",
    "#model.save_word2vec_format('todvenshrt0.model.bin', binary=True)\n",
    "# or, import word weights created by the (faster) C word2vec\n",
    "# this way, you can switch between the C/Python toolkits easily\n",
    "#model = word2vec.Word2Vec.load_word2vec_format('todvenshrt0.bin', binary=True)\n",
    " \n",
    "# \"boy\" is to \"father\" as \"girl\" is to ...?\n",
    "#print model.most_similar(['a', 't0'], ['b'], topn=3)\n",
    "    \n",
    "# which word doesn't go with the others?\n",
    "#print model.doesnt_match(\"a b c\".split())\n",
    "\n",
    "print model.most_similar(['t1'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
