{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download & Preprocess the IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download reviews.txt and labels.txt from here: https://github.com/udacity/deep-learning/tree/master/sentiment-network\n",
    "\n",
    "def pretty_print_review_and_label(i):\n",
    "   print(labels[i] + \"\\t:\\t\" + reviews[i][:80] + \"...\")\n",
    "\n",
    "g = open('reviews.txt','r') # What we know!\n",
    "reviews = list(map(lambda x:x[:-1],g.readlines()))\n",
    "g.close()\n",
    "\n",
    "g = open('labels.txt','r') # What we WANT to know!\n",
    "labels = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
    "g.close()\n",
    "\n",
    "\n",
    "# Preprocess dataset:\n",
    "\n",
    "import sys\n",
    "\n",
    "f = open('reviews.txt')\n",
    "raw_reviews = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('labels.txt')\n",
    "raw_labels = f.readlines()\n",
    "f.close()\n",
    "\n",
    "tokens = list(map(lambda x:set(x.split(\" \")),raw_reviews))\n",
    "\n",
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        if(len(word)>0):\n",
    "            vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "\n",
    "input_dataset = list()\n",
    "for sent in tokens:\n",
    "    sent_indices = list()\n",
    "    for word in sent:\n",
    "        try:\n",
    "            sent_indices.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(list(set(sent_indices)))\n",
    "\n",
    "target_dataset = list()\n",
    "for label in raw_labels:\n",
    "    if label == 'positive\\n':\n",
    "        target_dataset.append(1)\n",
    "    else:\n",
    "        target_dataset.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:0 Progress:95.92% Training Accuracy:0.8164039696438996%%\n",
      "Iter:1 Progress:95.92% Training Accuracy:0.8575090658997124%\n",
      "Test Accuracy:0.852\n"
     ]
    }
   ],
   "source": [
    "# NN (original version)\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoidd(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(x):\n",
    "    return (x>0) * x\n",
    "\n",
    "def relud(x):\n",
    "    return x>0\n",
    "\n",
    "alpha, iterations = (0.01, 2)\n",
    "hidden_size = 10\n",
    "\n",
    "weights_0_1 = 0.2*np.random.random((len(vocab), hidden_size)) - 0.1 # vocab to hidden\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size, 1)) - 0.1 # hidden to 1\n",
    "\n",
    "correct, total = (0, 0)\n",
    "\n",
    "for iter in range(iterations):\n",
    "    \n",
    "    # train on first 24,000\n",
    "    for i in range(len(input_dataset) - 1000):\n",
    "\n",
    "        x, y = (input_dataset[i], target_dataset[i]) # i-th element of dataset\n",
    "        \n",
    "        layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0)) # embed + sigmoid: one-hot encoding: summation faster than product\n",
    "        layer_2 = sigmoid(np.dot(layer_1, weights_1_2)) # linear + softmax\n",
    "\n",
    "        layer_2_delta = layer_2 - y # compare pred with truth = error\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) # backprop\n",
    "        \n",
    "        # W_1_2-=alpha * L2delta * sigmoidd(L1 * W_1_2) * L1 -> alpha * L2delta * L1\n",
    "        # W_0_1-=alpha * L1delta * sigmoidd(L1 * W_1_2) * sigmoidd(W_0_1 * x) * x -> alpha * L1delta # x dropped since W_0_1[x]\n",
    "        weights_0_1[x] -= layer_1_delta * alpha # weight updates: sig' = approx 1/4 (Taylor exp. 1/2+x/4+-...)\n",
    "        weights_1_2 -= np.outer(layer_1, layer_2_delta) * alpha\n",
    "        \n",
    "        if(np.abs(layer_2_delta) < 0.5): # is it a close fit?\n",
    "            correct += 1 # increment correct guesses\n",
    "            \n",
    "        total += 1 # total guesses\n",
    "        \n",
    "        if(i % 20 == 1):\n",
    "            progress = str(i/float(len(input_dataset)))\n",
    "            sys.stdout.write('\\rIter:'+str(iter)\\\n",
    "                             +' Progress:'+progress[2:4]\\\n",
    "                             +'.'+progress[4:6]\\\n",
    "                             +'% Training Accuracy:'\\\n",
    "                             + str(correct/float(total)) + '%')\n",
    "    \n",
    "    print()\n",
    "    \n",
    "correct, total = (0, 0)\n",
    "\n",
    "for i in range(len(input_dataset)-1000, len(input_dataset)): # use the last 1000 as test\n",
    "\n",
    "    x = input_dataset[i]\n",
    "    y = target_dataset[i]\n",
    "\n",
    "    layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0))\n",
    "    layer_2 = sigmoid(np.dot(layer_1, weights_1_2))\n",
    "    \n",
    "    if(np.abs(layer_2 - y) < 0.5):\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(\"Test Accuracy:\" + str(correct / float(total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Surprising Power of Averaged Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11629306 -7.33107774 -3.28265491  2.96615504 -8.57442025  7.42941586\n",
      "  7.04248413 -3.85775863 -2.56417646  4.24056126]\n",
      "['the characters are unlikeable and the sc', 'this movie is so bad  it can only be com', 'this is without a doubt the worst movie ', 'i  ve seen about    movies released betw', 'this was one of the worst movies i have ']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "norms = np.sum(weights_0_1 * weights_0_1, axis=1)\n",
    "norms.resize(norms.shape[0],1)\n",
    "normed_weights = weights_0_1 * norms\n",
    "\n",
    "def make_sent_vect(words):\n",
    "    indices = list(map(lambda x:word2index[x], filter(lambda x:x in word2index, words)))\n",
    "    return np.mean(normed_weights[indices], axis=0)\n",
    "\n",
    "reviews2vectors = list()\n",
    "for review in tokens: # tokenized reviews\n",
    "    reviews2vectors.append(make_sent_vect(review))\n",
    "reviews2vectors = np.array(reviews2vectors)\n",
    "\n",
    "def most_similar_reviews(review, ntop=3):\n",
    "    v = make_sent_vect(review)\n",
    "    print(v)\n",
    "    scores = Counter()\n",
    "    for i, val in enumerate(reviews2vectors.dot(v)): # dot-product similarity\n",
    "        scores[i] = val\n",
    "        \n",
    "    most_similar = list()\n",
    "    for idx, score in scores.most_common(ntop):\n",
    "        most_similar.append(raw_reviews[idx][0:40])\n",
    "        plt.plot(reviews2vectors[idx]) # show most similar review word vectors\n",
    "    plt.plot(v, 'o')\n",
    "    return most_similar\n",
    "\n",
    "print(most_similar_reviews(['boring', 'awful'], 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices that Change Absolutely Nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1,2,3])\n",
    "b = np.array([0.1,0.2,0.3])\n",
    "c = np.array([-1,-0.5,0])\n",
    "d = np.array([0,0,0])\n",
    "\n",
    "identity = np.eye(3)\n",
    "print(identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3.]\n",
      "[0.1 0.2 0.3]\n",
      "[-1.  -0.5  0. ]\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(a.dot(identity))\n",
    "print(b.dot(identity))\n",
    "print(c.dot(identity))\n",
    "print(d.dot(identity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13 15 17]\n",
      "[13. 15. 17.]\n"
     ]
    }
   ],
   "source": [
    "this = np.array([2,4,6])\n",
    "movie = np.array([10,10,10])\n",
    "rocks = np.array([1,1,1])\n",
    "\n",
    "print(this + movie + rocks)\n",
    "print((this.dot(identity) + movie).dot(identity) + rocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x_):\n",
    "    x = np.atleast_2d(x_)\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
    "\n",
    "word_vects = {}\n",
    "word_vects['yankees'] = np.array([[0.,0.,0.]])\n",
    "word_vects['bears'] = np.array([[0.,0.,0.]])\n",
    "word_vects['braves'] = np.array([[0.,0.,0.]])\n",
    "word_vects['red'] = np.array([[0.,0.,0.]])\n",
    "word_vects['socks'] = np.array([[0.,0.,0.]])\n",
    "word_vects['lose'] = np.array([[0.,0.,0.]])\n",
    "word_vects['defeat'] = np.array([[0.,0.,0.]])\n",
    "word_vects['beat'] = np.array([[0.,0.,0.]])\n",
    "word_vects['tie'] = np.array([[0.,0.,0.]])\n",
    "\n",
    "sent2output = np.random.rand(3, len(word_vects))\n",
    "\n",
    "identity = np.eye(3) # eye: returns id-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]]\n",
      "[[0.91063368 0.08028843 0.94321154 0.63338327 0.9655085  0.7291186\n",
      "  0.73045693 0.4120407  0.71376408]\n",
      " [0.67646957 0.27824081 0.21080892 0.38702292 0.42557053 0.14631184\n",
      "  0.36863566 0.25976257 0.15845981]\n",
      " [0.88471383 0.21714672 0.47664916 0.48413108 0.82606822 0.80933\n",
      "  0.75727878 0.49472755 0.59946693]]\n",
      "prediction: [[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "  0.11111111 0.11111111 0.11111111]]\n"
     ]
    }
   ],
   "source": [
    "layer_0 = word_vects['red']\n",
    "layer_1 = layer_0.dot(identity) + word_vects['socks']\n",
    "layer_2 = layer_1.dot(identity) + word_vects['defeat']\n",
    "\n",
    "pred = softmax(layer_2.dot(sent2output)) # forward prop\n",
    "\n",
    "print(layer_2)\n",
    "print(sent2output)\n",
    "print('prediction:', pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we Backpropagate into this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37308671 0.98699617 0.85995811 0.22377186 0.39743257 0.14405549\n",
      "  0.19941871 0.30156922 0.88931055]\n",
      " [0.11577173 0.69162203 0.0193296  0.53357674 0.92731954 0.195463\n",
      "  0.22111529 0.38083827 0.62331081]\n",
      " [0.02037032 0.50525552 0.14917324 0.64613454 0.54549646 0.17000143\n",
      "  0.56655313 0.43763237 0.97359202]]\n",
      "{'yankees': array([[0., 0., 0.]]), 'bears': array([[0., 0., 0.]]), 'braves': array([[0., 0., 0.]]), 'red': array([[-0.00226188, -0.0059255 , -0.0085133 ]]), 'socks': array([[-0.00226185, -0.00592542, -0.00851318]]), 'lose': array([[0., 0., 0.]]), 'defeat': array([[-0.00226182, -0.00592534, -0.00851306]]), 'beat': array([[0., 0., 0.]]), 'tie': array([[0., 0., 0.]])}\n"
     ]
    }
   ],
   "source": [
    "y = np.array([1,0,0,0,0,0,0,0,0]) # target one-hot vector for \"yankees\"\n",
    "\n",
    "pred_delta = pred - y\n",
    "layer_2_delta = pred_delta.dot(sent2output.T)\n",
    "defeat_delta = layer_2_delta * 1 # can ignore the \"1\" like prev. chapter\n",
    "layer_1_delta = layer_2_delta.dot(identity.T)\n",
    "socks_delta = layer_1_delta * 1 # again... can ignore the \"1\"\n",
    "layer_0_delta = layer_1_delta.dot(identity.T)\n",
    "\n",
    "alpha = 0.01\n",
    "word_vects['red'] -= layer_0_delta * alpha\n",
    "word_vects['socks'] -= socks_delta * alpha\n",
    "word_vects['defeat'] -= defeat_delta * alpha\n",
    "\n",
    "identity -= np.outer(layer_0, layer_1_delta) * alpha # weight updates\n",
    "identity -= np.outer(layer_1, layer_2_delta) * alpha\n",
    "sent2output -= np.outer(layer_2, pred_delta) * alpha\n",
    "\n",
    "print(sent2output)\n",
    "print(word_vects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['john', 'travelled', 'to', 'the', 'hallway.'], ['mary', 'journeyed', 'to', 'the', 'bathroom.'], ['where', 'is', 'john?', '\\thallway\\t1']]\n"
     ]
    }
   ],
   "source": [
    "import sys,random,math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "f = open('qa1_single-supporting-fact_train.txt','r')\n",
    "raw = f.readlines()\n",
    "f.close()\n",
    "\n",
    "tokens = list()\n",
    "for line in raw[0:1000]:\n",
    "    tokens.append(line.lower().replace(\"\\n\",\"\").split(\" \")[1:])\n",
    "\n",
    "print(tokens[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "\n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "    \n",
    "def words2indices(sentence):\n",
    "    idx = list()\n",
    "    for word in sentence:\n",
    "        idx.append(word2index[word])\n",
    "    return idx\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "embed_size = 10\n",
    "\n",
    "# word embeddings\n",
    "embed = (np.random.rand(len(vocab),embed_size) - 0.5) * 0.1\n",
    "\n",
    "# embedding -> embedding (initially the identity matrix)\n",
    "recurrent = np.eye(embed_size)\n",
    "\n",
    "# sentence embedding for empty sentence\n",
    "start = np.zeros(embed_size)\n",
    "\n",
    "# embedding -> output weights\n",
    "decoder = (np.random.rand(embed_size, len(vocab)) - 0.5) * 0.1\n",
    "\n",
    "# one hot lookups (for loss function)\n",
    "one_hot = np.eye(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation with Arbitrary Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sent):\n",
    "    \n",
    "    layers = list()\n",
    "    layer = {}\n",
    "    layer['hidden'] = start\n",
    "    layers.append(layer)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # forward propagate\n",
    "    preds = list()\n",
    "    for target_i in range(len(sent)):\n",
    "\n",
    "        layer = {}\n",
    "\n",
    "        # try to predict the next term\n",
    "        layer['pred'] = softmax(layers[-1]['hidden'].dot(decoder))\n",
    "\n",
    "        loss += -np.log(layer['pred'][sent[target_i]])\n",
    "\n",
    "        # generate the next hidden state\n",
    "        layer['hidden'] = layers[-1]['hidden'].dot(recurrent) + embed[sent[target_i]]\n",
    "        layers.append(layer)\n",
    "\n",
    "    return layers, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation with Arbitrary Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward\n",
    "for iter in range(30000):\n",
    "    alpha = 0.001\n",
    "    sent = words2indices(tokens[iter%len(tokens)][1:])\n",
    "    layers,loss = predict(sent) \n",
    "\n",
    "    # back propagate\n",
    "    for layer_idx in reversed(range(len(layers))):\n",
    "        layer = layers[layer_idx]\n",
    "        target = sent[layer_idx-1]\n",
    "\n",
    "        if(layer_idx > 0):  # if not the first layer\n",
    "            layer['output_delta'] = layer['pred'] - one_hot[target]\n",
    "            new_hidden_delta = layer['output_delta'].dot(decoder.transpose())\n",
    "\n",
    "            # if the last layer - don't pull from a later one becasue it doesn't exist\n",
    "            if(layer_idx == len(layers)-1):\n",
    "                layer['hidden_delta'] = new_hidden_delta\n",
    "            else:\n",
    "                layer['hidden_delta'] = new_hidden_delta + layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())\n",
    "        else: # if the first layer\n",
    "            layer['hidden_delta'] = layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Update with Arbitrary Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:81.08812874281222\n",
      "Perplexity:80.94057641998825\n",
      "Perplexity:80.74658557063212\n",
      "Perplexity:80.44086234167231\n",
      "Perplexity:79.90436968255443\n",
      "Perplexity:78.88251420717953\n",
      "Perplexity:76.7231013651752\n",
      "Perplexity:71.14663366447378\n",
      "Perplexity:48.54011702437526\n",
      "Perplexity:28.975543950395423\n",
      "Perplexity:22.50366907200615\n",
      "Perplexity:21.16184633378187\n",
      "Perplexity:20.08541174396922\n",
      "Perplexity:18.618579250105174\n",
      "Perplexity:16.26816238265141\n",
      "Perplexity:12.941013686366436\n",
      "Perplexity:9.744360094074626\n",
      "Perplexity:8.197901438571844\n",
      "Perplexity:7.276779848939655\n",
      "Perplexity:6.662493567341839\n",
      "Perplexity:6.2415647292337955\n",
      "Perplexity:5.961263401371822\n",
      "Perplexity:5.7735848775236756\n",
      "Perplexity:5.63525478280964\n",
      "Perplexity:5.513981683391327\n",
      "Perplexity:5.395991693487238\n",
      "Perplexity:5.2809916169580235\n",
      "Perplexity:5.173001832978402\n",
      "Perplexity:5.077100079485485\n",
      "Perplexity:4.994391131105561\n"
     ]
    }
   ],
   "source": [
    "# forward\n",
    "for iter in range(30000):\n",
    "    alpha = 0.001\n",
    "    sent = words2indices(tokens[iter%len(tokens)][1:])\n",
    "\n",
    "    layers,loss = predict(sent) \n",
    "\n",
    "    # back propagate\n",
    "    for layer_idx in reversed(range(len(layers))):\n",
    "        layer = layers[layer_idx]\n",
    "        target = sent[layer_idx-1]\n",
    "\n",
    "        if(layer_idx > 0):\n",
    "            layer['output_delta'] = layer['pred'] - one_hot[target]\n",
    "            new_hidden_delta = layer['output_delta'].dot(decoder.transpose())\n",
    "\n",
    "            # if the last layer - don't pull from a \n",
    "            # later one becasue it doesn't exist\n",
    "            if(layer_idx == len(layers)-1):\n",
    "                layer['hidden_delta'] = new_hidden_delta\n",
    "            else:\n",
    "                layer['hidden_delta'] = new_hidden_delta + layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())\n",
    "        else:\n",
    "            layer['hidden_delta'] = layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())\n",
    "\n",
    "    # update weights\n",
    "    start -= layers[0]['hidden_delta'] * alpha / float(len(sent))\n",
    "    for layer_idx,layer in enumerate(layers[1:]):\n",
    "        \n",
    "        decoder -= np.outer(layers[layer_idx]['hidden'], layer['output_delta']) * alpha / float(len(sent))\n",
    "        \n",
    "        embed_idx = sent[layer_idx]\n",
    "        embed[embed_idx] -= layers[layer_idx]['hidden_delta'] * alpha / float(len(sent))\n",
    "        recurrent -= np.outer(layers[layer_idx]['hidden'], layer['hidden_delta']) * alpha / float(len(sent))\n",
    "        \n",
    "    if(iter % 1000 == 0):\n",
    "        print(\"Perplexity:\" + str(np.exp(loss/len(sent))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution and Output Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['john', 'moved', 'to', 'the', 'bedroom.']\n",
      "Prev Input:john        True:moved          Pred:is\n",
      "Prev Input:moved       True:to             Pred:to\n",
      "Prev Input:to          True:the            Pred:the\n",
      "Prev Input:the         True:bedroom.       Pred:office.\n"
     ]
    }
   ],
   "source": [
    "sent_index = 4\n",
    "\n",
    "l,_ = predict(words2indices(tokens[sent_index]))\n",
    "\n",
    "print(tokens[sent_index])\n",
    "\n",
    "for i,each_layer in enumerate(l[1:-1]):\n",
    "    input = tokens[sent_index][i]\n",
    "    true = tokens[sent_index][i+1]\n",
    "    pred = vocab[each_layer['pred'].argmax()]\n",
    "    print(\"Prev Input:\" + input + (' ' * (12 - len(input))) +\\\n",
    "          \"True:\" + true + (\" \" * (15 - len(true))) + \"Pred:\" + pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
