{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# squares\n",
    "import itertools\n",
    "\n",
    "def checksum(l):\n",
    "    if l[0]+l[1]+l[3]+l[4] != 20:\n",
    "        return False\n",
    "    if l[1]+l[2]+l[4]+l[5] != 20:\n",
    "        return False\n",
    "    if l[3]+l[4]+l[6]+l[7] != 20:\n",
    "        return False\n",
    "    if l[4]+l[5]+l[7]+l[8] != 20:\n",
    "        return False\n",
    "    if l[0]+l[2]+l[6]+l[8] != 20:\n",
    "        return False\n",
    "    if l[1]+l[3]+l[5]+l[7] != 20:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "if __name__=='__main__':\n",
    "    testl = list(itertools.permutations(range(1,10)))\n",
    "    for e in testl:\n",
    "        if checksum(e):\n",
    "            print e\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restaurant visits\n",
    "import random\n",
    "\n",
    "def qual(nvis,nrk,nprob):\n",
    "    res = []\n",
    "    hiq = 0\n",
    "    for ivis in range(nvis):\n",
    "        if ivis < nprob:\n",
    "            thisrk = random.randint(1,nrk)\n",
    "            res.append(thisrk)\n",
    "            if thisrk > hiq:\n",
    "                hiq = thisrk\n",
    "        else:\n",
    "            res.append(hiq)\n",
    "    return res\n",
    "            \n",
    "if __name__=='__main__':\n",
    "    random.seed()\n",
    "    totr = 0.\n",
    "    for n in range(1000):\n",
    "        totr += sum(qual(4,5,2))/4.\n",
    "    print totr / 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two dice\n",
    "import random\n",
    "\n",
    "def twod():\n",
    "    r0 = random.randint(1,6)\n",
    "    r1 = random.randint(1,6)\n",
    "    if (r0+r1) % 2 == 0:\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    random.seed()\n",
    "    even = 0.\n",
    "    for i in range(10000):\n",
    "        if twod():\n",
    "            even += 1.\n",
    "    print even/10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yahtzee\n",
    "import random\n",
    "import time\n",
    "import itertools\n",
    "import operator\n",
    "\n",
    "def roll(n): \n",
    "    return [random.randint(1,6) for i in range(n)]\n",
    "\n",
    "def most_common(L):\n",
    "  # get an iterable of (item, iterable) pairs\n",
    "  SL = sorted((x, i) for i, x in enumerate(L))\n",
    "  # print 'SL:', SL\n",
    "  groups = itertools.groupby(SL, key=operator.itemgetter(0))\n",
    "  # auxiliary function to get \"quality\" for an item\n",
    "  def _auxfun(g):\n",
    "    item, iterable = g\n",
    "    count = 0\n",
    "    min_index = len(L)\n",
    "    for _, where in iterable:\n",
    "      count += 1\n",
    "      min_index = min(min_index, where)\n",
    "    # print 'item %r, count %r, minind %r' % (item, count, min_index)\n",
    "    return count, -min_index\n",
    "  # pick the highest-count/earliest item\n",
    "  return max(groups, key=_auxfun)[0]\n",
    "\n",
    "if __name__=='__main__':\n",
    "    random.seed(time.time())\n",
    "    yz = 0\n",
    "    ntrials = 10000000\n",
    "    for i in range(ntrials):\n",
    "        lst = roll(5)\n",
    "        #print lst\n",
    "        mc0 = most_common(lst)\n",
    "        #print mc0\n",
    "        netlst = [x for x in lst if x != mc0]\n",
    "        #print netlst\n",
    "        if len(netlst)==0:\n",
    "            yz += 1\n",
    "            continue\n",
    "        lst = roll(len(netlst))\n",
    "        #print lst\n",
    "        mc1 = most_common(lst)\n",
    "        netlst = [x for x in lst if x != mc0]\n",
    "        #print netlst\n",
    "        if len(netlst)==0:\n",
    "            yz += 1\n",
    "            continue\n",
    "        lst = roll(len(netlst))\n",
    "        #print lst\n",
    "        netlst = [x for x in lst if x != mc0]\n",
    "        #print netlst\n",
    "        if len(netlst)==0:\n",
    "            #print 'Yahtzee!'\n",
    "            yz += 1\n",
    "    print yz\n",
    "    print float(yz)/float(ntrials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def levenshtein(seq1, seq2):  \n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros ((size_x, size_y))\n",
    "    for x in xrange(size_x):\n",
    "        matrix [x, 0] = x\n",
    "    for y in xrange(size_y):\n",
    "        matrix [0, y] = y\n",
    "\n",
    "    for x in xrange(1, size_x):\n",
    "        for y in xrange(1, size_y):\n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1, y] + 1,\n",
    "                    matrix[x-1, y-1],\n",
    "                    matrix[x, y-1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1,y] + 1,\n",
    "                    matrix[x-1,y-1] + 1,\n",
    "                    matrix[x,y-1] + 1\n",
    "                )\n",
    "    print (matrix)\n",
    "    return (matrix[size_x - 1, size_y - 1])\n",
    "\n",
    "if __name__=='__main__':\n",
    "    print levenshtein('test', 'toast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# students and friends\n",
    "import itertools\n",
    "\n",
    "def factorial(n):\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n-1)\n",
    "    \n",
    "def elimfrnds(lst):\n",
    "    ectr = -1\n",
    "    ret = True\n",
    "    for elem in lst:\n",
    "        ectr += 1\n",
    "        if ectr == 0:\n",
    "            if elem+1 == lst[ectr+1]:\n",
    "                ret = False\n",
    "                break\n",
    "        if ectr == len(lst)-1:\n",
    "            if elem-1 == lst[ectr-1]:\n",
    "                ret = False\n",
    "                break\n",
    "        if ectr > 0 and ectr < len(lst)-1:\n",
    "            if elem+1 == lst[ectr+1] or elem-1 == lst[ectr-1]:\n",
    "                ret = False\n",
    "                break\n",
    "            if elem+1 == lst[ectr-1] or elem-1 == lst[ectr+1]:\n",
    "                ret = False\n",
    "                break\n",
    "    return ret\n",
    "\n",
    "if __name__=='__main__':\n",
    "    n = 10\n",
    "    perml = list(itertools.permutations(range(n)))\n",
    "    ctr = 0\n",
    "    for elem in perml:\n",
    "        if elimfrnds(elem):\n",
    "            ctr += 1\n",
    "    #print float(ctr) / float(factorial(n))\n",
    "    print ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, [2, 0, 1, 1, 1, 0, 1, 2, 2]]\n"
     ]
    }
   ],
   "source": [
    "# tic-tac-toe minimax\n",
    "\n",
    "def chkwin(bl,p): # check for win for board list\n",
    "    for i in range(0,6,3):\n",
    "        if bl[i]==p and bl[i+1]==p and bl[i+2]==p:\n",
    "            return True\n",
    "    for i in range(3):\n",
    "        if bl[i]==p and bl[i+3]==p and bl[i+6]==p:\n",
    "            return True\n",
    "    if bl[0]==p and bl[4]==p and bl[8]==p:\n",
    "            return True\n",
    "    if bl[2]==p and bl[4]==p and bl[6]==p:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def otherp(p):\n",
    "    if p==1:\n",
    "        return 2\n",
    "    return 1\n",
    "\n",
    "def mvopt(bl): # returns movement options for current board state\n",
    "    res = []\n",
    "    for i in range(len(bl)):\n",
    "        if bl[i]==0:\n",
    "            res.append(i)\n",
    "    return res\n",
    "\n",
    "def plyrmv(bl,i,p): # next player move\n",
    "    return [x if ind != i else p for ind, x in enumerate(bl)]\n",
    "\n",
    "def successors(bl,p): # return all successor board states\n",
    "    mo = mvopt(bl)\n",
    "    succs = []\n",
    "    for o in mo:\n",
    "        succs.append([0,plyrmv(bl,o,p)]) # [value=0 (default utility), state]\n",
    "    return succs\n",
    "\n",
    "def util(bl,p): # return utility of current board for player p\n",
    "    if chkwin(bl,p):\n",
    "        return 1\n",
    "    elif chkwin(bl,otherp(p)):\n",
    "        return -1\n",
    "    return 0\n",
    "\n",
    "def maxval(bl,p): # determine the max-value for player p\n",
    "    tt = util(bl,p)\n",
    "    if tt != 0:\n",
    "        return [tt,bl]\n",
    "    v = [-1000,bl]\n",
    "    succs = successors(bl,p)\n",
    "    for s in succs:\n",
    "        miv = minval(s[1],p)\n",
    "        if miv[0] > v[0]:\n",
    "            v = miv\n",
    "    return v\n",
    "\n",
    "def minval(bl,p): # determine the min-value for player p\n",
    "    tt = util(bl,p)\n",
    "    if tt != 0:\n",
    "        return [tt,bl]\n",
    "    v = [1000,bl]\n",
    "    succs = successors(bl,otherp(p))\n",
    "    for s in succs:\n",
    "        mav = maxval(s[1],p)\n",
    "        if mav[0] < v[0]:\n",
    "            v = mav\n",
    "    return v\n",
    "\n",
    "def minimaxdec(bl,p): # minimax decision player p\n",
    "    v = maxval(bl,p)\n",
    "    return v\n",
    "\n",
    "if __name__=='__main__':\n",
    "    brd = [2,0,1,1,0,0,1,2,2]\n",
    "    print minimaxdec(brd,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, [2, 0, 1, 1, 2, 0, 1, 2, 2, 1, 2, 1, 2, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# tic-tac-toe 4x4 minimax\n",
    "\n",
    "def chkwin(bl,p): # check for win for board list\n",
    "    for i in range(0,12,4):\n",
    "        if bl[i]==p and bl[i+1]==p and bl[i+2]==p and bl[i+3]==p:\n",
    "            return True\n",
    "    for i in range(4):\n",
    "        if bl[i]==p and bl[i+4]==p and bl[i+8]==p and bl[i+12]==p:\n",
    "            return True\n",
    "    if bl[0]==p and bl[5]==p and bl[10]==p and bl[15]==p:\n",
    "            return True\n",
    "    if bl[3]==p and bl[6]==p and bl[9]==p and bl[12]==p:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def otherp(p):\n",
    "    if p==1:\n",
    "        return 2\n",
    "    return 1\n",
    "\n",
    "def mvopt(bl): # returns movement options for current board state\n",
    "    res = []\n",
    "    for i in range(len(bl)):\n",
    "        if bl[i]==0:\n",
    "            res.append(i)\n",
    "    return res\n",
    "\n",
    "def plyrmv(bl,i,p): # next player move\n",
    "    return [x if ind != i else p for ind, x in enumerate(bl)]\n",
    "\n",
    "def successors(bl,p): # return all successor board states\n",
    "    mo = mvopt(bl)\n",
    "    succs = []\n",
    "    for o in mo:\n",
    "        succs.append([0,plyrmv(bl,o,p)]) # [value=0 (default utility), state]\n",
    "    return succs\n",
    "\n",
    "def util(bl,p): # return utility of current board for player p\n",
    "    if chkwin(bl,p):\n",
    "        return 1\n",
    "    elif chkwin(bl,otherp(p)):\n",
    "        return -1\n",
    "    return 0\n",
    "\n",
    "def maxval(bl,p): # determine the max-value for player p\n",
    "    tt = util(bl,p)\n",
    "    if tt != 0:\n",
    "        return [tt,bl]\n",
    "    v = [-1000,bl]\n",
    "    succs = successors(bl,p)\n",
    "    for s in succs:\n",
    "        miv = minval(s[1],p)\n",
    "        if miv[0] > v[0]:\n",
    "            v = miv\n",
    "    return v\n",
    "\n",
    "def minval(bl,p): # determine the min-value for player p\n",
    "    tt = util(bl,p)\n",
    "    if tt != 0:\n",
    "        return [tt,bl]\n",
    "    v = [1000,bl]\n",
    "    succs = successors(bl,otherp(p))\n",
    "    for s in succs:\n",
    "        mav = maxval(s[1],p)\n",
    "        if mav[0] < v[0]:\n",
    "            v = mav\n",
    "    return v\n",
    "\n",
    "def minimaxdec(bl,p): # minimax decision player p\n",
    "    v = maxval(bl,p)\n",
    "    return v\n",
    "\n",
    "if __name__=='__main__':\n",
    "    brd = [2,0,1,1,0,0,1,2,2,1,2,1,2,0,0,0]\n",
    "    print minimaxdec(brd,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex tests\n",
    "import re\n",
    "\n",
    "dailybugle ='Spider-Man Menaces City!'\n",
    "pattern = r'spider[- ]?man.' # r'' raw string syntax, [...] any char listed or in range, ? match 0 or 1 time, . any char\n",
    "\n",
    "if re.match(pattern, dailybugle, re.IGNORECASE):\n",
    "    print dailybugle\n",
    "\n",
    "date = '12/30/1969'\n",
    "regex = re.compile(r'^(\\d\\d)[-/](\\d\\d)[-/](\\d\\d(?:\\d\\d)?)$') # ^ start of string, (...) group subpatterns, \\d digit char\n",
    "# (?:...) non-capturing group, $ end of search string\n",
    "\n",
    "mmatch = regex.match(date)\n",
    "#print mmatch.string\n",
    "if mmatch:\n",
    "    month = mmatch.group(1)\n",
    "    day = mmatch.group(2)\n",
    "    year = mmatch.group(3)\n",
    "    print mmatch.groups([0])\n",
    "    print '%s %s %s' % (month, day, year)\n",
    "\n",
    "date = \"aaa_bbb\"\n",
    "regex = re.compile(r\"(?:aaa)(_bbb)\")\n",
    "mmatch = regex.match(date)\n",
    "if mmatch:\n",
    "    print mmatch.groups([0])\n",
    "#print re.findall(regex, date)\n",
    "date = \"111_222\"\n",
    "regex = re.compile(r\"(?:111)(_222)\")\n",
    "mmatch = regex.match(date)\n",
    "if mmatch:\n",
    "    print mmatch.groups([0])\n",
    "date = \"111_222\"\n",
    "regex = re.compile(r\"(?:111)(_\\d\\d\\d)\")\n",
    "mmatch = regex.match(date)\n",
    "if mmatch:\n",
    "    print mmatch.groups([0])\n",
    "date = \"111_222\"\n",
    "regex = re.compile(r\"(_\\d\\d\\d)*\")\n",
    "mmatch = regex.match(date)\n",
    "if mmatch:\n",
    "    print mmatch.groups([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gated Recurrent Unit (GRU) test (Cho 2014):\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    np.putmask(x, x > 100, 100) # limit array values of x (np.putmask(a, a >= m, m - 1))\n",
    "    np.putmask(x, x < -100, -100)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def dtanh(x): # actually sech**2\n",
    "    return 1.0 - np.tanh(x)**2\n",
    "\n",
    "def forget_layer(combine):\n",
    "    return sigmoid(np.dot(wr, np.transpose(combine)))\n",
    "    \n",
    "def update_layer(combine):\n",
    "    return sigmoid(np.dot(wz, np.transpose(combine)))\n",
    "    \n",
    "def output_layer(htempt):\n",
    "    return np.tanh(np.dot(w, htempt))\n",
    "    \n",
    "def GRU(prev_ht, xt): # a GRU forward propagation\n",
    "    combine = np.add(prev_ht,xt)\n",
    "    rt = forget_layer(combine)\n",
    "    zt = update_layer(combine)\n",
    "    htempt = np.add(np.multiply(rt, np.transpose(prev_ht)), np.transpose(xt))\n",
    "    htempt = output_layer(htempt)\n",
    "    ht = np.add((np.multiply(np.subtract(np.array([[1],[1],[1],[1]]), zt), np.transpose(prev_ht))), np.multiply(zt, htempt))\n",
    "    return ht\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    prev_ht = np.array([[1,1,0,0]])\n",
    "    xt = np.array([[0,0,1,1]])\n",
    "    \n",
    "    wr = np.random.rand(4,4) - np.random.rand(4,4)\n",
    "    wz = np.random.rand(4,4) - np.random.rand(4,4) # bias trick: last line - todo: add bias back!\n",
    "    w = np.random.rand(4,4) - np.random.rand(4,4)\n",
    "\n",
    "    np.set_printoptions(precision=3)\n",
    "    \n",
    "    print GRU(prev_ht, xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gated Recurrent Unit (GRU) test (Cho 2014) with backprop:\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    np.putmask(x, x > 100, 100) # limit array values of x (np.putmask(a, a >= m, m - 1))\n",
    "    np.putmask(x, x < -100, -100)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def dtanh(x): # actually sech**2\n",
    "    return 1.0 - np.tanh(x)**2\n",
    "\n",
    "def forget_layer(combine):\n",
    "    return sigmoid(np.dot(wr, np.transpose(combine)))\n",
    "    \n",
    "def update_layer(combine):\n",
    "    return sigmoid(np.dot(wz, np.transpose(combine)))\n",
    "    \n",
    "def output_layer(htempt):\n",
    "    return np.tanh(np.dot(w, htempt))\n",
    "    \n",
    "def GRU(prev_ht, xt): # a GRU forward propagation\n",
    "    combine = np.add(prev_ht,xt)\n",
    "    rt = forget_layer(combine)\n",
    "    zt = update_layer(combine)\n",
    "    htempt = np.add(np.multiply(rt, np.transpose(prev_ht)), np.transpose(xt))\n",
    "    htempt = output_layer(htempt)\n",
    "    ht = np.add((np.multiply(np.subtract(np.array([[1],[1],[1],[1]]), zt), np.transpose(prev_ht))), np.multiply(zt, htempt))\n",
    "    return ht\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) # only difference\n",
    "\n",
    "def forward(x, y, Uz, Ur, Uc, Wz, Wr, Wc, bz, br, bc, V, bV, s0):\n",
    "    vocabularysize =len(x)\n",
    "    sentencesize = len(x)\n",
    "    iMemsize = len(V)\n",
    "    s = np.zeros( (iMemsize, sentencesize) ) # np.zeros((nrows, ncols))\n",
    "    yhat = np.zeros( (vocabularysize, sentencesize) )\n",
    "    L = np.zeros( (sentencesize, 1) )\n",
    "    z = np.zeros( (iMemsize, sentencesize) )\n",
    "    r = np.zeros( (iMemsize, sentencesize) )\n",
    "    c = np.zeros( (iMemsize, sentencesize) )\n",
    "    \n",
    "    # calculate result for step 1 since s 0 is not in s\n",
    "    #tst0 = Wz.dot(np.transpose(s0))\n",
    "    z = sigmoid( Uz.dot(x) + Wz.dot(np.transpose(s0)) + bz ) # np.multiply() equivalent to *\n",
    "    r = sigmoid( Ur.dot(x) + Wr.dot(np.transpose(s0)) + br )\n",
    "    c = np.tanh( Uc.dot(x) + Wc.dot(s0*r ) + bc )\n",
    "    s = np.subtract(np.ones(len(z)),z)*c + z*s0 # .∗ = element-wise mult\n",
    "    yhat = softmax( V.dot(s) + bV )\n",
    "    L = sum(np.negative(y)*np.log( yhat ) )\n",
    "    \n",
    "    res = []\n",
    "    res.append(s)\n",
    "    res.append(yhat)\n",
    "    res.append(L)\n",
    "    res.append(z)\n",
    "    res.append(r)\n",
    "    res.append(c) # c==h\n",
    "    \n",
    "    #for wordI in range(2, sentencesize):\n",
    "        #z[:, wordI] = sigmoid( Uz.dot(x[:,wordI]) + Wz.dot(s[:,wordI−1]) + bz )\n",
    "        #r[:, wordI] = sigmoid( Ur.dot(x[:,wordI]) + Wr.dot(s[:,wordI−1]) + br )\n",
    "        #c[:, wordI] = tanh( Uc.dot(x[:,wordI]) + Wc.dot(s[:,wordI−1]*r[:,wordI] ) + bc )\n",
    "        #s[:, wordI] = np.subtract(np.ones(len(z)),z[:,wordI] )*c[:,wordI] + z[:,wordI]*s[:,wordI−1]\n",
    "        #yhat[:, wordI] = softmax( V.dot(s[:,wordI]) + bV )\n",
    "        #L(wordI) = sum(np.negative(y[:,wordI])*np.log( yhat[:,wordI] ) )\n",
    "    return res\n",
    "\n",
    "def backward_direct(x, y, Uz, Ur, Uc, Wz, Wr, Wc, bz, br, bc, V, bV, s0):\n",
    "    \n",
    "    fwddata = forward (x, y, Uz, Ur, Uc, Wz, Wr, Wc, bz, br, bc, V, bV, s0) # forward propagate\n",
    "    # S-internal state, Yhat-predictions, L-losses, Z,R,C-intermediate results (C=H=ht)\n",
    "    s=fwddata[0]\n",
    "    yhat=fwddata[1]\n",
    "    L=fwddata[2]\n",
    "    z=fwddata[3]\n",
    "    r=fwddata[4]\n",
    "    c=fwddata[5] # actually h\n",
    "    \n",
    "    sentencesize = len(x)\n",
    "    # calculate gradient using chain rule\n",
    "    deltay = np.subtract(yhat, y) # output y_hat - truth y\n",
    "    dbV = np.sum( deltay, 1 )\n",
    "    dV = np.zeros( len(V) )\n",
    "    for wordI in range(1, sentencesize):\n",
    "        dV = dV + np.dot(deltay[:, wordI], s[:, wordI]) # calculate dL/dS=VT(yhat-y)\n",
    "    \n",
    "    ds0 = np.zeros( len(s0) ) # initialize weight increments with 0s \n",
    "    dUc = np.zeros( len(Uc) )\n",
    "    dUr = np.zeros( len(Ur) )\n",
    "    dUz = np.zeros( len(Uz) )\n",
    "    dWc = np.zeros( len(Wc) )\n",
    "    dWr = np.zeros( len(Wr) )\n",
    "    dWz = np.zeros( len(Wz) )\n",
    "    dbz = np.zeros( len(bz) )\n",
    "    dbr = np.zeros( len(br) )\n",
    "    dbc = np.zeros( len(bc) ) # c==h\n",
    "    dssingle = np.dot(np.transpose(V), deltay) # ' = transpose; V*(yhat-y) : single state change from difference truth-predctn\n",
    "    \n",
    "    dscur = np.zeros((len(dssingle),1)) # np.zeros((2, 1)) - array([[ 0.], [ 0.]]) : current state change\n",
    "    \n",
    "    for wordJ in range(sentencesize-1, 1, -1): # index t : counting down from nw=sentencesize\n",
    "        wordJm1 = wordJ-1\n",
    "        \n",
    "        # A(:,3) is the 3rd column of A -> numpy: test[:,0] cols, test[1,:] rows\n",
    "        dscur = dscur + dssingle[:, wordJ] # current state change ds\n",
    "        dscurbk = dscur # backup of current state change\n",
    "\n",
    "        # dtanhInput = ds*(1-zt)*(1-ht)*ht (should be: ds*(1-zt)*(1-ht)*(1+ht) - derivative of tanh is 1-tanh**2 )\n",
    "        dtanhInput = dscur*(np.subtract(np.ones((len(z),1)),z))*(np.subtract(np.ones((len(z),1)),c[:, wordJ]*c[:, wordJ] ))\n",
    "        dbc = dbc + dtanhInput\n",
    "        dUc = dUc + dtanhInput.dot(x) # ds*(1-zt)*(1-ht)*ht*xt\n",
    "        \n",
    "        dWc = dWc + dtanhInput.dot(s*r) # ds*(1-zt)*(1-ht)*ht*st-1*rt\n",
    "        dsr = np.dot(np.transpose(Wc), dtanhInput) # ' = transpose\n",
    "        dscur = dsr*r\n",
    "        dsigInputr = dsr*s*r*(np.subtract(np.ones((len(r),1)),r)) # dsr*(st-1)*rt*(1-rt)\n",
    "        dbr = dbr + dsigInputr\n",
    "        dUr = dUr + dsigInputr.dot(x) # dsr*(st-1)*rt*(1-rt)*xt\n",
    "\n",
    "        dWr = dWr + np.dot(dsigInputr, s) # dsr*(st-1)*rt*(1-rt)*(st-1)\n",
    "        dscur = dscur + np.dot(np.transpose(Wr), dsigInputr) # Wr*dsr*(st-1)*rt*(1-rt)\n",
    "        dscur = dscur + dscurbk*z\n",
    "        dz = dscurbk*( np.subtract(s, c) ) # ds*(st-1 - ht)\n",
    "        dsigInputz = dz*z*(np.subtract(np.ones((len(z),1)),z)) # dz*zt*(1-zt)\n",
    "        dbz = dbz + dsigInputz\n",
    "        dUz = dUz + dsigInputz.dot(x) # dz*zt*(1-zt)*xt\n",
    "\n",
    "        dWz = dWz + dsigInputz.dot(s)\n",
    "        dscur = dscur + np.dot(np.transpose(Wz), dsigInputz)\n",
    "    \n",
    "    dscur = dscur + dssingle[:,1]\n",
    "    dtanhInput = ( dscur*(np.subtract(np.ones((len(z),1)), z))*(np.subtract(np.ones((len(c),1)), c[:,1]*c[:,1] ) ))\n",
    "    dbc = dbc + dtanhInput\n",
    "    dUc = dUc + dtanhInput.dot(x)\n",
    "    \n",
    "    dWc = dWc + np.dot(dtanhInput, ( s0*r ))\n",
    "    dsr = np.dot(np.transpose(Wc), dtanhInput)\n",
    "    ds0 = ds0 + dsr*r\n",
    "    dsigInputr = dsr*s0*r*(np.subtract(np.ones((len(r),1)),r))\n",
    "    dbr = dbr + dsigInputr\n",
    "    dUr = dUr + dsigInputr.dot(x)\n",
    "    \n",
    "    dWr = dWr + s0.dot(dsigInputr)\n",
    "    ds0 = ds0 + np.dot(np.transpose(Wr), dsigInputr)\n",
    "    ds0 = ds0 + dscur*z\n",
    "    dz = dscur*( np.subtract(s0, c) )\n",
    "    dsigInputz = dz*z*(np.subtract(np.ones((len(z),1)),z))\n",
    "    dbz = dbz + dsigInputz\n",
    "    dUz = dUz + dsigInputz.dot(x)\n",
    "    \n",
    "    dWz = dWz + s0.dot(dsigInputr)\n",
    "    ds0 = ds0 + np.dot(np.transpose(Wz), dsigInputz)\n",
    "    return ds0\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    # dimensions\n",
    "    D = 4 # input dimensions\n",
    "    H = 2 # hidden dimensions\n",
    "    N = 4 # minibatch size\n",
    "    \n",
    "    prev_ht = np.transpose(np.array([[1,1,0,0]]))\n",
    "    xt = np.transpose(np.array([[0,0,1,1]]))\n",
    "    yt = np.transpose(np.array([[0,0,0,1]]))\n",
    "    \n",
    "    ur = np.random.rand(4,4) - np.random.rand(4,4)\n",
    "    uz = np.random.rand(4,4) - np.random.rand(4,4)\n",
    "    uc = np.random.rand(4,4) - np.random.rand(4,4)\n",
    "    \n",
    "    wr = np.random.rand(4,4) - np.random.rand(4,4)\n",
    "    wz = np.random.rand(4,4) - np.random.rand(4,4) \n",
    "    wc = np.random.rand(4,4) - np.random.rand(4,4)\n",
    "    \n",
    "    br = np.random.rand(4,1) - np.random.rand(4,1)\n",
    "    bz = np.random.rand(4,1) - np.random.rand(4,1)\n",
    "    bc = np.random.rand(4,1) - np.random.rand(4,1)\n",
    "    \n",
    "    V = np.random.rand(4,4) - np.random.rand(4,4)\n",
    "    bv = np.random.rand(4,1) - np.random.rand(4,1)\n",
    "    \n",
    "    s0 = np.array([[0,0,0,0]])\n",
    "\n",
    "    np.set_printoptions(precision=3)\n",
    "    \n",
    "    #print forward(xt, yt, uz, ur, uc, wz, wr, wc, bz, br, bc, V, bv, s0)\n",
    "    print backward_direct(xt, yt, uz, ur, uc, wz, wr, wc, bz, br, bc, V, bv, s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gated Recurrent Unit (GRU) test (Cho 2014) with backprop:\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    np.putmask(x, x > 100, 100) # limit array values of x (np.putmask(a, a >= m, m - 1))\n",
    "    np.putmask(x, x < -100, -100)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def dtanh(x): # actually sech**2\n",
    "    return 1.0 - np.tanh(x)**2\n",
    "\n",
    "def forget_layer(combine):\n",
    "    return sigmoid(np.dot(wr, np.transpose(combine)))\n",
    "    \n",
    "def update_layer(combine):\n",
    "    return sigmoid(np.dot(wz, np.transpose(combine)))\n",
    "    \n",
    "def output_layer(htempt):\n",
    "    return np.tanh(np.dot(w, htempt))\n",
    "    \n",
    "def GRU(prev_ht, xt): # a GRU forward propagation\n",
    "    combine = np.add(prev_ht,xt)\n",
    "    rt = forget_layer(combine)\n",
    "    zt = update_layer(combine)\n",
    "    htempt = np.add(np.multiply(rt, np.transpose(prev_ht)), np.transpose(xt))\n",
    "    htempt = output_layer(htempt)\n",
    "    ht = np.add((np.multiply(np.subtract(np.array([[1],[1],[1],[1]]), zt), np.transpose(prev_ht))), np.multiply(zt, htempt))\n",
    "    return ht\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) # only difference\n",
    "\n",
    "def forward(x, y, Uz, Ur, Uc, Wz, Wr, Wc, bz, br, bc, V, bV, s0):\n",
    "    vocabularysize = np.shape(x)[0]\n",
    "    sentencesize = np.shape(x)[0]\n",
    "    iMemsize = np.shape(V)[0]\n",
    "    s = np.zeros( (iMemsize, sentencesize) ) # np.zeros((nrows, ncols))\n",
    "    yhat = np.zeros( (vocabularysize, sentencesize) )\n",
    "    L = np.zeros( (sentencesize, 1) )\n",
    "    z = np.zeros( (iMemsize, sentencesize) )\n",
    "    r = np.zeros( (iMemsize, sentencesize) )\n",
    "    c = np.zeros( (iMemsize, sentencesize) )\n",
    "    \n",
    "    # calculate result for step 1 since s 0 is not in s\n",
    "    #tst0 = Wz.dot(np.transpose(s0))\n",
    "    z = sigmoid( Uz.dot(x) + Wz.dot(s0) + bz ) # np.multiply() equivalent to *\n",
    "    r = sigmoid( Ur.dot(x) + Wr.dot(s0) + br )\n",
    "    c = np.tanh( Uc.dot(x) + Wc.dot(s0*r) + bc )\n",
    "    print np.ones(np.shape(z)[0],np.shape(z)[1])\n",
    "    s = np.subtract(np.ones(np.shape(z)[0],np.shape(z)[1]),z)*c + z*s0 # .∗ = element-wise mult\n",
    "    yhat = softmax( V.dot(s) + bV )\n",
    "    L = sum(np.negative(y)*np.log( yhat ) )\n",
    "    \n",
    "    res = []\n",
    "    res.append(s)\n",
    "    res.append(yhat)\n",
    "    res.append(L)\n",
    "    res.append(z)\n",
    "    res.append(r)\n",
    "    res.append(c) # c==h\n",
    "    \n",
    "    for wordI in range(1, sentencesize):\n",
    "        wordIm1 = wordI - 1\n",
    "        # A(:,3) is the 3rd column of A -> numpy: test[:,0] cols, test[1,:] rows\n",
    "        z[:, wordI] = sigmoid( Uz.dot(x[:, wordI]) + Wz.dot(s[:, wordIm1]) + bz )\n",
    "        r[:, wordI] = sigmoid( Ur.dot(x[:, wordI]) + Wr.dot(s[:, wordIm1]) + br )\n",
    "        c[:, wordI] = tanh( Uc.dot(x[:, wordI]) + Wc.dot(s[:, wordIm1]*r[:, wordI] ) + bc )\n",
    "        s[:, wordI] = np.subtract(np.ones(len(z)),z[:, wordI] )*c[:, wordI] + z[:, wordI]*s[:, wordIm1]\n",
    "        yhat[:, wordI] = softmax( V.dot(s[:,wordI]) + bV )\n",
    "        L[wordI] = sum(np.negative(y[:, wordI])*np.log( yhat[:, wordI] ) )\n",
    "    return res\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    # dimensions\n",
    "    D = 4 # input dimensions\n",
    "    H = 2 # hidden dimensions\n",
    "    N = 4 # minibatch size\n",
    "    \n",
    "    prev_ht = np.transpose(np.array([[1,1,0,0]]))\n",
    "    xt = np.transpose(np.array([[0,0,0,1],[0,0,1,0],[1,0,0,0]]))\n",
    "    yt = np.transpose(np.array([[0,0,0,1],[0,0,0,1],[1,0,0,0]]))\n",
    "    \n",
    "    ur = np.random.rand(4,4) - np.random.rand(4,4)\n",
    "    uz = np.random.rand(4,4) - np.random.rand(4,4)\n",
    "    uc = np.random.rand(4,4) - np.random.rand(4,4)\n",
    "    \n",
    "    wr = np.random.rand(4,4) - np.random.rand(4,4)\n",
    "    wz = np.random.rand(4,4) - np.random.rand(4,4) \n",
    "    wc = np.random.rand(4,4) - np.random.rand(4,4)\n",
    "    \n",
    "    br = np.random.rand(4,1) - np.random.rand(4,1)\n",
    "    bz = np.random.rand(4,1) - np.random.rand(4,1)\n",
    "    bc = np.random.rand(4,1) - np.random.rand(4,1)\n",
    "    \n",
    "    V = np.random.rand(4,4) - np.random.rand(4,4)\n",
    "    bv = np.random.rand(4,1) - np.random.rand(4,1)\n",
    "    \n",
    "    s0 = np.transpose(np.array([[0,0,0,0],[0,0,0,0],[0,0,0,0]]))\n",
    "\n",
    "    np.set_printoptions(precision=3)\n",
    "    \n",
    "    print forward(xt, yt, uz, ur, uc, wz, wr, wc, bz, br, bc, V, bv, s0)\n",
    "    #print backward_direct(xt, yt, uz, ur, uc, wz, wr, wc, bz, br, bc, V, bv, s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU with Minimal Gated Units\n",
    "import numpy as np\n",
    "\n",
    "# Seed random\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read data and setup maps for integer encoding and decoding.\n",
    "data = open('input.txt', 'r').read()\n",
    "chars = sorted(list(set(data))) # Sort makes model predictable (if seeded).\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) } # dict from enumerate: char and index\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) } # dict from enumerate: index and char\n",
    "\n",
    "# Activation functions\n",
    "# NOTE: Derivatives are calculated using outcomes of their primitives (which are already calculated during forward prop).\n",
    "def sigmoid(input, deriv=False):\n",
    "    if deriv:\n",
    "        return input*(1-input) # sig' = sig(1-sig)\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def tanh(input, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - input ** 2 # tanh'=1-tanh**2\n",
    "    else:\n",
    "        return np.tanh(input)\n",
    "\n",
    "# Derivative is directly calculated in backprop (in combination with cross-entropy loss function).\n",
    "def softmax(input):\n",
    "    # Subtraction of max value improves numerical stability.\n",
    "    e_input = np.exp(input - np.max(input))\n",
    "    return e_input / e_input.sum()\n",
    "\n",
    "# Hyper parameters\n",
    "N, h_size, o_size = vocab_size, vocab_size, vocab_size # Hidden size is set to vocab_size, assuming that level of abstractness is approximately proportional to vocab_size (but can be set to any other value).\n",
    "seq_length = 15 # Longer sequence lengths allow for lengthier latent dependencies to be trained.\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# Model parameter initialization\n",
    "Wz = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Uz = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "bz = np.zeros((h_size, 1))\n",
    "\n",
    "Wr = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Ur = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "br = np.zeros((h_size, 1))\n",
    "\n",
    "Wh = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "Uh = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "bh = np.zeros((h_size, 1))\n",
    "\n",
    "Wy = np.random.rand(o_size, h_size) * 0.1 - 0.05\n",
    "by = np.zeros((o_size, 1))\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    # Initialize variables\n",
    "    x, z, r, h_hat, h, y, p = {}, {}, {}, {}, {-1: hprev}, {}, {} # Dictionaries contain variables for each timestep.\n",
    "    sequence_loss = 0\n",
    "\n",
    "    # Forward prop\n",
    "    for t in range(len(inputs)):\n",
    "        # Set up one-hot encoded input\n",
    "        x[t] = np.zeros((vocab_size, 1)) # for each time step t a one-hot over vocabulary size - init with all 0\n",
    "        x[t][inputs[t]] = 1 # set t'th input to one (current word in one-hot)\n",
    "        \n",
    "        # Calculate update and reset gates\n",
    "        z[t] = sigmoid(np.dot(Wz, x[t]) + np.dot(Uz, h[t-1]) + bz) # update gate\n",
    "        r[t] = sigmoid(np.dot(Wr, x[t]) + np.dot(Ur, h[t-1]) + br) # reset gate\n",
    "        \n",
    "        # Calculate hidden units\n",
    "        h_hat[t] = tanh(np.dot(Wh, x[t]) + np.dot(Uh, np.multiply(r[t], h[t-1])) + bh)\n",
    "        h[t] = np.multiply(z[t], h[t-1]) + np.multiply((1 - z[t]), h_hat[t]) # sometimes denoted s[t]\n",
    "        \n",
    "        # Regular output unit\n",
    "        y[t] = np.dot(Wy, h[t]) + by\n",
    "        \n",
    "        # Probability distribution\n",
    "        p[t] = softmax(y[t])\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        # targets[t]'s entry of p[t]: since the output is 0 or 1 only one entry contributes (and yj*log(pj) becomes just log(pj))\n",
    "        loss = -np.sum(np.log(p[t][targets[t]])) # dict p: or time t (=one-hot position), whats the corresponsing target value?\n",
    "        sequence_loss += loss\n",
    "\n",
    "    # Parameter gradient initialization\n",
    "    dWy, dWh, dWr, dWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n",
    "    dUh, dUr, dUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n",
    "    dby, dbh, dbr, dbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n",
    "    dhnext = np.zeros_like(h[0])\n",
    "    \n",
    "    # Backward prop\n",
    "    for t in reversed(range(len(inputs))): # index t counting down\n",
    "        # ∂loss/∂y\n",
    "        dy = np.copy(p[t]) # copy output\n",
    "        dy[targets[t]] -= 1 # the current target (truth) is 1 for the current t (an 0 for all other t's)\n",
    "        \n",
    "        # ∂loss/∂Wy and ∂loss/∂by\n",
    "        dWy += np.dot(dy, h[t].T) # weight updates: Wy -> Wy - etha * d loss / dWy -> dWy += etha * d loss / dWy\n",
    "        dby += dy # weight for bias is just 1\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        dh = np.dot(Wy.T, dy) + dhnext\n",
    "        dh_hat = np.multiply(dh, (1 - z[t]))\n",
    "        dh_hat_l = dh_hat * tanh(h_hat[t], deriv=True) # = Wy*dy*(1-zt) * (1-tanh**2(h_hat))\n",
    "        \n",
    "        # ∂loss/∂Wh, ∂loss/∂Uh and ∂loss/∂bh\n",
    "        dWh += np.dot(dh_hat_l, x[t].T) # Wy*dy*(1-zt) * (1-tanh**2(h_hat)) * xt\n",
    "        dUh += np.dot(dh_hat_l, np.multiply(r[t], h[t-1]).T) # Wy*dy*(1-zt) * (1-tanh**2(h_hat)) * (rt*ht-1)\n",
    "        dbh += dh_hat_l # weight for bias is just 1\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        drhp = np.dot(Uh.T, dh_hat_l) # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat))\n",
    "        dr = np.multiply(drhp, h[t-1]) # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat)) * ht-1\n",
    "        dr_l = dr * sigmoid(r[t], deriv=True) # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat))*ht-1*sig(rt)*(1-sig(rt))\n",
    "        \n",
    "        # ∂loss/∂Wr, ∂loss/∂Ur and ∂loss/∂br\n",
    "        dWr += np.dot(dr_l, x[t].T) # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat))*sig(rt)*(1-sig(rt))*xt\n",
    "        dUr += np.dot(dr_l, h[t-1].T) # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat))*sig(rt)*(1-sig(rt))*ht-1\n",
    "        dbr += dr_l # # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat))*sig(rt)*(1-sig(rt))\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        dz = np.multiply(dh, h[t-1] - h_hat[t]) # dh * (ht-1 - h_hatt)\n",
    "        dz_l = dz * sigmoid(z[t], deriv=True) # dz * sig(zt)*(1-sig(zt))\n",
    "        \n",
    "        # ∂loss/∂Wz, ∂loss/∂Uz and ∂loss/∂bz\n",
    "        dWz += np.dot(dz_l, x[t].T) # dz * sig(zt)*(1-sig(zt))*xt\n",
    "        dUz += np.dot(dz_l, h[t-1].T) # # dz * sig(zt)*(1-sig(zt))*ht-1\n",
    "        dbz += dz_l # # dz * sig(zt)*(1-sig(zt))\n",
    "        \n",
    "        # All influences of previous layer to loss\n",
    "        dh_fz_inner = np.dot(Uz.T, dz_l) # Uz * dz * sig(zt)*(1-sig(zt))\n",
    "        dh_fz = np.multiply(dh, z[t]) # dh * zt\n",
    "        dh_fhh = np.multiply(drhp, r[t]) # Uh * Wy*dy*(1-zt) * (1-tanh**2(Wy*dy*(1-zt))) * rt\n",
    "        dh_fr = np.dot(Ur.T, dr_l) # Ur * Uh*Wy*dy*(1-zt)*ht-1*sig(rt)*(1-sig(rt))\n",
    "        \n",
    "        # ∂loss/∂h𝑡₋₁\n",
    "        dhnext = dh_fz_inner + dh_fz + dh_fhh + dh_fr # \n",
    "\n",
    "    return sequence_loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, h[len(inputs) - 1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    # Initialize first word of sample ('seed') as one-hot encoded vector.\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = [seed_ix]\n",
    "    \n",
    "    for t in range(n):\n",
    "        # Calculate update and reset gates\n",
    "        z = sigmoid(np.dot(Wz, x) + np.dot(Uz, h) + bz)\n",
    "        r = sigmoid(np.dot(Wr, x) + np.dot(Ur, h) + br)\n",
    "        \n",
    "        # Calculate hidden units\n",
    "        h_hat = tanh(np.dot(Wh, x) + np.dot(Uh, np.multiply(r, h)) + bh)\n",
    "        h = np.multiply(z, h) + np.multiply((1 - z), h_hat)\n",
    "        \n",
    "        # Regular output unit\n",
    "        y = np.dot(Wy, h) + by\n",
    "        \n",
    "        # Probability distribution\n",
    "        p = softmax(y)\n",
    "\n",
    "        # Choose next char according to the distribution\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel()) # ravel returns flattened array, P are the probabilities for choice\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    \n",
    "    return ixes\n",
    "\n",
    "# Initialize sampling parameters and memory gradients (for adagrad)\n",
    "n, p = 0, 0\n",
    "mdWy, mdWh, mdWr, mdWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n",
    "mdUh, mdUr, mdUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n",
    "mdby, mdbh, mdbr, mdbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length\n",
    "\n",
    "print_interval = 1000\n",
    "n_runs = 1\n",
    "\n",
    "while True:\n",
    "    # Reset memory if appropriate\n",
    "    if p + seq_length + 1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((h_size, 1))\n",
    "        p = 0 # current position\n",
    "    \n",
    "    # Get input and target sequence - each an index list of characters\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]] # current position to sequence length\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]] # current position+1 to sequence length+1 (next state)\n",
    "\n",
    "    # Occasionally sample from model and print result\n",
    "    if n % print_interval == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 1000)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n%s\\n----' % (txt, ))\n",
    "\n",
    "    # Get gradients for current model based on input and target sequences\n",
    "    loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    # Occasionally print loss information\n",
    "    if n % print_interval == 0:\n",
    "        print('iter %d, loss: %f, smooth loss: %f' % (n, loss, smooth_loss))\n",
    "\n",
    "    # Update model with adagrad (stochastic) gradient descent\n",
    "    for param, dparam, mem in zip([Wy,  Wh,  Wr,  Wz,  Uh,  Ur,  Uz,  by,  bh,  br,  bz],\n",
    "                                  [dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz],\n",
    "                                  [mdWy,mdWh,mdWr,mdWz,mdUh,mdUr,mdUz,mdby,mdbh,mdbr,mdbz]):\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # Small added term for numerical stability\n",
    "\n",
    "    # Prepare for next iteration\n",
    "    p += seq_length\n",
    "    n += 1\n",
    "    \n",
    "    # check exit\n",
    "    if n>n_runs:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# longer test output with trained weights\n",
    "sample_ix = sample(hprev, inputs[0], 5000)\n",
    "txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "print txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some GRU tests\n",
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "# NOTE: Derivatives are calculated using outcomes of their primitives (which are already calculated during forward prop).\n",
    "def sigmoid(input, deriv=False):\n",
    "    if deriv:\n",
    "        return input*(1-input) # sig' = sig(1-sig)\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def tanh(input, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - input ** 2 # tanh'=1-tanh**2\n",
    "    else:\n",
    "        return np.tanh(input)\n",
    "\n",
    "# Derivative is directly calculated in backprop (in combination with cross-entropy loss function).\n",
    "def softmax(input):\n",
    "    # Subtraction of max value improves numerical stability.\n",
    "    e_input = np.exp(input - np.max(input))\n",
    "    return e_input / e_input.sum()\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    # Initialize variables\n",
    "    x, z, r, h_hat, h, y, p = {}, {}, {}, {}, {-1: hprev}, {}, {} # Dictionaries contain variables for each timestep.\n",
    "    sequence_loss = 0\n",
    "\n",
    "    # Forward prop\n",
    "    for t in range(len(inputs)):\n",
    "        # Set up one-hot encoded input\n",
    "        x[t] = np.zeros((vocab_size, 1)) # for each time step t a one-hot over vocabulary size - init with all 0\n",
    "        x[t][inputs[t]] = 1 # set t'th input to one (current word in one-hot)\n",
    "        \n",
    "        # Calculate update and reset gates\n",
    "        z[t] = sigmoid(np.dot(Wz, x[t]) + np.dot(Uz, h[t-1]) + bz) # update gate\n",
    "        r[t] = sigmoid(np.dot(Wr, x[t]) + np.dot(Ur, h[t-1]) + br) # reset gate\n",
    "        \n",
    "        # Calculate hidden units\n",
    "        h_hat[t] = tanh(np.dot(Wh, x[t]) + np.dot(Uh, np.multiply(r[t], h[t-1])) + bh)\n",
    "        h[t] = np.multiply(z[t], h[t-1]) + np.multiply((1 - z[t]), h_hat[t]) # sometimes denoted s[t]\n",
    "        \n",
    "        # Regular output unit\n",
    "        y[t] = np.dot(Wy, h[t]) + by\n",
    "        \n",
    "        # Probability distribution\n",
    "        p[t] = softmax(y[t])\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        loss = -np.sum(np.log(p[t][targets[t]]))\n",
    "        sequence_loss += loss\n",
    "    return p\n",
    "\n",
    "def tstloss(inputs, targets, vsize):\n",
    "    p = {}\n",
    "    y = {}\n",
    "    h = {}\n",
    "    Wy = np.random.rand(vsize, vsize) * 0.1 - 0.05\n",
    "    h = np.zeros((vsize, 1))\n",
    "    h[3] = 1\n",
    "    by = np.zeros((vsize, 1))\n",
    "    for t in range(len(inputs)): \n",
    "        y[t] = np.dot(Wy, h) + by\n",
    "        p[t] = softmax(y[t])\n",
    "        loss = -np.sum(np.log(p[t][targets[t]])) # targets[t]'s entry of p[t]: since the output is 0 or 1 only one entry contributes\n",
    "    return loss\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    seq_length = 5\n",
    "    data = 'A king there was in days of old'\n",
    "    chars = sorted(list(set(data))) # sorted list of characters in text\n",
    "    data_size, vocab_size = len(data), len(chars)\n",
    "    char_to_ix = { ch:i for i,ch in enumerate(chars) } # dict from enumerate: char and index\n",
    "    ix_to_char = { i:ch for i,ch in enumerate(chars) } # dict from enumerate: index and char\n",
    "    p=0\n",
    "    # Get input and target sequence - each an index list of characters\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]] # current position to sequence length\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]] # current position+1 to sequence length+1 (next state)\n",
    "    print tstloss(inputs, targets, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import ode\n",
    "\n",
    "y0, t0 = [1.0j, 2.0], 0\n",
    "\n",
    "def f(t, y, arg1):\n",
    "    return [1j*arg1*y[0] + y[1], -arg1*y[1]**2]\n",
    "\n",
    "def jac(t, y, arg1):\n",
    "    return [[1j*arg1, 1], [0, -arg1*2*y[1]]]\n",
    "\n",
    "rr = ode(f, jac).set_integrator('zvode', method='bdf', with_jacobian=True)\n",
    "rr.set_initial_value(y0, t0).set_f_params(2.0).set_jac_params(2.0)\n",
    "t1 = 10\n",
    "dt = 1\n",
    "while rr.successful() and rr.t < t1:\n",
    "    rr.integrate(rr.t+dt)\n",
    "    print rr.t\n",
    "    print rr.y\n",
    "    #print \"%g %g\" % (rr.t, rr.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy . integrate\n",
    "import autograd .numpy as np\n",
    "from autograd . extend import primitive , defvjp_argnums\n",
    "from autograd import make_vjp\n",
    "from autograd . misc import flatten\n",
    "from autograd . builtins import tuple\n",
    "\n",
    "odeint = primitive ( scipy . integrate . odeint )\n",
    "\n",
    "def grad_odeint_all (yt , func , y0 , t , func_args , ∗∗kwargs ): # Extended from \" Scalable Inference of Ordinary Differential\n",
    "    # Equation Models of Biochemical Processes \" , Sec . 2.4.2\n",
    "    # Fabian Froehlich , Carolin Loos , Jan Hasenauer , 2017\n",
    "    # https :/ / arxiv . org / pdf /1711.08079. pdf\n",
    "    T, D = np . shape ( yt )\n",
    "    flat_args , unflatten = flatten ( func_args )\n",
    "    def flat_func (y, t , flat_args ):\n",
    "        return func (y , t , ∗unflatten ( flat_args ))\n",
    "    \n",
    "    def unpack(x ): # y , vjp_y , vjp_t , vjp_args\n",
    "        return x[0:D] , x[D:2 ∗ D] , x[2 ∗ D] , x[2 ∗ D + 1:]\n",
    "    \n",
    "    def augmented_dynamics ( augmented_state , t , flat_args ):\n",
    "    # Orginal system augmented with vjp_y , vjp_t and vjp_args\n",
    "        y , vjp_y , _ , _ = unpack( augmented_state )\n",
    "        vjp_all , dy_dt = make_vjp( flat_func , argnum=(0 , 1, 2))(y , t , flat_args )\n",
    "        vjp_y , vjp_t , vjp_args = vjp_all(−vjp_y )\n",
    "        return np . hstack (( dy_dt , vjp_y , vjp_t , vjp_args ))\n",
    "    \n",
    "    def vjp_all (g,∗∗kwargs ):\n",
    "        vjp_y = g[−1, :]\n",
    "        vjp_t0 = 0\n",
    "        time_vjp_list = []\n",
    "        vjp_args = np. zeros (np . size ( flat_args ))\n",
    "        \n",
    "        for i in range (T − 1, 0, −1):\n",
    "            # Compute effect of moving current time\n",
    "            vjp_cur_t = np . dot ( func ( yt [i , :] , t [ i ] , ∗func_args ) , g[ i , :])\n",
    "            time_vjp_list . append( vjp_cur_t )\n",
    "            vjp_t0 = vjp_t0 − vjp_cur_t\n",
    "            \n",
    "            # Run augmented system backwards to the previous observation\n",
    "            aug_y0 = np . hstack (( yt [ i , :] , vjp_y , vjp_t0 , vjp_args ))\n",
    "            aug_ans = odeint (augmented_dynamics , aug_y0 , np . array ([ t [ i ] , t [ i − 1]]) , tuple (( flat_args ,)) , ∗∗kwargs)\n",
    "            _ , vjp_y , vjp_t0 , vjp_args = unpack( aug_ans [1])\n",
    "            \n",
    "            # Add gradient from current output\n",
    "            vjp_y = vjp_y + g[ i − 1, :]\n",
    "            \n",
    "        time_vjp_list . append( vjp_t0 )\n",
    "        vjp_times = np . hstack ( time_vjp_list )[:: −1]\n",
    "        \n",
    "        return None , vjp_y , vjp_times , unflatten ( vjp_args )\n",
    "    return vjp_all\n",
    "\n",
    "def grad_argnums_wrapper ( all_vjp_builder ):\n",
    "    # A generic autograd helper function . Takes a function that\n",
    "    # builds vjps for all arguments , and wraps it to return only required vjps\n",
    "    def build_selected_vjps (argnums , ans , combined_args , kwargs ):\n",
    "        vjp_func = all_vjp_builder (ans , ∗combined_args , ∗∗kwargs)\n",
    "        def chosen_vjps (g ):\n",
    "            # Return whichever vjps were asked for\n",
    "            all_vjps = vjp_func (g)\n",
    "            return [ all_vjps [argnum] for argnum in argnums]\n",
    "        return chosen_vjps\n",
    "    return build_selected_vjps\n",
    "\n",
    "defvjp_argnums ( odeint , grad_argnums_wrapper ( grad_odeint_all ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
