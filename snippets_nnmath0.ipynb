{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23e6b1a8-93f4-41d0-ae9c-d2af6932d607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# NN maths simple example\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ninputs = 64\n",
    "noutputs = 2\n",
    "nhidden0 = 8\n",
    "\n",
    "ainputs = np.random.random((ninputs,))\n",
    "bhidden0 = np.array((nhidden0,))\n",
    "coutputs = np.array((noutputs,))\n",
    "\n",
    "winphidden0 = np.zeros((ninputs, nhidden0))\n",
    "whidden0out = np.zeros((nhidden0, noutputs))\n",
    "\n",
    "winphidden0idt = np.identity(nhidden0, like=winphidden0)\n",
    "whidden0outidt = np.identity(nhidden0, like=whidden0out)\n",
    "\n",
    "winphidden0idt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d7ec51f-3065-43fb-9342-0609f19068df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "print('.', end='\\r')\n",
    "\n",
    "time.sleep(1.5)\n",
    "\n",
    "print('+', end='\\r')\n",
    "print()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "412c440c-fe2e-4c57-b500-e57af6613e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number:  648024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "guessthis = [str(random.randint(0, 9)) for i in range(6)]\n",
    "\n",
    "time.sleep(0.8)\n",
    "for el in guessthis:\n",
    "    print(el, end='\\r')\n",
    "    time.sleep(1.2)\n",
    "    print(' ', end='\\r')\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "print()\n",
    "\n",
    "guessednumber = input(\"Enter the number: \")\n",
    "\n",
    "if guessednumber == ''.join(guessthis):\n",
    "    print('correct')\n",
    "else:\n",
    "    print('incorrect', ''.join(guessthis))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f34552e-4ca2-4afa-8818-563ede04711e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Add 1 to the 3. number.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number:  038134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "lenguess = 6\n",
    "\n",
    "guessthis = [str(random.randint(0, 9)) for i in range(lenguess)]\n",
    "\n",
    "time.sleep(0.8)\n",
    "for el in guessthis:\n",
    "    print(el, end='\\r')\n",
    "    time.sleep(1.2)\n",
    "    print(' ', end='\\r')\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "print()\n",
    "\n",
    "modifier = random.randint(1, 9)\n",
    "idxmodifier = random.randint(0, lenguess-1)\n",
    "\n",
    "print(f\"Add {modifier} to the {idxmodifier+1}. number.\")\n",
    "guessthis[idxmodifier] = str(int(guessthis[idxmodifier]) + modifier)\n",
    "\n",
    "guessednumber = input(\"Enter the number: \")\n",
    "\n",
    "if guessednumber == ''.join(guessthis):\n",
    "    print('correct')\n",
    "else:\n",
    "    print('incorrect', ''.join(guessthis))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f130f2-a6b7-41f6-93b2-b81d7643fa28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 1\n",
    "eval('x + 1')\n",
    "\n",
    "#eval('x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd5022c6-7e75-4ad9-b327-63499a6432d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.choice([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6e73f28-9440-400c-af40-c8c1ca5d04fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99929937]\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp, array, random, dot\n",
    "\n",
    "training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_set_outputs = array([[0, 1, 1, 0]]).T\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "synaptic_weights = 2 * random.random((3, 1)) - 1\n",
    "\n",
    "for iteration in range(1000):\n",
    "    output = 1 / (1 + exp(-(dot(training_set_inputs, synaptic_weights))))\n",
    "    synaptic_weights += dot(training_set_inputs.T, (training_set_outputs - output) * output * (1 - output))  # quadratic loss\n",
    "    \n",
    "print(1 / (1 + exp(-(dot(array([1, 0, 0]), synaptic_weights)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "cb2e1b67-7dfd-4019-9d3c-bf27ee3b1324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98404014]\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp, array, random, dot\n",
    "\n",
    "training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_set_outputs = array([[0, 1, 1, 0]]).T\n",
    "\n",
    "#random.seed(1)\n",
    "\n",
    "synaptic_weights = 2 * random.random((3, 1)) - 1\n",
    "\n",
    "for iteration in range(10):\n",
    "    output = 1 / (1 + exp(-(dot(training_set_inputs, synaptic_weights))))\n",
    "    synaptic_weights += dot(training_set_inputs.T, (training_set_outputs - output))  # cross entropy loss\n",
    "    \n",
    "print(1 / (1 + exp(-(dot(array([1, 0, 0]), synaptic_weights)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5cf63774-0c9d-4407-a088-136230c7fdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16077543]\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp, array, random, dot\n",
    "\n",
    "training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_set_outputs = array([[0, 1, 1, 0]]).T\n",
    "\n",
    "t_input = array([0, 0, 1])\n",
    "t_output = array([0])\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "synaptic_weights = 2 * random.random((3, 1)) - 1\n",
    "\n",
    "for iteration in range(100):\n",
    "    output = 1 / (1 + exp(-(dot(t_input, synaptic_weights))))\n",
    "    synaptic_weights += dot(t_input.T, array([1, 1, 1]) * (t_output - output) * output * (1 - output))\n",
    "    \n",
    "print(1 / (1 + exp(-(dot(array([1, 0, 0]), synaptic_weights)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "db741fb3-4324-4c9b-bcdb-966d9b676122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02232899]\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp, array, random, dot\n",
    "\n",
    "training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_set_outputs = array([[0, 1, 1, 0]]).T\n",
    "\n",
    "t_input = array([0, 0, 1])\n",
    "t_output = array([0])\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "synaptic_weights = 2 * random.random((3, 1)) - 1\n",
    "\n",
    "for iteration in range(100):\n",
    "    output = 1 / (1 + exp(-(dot(t_input, synaptic_weights))))\n",
    "    synaptic_weights += dot(t_input.T, array([1, 1, 1]) * (t_output - output))  # cross entropy loss\n",
    "    \n",
    "print(1 / (1 + exp(-(dot(array([1, 0, 0]), synaptic_weights)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a01829-3f5a-49a2-aee1-dde073248a51",
   "metadata": {},
   "source": [
    "$$c = \\sqrt{a^2 + b^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "5ce749cf-1e3b-4d2f-a0f5-3b37d7b1d6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0 0 1] [0] [0.05248225]\n",
      "1 [1 1 1] [1] [0.94024712]\n",
      "2 [1 0 1] [1] [0.95228751]\n",
      "3 [0 1 1] [0] [0.04184173]\n",
      "[0.99723251]\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp, array, random, dot, max\n",
    "\n",
    "training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_set_outputs = array([[0, 1, 1, 0]]).T\n",
    "\n",
    "etha = 1.\n",
    "synaptic_weights = 2 * random.random((3, 1)) - 1  # 3 weights + bias\n",
    "\n",
    "for iteration in range(100):\n",
    "    tmpdot = 0\n",
    "    i = random.randint(4)\n",
    "    t_input = array([training_set_inputs[i]])\n",
    "    t_output = training_set_outputs[i]\n",
    "    tmpdot += (dot(t_input, synaptic_weights))\n",
    "    output = 1 / (1 + exp(-tmpdot))\n",
    "    synaptic_weights += etha * t_input.T * (t_output - output)  # cross entropy loss\n",
    "    \n",
    "for i in range(4):\n",
    "    print(i, training_set_inputs[i], training_set_outputs[i], 1 / (1 + exp(-(dot(training_set_inputs[i], synaptic_weights)))))\n",
    "\n",
    "print(1 / (1 + exp(-(dot(array([1, 0, 0]), synaptic_weights)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ab1df5-b396-40f5-9a7c-a411d5472269",
   "metadata": {},
   "source": [
    "**Core Concepts Illustrated**\n",
    "\n",
    "The code demonstrates the three fundamental steps of the attention mechanism:\n",
    "\n",
    "**Similarity Scores (Dot Product)**: The np.dot(keys, query) step calculates how relevant each Key is to the Query. A larger dot product means greater similarity.\n",
    "\n",
    "**Attention Weights (Softmax)**: The Softmax function $\\left(\\frac{e^{score_i}}{\\sum_j e^{score_j}}\\right)$ is applied to the raw scores. This step normalizes the scores into a probability distribution, ensuring all weights are between 0 and 1 and sum up to 1. These weights indicate how much to pay attention to each value.\n",
    "\n",
    "**Context Vector (Weighted Sum)**: The final output is computed by multiplying each Value by its corresponding Weight and summing the results ($\\sum_{i} W_i \\cdot V_i$). The resulting Context Vector is a representation that captures the most relevant information from the Values, steered by the Query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "a7f905b9-4e61-46d9-b543-4cc6b4fb2558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Simple Attention Example ---\n",
      "Query: [1.  0.5 0.1 0. ]\n",
      "-----------------------------------\n",
      "Keys (K):\n",
      "[[1.1 0.4 0.  0.1]\n",
      " [0.2 0.8 0.9 0.7]\n",
      " [0.9 0.6 0.1 0. ]]\n",
      "Values (V):\n",
      "[[5. 5. 5. 5.]\n",
      " [1. 1. 1. 1.]\n",
      " [3. 3. 3. 3.]]\n",
      "-----------------------------------\n",
      "Attention Weights (W):\n",
      "[0.40695369 0.22111864 0.37192767]\n",
      "-----------------------------------\n",
      "Context Vector (Output):\n",
      "[3.3716701 3.3716701 3.3716701 3.3716701]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simple_attention(query, keys, values):\n",
    "    \"\"\"\n",
    "    A simplified, illustrative example of the attention mechanism.\n",
    "\n",
    "    Args:\n",
    "        query (np.ndarray): The vector representing what we're looking for.\n",
    "        keys (np.ndarray): A set of vectors to compare the query against.\n",
    "        values (np.ndarray): The values associated with the keys.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The context vector (weighted sum of values).\n",
    "    \"\"\"\n",
    "    # 1. Calculate Similarity Scores (e.g., Dot Product)\n",
    "    # The dot product measures how related the query is to each key.\n",
    "    # Scores shape will be (num_keys,)\n",
    "    scores = np.dot(keys, query)\n",
    "\n",
    "    # 2. Normalize Scores to get Attention Weights (using Softmax)\n",
    "    # Softmax converts scores into probabilities (weights) that sum to 1.\n",
    "    # The higher the score, the higher the weight.\n",
    "    # Weights shape will be (num_keys,)\n",
    "    # np.exp(scores - np.max(scores)) is used for numerical stability\n",
    "    exp_scores = np.exp(scores - np.max(scores))\n",
    "    weights = exp_scores / np.sum(exp_scores)\n",
    "\n",
    "    # 3. Compute Context Vector (Weighted Sum of Values)\n",
    "    # The final output (context vector) is the sum of (weight * value).\n",
    "    # This vector represents the \"attended\" information.\n",
    "    # values shape is (num_keys, value_dim), weights shape is (num_keys,)\n",
    "    # This results in (value_dim,)\n",
    "    context_vector = np.dot(weights, values)\n",
    "\n",
    "    return context_vector, weights\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# Define the input data:\n",
    "# Here, the dimension for Q, K, V is 4. We have 3 key-value pairs.\n",
    "# Imagine Q is a word, and K/V are surrounding words.\n",
    "\n",
    "# The Query (Q): What we are looking for (e.g., current word's embedding)\n",
    "query = np.array([1.0, 0.5, 0.1, 0.0]) # Shape (4,)\n",
    "\n",
    "# The Keys (K): What we compare against (e.g., other words' embeddings)\n",
    "keys = np.array([\n",
    "    [1.1, 0.4, 0.0, 0.1],  # Key 1: Very similar to Q\n",
    "    [0.2, 0.8, 0.9, 0.7],  # Key 2: Less similar\n",
    "    [0.9, 0.6, 0.1, 0.0]   # Key 3: Similar\n",
    "]) # Shape (3, 4)\n",
    "\n",
    "# The Values (V): The actual information to be aggregated\n",
    "# (In self-attention, K and V are often the same, but they don't have to be)\n",
    "values = np.array([\n",
    "    [5.0, 5.0, 5.0, 5.0],  # Value 1 (High relevance expected)\n",
    "    [1.0, 1.0, 1.0, 1.0],  # Value 2 (Low relevance expected)\n",
    "    [3.0, 3.0, 3.0, 3.0]   # Value 3 (Medium relevance expected)\n",
    "]) # Shape (3, 4)\n",
    "\n",
    "# Run the attention mechanism\n",
    "context_vector, weights = simple_attention(query, keys, values)\n",
    "\n",
    "print(\"--- Simple Attention Example ---\")\n",
    "print(f\"Query: {query}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(f\"Keys (K):\\n{keys}\")\n",
    "print(f\"Values (V):\\n{values}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(f\"Attention Weights (W):\\n{weights}\")\n",
    "# Expected: weights[0] and weights[2] should be high because Key 1 and Key 3 \n",
    "# are more similar to the Query than Key 2.\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(f\"Context Vector (Output):\\n{context_vector}\")\n",
    "# Expected: The output should be a blend of the values, dominated by Value 1 \n",
    "# and Value 3 due to their higher weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276c9e9b-76f4-4378-9ec3-f16bf430208e",
   "metadata": {},
   "source": [
    "**Hierarchical Attention** is an extension of the standard attention mechanism used in deep learning, primarily designed to process information with an inherent, nested structure, such as documents, which are composed of sentences, which are in turn composed of words.\n",
    "\n",
    "It works by applying the attention mechanism at multiple levels of granularity:\n",
    "\n",
    "### 1. The Concept\n",
    "\n",
    "Instead of calculating a single context vector for the entire input sequence, Hierarchical Attention applies attention sequentially:\n",
    "\n",
    "1.  **Lower Level (e.g., Word-to-Sentence Attention):** It first processes the elements at the lowest level (e.g., words in a sentence) and computes an intermediate representation (e.g., a **sentence vector**) by selectively weighing the importance of individual words.\n",
    "2.  **Higher Level (e.g., Sentence-to-Document Attention):** These intermediate representations (sentence vectors) are then processed at the higher level. The mechanism selectively weighs the importance of these elements (sentences) to form the final, high-level output (e.g., a **document vector** or final prediction).\n",
    "\n",
    "This multi-layered approach allows the model to first figure out which **words are important within a sentence** and then figure out which **sentences are important within the document**, leading to more robust and context-aware representations.\n",
    "\n",
    "***\n",
    "\n",
    "### 2. Key References\n",
    "\n",
    "The seminal work introducing and popularizing this specific architecture is:\n",
    "\n",
    "* **Yang, Z., Yang, D., Cohen, S., Salakhutdinov, R., & Hovy, E. (2016). Hierarchical Attention Networks for Document Classification.** *Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL).*\n",
    "    * **Focus:** This paper introduced the **Hierarchical Attention Network (HAN)** model, specifically for document classification, which is the standard implementation of hierarchical attention.\n",
    "\n",
    "Other related and foundational works on attention include:\n",
    "\n",
    "* **Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate.** *International Conference on Learning Representations (ICLR).*\n",
    "    * **Focus:** The original introduction of the **attention mechanism** (additive attention) in the context of sequence-to-sequence models (Neural Machine Translation).\n",
    "\n",
    "* **Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention Is All You Need.** *Advances in Neural Information Processing Systems (NeurIPS).*\n",
    "    * **Focus:** Introduced the **Transformer** architecture and the **Self-Attention** (or scaled dot-product attention) mechanism, which underpins modern models and is often adapted for hierarchical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd80d684-8f13-413f-be70-0e30ae546bf6",
   "metadata": {},
   "source": [
    "ðŸ’¡ **BPE Overview**\n",
    "\n",
    "Byte Pair Encoding is a subword tokenization technique used to balance between character-level and word-level models.\n",
    "\n",
    "Goal: Efficiently handle a large vocabulary and the \"out-of-vocabulary\" (OOV) problem, especially common with large language models (LLMs).\n",
    "\n",
    "Process:\n",
    "\n",
    "* Start by treating every character as an initial token.\n",
    "\n",
    "* Iteratively find the most frequent adjacent pair of tokens (which can be characters or already-merged subwords).\n",
    "\n",
    "* Replace all occurrences of that pair with a new, single token.\n",
    "\n",
    "* Repeat the process for a fixed number of merge operations or until no more frequent pairs exist.\n",
    "\n",
    "The provided Python code illustrates these steps by tracking tokens as space-separated strings and using a Counter to find the most frequent pairs.\n",
    "\n",
    "This video provides an excellent, detailed explanation of how Byte Pair Encoding works, which is highly relevant to understanding the code. Lecture 8: The GPT Tokenizer: Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "ce08b9d8-847a-402b-aea8-efb8aa48830b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BPE training on: 'low lower newest newest newest wide wide' for 5 merges.\n",
      "\n",
      "Initial Vocabulary: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w']\n",
      "--------------------\n",
      "Merge 1: Merged ('w', 'e') into 'we'\n",
      "Current Tokens (word: frequency): {'l o w': 1, 'l o we r': 1, 'n e we s t': 3, 'w i d e': 2}\n",
      "Current Vocabulary: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', 'we']\n",
      "--------------------\n",
      "Merge 2: Merged ('n', 'e') into 'ne'\n",
      "Current Tokens (word: frequency): {'l o w': 1, 'l o we r': 1, 'ne we s t': 3, 'w i d e': 2}\n",
      "Current Vocabulary: ['d', 'e', 'i', 'l', 'ne', 'o', 'r', 's', 't', 'w', 'we']\n",
      "--------------------\n",
      "Merge 3: Merged ('ne', 'we') into 'newe'\n",
      "Current Tokens (word: frequency): {'l o w': 1, 'l o we r': 1, 'newe s t': 3, 'w i d e': 2}\n",
      "Current Vocabulary: ['d', 'e', 'i', 'l', 'newe', 'o', 'r', 's', 't', 'w', 'we']\n",
      "--------------------\n",
      "Merge 4: Merged ('newe', 's') into 'newes'\n",
      "Current Tokens (word: frequency): {'l o w': 1, 'l o we r': 1, 'newes t': 3, 'w i d e': 2}\n",
      "Current Vocabulary: ['d', 'e', 'i', 'l', 'newes', 'o', 'r', 't', 'w', 'we']\n",
      "--------------------\n",
      "Merge 5: Merged ('newes', 't') into 'newest'\n",
      "Current Tokens (word: frequency): {'l o w': 1, 'l o we r': 1, 'newest': 3, 'w i d e': 2}\n",
      "Current Vocabulary: ['d', 'e', 'i', 'l', 'newest', 'o', 'r', 'w', 'we']\n",
      "--------------------\n",
      "Final Learned Merges: {('w', 'e'): 'we', ('n', 'e'): 'ne', ('ne', 'we'): 'newe', ('newe', 's'): 'newes', ('newes', 't'): 'newest'}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_pair_counts(tokens):\n",
    "    \"\"\"Count the frequency of adjacent pairs in a list of tokens.\"\"\"\n",
    "    counts = Counter()\n",
    "    for word, freq in tokens.items():\n",
    "        token_list = word.split(' ')\n",
    "        for i in range(len(token_list) - 1):\n",
    "            pair = (token_list[i], token_list[i+1])\n",
    "            counts[pair] += freq\n",
    "    return counts\n",
    "\n",
    "def merge_pair(tokens, pair, new_token):\n",
    "    \"\"\"Replace all occurrences of a pair in the token list with a new token.\"\"\"\n",
    "    merged_tokens = {}\n",
    "    bigram_str = ' '.join(pair)\n",
    "    new_str = new_token\n",
    "    \n",
    "    for word, freq in tokens.items():\n",
    "        # Replace the pair with the new token, maintaining token separation\n",
    "        # using a simple string replacement on the space-separated tokens.\n",
    "        new_word = word.replace(bigram_str, new_str)\n",
    "        merged_tokens[new_word] = freq\n",
    "    return merged_tokens\n",
    "\n",
    "def train_bpe(corpus, num_merges):\n",
    "    \"\"\"Main BPE training function.\"\"\"\n",
    "    \n",
    "    # 1. Initial Tokens: Split each word into characters and track word frequencies.\n",
    "    # The dictionary keys represent space-separated tokens (e.g., \"h e l l o\"), \n",
    "    # and the values are the frequencies of the *words* in the corpus.\n",
    "    tokens = Counter(corpus.split())\n",
    "    tokens = {' '.join(list(k)) : v for k, v in tokens.items()}\n",
    "    \n",
    "    print(\"Initial Vocabulary:\", sorted(list(set(' '.join(tokens.keys()).split(' ')))) )\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    merges = {}\n",
    "    for i in range(num_merges):\n",
    "        # 2. Find the most frequent pair\n",
    "        pair_counts = get_pair_counts(tokens)\n",
    "        if not pair_counts:\n",
    "            break\n",
    "            \n",
    "        # Get the most frequent pair\n",
    "        best_pair = pair_counts.most_common(1)[0][0]\n",
    "        \n",
    "        # 3. Create a new token (merge)\n",
    "        new_token = ''.join(best_pair)\n",
    "        merges[best_pair] = new_token\n",
    "        \n",
    "        # 4. Apply the merge to the corpus\n",
    "        tokens = merge_pair(tokens, best_pair, new_token)\n",
    "        \n",
    "        print(f\"Merge {i+1}: Merged {best_pair} into '{new_token}'\")\n",
    "        print(\"Current Tokens (word: frequency):\", tokens)\n",
    "        print(\"Current Vocabulary:\", sorted(list(set(' '.join(tokens.keys()).split(' ')))) )\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "    return merges\n",
    "\n",
    "# --- Example Usage ---\n",
    "corpus = \"low lower newest newest newest wide wide\"\n",
    "num_merges = 5\n",
    "\n",
    "print(f\"Starting BPE training on: '{corpus}' for {num_merges} merges.\\n\")\n",
    "learned_merges = train_bpe(corpus, num_merges)\n",
    "\n",
    "print(\"Final Learned Merges:\", learned_merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf9d307-eb75-478f-a314-b70cdae67a8e",
   "metadata": {},
   "source": [
    "the key question here is the **levels of attention and concepts**: byte pairs -> words -> sentences -> documents are obvious, but I think there is a concept layer between sentences and documents. How, then, about higher and more abstract concept layers? Can this be done dynamically? What is the separation criterion? I imagine someting like information gain as used as a split criterion in random trees.\n",
    "\n",
    "\n",
    "The **Information Gain (IG)** criterion is a key metric used in training Decision Trees to decide which attribute (feature) to split the data on at each node. Its goal is to **maximize the purity of the resulting child nodes**, meaning the children should ideally **contain data points belonging to a single class**.\n",
    "\n",
    "ðŸŒ³ **Information Gain Explained**\n",
    "\n",
    "Information Gain is calculated by finding the difference between the entropy of the parent node (before the split) and the weighted average of the entropy of the child nodes (after the split).$$IG(\\text{S}, \\text{A}) = \\text{Entropy}(\\text{S}) - \\sum_{v \\in \\text{Values}(\\text{A})} \\frac{|\\text{S}_v|}{|\\text{S}|} \\text{Entropy}(\\text{S}_v)$$Where:\n",
    "$S$ is the set of data at the parent node.\n",
    "$A$ is the attribute being considered for the split.\n",
    "$\\text{Values}(A)$ is the set of possible values for attribute $A$.$S_v$ is the subset of $S$ where attribute $A$ has value $v$.$|S|$ and\n",
    "$|S_v|$ are the number of elements in the respective sets.\n",
    "\n",
    "Entropy ($\\text{Entropy}(S)$): This is a measure of the impurity or randomness in the data. **Low entropy means the data is mostly of one class (pure)**, and high entropy means the classes are mixed (impure). The goal of a split is to reduce this value. $$\\text{Entropy}(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i)$$ Where $c$ is the number of classes, and $p_i$ is the proportion of samples belonging to class $i$.\n",
    "\n",
    "**Information Gain**: The split that results in the highest Information Gain is chosen because it is the most effective at reducing impurity and making the resulting subsets (children) purer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96491425-96b4-4aa8-864f-059199e8a309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent Entropy: 1.000\n",
      "Weighted Child Entropy: 0.722\n",
      "Information Gain: 0.278\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Define Entropy Function\n",
    "def calculate_entropy(class_counts):\n",
    "    \"\"\"Calculates entropy given a list of class counts.\"\"\"\n",
    "    total_samples = sum(class_counts)\n",
    "    if total_samples == 0:\n",
    "        return 0\n",
    "    entropy = 0\n",
    "    for count in class_counts:\n",
    "        # p_i is the proportion of the class\n",
    "        p_i = count / total_samples\n",
    "        if p_i > 0:\n",
    "            # - p_i * log2(p_i)\n",
    "            entropy -= p_i * np.log2(p_i)\n",
    "    return entropy\n",
    "\n",
    "# 2. Sample Data (Parent Node)\n",
    "# The data has two classes: [Class A, Class B]\n",
    "parent_counts = [5, 5]  # 5 A's and 5 B's (Total 10)\n",
    "entropy_parent = calculate_entropy(parent_counts)\n",
    "# Entropy here will be 1.0 (maximum impurity)\n",
    "\n",
    "# 3. Define a Hypothetical Split (Feature 'Color' with values 'Red' and 'Blue')\n",
    "# Split 1 (Red): [4 A's, 1 B] -> 5 samples\n",
    "split_red_counts = [4, 1]\n",
    "entropy_red = calculate_entropy(split_red_counts)\n",
    "\n",
    "# Split 2 (Blue): [1 A, 4 B's] -> 5 samples\n",
    "split_blue_counts = [1, 4]\n",
    "entropy_blue = calculate_entropy(split_blue_counts)\n",
    "\n",
    "#    A   B\n",
    "#    5   5\n",
    "#-----------\n",
    "# R  4   1\n",
    "# B  1   4\n",
    "\n",
    "# 4. Calculate Weighted Average Entropy of Children\n",
    "total_samples = sum(parent_counts)\n",
    "weighted_child_entropy = (\n",
    "    (sum(split_red_counts) / total_samples) * entropy_red +\n",
    "    (sum(split_blue_counts) / total_samples) * entropy_blue\n",
    ")\n",
    "\n",
    "# 5. Calculate Information Gain\n",
    "information_gain = entropy_parent - weighted_child_entropy\n",
    "\n",
    "print(f\"Parent Entropy: {entropy_parent:.3f}\")\n",
    "print(f\"Weighted Child Entropy: {weighted_child_entropy:.3f}\")\n",
    "print(f\"Information Gain: {information_gain:.3f}\") \n",
    "\n",
    "# The highest IG value indicates the best split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55ffe210-b7df-4181-a086-e870fe99410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy words - sentences\n",
    "# not really meaningful: for characters simple: count a,b,c,... and count frequencies\n",
    "# same for byte pairs and words\n",
    "# for sentences there will likely rarely be the same sentence, so all counts are 1\n",
    "# in a way also not necessary: its language bound and human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de71a234-e312-4f51-8440-cfd4c6278fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: C:\\Users\\bscho\\Documents\\DL\\lrgtxt0\n",
      "Number of words: 1067857\n",
      "Number of sentences: 82597\n",
      "Words per sentences: 12.928520406310156\n"
     ]
    }
   ],
   "source": [
    "# small sample analysis: ratios of byte pairs, words, sentences in docs - relationship to split ratio?\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Simple text analysis: count words and sentences\n",
    "\n",
    "def analyze_text(text):\n",
    "    # Count words (split by whitespace)\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    \n",
    "    # Count sentences (split by '.', '!', '?')\n",
    "    import re\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    # Remove empty strings after splitting\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    num_sentences = len(sentences)\n",
    "    \n",
    "    return num_words, num_sentences\n",
    "\n",
    "# Example usage\n",
    "#text = \"\"\"Artificial intelligence is transforming industries. \n",
    "#It helps automate complex tasks, improve accuracy, and reveal insights! \n",
    "#Can you imagine business without it?\"\"\"\n",
    "\n",
    "#words, sentences = analyze_text(text)\n",
    "#print(f\"Number of words: {words}\")\n",
    "#print(f\"Number of sentences: {sentences}\")\n",
    "\n",
    "\n",
    "print(\"Current directory:\" , os.getcwd())\n",
    "os.chdir(r\"C:\\Users\\bscho\\Documents\\DL\\lrgtxt0\")\n",
    "\n",
    "with open(\"bible.txt\", encoding=\"utf8\") as f:\n",
    "  text = f.read()\n",
    "\n",
    "words, sentences = analyze_text(text)\n",
    "print(f\"Number of words: {words}\")\n",
    "print(f\"Number of sentences: {sentences}\")\n",
    "print(f\"Words per sentences: {words / sentences}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58683677-0cb2-44b0-ae7a-a40914e4587d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a95e5a7-ba56-4f44-8259-b9dbd9ebef87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Simple Attention Example ---\n",
      "Query: [0.75 0.05 0.75 0.05]\n",
      "-----------------------------------\n",
      "Keys (K):\n",
      "[[0.75 0.05 0.75 0.05]\n",
      " [0.5  0.8  0.5  0.8 ]\n",
      " [0.75 0.05 0.75 0.05]]\n",
      "Values (V):\n",
      "[[4.5 5.  5.  5. ]\n",
      " [1.  1.  1.5 1. ]\n",
      " [3.  3.5 3.  3. ]]\n",
      "-----------------------------------\n",
      "Attention Weights (W):\n",
      "[0.36485455 0.2702909  0.36485455]\n",
      "-----------------------------------\n",
      "Context Vector (Output):\n",
      "[3.00670003 3.37155458 3.32427275 3.1891273 ]\n"
     ]
    }
   ],
   "source": [
    "# Hierarchical Attention\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def simple_attention(query, keys, values):\n",
    "    \"\"\"\n",
    "    A simplified, illustrative example of the attention mechanism.\n",
    "\n",
    "    Args:\n",
    "        query (np.ndarray): The vector representing what we're looking for.\n",
    "        keys (np.ndarray): A set of vectors to compare the query against.\n",
    "        values (np.ndarray): The values associated with the keys.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The context vector (weighted sum of values).\n",
    "    \"\"\"\n",
    "    # 1. Calculate Similarity Scores (e.g., Dot Product)\n",
    "    # The dot product measures how related the query is to each key.\n",
    "    # Scores shape will be (num_keys,)\n",
    "    scores = np.dot(keys, query)\n",
    "\n",
    "    # 2. Normalize Scores to get Attention Weights (using Softmax)\n",
    "    # Softmax converts scores into probabilities (weights) that sum to 1.\n",
    "    # The higher the score, the higher the weight.\n",
    "    # Weights shape will be (num_keys,)\n",
    "    # np.exp(scores - np.max(scores)) is used for numerical stability\n",
    "    exp_scores = np.exp(scores - np.max(scores))\n",
    "    weights = exp_scores / np.sum(exp_scores)\n",
    "\n",
    "    # 3. Compute Context Vector (Weighted Sum of Values)\n",
    "    # The final output (context vector) is the sum of (weight * value).\n",
    "    # This vector represents the \"attended\" information.\n",
    "    # values shape is (num_keys, value_dim), weights shape is (num_keys,)\n",
    "    # This results in (value_dim,)\n",
    "    context_vector = np.dot(weights, values)\n",
    "\n",
    "    return context_vector, weights\n",
    "\n",
    "# --- Example Usage ---\n",
    "#  here we have to expand: the issue is not the attention mechanism, its the hierarchy definition and aggregation\n",
    "\n",
    "# Define the input data:\n",
    "# Here, the dimension for Q, K, V is 8. We have 3 key-value pairs.\n",
    "# Imagine Q is a word, and K/V are surrounding words.\n",
    "\n",
    "# The Query (Q): What we are looking for (e.g., current word's embedding)\n",
    "query = np.array([1.0, 0.5, 0.1, 0.0, 1.0, 0.5, 0.1, 0.0]) # Shape (8,)\n",
    "\n",
    "# The Keys (K): What we compare against (e.g., other words' embeddings)\n",
    "keys = np.array([\n",
    "    [1.1, 0.4, 0.0, 0.1, 1.1, 0.4, 0.0, 0.1],  # Key 1: Very similar to Q\n",
    "    [0.2, 0.8, 0.9, 0.7, 0.2, 0.8, 0.9, 0.7],  # Key 2: Less similar\n",
    "    [0.9, 0.6, 0.1, 0.0, 0.9, 0.6, 0.1, 0.0]   # Key 3: Similar\n",
    "]) # Shape (3, 8)\n",
    "\n",
    "# The Values (V): The actual information to be aggregated\n",
    "# (In self-attention, K and V are often the same, but they don't have to be)\n",
    "values = np.array([\n",
    "    [5.0, 4.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0],  # Value 1 (High relevance expected)\n",
    "    [1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0],  # Value 2 (Low relevance expected)\n",
    "    [3.0, 3.0, 4.0, 3.0, 3.0, 3.0, 3.0, 3.0]   # Value 3 (Medium relevance expected)\n",
    "]) # Shape (3, 8)\n",
    "\n",
    "# aggregate QKV\n",
    "matavrg2 = 0.5 * np.array([[1., 1., 0., 0., 0., 0., 0., 0.], [0., 0., 1., 1., 0., 0., 0., 0.], \n",
    "                         [0., 0., 0., 0., 1., 1., 0., 0.],[0., 0., 0., 0., 0., 0., 1., 1.]])\n",
    "query = np.dot(matavrg2, query)\n",
    "keys = np.dot(keys, matavrg2.T)\n",
    "values = np.dot(values, matavrg2.T)\n",
    "\n",
    "# Run the attention mechanism\n",
    "context_vector, weights = simple_attention(query, keys, values)\n",
    "\n",
    "print(\"--- Simple Attention Example ---\")\n",
    "print(f\"Query: {query}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(f\"Keys (K):\\n{keys}\")\n",
    "print(f\"Values (V):\\n{values}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(f\"Attention Weights (W):\\n{weights}\")\n",
    "# Expected: weights[0] and weights[2] should be high because Key 1 and Key 3 \n",
    "# are more similar to the Query than Key 2.\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(f\"Context Vector (Output):\\n{context_vector}\")\n",
    "# Expected: The output should be a blend of the values, dominated by Value 1 \n",
    "# and Value 3 due to their higher weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfac5c7-10b6-4722-8f0f-207f91ad8b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e218354-9b3c-43db-97cf-59b0b5b034c1",
   "metadata": {},
   "source": [
    "# clarify: to what degree does the stacking in transformers provide hierarchical attention?\n",
    "\n",
    "Excellent question â€” and yes, the **GPT architecture does enable multi-level attention through its stacked transformer blocks**, but with some subtle nuances worth unpacking carefully.\n",
    "\n",
    "Letâ€™s go step by step.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  1. The GPT Stack Structure\n",
    "\n",
    "GPT (Generative Pretrained Transformer) is composed of **a stack of Transformer decoder blocks**, typically 12â€“96 layers deep depending on the model size (GPT-2, GPT-3, GPT-4, etc.).\n",
    "\n",
    "Each block consists of:\n",
    "\n",
    "* A **self-attention sublayer** (with multi-head attention)\n",
    "* A **feedforward (MLP) sublayer**\n",
    "* Residual connections + layer normalization around both\n",
    "\n",
    "Formally:\n",
    "$x_{l+1} = x_l + \\text{FFN}\\big(x_l + \\text{MHA}(x_l)\\big)$\n",
    "\n",
    "Each layerâ€™s output becomes the input to the next, meaning that **attention information is propagated hierarchically upward through the stack**.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ” 2. Multi-Level Attention: What That Really Means\n",
    "\n",
    "Each self-attention block computes attention over *the same sequence* of tokens, but since each layer operates on **a progressively more abstract representation**, the attention maps become increasingly *semantic* rather than *surface-level*.\n",
    "\n",
    "So yes â€” GPT realizes a **multi-level attention hierarchy**, because:\n",
    "\n",
    "* **Lower layers** attend mostly to local and syntactic relations (e.g. agreement, word order).\n",
    "* **Middle layers** capture phrasal and compositional patterns.\n",
    "* **Higher layers** encode global or conceptual relationships (narrative flow, topic coherence, reasoning steps).\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ 3. How Layers â€œFeed Into Each Otherâ€\n",
    "\n",
    "Attention itself isnâ€™t directly multi-level â€” thereâ€™s no explicit hierarchical routing between attention heads across layers.\n",
    "However:\n",
    "\n",
    "* The **output of each layer (post-attention)** is passed to the next layer.\n",
    "* Each subsequent layerâ€™s attention operates **on the transformed hidden states** from all previous processing.\n",
    "\n",
    "That means the **contextual embeddings progressively integrate** all prior attention computations.\n",
    "So the hierarchy emerges *implicitly*, not because attention heads â€œtalk toâ€ one another directly, but because **the representations they produce are recursively refined**.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§© 4. Multi-Head Attention as Parallel Abstraction\n",
    "\n",
    "Within each layer, **multi-head attention** adds another axis of specialization:\n",
    "\n",
    "* Different heads focus on different types of relationships.\n",
    "* The model combines these to form a richer contextual embedding.\n",
    "\n",
    "Stacking layers effectively multiplies this process, giving rise to a **multi-head, multi-level attention system** â€” each layer refining, aggregating, and recontextualizing what came before.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”„ 5. Summary Analogy\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "| Level     | Focus                             | Analogy                   |\n",
    "| --------- | --------------------------------- | ------------------------- |\n",
    "| Layer 1â€“3 | Syntax, local dependencies        | \"Hearing words\"           |\n",
    "| Layer 4â€“8 | Phrases, semantic roles           | \"Understanding sentences\" |\n",
    "| Layer 9+  | Discourse, reasoning, abstraction | \"Grasping meaning\"        |\n",
    "\n",
    "Each levelâ€™s â€œattentionâ€ feeds the nextâ€™s, producing **emergent hierarchical understanding** â€” even though the architecture itself is *flat* (stacked blocks, no explicit tree).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§® Bonus: Variants That Make Hierarchy More Explicit\n",
    "\n",
    "Some architectures extend GPT to make this multi-level hierarchy more explicit:\n",
    "\n",
    "* **Perceiver / Perceiver IO:** introduces latent hierarchical attention bottlenecks.\n",
    "* **Longformer / BigBird:** use attention patterns that change with depth.\n",
    "* **Transformer-XL / RWKV:** add recurrence across layers or time.\n",
    "* **RETRO / RAG:** augment attention with external retrieval (explicit multi-level context).\n",
    "\n",
    "---\n",
    "\n",
    "If youâ€™d like, I can show a **diagram or code snippet** illustrating how multi-level attention emerges across blocks â€” would you like that visual explanation?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5fb9d2-2108-4c92-9702-8d8253615064",
   "metadata": {},
   "source": [
    "![MHAtt](\"C:\\Users\\bscho\\Documents\\DLmhatt.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec7cb454-fdd1-458b-856f-63e2247bc5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model parameters: 10786625\n",
      "Logits shape: torch.Size([1024, 65])\n",
      "Loss: 4.3347\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# --- 1. Multi-Head Causal Self-Attention ---\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, block_size, dropout):\n",
    "        super().__init__()\n",
    "        assert embed_size % num_heads == 0\n",
    "        \n",
    "        self.head_size = embed_size // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_size = embed_size\n",
    "        self.block_size = block_size  # Max sequence length\n",
    "        \n",
    "        # Key, Query, Value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(embed_size, 3 * embed_size, bias=False)\n",
    "        # Output projection\n",
    "        self.c_proj = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Causal mask to ensure we don't look into the future\n",
    "        # (block_size, block_size)\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size))\n",
    "                                     .view(1, 1, block_size, block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # Batch size, sequence length, embedding dimension\n",
    "        \n",
    "        # Calculate query, key, values for all heads in parallel\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.embed_size, dim=2)\n",
    "        \n",
    "        # Reshape for multi-head attention: (B, T, C) -> (B, num_heads, T, head_size)\n",
    "        k = k.view(B, T, self.num_heads, self.head_size).transpose(1, 2) \n",
    "        q = q.view(B, T, self.num_heads, self.head_size).transpose(1, 2) \n",
    "        v = v.view(B, T, self.num_heads, self.head_size).transpose(1, 2) \n",
    "\n",
    "        # Scaled Dot-Product Attention (manual for clarity, or use torch.nn.functional.scaled_dot_product_attention in newer PyTorch)\n",
    "        # (B, num_heads, T, head_size) @ (B, num_heads, head_size, T) -> (B, num_heads, T, T)\n",
    "        attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_size))\n",
    "        \n",
    "        # Apply the causal (look-ahead) mask\n",
    "        # We only need the mask up to the current sequence length T\n",
    "        attn = attn.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        \n",
    "        # (B, num_heads, T, T) @ (B, num_heads, T, head_size) -> (B, num_heads, T, head_size)\n",
    "        y = attn @ v \n",
    "        \n",
    "        # Re-assemble all head outputs side by side: (B, T, embed_size)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) \n",
    "\n",
    "        # Output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "# --- 2. Transformer Block (GPT Layer) ---\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, block_size, dropout):\n",
    "        super().__init__()\n",
    "        # LayerNorm before the self-attention\n",
    "        self.ln_1 = nn.LayerNorm(embed_size)\n",
    "        self.attn = CausalSelfAttention(embed_size, num_heads, block_size, dropout)\n",
    "        # LayerNorm before the MLP\n",
    "        self.ln_2 = nn.LayerNorm(embed_size)\n",
    "        # MLP (Feed-Forward Network)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),  # Standard 4x expansion\n",
    "            nn.GELU(),  # GELU activation, common in GPT\n",
    "            nn.Linear(4 * embed_size, embed_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply self-attention with residual connection and LayerNorm (pre-normalization)\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        # Apply MLP with residual connection and LayerNorm (pre-normalization)\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "# --- 3. Minimal GPT Model ---\n",
    "class MinimalGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, block_size, num_layers, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # Token and Positional Embeddings\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Stack of Transformer Blocks\n",
    "        self.blocks = nn.Sequential(*[Block(embed_size, num_heads, block_size, dropout) for _ in range(num_layers)])\n",
    "        \n",
    "        # Final Layer Norm and Linear head for logits\n",
    "        self.ln_f = nn.LayerNorm(embed_size)\n",
    "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        \n",
    "        # Token and positional embeddings\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        pos = torch.arange(T, device=idx.device)  # (T)\n",
    "        pos_emb = self.position_embedding_table(pos)  # (T, C)\n",
    "        x = self.dropout(tok_emb + pos_emb)  # (B, T, C)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        x = self.blocks(x)\n",
    "        \n",
    "        # Final layer normalization and logits\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # Reshape for cross-entropy loss (Pytorch expects (B*T, vocab_size) and (B*T))\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "# Example Usage:\n",
    "# Define model configuration\n",
    "VOCAB_SIZE = 65    # Example: character-level encoding (like in minGPT)\n",
    "EMBED_SIZE = 384   # Embedding dimension\n",
    "BLOCK_SIZE = 256   # Context length\n",
    "NUM_LAYERS = 6     # Number of decoder blocks\n",
    "NUM_HEADS = 6      # Number of attention heads\n",
    "DROPOUT = 0.2      # Dropout rate\n",
    "\n",
    "# Instantiate the model\n",
    "model = MinimalGPT(VOCAB_SIZE, EMBED_SIZE, BLOCK_SIZE, NUM_LAYERS, NUM_HEADS, DROPOUT)\n",
    "print(f\"Total model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Example input (dummy data: B=4, T=BLOCK_SIZE)\n",
    "dummy_input = torch.randint(0, VOCAB_SIZE, (4, BLOCK_SIZE))\n",
    "dummy_targets = torch.randint(0, VOCAB_SIZE, (4, BLOCK_SIZE))\n",
    "\n",
    "# Forward pass\n",
    "logits, loss = model(dummy_input, dummy_targets)\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3704fee3-98e2-435b-84ad-1b469fd24725",
   "metadata": {},
   "source": [
    "**Key GPT Characteristics in the Code**:\n",
    "\n",
    "* Decoder-Only: The model is built solely with Transformer Blocks, without an Encoder.\n",
    "\n",
    "* Causal Self-Attention: The CausalSelfAttention module uses a triangular mask (self.mask) to ensure each token can only attend to tokens that came before it in the sequence, which is essential for language generation (autoregression).\n",
    "\n",
    "* Positional Encoding: Both token embeddings (self.token_embedding_table) and positional embeddings (self.position_embedding_table) are summed together to inject sequence order information, as the attention mechanism is permutation-invariant.\n",
    "\n",
    "* Pre-Normalization: The Block uses Layer Normalization (self.ln_1, self.ln_2) before the self-attention and MLP layers, which is a common practice in modern LLMs like GPT-2/3.\n",
    "\n",
    "* Residual Connections: The addition operations (x = x + self.attn(...), x = x + self.mlp(...)) implement the crucial residual connections that allow gradients to flow easily through the deep network.\n",
    "\n",
    "You can find a more in-depth and complete implementation with training and data handling in this video that explains coding a ChatGPT-like transformer: Coding a ChatGPT Like Transformer From Scratch in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e693c01d-e6f8-478c-be4c-ac8d46c03de3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddccb8bc-4943-4864-85d7-dd75800f68de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bea7915-6e2c-4c54-9b24-380d06319ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7358897d-ad3a-444c-b185-3c8d188fe044",
   "metadata": {},
   "source": [
    "Excellent question â€” and one that goes to the **core of what makes a Transformer more than just an attention machine.**\n",
    "\n",
    "Letâ€™s unpack it precisely:\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Context: Transformer Block Structure\n",
    "\n",
    "Each Transformer layer (or block) has **two main sublayers**:\n",
    "\n",
    "1. **Multi-Head Self-Attention (MHSA)**\n",
    "2. **Position-wise Feedforward Network (FFN or MLP)**\n",
    "\n",
    "Both are wrapped in residual connections and layer normalization:\n",
    "$x_{l+1} = \\text{LayerNorm}\\big(x_l + \\text{FFN}(\\text{LayerNorm}(x_l + \\text{MHSA}(x_l)))\\big)$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ” What the Attention Layer Does\n",
    "\n",
    "The **attention sublayer** performs *information routing*:\n",
    "It lets each token (word, embedding vector, etc.) **selectively mix information from other tokens**.\n",
    "You can think of it as a **content-based lookup** â€” finding and aggregating relevant context across the sequence.\n",
    "\n",
    "But thatâ€™s *all it does* â€” it doesnâ€™t *transform* the representation nonlinearly beyond mixing and re-weighting.\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ What the Feedforward (MLP) Sublayer Does\n",
    "\n",
    "Once the attention has gathered contextual information, the **feedforward network** applies **nonlinear transformations** to each tokenâ€™s representation *independently*.\n",
    "Itâ€™s often called **position-wise** because it processes each tokenâ€™s embedding separately, with the *same weights* across all tokens.\n",
    "\n",
    "Mathematically:\n",
    "$\\text{FFN}(x) = W_2 \\cdot \\text{ReLU}(W_1 x + b_1) + b_2$\n",
    "\n",
    "(sometimes with GELU or SwiGLU instead of ReLU).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ Purpose and Intuition\n",
    "\n",
    "| Function                               | Explanation                                                                                                                                                                                            |\n",
    "| -------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| **Feature Expansion & Transformation** | The MLP expands dimensionality (e.g., (d_{model} â†’ 4d_{model} â†’ d_{model})) to create a richer nonlinear feature space before compressing back. This allows complex pattern transformations.           |\n",
    "| **Nonlinearity Injection**             | Attention is mostly linear in the input embeddings (except for softmax). The MLP adds crucial *nonlinear capacity* â€” turning a linear attention mechanism into a true universal function approximator. |\n",
    "| **Local Reasoning**                    | While attention handles *global dependencies*, the MLP refines *local representations*: it can abstract over the combined context (e.g., detect that â€œnot goodâ€ â†’ â€œbadâ€).                              |\n",
    "| **Channel Mixing**                     | It allows interaction *between feature dimensions* (hidden units), unlike attention which mixes *between tokens*. So: **attention = spatial mixing**, **MLP = channel mixing**.                        |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”„ Summary Analogy\n",
    "\n",
    "| Layer                 | Mixes What?                        | Role                                           |\n",
    "| --------------------- | ---------------------------------- | ---------------------------------------------- |\n",
    "| **Attention**         | Mixes *tokens* across the sequence | Contextual integration (â€œwho talks to whomâ€)   |\n",
    "| **Feedforward (MLP)** | Mixes *features* within each token | Nonlinear transformation (â€œwhatâ€™s being saidâ€) |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§© In short\n",
    "\n",
    "> The feedforward sublayer gives the Transformer the ability to **nonlinearly transform contextualized representations**, providing **depth and expressive power** that attention alone cannot.\n",
    "\n",
    "---\n",
    "\n",
    "If youâ€™d like, I can also show how **removing the MLP** from a Transformer degrades its expressive power â€” for instance, showing mathematically or visually that pure attention is mostly a *linear map over inputs*. Would you like that next?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daef7017-4103-4145-9f2a-470b1cd2edca",
   "metadata": {},
   "source": [
    "Here's a clear toy example demonstrating the MLP (feedforward) component in transformers:This example demonstrates the key aspects of the transformer feedforward network:\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "1. **Two linear layers with expansion**: The first layer expands from `d_model` to `d_ff` (typically 4x larger), and the second projects back down\n",
    "\n",
    "2. **Position-wise**: Each token position is processed independently - no mixing of information across the sequence\n",
    "\n",
    "3. **Non-linearity**: GELU (or ReLU) activation between the layers provides the non-linear transformation\n",
    "\n",
    "4. **Residual connection**: In a full transformer, this output would be added to the input (residual connection) and layer-normalized\n",
    "\n",
    "The expansion to a higher dimension (`d_ff`) and back down allows the network to learn complex transformations while maintaining the same `d_model` dimension throughout the transformer stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1727df63-1b0d-47fe-904d-3a1884db7c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 8])\n",
      "Input (first sequence, first token):\n",
      " tensor([ 0.5163,  1.6060, -0.9815,  0.5361,  0.9226,  0.4872,  1.4920,  0.0133])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 8])\n",
      "Output (first sequence, first token):\n",
      " tensor([-0.0074, -0.2296,  0.3912,  0.1630, -0.0833, -0.0522, -0.0185,  0.2283],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "============================================================\n",
      "Key property: Position-wise application\n",
      "============================================================\n",
      "Processing position 0 separately:\n",
      "tensor([ 0.0810, -0.1133,  0.3722], grad_fn=<SliceBackward0>)\n",
      "\n",
      "From full forward pass (position 0):\n",
      "tensor([-0.0074, -0.2296,  0.3912], grad_fn=<SliceBackward0>)\n",
      "\n",
      "============================================================\n",
      "Architecture summary:\n",
      "============================================================\n",
      "d_model: 8 -> d_ff: 32 -> d_model: 8\n",
      "Total parameters: 552\n",
      "  - W1: 8 x 32 = 256\n",
      "  - b1: 32\n",
      "  - W2: 32 x 8 = 256\n",
      "  - b2: 8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForwardMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    The MLP/Feedforward network used between transformer blocks.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Linear projection to expand dimension (d_model -> d_ff)\n",
    "    2. Non-linear activation (typically ReLU or GELU)\n",
    "    3. Linear projection back down (d_ff -> d_model)\n",
    "    4. Dropout for regularization\n",
    "    \n",
    "    This is applied independently to each position.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.activation = nn.GELU()  # Can also use ReLU\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        x = self.linear1(x)      # -> (batch, seq_len, d_ff)\n",
    "        x = self.activation(x)   # Apply non-linearity\n",
    "        x = self.dropout(x)      # Regularization\n",
    "        x = self.linear2(x)      # -> (batch, seq_len, d_model)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Toy example with small dimensions\n",
    "def main():\n",
    "    # Hyperparameters (small for toy example)\n",
    "    batch_size = 2\n",
    "    seq_len = 4\n",
    "    d_model = 8       # Model dimension (embedding size)\n",
    "    d_ff = 32         # Hidden dimension (typically 4x d_model)\n",
    "    \n",
    "    # Create the feedforward network\n",
    "    ffn = FeedForwardMLP(d_model, d_ff, dropout=0.1)\n",
    "    \n",
    "    # Create toy input (output from attention layer)\n",
    "    # In practice, this comes from the multi-head attention\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Input (first sequence, first token):\\n\", x[0, 0])\n",
    "    print()\n",
    "    \n",
    "    # Forward pass through MLP\n",
    "    output = ffn(x)\n",
    "    \n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Output (first sequence, first token):\\n\", output[0, 0])\n",
    "    print()\n",
    "    \n",
    "    # Key insight: Each position is processed independently\n",
    "    # The FFN doesn't mix information across sequence positions\n",
    "    print(\"=\"*60)\n",
    "    print(\"Key property: Position-wise application\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Process positions separately to verify independence\n",
    "    output_pos0 = ffn(x[:, 0:1, :])  # Process only position 0\n",
    "    output_pos1 = ffn(x[:, 1:2, :])  # Process only position 1\n",
    "    \n",
    "    print(\"Processing position 0 separately:\")\n",
    "    print(output_pos0[0, 0, :3])  # First 3 values\n",
    "    print(\"\\nFrom full forward pass (position 0):\")\n",
    "    print(output[0, 0, :3])  # Should match!\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Architecture summary:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"d_model: {d_model} -> d_ff: {d_ff} -> d_model: {d_model}\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in ffn.parameters()):,}\")\n",
    "    print(f\"  - W1: {d_model} x {d_ff} = {d_model * d_ff:,}\")\n",
    "    print(f\"  - b1: {d_ff}\")\n",
    "    print(f\"  - W2: {d_ff} x {d_model} = {d_ff * d_model:,}\")\n",
    "    print(f\"  - b2: {d_model}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be442bf-2135-44c1-aea0-0d0293a8ce8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0261b52-97ab-4fca-8f3f-93bf8ec8d431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf5e011b-f554-4ec9-84d8-e1072bf56752",
   "metadata": {},
   "source": [
    "Excellent â€” this gets to the **engineering and mathematical stability** that makes the Transformer architecture actually *trainable* and *deep*.\n",
    "\n",
    "Letâ€™s break this down clearly and precisely.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§± Transformer Block Recap\n",
    "\n",
    "Each Transformer layer typically looks like this (in **Post-LN** form, the original Vaswani et al. 2017 version):\n",
    "\n",
    "$y = x + \\text{Attention}(\\text{LayerNorm}(x))$\n",
    "\n",
    "$z = y + \\text{FFN}(\\text{LayerNorm}(y))$\n",
    "\n",
    "Or in **Pre-LN** form (used in most modern implementations for stability):\n",
    "\n",
    "$y = x + \\text{Attention}(\\text{LayerNorm}(x))$\n",
    "\n",
    "$z = y + \\text{FFN}(\\text{LayerNorm}(y))$\n",
    "\n",
    "(Same structure, but normalization happens *before* each sublayer.)\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Purpose of Residual Connections\n",
    "\n",
    "Residual (or *skip*) connections were first popularized by ResNet.\n",
    "Their main purpose is to make **deep networks trainable** by preserving **gradient flow** and **semantic continuity** across layers.\n",
    "\n",
    "### 1. **Gradient Flow Stabilization**\n",
    "\n",
    "* Without residuals, deep layers can cause *vanishing/exploding gradients*.\n",
    "* The skip path (`x + something`) creates a **direct gradient highway** from later to earlier layers:\n",
    "  $\n",
    "  \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} (1 + \\frac{\\partial f(x)}{\\partial x})\n",
    "  $\n",
    "  So even if the learned transformation (f(x)) collapses or saturates, the identity path keeps gradients alive.\n",
    "\n",
    "### 2. **Preserve Information**\n",
    "\n",
    "* The model can always â€œfall backâ€ to the identity function if a sublayer learns nothing useful.\n",
    "* This gives every layer a *safe baseline* to start from, enabling gradual improvement rather than catastrophic change.\n",
    "\n",
    "### 3. **Eases Optimization**\n",
    "\n",
    "* Each sublayer learns *residual refinements* rather than complete transformations.\n",
    "* Instead of â€œlearn the full mapping,â€ itâ€™s â€œlearn the small correction.â€\n",
    "\n",
    "So:\n",
    "\n",
    "> **Residual connections make each layer a refinement, not a replacement, of its input.**\n",
    "\n",
    "---\n",
    "\n",
    "## âš–ï¸ Purpose of Layer Normalization\n",
    "\n",
    "Layer Normalization (LN) stabilizes the activations within each token vector.\n",
    "\n",
    "Given a token embedding $(x \\in \\mathbb{R}^{d})$:\n",
    "$\n",
    "\\text{LN}(x) = \\frac{x - \\mu}{\\sigma} \\cdot \\gamma + \\beta\n",
    "$\n",
    "where $(\\mu)$ and $(\\sigma)$ are the mean and standard deviation over the tokenâ€™s features.\n",
    "\n",
    "### 1. **Prevents Internal Covariate Shift**\n",
    "\n",
    "* Keeps activations on a stable scale and distribution as they pass through many layers.\n",
    "* This means training dynamics donâ€™t oscillate or blow up due to inconsistent activation scales.\n",
    "\n",
    "### 2. **Improves Convergence Speed**\n",
    "\n",
    "* By normalizing, the optimization surface becomes smoother â†’ allows larger learning rates and faster convergence.\n",
    "\n",
    "### 3. **Balances Attention and Feedforward Outputs**\n",
    "\n",
    "* Since attention and MLP sublayers may output vectors with very different norms, LN ensures consistent scaling before residual addition.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¡ Combined Purpose: Residual + LN Together\n",
    "\n",
    "| Mechanism               | Primary Role                         | Effect                                                     |\n",
    "| ----------------------- | ------------------------------------ | ---------------------------------------------------------- |\n",
    "| **Residual Connection** | Keep information & gradients flowing | Enables deep stacking and stable gradient propagation      |\n",
    "| **Layer Normalization** | Keep activations numerically stable  | Prevents exploding/vanishing activations, smooths training |\n",
    "\n",
    "Together they:\n",
    "\n",
    "* Maintain a **consistent scale and signal path** through the entire Transformer.\n",
    "* Allow extremely deep architectures (e.g., GPT-4, 100+ layers) to train stably.\n",
    "* Enable **additive composition of knowledge** across layers â€” each layer makes a controlled refinement of the representation.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© Conceptual Summary\n",
    "\n",
    "You can think of it like this:\n",
    "\n",
    "> The residual path is the **highway** that carries meaning forward,\n",
    "> and the layer norm is the **traffic regulator** that keeps that flow smooth and stable.\n",
    "\n",
    "---\n",
    "\n",
    "If youâ€™d like, I can also show **what goes wrong** (numerically and conceptually) when you remove each component â€” e.g., plots of gradient norms with vs. without residuals/LN.\n",
    "Would you like that next?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a67a72-2fba-44cf-a33c-0fef552d0936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d4d87b0-fddc-4343-b363-7d87cd2f37a6",
   "metadata": {},
   "source": [
    "I'll create a simple example of a GPT-like transformer with hierarchical attention. This implementation shows how to process text at multiple levels of granularity (e.g., tokens â†’ phrases â†’ sentences).I've created a hierarchical attention transformer that demonstrates key concepts:\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "1. **Two-Level Hierarchy:**\n",
    "   - **Local attention**: Operates within small chunks (e.g., 4 tokens) to capture fine-grained patterns\n",
    "   - **Global attention**: Operates between chunk representations to capture high-level structure\n",
    "\n",
    "2. **How it works:**\n",
    "   - Splits the sequence into chunks\n",
    "   - Applies attention within each chunk independently\n",
    "   - Pools chunks into single representations\n",
    "   - Applies attention between these chunk representations\n",
    "   - Broadcasts global context back to all tokens\n",
    "\n",
    "3. **Benefits:**\n",
    "   - More efficient than full attention (reduced computation)\n",
    "   - Captures both local patterns and long-range dependencies\n",
    "   - Mimics how language has hierarchical structure (words â†’ phrases â†’ sentences)\n",
    "\n",
    "**The example includes:**\n",
    "- A complete working transformer with hierarchical attention\n",
    "- Causal masking for autoregressive generation\n",
    "- A simple generation demo\n",
    "\n",
    "You can modify `chunk_size` to control the granularity of the hierarchy. Smaller chunks = more local focus, larger chunks = more global processing per level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "903b8700-9fb7-40c7-b4ad-925fc6a030bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 16])\n",
      "Output shape: torch.Size([2, 16, 1000])\n",
      "\n",
      "Model has 762,856 parameters\n",
      "\n",
      "=== Example Generation ===\n",
      "Starting sequence: [327, 543, 982, 593]\n",
      "Generated sequence: [327, 543, 982, 593, 5, 459, 641, 833, 452, 500, 618, 681]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Standard multi-head attention mechanism\"\"\"\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project and reshape to (batch, n_heads, seq_len, head_dim)\n",
    "        q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Reshape back\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "\n",
    "class HierarchicalAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Hierarchical attention that operates at two levels:\n",
    "    1. Local attention within chunks (low-level patterns)\n",
    "    2. Global attention between chunk representations (high-level structure)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, chunk_size=4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "        # Local attention within chunks\n",
    "        self.local_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        \n",
    "        # Global attention between chunks\n",
    "        self.global_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        \n",
    "        # Pooling to create chunk representations\n",
    "        self.chunk_pool = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Expand chunk representation back to tokens\n",
    "        self.chunk_expand = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Pad sequence to be divisible by chunk_size\n",
    "        pad_len = (self.chunk_size - seq_len % self.chunk_size) % self.chunk_size\n",
    "        if pad_len > 0:\n",
    "            x = F.pad(x, (0, 0, 0, pad_len))\n",
    "            if mask is not None:\n",
    "                mask = F.pad(mask, (0, pad_len, 0, pad_len))\n",
    "        \n",
    "        padded_len = x.shape[1]\n",
    "        n_chunks = padded_len // self.chunk_size\n",
    "        \n",
    "        # === Level 1: Local attention within chunks ===\n",
    "        # Reshape to (batch * n_chunks, chunk_size, d_model)\n",
    "        x_chunks = x.view(batch_size, n_chunks, self.chunk_size, d_model)\n",
    "        x_chunks = x_chunks.view(batch_size * n_chunks, self.chunk_size, d_model)\n",
    "        \n",
    "        # Apply local attention\n",
    "        local_out = self.local_attn(x_chunks)\n",
    "        local_out = self.norm1(x_chunks + local_out)\n",
    "        \n",
    "        # Reshape back\n",
    "        local_out = local_out.view(batch_size, n_chunks, self.chunk_size, d_model)\n",
    "        \n",
    "        # === Level 2: Create chunk representations ===\n",
    "        # Pool each chunk to a single vector (mean pooling)\n",
    "        chunk_repr = local_out.mean(dim=2)  # (batch, n_chunks, d_model)\n",
    "        chunk_repr = self.chunk_pool(chunk_repr)\n",
    "        \n",
    "        # === Level 3: Global attention between chunks ===\n",
    "        global_out = self.global_attn(chunk_repr)\n",
    "        global_out = self.norm2(chunk_repr + global_out)\n",
    "        \n",
    "        # === Level 4: Broadcast chunk context back to tokens ===\n",
    "        # Expand chunk representations back to all tokens in each chunk\n",
    "        global_expanded = global_out.unsqueeze(2).expand(-1, -1, self.chunk_size, -1)\n",
    "        global_expanded = global_expanded.reshape(batch_size, padded_len, d_model)\n",
    "        global_expanded = self.chunk_expand(global_expanded)\n",
    "        \n",
    "        # Combine local and global information\n",
    "        local_flat = local_out.view(batch_size, padded_len, d_model)\n",
    "        output = self.norm3(local_flat + global_expanded)\n",
    "        \n",
    "        # Remove padding\n",
    "        if pad_len > 0:\n",
    "            output = output[:, :-pad_len, :]\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class HierarchicalTransformer(nn.Module):\n",
    "    \"\"\"Simple GPT-like transformer with hierarchical attention\"\"\"\n",
    "    def __init__(self, vocab_size, d_model=128, n_heads=4, n_layers=3, \n",
    "                 chunk_size=4, max_seq_len=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        # Stack of hierarchical attention layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            HierarchicalAttentionLayer(d_model, n_heads, chunk_size)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Embeddings\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        x = self.embedding(x) + self.pos_encoding(positions)\n",
    "        \n",
    "        # Create causal mask for autoregressive generation\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf')).masked_fill(mask == 0, 1.0)\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, seq_len)\n",
    "        \n",
    "        # Apply hierarchical attention layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = self.output_proj(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# === Example Usage ===\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    vocab_size = 1000\n",
    "    batch_size = 2\n",
    "    seq_len = 16\n",
    "    \n",
    "    # Create model\n",
    "    model = HierarchicalTransformer(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=128,\n",
    "        n_heads=4,\n",
    "        n_layers=3,\n",
    "        chunk_size=4,  # Process text in chunks of 4 tokens\n",
    "        max_seq_len=64\n",
    "    )\n",
    "    \n",
    "    # Create sample input (random token IDs)\n",
    "    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = model(input_ids)\n",
    "    \n",
    "    print(f\"Input shape: {input_ids.shape}\")\n",
    "    print(f\"Output shape: {logits.shape}\")\n",
    "    print(f\"\\nModel has {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "    \n",
    "    # Demonstrate generation\n",
    "    print(\"\\n=== Example Generation ===\")\n",
    "    with torch.no_grad():\n",
    "        # Start with a sequence\n",
    "        generated = input_ids[0:1, :4]  # Take first 4 tokens\n",
    "        print(f\"Starting sequence: {generated.tolist()[0]}\")\n",
    "        \n",
    "        # Generate next 8 tokens\n",
    "        for _ in range(8):\n",
    "            logits = model(generated)\n",
    "            next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "        \n",
    "        print(f\"Generated sequence: {generated.tolist()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213823e9-42c9-42ca-89bf-9d330050ada6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15fbabda-c67c-4831-a39d-0bfda59761d4",
   "metadata": {},
   "source": [
    "A **Convolutional Network (ConvNet or CNN)** is a class of deep neural networks primarily designed to process data with a known grid-like topology, such as images (2D grid of pixels) or time-series data (1D grid). ðŸ–¼ï¸\n",
    "The core idea is to automatically and efficiently learn spatial hierarchies of features from the raw input data.\n",
    "\n",
    "Key Components and Concepts\n",
    "CNNs gain their power from three types of specialized layers:\n",
    "\n",
    "1. **Convolutional Layer (The Core)**\n",
    "This layer applies a learnable set of filters (also called kernels) across the input image.\n",
    "Filter (Kernel): A small matrix (e.g., 3x3) that slides over the input (a process called convolution).\n",
    "Feature Mapping: At each location, the filter computes a dot product between its values and the corresponding values of the input. This result is stored in an output structure called a feature map.\n",
    "Feature Learning: Different filters learn to recognize different basic features, such as edges, textures, or corners, regardless of where they appear in the image (this is called translation invariance).\n",
    "\n",
    "3. **Pooling Layer (Downsampling)**\n",
    "This layer systematically reduces the spatial size (width and height) of the feature maps, which serves two main purposes:\n",
    "Reduces Complexity: Lowers the number of parameters and computation in the network.\n",
    "Increases Robustness: Makes the model less sensitive to minor shifts or distortions in the input image.\n",
    "Max Pooling: The most common form, which takes the maximum value from a small window (e.g., 2x2) in the feature map.\n",
    "\n",
    "\n",
    "5. **Fully Connected (FC) Layer (Classification)**\n",
    "After several stacked Conv and Pooling layers have extracted high-level features, the feature maps are flattened into a single vector. This vector is then fed into one or more standard fully connected layers, which perform the final classification (e.g., determining if the image is a cat, dog, or bird)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "0c762f64-7cba-4c4e-a9cc-8416b1df91be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9912422/9912422 [00:00<00:00, 10214828.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28881/28881 [00:00<00:00, 271850.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1648877/1648877 [00:00<00:00, 2677371.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4542/4542 [00:00<00:00, 998769.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Starting PyTorch CNN Training...\n",
      "Epoch 1/5, Loss: 0.1287\n",
      "Epoch 2/5, Loss: 0.0426\n",
      "Epoch 3/5, Loss: 0.0297\n",
      "Epoch 4/5, Loss: 0.0224\n",
      "Epoch 5/5, Loss: 0.0173\n",
      "-----------------------------------\n",
      "Test Set Results:\n",
      "Average Loss: 0.0296\n",
      "Accuracy: 9905/10000 (99.05%)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- 1. Define the CNN Architecture (The Model) ---\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # 1. Convolutional Layer 1\n",
    "        # Input: 1 channel (grayscale), Output: 32 feature maps, 3x3 kernel\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "\n",
    "        # 2. Convolutional Layer 2\n",
    "        # Input: 32 feature maps, Output: 64 feature maps, 3x3 kernel\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "\n",
    "        # 3. Pooling Layer\n",
    "        # Reduces spatial size (2x2 window)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 4. Fully Connected (Dense) Layer\n",
    "        # The size 7*7*64 comes from:\n",
    "        # 28x28 (input) -> 14x14 (after pool1) -> 7x7 (after pool2) * 64 channels\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 128)\n",
    "\n",
    "        # 5. Output Layer (10 classes for digits 0-9)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply Conv1 -> ReLU -> Pool\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "\n",
    "        # Apply Conv2 -> ReLU -> Pool\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Flatten the feature maps for the Dense layers\n",
    "        x = x.view(-1, 7 * 7 * 64)\n",
    "\n",
    "        # Apply FC1 -> ReLU\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # Output Layer (No softmax here; it's usually included in the loss function in PyTorch)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# --- 2. Data Loading and Preprocessing ---\n",
    "# Define a standard transformation: convert to tensor and normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # Standard MNIST mean and std\n",
    "])\n",
    "\n",
    "# Download and load the datasets\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "# Create DataLoaders (for batching and shuffling)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# --- 3. Model Initialization, Loss, and Optimizer ---\n",
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 5\n",
    "\n",
    "# --- 4. Training Loop ---\n",
    "print(\"Starting PyTorch CNN Training...\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train() # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: compute predicted outputs\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass: compute gradient of the loss w.r.t model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch}/{epochs}, Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "# --- 5. Evaluation Loop ---\n",
    "model.eval() # Set the model to evaluation mode\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad(): # Disable gradient calculations during evaluation\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()  # Sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset) / 1000 # Correct average loss calculation\n",
    "\n",
    "print(\"-\" * 35)\n",
    "print(f\"Test Set Results:\")\n",
    "print(f\"Average Loss: {test_loss:.4f}\")\n",
    "print(f\"Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28128c71-b626-4447-a70c-08a224599278",
   "metadata": {},
   "source": [
    "Key PyTorch Components\n",
    "torch.nn.Module: All PyTorch models inherit from this base class. The constructor (__init__) defines the layers, and the forward method defines how data flows through the network.\n",
    "\n",
    "nn.Conv2d: The convolutional layer. It expects input in the shape (Batch_Size, Channels, Height, Width).\n",
    "\n",
    "nn.MaxPool2d: The pooling layer for downsampling features.\n",
    "\n",
    "nn.Linear: A standard fully connected (dense) layer.\n",
    "\n",
    "F.relu: The ReLU activation function from the functional API (torch.nn.functional).\n",
    "\n",
    "x.view(-1, ...): This is the PyTorch equivalent of Flattening. It reshapes the 4D tensor (from Conv/Pool) into a 2D tensor (Batch_Size, Feature_Vector_Length) where -1 automatically infers the batch size.\n",
    "\n",
    "torch.no_grad(): Used during evaluation to save memory and computations by ensuring no gradients are calculated, as they aren't needed for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a87f5f5-fa9d-4de2-8958-c5d282dd6fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff23e452-45c8-4205-94c6-12224ce69bfc",
   "metadata": {},
   "source": [
    "**Mixture-of-Experts (MoE) Architecture**\n",
    "\n",
    "The Mixture-of-Experts (MoE) Architecture is a neural network design that replaces a standard, monolithic feed-forward layer with a sparsely activated system of multiple, smaller expert networks. This allows models to scale up the number of parameters significantly without proportionally increasing the computational cost (FLOPs) during training or inference. ðŸš€\n",
    "\n",
    "Core Concept: Divide and Conquer\n",
    "\n",
    "In a traditional large language model (LLM) Transformer block, every input token goes through the same feed-forward network. In an MoE layer, this process is replaced:\n",
    "* **Gate (Router) Network**: A small neural network (or linear layer) determines which of the multiple experts should process the current input token. It outputs a probability distribution over the experts.\n",
    "* **Sparse Activation**: Crucially, the gate is typically configured to select and activate only a sparse subset of the experts (e.g., the top 1 or top 2 experts) for any given input token.\n",
    "* **Experts**: These are independent, specialized feed-forward networks (MLPs). Each chosen expert processes the input, and its output is weighted by the probability assigned by the gate.\n",
    "* **Output Combination**: The outputs from the selected experts are combined (usually by a weighted sum) to form the final output of the MoE layer.\n",
    "\n",
    "The benefit is computational efficiency: a model might have billions of parameters across dozens of experts, but only a small fraction of those parameters are used for any single input, making the model faster and cheaper to run than a dense model of comparable size.\n",
    "\n",
    "Key References\n",
    "\n",
    "The concept of MoE dates back to the early 1990s, but its modern large-scale application in Transformer models is credited to the following key papers:\n",
    "* Jacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton, G. E. (1991). Adaptive mixtures of local experts. Neural computation.\n",
    "Focus: The foundational work introducing the Mixture-of-Experts idea with a routing mechanism, initially applied to regression.\n",
    "* Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Chen, W., ... & Dean, J. (2017). Outrageously large neural networks: The sparsely-gated Mixture-of-Experts layer. International Conference on Learning Representations (ICLR).\n",
    "Focus: The modern, large-scale application of MoE to sequence-to-sequence models, demonstrating massive parameter counts (trillions) with efficient sparsity. This paper showed how to make the gating mechanism differentiable and scalable.\n",
    "* Lepikhin, T., Xu, H., Chen, Y., Schoppe, E., Roberts, A., Lee, J., ... & Shazeer, N. (2021). GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding. International Conference on Machine Learning (ICML).\n",
    "Focus: Further scaling the MoE approach by integrating it into the Transformer's feed-forward layers and demonstrating its effectiveness in achieving state-of-the-art results for machine translation, setting the stage for models like GPT-4 and Google's Gemini.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcb4e27-2bd1-4823-9874-48067e0d6408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5790533-5667-4c51-9636-30f43149e47a",
   "metadata": {},
   "source": [
    "**Neuro-Symbolic Integration and Active Learning**\n",
    "    \n",
    "Neuro-Symbolic Integration is an approach in Artificial Intelligence that aims to combine the strengths of two historically separate paradigms: neural (connectionist) networks and symbolic (logic-based) AI. ðŸ§  ðŸ’¡\n",
    "\n",
    "The goal is to create more robust, transparent, and generalizable AI systems by melding the capabilities of each:\n",
    "\n",
    "| Paradigm | Strengths (Contribution to the Hybrid) | Weaknesses (Addressed by the Hybrid) |\n",
    "| --- | --- | --- |\n",
    "| Neural (Deep Learning) | Perception & Learning: Excellent at pattern recognition, handling noisy data, and learning complex features from raw data (e.g., images, text). | Reasoning & Explanation: Poor at logical deduction, high-level planning, and providing transparent, step-by-step explanations. |  \n",
    "| Symbolic (Logic & Rules) | Reasoning & Knowledge: Excellent at representing explicit knowledge, performing logical inference, and ensuring results adhere to known rules. | Perception & Robustness: Poor at learning from raw data, handling uncertainty, and adapting to novel, unseen inputs. |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Integration Approaches**\n",
    "\n",
    "Neuro-Symbolic AI attempts to bridge this gap, typically using one of the following main styles:\n",
    "* Symbolic-Guided Neural Networks: The symbolic component (e.g., a knowledge graph or a set of logical rules) is used to constrain or guide the training of a neural network, ensuring the network's predictions adhere to known facts or logical consistency.\n",
    "* Neural-Guided Symbolic Reasoning: The neural network component processes raw data (e.g., an image) and translates it into a structured, symbolic representation (e.g., \"There is a cat and a dog in the image\"). This symbolic output is then passed to a traditional reasoning system to perform complex deduction.\n",
    "* End-to-End Hybrid Models: The two components operate simultaneously, often iteratively passing information back and forth. For example, a neural network proposes a solution, which a symbolic module verifies and refines based on logical constraints.\n",
    "\n",
    "The ultimate aim of Neuro-Symbolic AI is to achieve the human-like ability to seamlessly switch between intuitive pattern recognition and deliberate, logical thought.\n",
    "   \n",
    "Here are a few key references for Neuro-Symbolic Integration (NSI), covering its foundational concepts and modern applications:\n",
    "\n",
    "**Foundational Concepts and Early Work**\n",
    "* Valiant, L. G. (1994). A theory of the learnable. Communications of the ACM, 37(11), 113-122.\n",
    "Focus: Though broad, Valiant's work on \"Neuroidal\" computation (a hybrid of neural and logical processing) is often cited as a conceptual precursor to modern NSI, arguing for systems that are both robust (like neural nets) and able to perform logical reasoning.\n",
    "\n",
    "**Key Modern Frameworks**\n",
    "* Besold, T. R., P. Moeller, M. P. G. Moos, & U. Schmid (2017). Neural-Symbolic Learning and Reasoning: A Survey and Perspective. In European Conference on Cognitive Modelling (ECCM).\n",
    "Focus: A comprehensive survey that categorized various NSI approaches, clearly outlining the challenges and future directions for the field.\n",
    "* Garnelo, M., & Shanahan, M. (2019). Relational Deep Learning. Foundations and Trends in Machine Learning, 12(4), 329-457.\n",
    "Focus: Discusses methods for integrating relational structures (symbolic concepts) into deep learning, particularly relevant to Neural-Symbolic reasoning on structured data like graphs.\n",
    "\n",
    "**Modern Practical Applications (Deep Learning Era)**\n",
    "* Manhaeve, R., DumanÄiÄ‡, S., Kimmig, A., & De Raedt, L. (2018). DeeProblog: Differentiable Probabilistic Logic Programming. Advances in Neural Information Processing Systems (NeurIPS).\n",
    "Focus: A prominent example of symbolic-guided neural networks. DeeProblog shows how to embed differentiable neural components into a probabilistic logic programming framework, allowing for end-to-end learning that respects logical constraints.\n",
    "* Mao, J., Gan, C., Kohli, P., Tenenbaum, J. B., & Wu, J. (2019). The Neuro-Symbolic Concept Learner. International Conference on Learning Representations (ICLR).\n",
    "Focus: A successful demonstration of neural-guided symbolic reasoning in computer vision. It uses a neural perception module to parse images into symbolic representations, which are then processed by a symbolic reasoning engine to answer complex visual questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f09a072-a2f1-4ffc-8821-4793d86a9088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec3e49c6-9e97-40ef-944d-ce6021eac9da",
   "metadata": {},
   "source": [
    "**Predictive and Generative World Models**\n",
    "\n",
    "World Models in AI are internal, learned simulations or representations of an agent's environment, inspired by human cognition. These models allow an agent (especially in Reinforcement Learning, or RL) to anticipate outcomes, plan future actions, and even learn complex tasks entirely within the simulated world, sometimes referred to as \"dreaming.\"\n",
    "\n",
    "The architecture of a World Model often consists of two key components that perform Predictive and Generative functions.\n",
    "\n",
    "**Brief Description**\n",
    "\n",
    "**Predictive World Models (Forward Models)**\n",
    "The primary role of the predictive component is to act as a forward model, forecasting the next state of the environment given the current state and a potential action.\n",
    "Function: Given $s_t$ (current state) and $a_t$ (action), it predicts $s_{t+1}$ (next state) and $r_{t+1}$ (next reward).\n",
    "Goal: To learn the dynamics and transition probabilities of the environment.\n",
    "Benefit: Enables planning; the agent can mentally \"roll out\" sequences of actions within the model to evaluate a policy before executing them in the real world, saving time and resources.\n",
    "\n",
    "**Generative World Models**\n",
    "The generative component focuses on creating and reconstructing the observations or states of the environment, often operating on a compressed, latent space representation.\n",
    "Function:\n",
    "Compression (Encoder/Vision Model): Converts high-dimensional input (like raw image pixels) into a low-dimensional, compressed latent vector ($z_t$).\n",
    "Generation (Decoder): Reconstructs the original high-dimensional observation from the latent vector.\n",
    "Goal: To provide a robust, compact, and essential representation of the state for the predictive model and the policy. It is often responsible for generating the \"dream\" environment for internal training.\n",
    "In the most successful architectures, a deep learning agent uses the Generative model to encode the sensory input ($V$) and the Predictive model to forecast the future in the compact latent space ($M$), while a small Controller ($C$) learns the optimal actions using these internal representations.\n",
    "\n",
    "**Key References**\n",
    "\n",
    "**Foundational World Model Architecture**:\n",
    "Ha, D., & Schmidhuber, J. (2018). World Models. arXiv preprint arXiv:1803.10122.\n",
    "Significance: The seminal paper that popularized the modern, tripartite World Model architecture (Vision/Encoder, Memory/Predictor, Controller) and famously showed an agent could be trained entirely inside its simulated \"dream\" environment.\n",
    "\n",
    "**Advanced Model-Based RL**:\n",
    "Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Wierstra, D., & Chen, R. (2019). Dreamer: Learning Behavior from High-Dimensional Observations with Deep Reinforcement Learning. International Conference on Learning Representations (ICLR).\n",
    "Significance: Introduced Dreamer, a highly successful framework that refined the World Model approach, enabling efficient learning in complex, high-dimensional visual environments by training an agent using imagined latent trajectories. Successive versions (DreamerV2, V3) further improved performance.\n",
    "\n",
    "**Early Vision and Prediction**:\n",
    "Lotter, W., Kreiman, G., & Cox, D. (2016). Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning. International Conference on Learning Representations (ICLR).\n",
    "Significance: Explored Predictive Coding, a concept inspired by neuroscience where the brain constantly tries to predict the next sensory input and only propagates the prediction error. This idea is a core mechanism in predictive world models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c004cb7-c563-4698-ae11-3e46f3337310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b39656d-352e-4ee8-b2d7-e7640d516740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
