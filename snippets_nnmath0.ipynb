{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23e6b1a8-93f4-41d0-ae9c-d2af6932d607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# NN maths simple example\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ninputs = 64\n",
    "noutputs = 2\n",
    "nhidden0 = 8\n",
    "\n",
    "ainputs = np.random.random((ninputs,))\n",
    "bhidden0 = np.array((nhidden0,))\n",
    "coutputs = np.array((noutputs,))\n",
    "\n",
    "winphidden0 = np.zeros((ninputs, nhidden0))\n",
    "whidden0out = np.zeros((nhidden0, noutputs))\n",
    "\n",
    "winphidden0idt = np.identity(nhidden0, like=winphidden0)\n",
    "whidden0outidt = np.identity(nhidden0, like=whidden0out)\n",
    "\n",
    "winphidden0idt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d7ec51f-3065-43fb-9342-0609f19068df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "print('.', end='\\r')\n",
    "\n",
    "time.sleep(1.5)\n",
    "\n",
    "print('+', end='\\r')\n",
    "print()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "412c440c-fe2e-4c57-b500-e57af6613e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number:  648024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "guessthis = [str(random.randint(0, 9)) for i in range(6)]\n",
    "\n",
    "time.sleep(0.8)\n",
    "for el in guessthis:\n",
    "    print(el, end='\\r')\n",
    "    time.sleep(1.2)\n",
    "    print(' ', end='\\r')\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "print()\n",
    "\n",
    "guessednumber = input(\"Enter the number: \")\n",
    "\n",
    "if guessednumber == ''.join(guessthis):\n",
    "    print('correct')\n",
    "else:\n",
    "    print('incorrect', ''.join(guessthis))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f34552e-4ca2-4afa-8818-563ede04711e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Add 1 to the 3. number.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number:  038134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "lenguess = 6\n",
    "\n",
    "guessthis = [str(random.randint(0, 9)) for i in range(lenguess)]\n",
    "\n",
    "time.sleep(0.8)\n",
    "for el in guessthis:\n",
    "    print(el, end='\\r')\n",
    "    time.sleep(1.2)\n",
    "    print(' ', end='\\r')\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "print()\n",
    "\n",
    "modifier = random.randint(1, 9)\n",
    "idxmodifier = random.randint(0, lenguess-1)\n",
    "\n",
    "print(f\"Add {modifier} to the {idxmodifier+1}. number.\")\n",
    "guessthis[idxmodifier] = str(int(guessthis[idxmodifier]) + modifier)\n",
    "\n",
    "guessednumber = input(\"Enter the number: \")\n",
    "\n",
    "if guessednumber == ''.join(guessthis):\n",
    "    print('correct')\n",
    "else:\n",
    "    print('incorrect', ''.join(guessthis))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f130f2-a6b7-41f6-93b2-b81d7643fa28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 1\n",
    "eval('x + 1')\n",
    "\n",
    "#eval('x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd5022c6-7e75-4ad9-b327-63499a6432d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.choice([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6e73f28-9440-400c-af40-c8c1ca5d04fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99929937]\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp, array, random, dot\n",
    "\n",
    "training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_set_outputs = array([[0, 1, 1, 0]]).T\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "synaptic_weights = 2 * random.random((3, 1)) - 1\n",
    "\n",
    "for iteration in range(1000):\n",
    "    output = 1 / (1 + exp(-(dot(training_set_inputs, synaptic_weights))))\n",
    "    synaptic_weights += dot(training_set_inputs.T, (training_set_outputs - output) * output * (1 - output))  # quadratic loss\n",
    "    \n",
    "print(1 / (1 + exp(-(dot(array([1, 0, 0]), synaptic_weights)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "cb2e1b67-7dfd-4019-9d3c-bf27ee3b1324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98404014]\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp, array, random, dot\n",
    "\n",
    "training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_set_outputs = array([[0, 1, 1, 0]]).T\n",
    "\n",
    "#random.seed(1)\n",
    "\n",
    "synaptic_weights = 2 * random.random((3, 1)) - 1\n",
    "\n",
    "for iteration in range(10):\n",
    "    output = 1 / (1 + exp(-(dot(training_set_inputs, synaptic_weights))))\n",
    "    synaptic_weights += dot(training_set_inputs.T, (training_set_outputs - output))  # cross entropy loss\n",
    "    \n",
    "print(1 / (1 + exp(-(dot(array([1, 0, 0]), synaptic_weights)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5cf63774-0c9d-4407-a088-136230c7fdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16077543]\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp, array, random, dot\n",
    "\n",
    "training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_set_outputs = array([[0, 1, 1, 0]]).T\n",
    "\n",
    "t_input = array([0, 0, 1])\n",
    "t_output = array([0])\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "synaptic_weights = 2 * random.random((3, 1)) - 1\n",
    "\n",
    "for iteration in range(100):\n",
    "    output = 1 / (1 + exp(-(dot(t_input, synaptic_weights))))\n",
    "    synaptic_weights += dot(t_input.T, array([1, 1, 1]) * (t_output - output) * output * (1 - output))\n",
    "    \n",
    "print(1 / (1 + exp(-(dot(array([1, 0, 0]), synaptic_weights)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "db741fb3-4324-4c9b-bcdb-966d9b676122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02232899]\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp, array, random, dot\n",
    "\n",
    "training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_set_outputs = array([[0, 1, 1, 0]]).T\n",
    "\n",
    "t_input = array([0, 0, 1])\n",
    "t_output = array([0])\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "synaptic_weights = 2 * random.random((3, 1)) - 1\n",
    "\n",
    "for iteration in range(100):\n",
    "    output = 1 / (1 + exp(-(dot(t_input, synaptic_weights))))\n",
    "    synaptic_weights += dot(t_input.T, array([1, 1, 1]) * (t_output - output))  # cross entropy loss\n",
    "    \n",
    "print(1 / (1 + exp(-(dot(array([1, 0, 0]), synaptic_weights)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a01829-3f5a-49a2-aee1-dde073248a51",
   "metadata": {},
   "source": [
    "$$c = \\sqrt{a^2 + b^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "5ce749cf-1e3b-4d2f-a0f5-3b37d7b1d6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0 0 1] [0] [0.05248225]\n",
      "1 [1 1 1] [1] [0.94024712]\n",
      "2 [1 0 1] [1] [0.95228751]\n",
      "3 [0 1 1] [0] [0.04184173]\n",
      "[0.99723251]\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp, array, random, dot, max\n",
    "\n",
    "training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_set_outputs = array([[0, 1, 1, 0]]).T\n",
    "\n",
    "etha = 1.\n",
    "synaptic_weights = 2 * random.random((3, 1)) - 1  # 3 weights + bias\n",
    "\n",
    "for iteration in range(100):\n",
    "    tmpdot = 0\n",
    "    i = random.randint(4)\n",
    "    t_input = array([training_set_inputs[i]])\n",
    "    t_output = training_set_outputs[i]\n",
    "    tmpdot += (dot(t_input, synaptic_weights))\n",
    "    output = 1 / (1 + exp(-tmpdot))\n",
    "    synaptic_weights += etha * t_input.T * (t_output - output)  # cross entropy loss\n",
    "    \n",
    "for i in range(4):\n",
    "    print(i, training_set_inputs[i], training_set_outputs[i], 1 / (1 + exp(-(dot(training_set_inputs[i], synaptic_weights)))))\n",
    "\n",
    "print(1 / (1 + exp(-(dot(array([1, 0, 0]), synaptic_weights)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ab1df5-b396-40f5-9a7c-a411d5472269",
   "metadata": {},
   "source": [
    "**Core Concepts Illustrated**\n",
    "\n",
    "The code demonstrates the three fundamental steps of the attention mechanism:\n",
    "\n",
    "**Similarity Scores (Dot Product)**: The np.dot(keys, query) step calculates how relevant each Key is to the Query. A larger dot product means greater similarity.\n",
    "\n",
    "**Attention Weights (Softmax)**: The Softmax function $\\left(\\frac{e^{score_i}}{\\sum_j e^{score_j}}\\right)$ is applied to the raw scores. This step normalizes the scores into a probability distribution, ensuring all weights are between 0 and 1 and sum up to 1. These weights indicate how much to pay attention to each value.\n",
    "\n",
    "**Context Vector (Weighted Sum)**: The final output is computed by multiplying each Value by its corresponding Weight and summing the results ($\\sum_{i} W_i \\cdot V_i$). The resulting Context Vector is a representation that captures the most relevant information from the Values, steered by the Query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "a7f905b9-4e61-46d9-b543-4cc6b4fb2558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Simple Attention Example ---\n",
      "Query: [1.  0.5 0.1 0. ]\n",
      "-----------------------------------\n",
      "Keys (K):\n",
      "[[1.1 0.4 0.  0.1]\n",
      " [0.2 0.8 0.9 0.7]\n",
      " [0.9 0.6 0.1 0. ]]\n",
      "Values (V):\n",
      "[[5. 5. 5. 5.]\n",
      " [1. 1. 1. 1.]\n",
      " [3. 3. 3. 3.]]\n",
      "-----------------------------------\n",
      "Attention Weights (W):\n",
      "[0.40695369 0.22111864 0.37192767]\n",
      "-----------------------------------\n",
      "Context Vector (Output):\n",
      "[3.3716701 3.3716701 3.3716701 3.3716701]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simple_attention(query, keys, values):\n",
    "    \"\"\"\n",
    "    A simplified, illustrative example of the attention mechanism.\n",
    "\n",
    "    Args:\n",
    "        query (np.ndarray): The vector representing what we're looking for.\n",
    "        keys (np.ndarray): A set of vectors to compare the query against.\n",
    "        values (np.ndarray): The values associated with the keys.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The context vector (weighted sum of values).\n",
    "    \"\"\"\n",
    "    # 1. Calculate Similarity Scores (e.g., Dot Product)\n",
    "    # The dot product measures how related the query is to each key.\n",
    "    # Scores shape will be (num_keys,)\n",
    "    scores = np.dot(keys, query)\n",
    "\n",
    "    # 2. Normalize Scores to get Attention Weights (using Softmax)\n",
    "    # Softmax converts scores into probabilities (weights) that sum to 1.\n",
    "    # The higher the score, the higher the weight.\n",
    "    # Weights shape will be (num_keys,)\n",
    "    # np.exp(scores - np.max(scores)) is used for numerical stability\n",
    "    exp_scores = np.exp(scores - np.max(scores))\n",
    "    weights = exp_scores / np.sum(exp_scores)\n",
    "\n",
    "    # 3. Compute Context Vector (Weighted Sum of Values)\n",
    "    # The final output (context vector) is the sum of (weight * value).\n",
    "    # This vector represents the \"attended\" information.\n",
    "    # values shape is (num_keys, value_dim), weights shape is (num_keys,)\n",
    "    # This results in (value_dim,)\n",
    "    context_vector = np.dot(weights, values)\n",
    "\n",
    "    return context_vector, weights\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# Define the input data:\n",
    "# Here, the dimension for Q, K, V is 4. We have 3 key-value pairs.\n",
    "# Imagine Q is a word, and K/V are surrounding words.\n",
    "\n",
    "# The Query (Q): What we are looking for (e.g., current word's embedding)\n",
    "query = np.array([1.0, 0.5, 0.1, 0.0]) # Shape (4,)\n",
    "\n",
    "# The Keys (K): What we compare against (e.g., other words' embeddings)\n",
    "keys = np.array([\n",
    "    [1.1, 0.4, 0.0, 0.1],  # Key 1: Very similar to Q\n",
    "    [0.2, 0.8, 0.9, 0.7],  # Key 2: Less similar\n",
    "    [0.9, 0.6, 0.1, 0.0]   # Key 3: Similar\n",
    "]) # Shape (3, 4)\n",
    "\n",
    "# The Values (V): The actual information to be aggregated\n",
    "# (In self-attention, K and V are often the same, but they don't have to be)\n",
    "values = np.array([\n",
    "    [5.0, 5.0, 5.0, 5.0],  # Value 1 (High relevance expected)\n",
    "    [1.0, 1.0, 1.0, 1.0],  # Value 2 (Low relevance expected)\n",
    "    [3.0, 3.0, 3.0, 3.0]   # Value 3 (Medium relevance expected)\n",
    "]) # Shape (3, 4)\n",
    "\n",
    "# Run the attention mechanism\n",
    "context_vector, weights = simple_attention(query, keys, values)\n",
    "\n",
    "print(\"--- Simple Attention Example ---\")\n",
    "print(f\"Query: {query}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(f\"Keys (K):\\n{keys}\")\n",
    "print(f\"Values (V):\\n{values}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(f\"Attention Weights (W):\\n{weights}\")\n",
    "# Expected: weights[0] and weights[2] should be high because Key 1 and Key 3 \n",
    "# are more similar to the Query than Key 2.\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(f\"Context Vector (Output):\\n{context_vector}\")\n",
    "# Expected: The output should be a blend of the values, dominated by Value 1 \n",
    "# and Value 3 due to their higher weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276c9e9b-76f4-4378-9ec3-f16bf430208e",
   "metadata": {},
   "source": [
    "**Hierarchical Attention** is an extension of the standard attention mechanism used in deep learning, primarily designed to process information with an inherent, nested structure, such as documents, which are composed of sentences, which are in turn composed of words.\n",
    "\n",
    "It works by applying the attention mechanism at multiple levels of granularity:\n",
    "\n",
    "### 1. The Concept\n",
    "\n",
    "Instead of calculating a single context vector for the entire input sequence, Hierarchical Attention applies attention sequentially:\n",
    "\n",
    "1.  **Lower Level (e.g., Word-to-Sentence Attention):** It first processes the elements at the lowest level (e.g., words in a sentence) and computes an intermediate representation (e.g., a **sentence vector**) by selectively weighing the importance of individual words.\n",
    "2.  **Higher Level (e.g., Sentence-to-Document Attention):** These intermediate representations (sentence vectors) are then processed at the higher level. The mechanism selectively weighs the importance of these elements (sentences) to form the final, high-level output (e.g., a **document vector** or final prediction).\n",
    "\n",
    "This multi-layered approach allows the model to first figure out which **words are important within a sentence** and then figure out which **sentences are important within the document**, leading to more robust and context-aware representations.\n",
    "\n",
    "***\n",
    "\n",
    "### 2. Key References\n",
    "\n",
    "The seminal work introducing and popularizing this specific architecture is:\n",
    "\n",
    "* **Yang, Z., Yang, D., Cohen, S., Salakhutdinov, R., & Hovy, E. (2016). Hierarchical Attention Networks for Document Classification.** *Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL).*\n",
    "    * **Focus:** This paper introduced the **Hierarchical Attention Network (HAN)** model, specifically for document classification, which is the standard implementation of hierarchical attention.\n",
    "\n",
    "Other related and foundational works on attention include:\n",
    "\n",
    "* **Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate.** *International Conference on Learning Representations (ICLR).*\n",
    "    * **Focus:** The original introduction of the **attention mechanism** (additive attention) in the context of sequence-to-sequence models (Neural Machine Translation).\n",
    "\n",
    "* **Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention Is All You Need.** *Advances in Neural Information Processing Systems (NeurIPS).*\n",
    "    * **Focus:** Introduced the **Transformer** architecture and the **Self-Attention** (or scaled dot-product attention) mechanism, which underpins modern models and is often adapted for hierarchical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd80d684-8f13-413f-be70-0e30ae546bf6",
   "metadata": {},
   "source": [
    "üí° BPE Overview\n",
    "Byte Pair Encoding is a subword tokenization technique used to balance between character-level and word-level models.\n",
    "\n",
    "Goal: Efficiently handle a large vocabulary and the \"out-of-vocabulary\" (OOV) problem, especially common with large language models (LLMs).\n",
    "\n",
    "Process:\n",
    "\n",
    "Start by treating every character as an initial token.\n",
    "\n",
    "Iteratively find the most frequent adjacent pair of tokens (which can be characters or already-merged subwords).\n",
    "\n",
    "Replace all occurrences of that pair with a new, single token.\n",
    "\n",
    "Repeat the process for a fixed number of merge operations or until no more frequent pairs exist.\n",
    "\n",
    "The provided Python code illustrates these steps by tracking tokens as space-separated strings and using a Counter to find the most frequent pairs.\n",
    "\n",
    "This video provides an excellent, detailed explanation of how Byte Pair Encoding works, which is highly relevant to understanding the code. Lecture 8: The GPT Tokenizer: Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "ce08b9d8-847a-402b-aea8-efb8aa48830b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BPE training on: 'low lower newest newest newest wide wide' for 5 merges.\n",
      "\n",
      "Initial Vocabulary: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w']\n",
      "--------------------\n",
      "Merge 1: Merged ('w', 'e') into 'we'\n",
      "Current Tokens (word: frequency): {'l o w': 1, 'l o we r': 1, 'n e we s t': 3, 'w i d e': 2}\n",
      "Current Vocabulary: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', 'we']\n",
      "--------------------\n",
      "Merge 2: Merged ('n', 'e') into 'ne'\n",
      "Current Tokens (word: frequency): {'l o w': 1, 'l o we r': 1, 'ne we s t': 3, 'w i d e': 2}\n",
      "Current Vocabulary: ['d', 'e', 'i', 'l', 'ne', 'o', 'r', 's', 't', 'w', 'we']\n",
      "--------------------\n",
      "Merge 3: Merged ('ne', 'we') into 'newe'\n",
      "Current Tokens (word: frequency): {'l o w': 1, 'l o we r': 1, 'newe s t': 3, 'w i d e': 2}\n",
      "Current Vocabulary: ['d', 'e', 'i', 'l', 'newe', 'o', 'r', 's', 't', 'w', 'we']\n",
      "--------------------\n",
      "Merge 4: Merged ('newe', 's') into 'newes'\n",
      "Current Tokens (word: frequency): {'l o w': 1, 'l o we r': 1, 'newes t': 3, 'w i d e': 2}\n",
      "Current Vocabulary: ['d', 'e', 'i', 'l', 'newes', 'o', 'r', 't', 'w', 'we']\n",
      "--------------------\n",
      "Merge 5: Merged ('newes', 't') into 'newest'\n",
      "Current Tokens (word: frequency): {'l o w': 1, 'l o we r': 1, 'newest': 3, 'w i d e': 2}\n",
      "Current Vocabulary: ['d', 'e', 'i', 'l', 'newest', 'o', 'r', 'w', 'we']\n",
      "--------------------\n",
      "Final Learned Merges: {('w', 'e'): 'we', ('n', 'e'): 'ne', ('ne', 'we'): 'newe', ('newe', 's'): 'newes', ('newes', 't'): 'newest'}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_pair_counts(tokens):\n",
    "    \"\"\"Count the frequency of adjacent pairs in a list of tokens.\"\"\"\n",
    "    counts = Counter()\n",
    "    for word, freq in tokens.items():\n",
    "        token_list = word.split(' ')\n",
    "        for i in range(len(token_list) - 1):\n",
    "            pair = (token_list[i], token_list[i+1])\n",
    "            counts[pair] += freq\n",
    "    return counts\n",
    "\n",
    "def merge_pair(tokens, pair, new_token):\n",
    "    \"\"\"Replace all occurrences of a pair in the token list with a new token.\"\"\"\n",
    "    merged_tokens = {}\n",
    "    bigram_str = ' '.join(pair)\n",
    "    new_str = new_token\n",
    "    \n",
    "    for word, freq in tokens.items():\n",
    "        # Replace the pair with the new token, maintaining token separation\n",
    "        # using a simple string replacement on the space-separated tokens.\n",
    "        new_word = word.replace(bigram_str, new_str)\n",
    "        merged_tokens[new_word] = freq\n",
    "    return merged_tokens\n",
    "\n",
    "def train_bpe(corpus, num_merges):\n",
    "    \"\"\"Main BPE training function.\"\"\"\n",
    "    \n",
    "    # 1. Initial Tokens: Split each word into characters and track word frequencies.\n",
    "    # The dictionary keys represent space-separated tokens (e.g., \"h e l l o\"), \n",
    "    # and the values are the frequencies of the *words* in the corpus.\n",
    "    tokens = Counter(corpus.split())\n",
    "    tokens = {' '.join(list(k)) : v for k, v in tokens.items()}\n",
    "    \n",
    "    print(\"Initial Vocabulary:\", sorted(list(set(' '.join(tokens.keys()).split(' ')))) )\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    merges = {}\n",
    "    for i in range(num_merges):\n",
    "        # 2. Find the most frequent pair\n",
    "        pair_counts = get_pair_counts(tokens)\n",
    "        if not pair_counts:\n",
    "            break\n",
    "            \n",
    "        # Get the most frequent pair\n",
    "        best_pair = pair_counts.most_common(1)[0][0]\n",
    "        \n",
    "        # 3. Create a new token (merge)\n",
    "        new_token = ''.join(best_pair)\n",
    "        merges[best_pair] = new_token\n",
    "        \n",
    "        # 4. Apply the merge to the corpus\n",
    "        tokens = merge_pair(tokens, best_pair, new_token)\n",
    "        \n",
    "        print(f\"Merge {i+1}: Merged {best_pair} into '{new_token}'\")\n",
    "        print(\"Current Tokens (word: frequency):\", tokens)\n",
    "        print(\"Current Vocabulary:\", sorted(list(set(' '.join(tokens.keys()).split(' ')))) )\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "    return merges\n",
    "\n",
    "# --- Example Usage ---\n",
    "corpus = \"low lower newest newest newest wide wide\"\n",
    "num_merges = 5\n",
    "\n",
    "print(f\"Starting BPE training on: '{corpus}' for {num_merges} merges.\\n\")\n",
    "learned_merges = train_bpe(corpus, num_merges)\n",
    "\n",
    "print(\"Final Learned Merges:\", learned_merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf9d307-eb75-478f-a314-b70cdae67a8e",
   "metadata": {},
   "source": [
    "the key question here is the levels of attention and concepts: byte pairs -> words -> sentences -> documents are obvious, but I think there is a concept layer between sentences and documents. How, then, about higher and more abstract concept layers? Can this be done dynamically? What is the separation criterion? I imagine someting like information gain as used as a split criterion in random trees.\n",
    "\n",
    "\n",
    "The Information Gain (IG) criterion is a key metric used in training Decision Trees to decide which attribute (feature) to split the data on at each node. Its goal is to maximize the purity of the resulting child nodes, meaning the children should ideally contain data points belonging to a single class.üå≥ Information Gain ExplainedInformation Gain is calculated by finding the difference between the entropy of the parent node (before the split) and the weighted average of the entropy of the child nodes (after the split).$$IG(\\text{S}, \\text{A}) = \\text{Entropy}(\\text{S}) - \\sum_{v \\in \\text{Values}(\\text{A})} \\frac{|\\text{S}_v|}{|\\text{S}|} \\text{Entropy}(\\text{S}_v)$$Where:$S$ is the set of data at the parent node.$A$ is the attribute being considered for the split.$\\text{Values}(A)$ is the set of possible values for attribute $A$.$S_v$ is the subset of $S$ where attribute $A$ has value $v$.$|S|$ and $|S_v|$ are the number of elements in the respective sets.Entropy ($\\text{Entropy}(S)$): This is a measure of the impurity or randomness in the data. Low entropy means the data is mostly of one class (pure), and high entropy means the classes are mixed (impure). The goal of a split is to reduce this value.$$\\text{Entropy}(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i)$$Where $c$ is the number of classes, and $p_i$ is the proportion of samples belonging to class $i$.Information Gain: The split that results in the highest Information Gain is chosen because it is the most effective at reducing impurity and making the resulting subsets (children) purer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "96491425-96b4-4aa8-864f-059199e8a309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent Entropy: 1.000\n",
      "Weighted Child Entropy: 0.722\n",
      "Information Gain: 0.278\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Define Entropy Function\n",
    "def calculate_entropy(class_counts):\n",
    "    \"\"\"Calculates entropy given a list of class counts.\"\"\"\n",
    "    total_samples = sum(class_counts)\n",
    "    if total_samples == 0:\n",
    "        return 0\n",
    "    \n",
    "    entropy = 0\n",
    "    for count in class_counts:\n",
    "        # p_i is the proportion of the class\n",
    "        p_i = count / total_samples\n",
    "        if p_i > 0:\n",
    "            # - p_i * log2(p_i)\n",
    "            entropy -= p_i * np.log2(p_i)\n",
    "    return entropy\n",
    "\n",
    "# 2. Sample Data (Parent Node)\n",
    "# The data has two classes: [Class A, Class B]\n",
    "parent_counts = [5, 5]  # 5 A's and 5 B's (Total 10)\n",
    "entropy_parent = calculate_entropy(parent_counts)\n",
    "# Entropy here will be 1.0 (maximum impurity)\n",
    "\n",
    "# 3. Define a Hypothetical Split (Feature 'Color' with values 'Red' and 'Blue')\n",
    "# Split 1 (Red): [4 A's, 1 B] -> 5 samples\n",
    "split_red_counts = [4, 1]\n",
    "entropy_red = calculate_entropy(split_red_counts)\n",
    "\n",
    "# Split 2 (Blue): [1 A, 4 B's] -> 5 samples\n",
    "split_blue_counts = [1, 4]\n",
    "entropy_blue = calculate_entropy(split_blue_counts)\n",
    "\n",
    "# 4. Calculate Weighted Average Entropy of Children\n",
    "total_samples = sum(parent_counts)\n",
    "weighted_child_entropy = (\n",
    "    (sum(split_red_counts) / total_samples) * entropy_red +\n",
    "    (sum(split_blue_counts) / total_samples) * entropy_blue\n",
    ")\n",
    "\n",
    "# 5. Calculate Information Gain\n",
    "information_gain = entropy_parent - weighted_child_entropy\n",
    "\n",
    "print(f\"Parent Entropy: {entropy_parent:.3f}\")\n",
    "print(f\"Weighted Child Entropy: {weighted_child_entropy:.3f}\")\n",
    "print(f\"Information Gain: {information_gain:.3f}\") \n",
    "\n",
    "# The highest IG value indicates the best split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "7a95e5a7-ba56-4f44-8259-b9dbd9ebef87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Simple Attention Example ---\n",
      "Query: [1.  0.5 0.1 0. ]\n",
      "-----------------------------------\n",
      "Keys (K):\n",
      "[[1.1 0.4 0.  0.1]\n",
      " [0.2 0.8 0.9 0.7]\n",
      " [0.9 0.6 0.1 0. ]]\n",
      "Values (V):\n",
      "[[5. 5. 5. 5.]\n",
      " [1. 1. 1. 1.]\n",
      " [3. 3. 3. 3.]]\n",
      "-----------------------------------\n",
      "Attention Weights (W):\n",
      "[0.40695369 0.22111864 0.37192767]\n",
      "-----------------------------------\n",
      "Context Vector (Output):\n",
      "[3.3716701 3.3716701 3.3716701 3.3716701]\n"
     ]
    }
   ],
   "source": [
    "# Hierarchical Attention\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def simple_attention(query, keys, values):\n",
    "    \"\"\"\n",
    "    A simplified, illustrative example of the attention mechanism.\n",
    "\n",
    "    Args:\n",
    "        query (np.ndarray): The vector representing what we're looking for.\n",
    "        keys (np.ndarray): A set of vectors to compare the query against.\n",
    "        values (np.ndarray): The values associated with the keys.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The context vector (weighted sum of values).\n",
    "    \"\"\"\n",
    "    # 1. Calculate Similarity Scores (e.g., Dot Product)\n",
    "    # The dot product measures how related the query is to each key.\n",
    "    # Scores shape will be (num_keys,)\n",
    "    scores = np.dot(keys, query)\n",
    "\n",
    "    # 2. Normalize Scores to get Attention Weights (using Softmax)\n",
    "    # Softmax converts scores into probabilities (weights) that sum to 1.\n",
    "    # The higher the score, the higher the weight.\n",
    "    # Weights shape will be (num_keys,)\n",
    "    # np.exp(scores - np.max(scores)) is used for numerical stability\n",
    "    exp_scores = np.exp(scores - np.max(scores))\n",
    "    weights = exp_scores / np.sum(exp_scores)\n",
    "\n",
    "    # 3. Compute Context Vector (Weighted Sum of Values)\n",
    "    # The final output (context vector) is the sum of (weight * value).\n",
    "    # This vector represents the \"attended\" information.\n",
    "    # values shape is (num_keys, value_dim), weights shape is (num_keys,)\n",
    "    # This results in (value_dim,)\n",
    "    context_vector = np.dot(weights, values)\n",
    "\n",
    "    return context_vector, weights\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "\n",
    "\n",
    "#  here we have to expand: the issue is not the attention mechanism, its the hierarchy definition and aggregation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the input data:\n",
    "# Here, the dimension for Q, K, V is 4. We have 3 key-value pairs.\n",
    "# Imagine Q is a word, and K/V are surrounding words.\n",
    "\n",
    "# The Query (Q): What we are looking for (e.g., current word's embedding)\n",
    "query = np.array([1.0, 0.5, 0.1, 0.0]) # Shape (4,)\n",
    "\n",
    "# The Keys (K): What we compare against (e.g., other words' embeddings)\n",
    "keys = np.array([\n",
    "    [1.1, 0.4, 0.0, 0.1],  # Key 1: Very similar to Q\n",
    "    [0.2, 0.8, 0.9, 0.7],  # Key 2: Less similar\n",
    "    [0.9, 0.6, 0.1, 0.0]   # Key 3: Similar\n",
    "]) # Shape (3, 4)\n",
    "\n",
    "# The Values (V): The actual information to be aggregated\n",
    "# (In self-attention, K and V are often the same, but they don't have to be)\n",
    "values = np.array([\n",
    "    [5.0, 5.0, 5.0, 5.0],  # Value 1 (High relevance expected)\n",
    "    [1.0, 1.0, 1.0, 1.0],  # Value 2 (Low relevance expected)\n",
    "    [3.0, 3.0, 3.0, 3.0]   # Value 3 (Medium relevance expected)\n",
    "]) # Shape (3, 4)\n",
    "\n",
    "# Run the attention mechanism\n",
    "context_vector, weights = simple_attention(query, keys, values)\n",
    "\n",
    "print(\"--- Simple Attention Example ---\")\n",
    "print(f\"Query: {query}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(f\"Keys (K):\\n{keys}\")\n",
    "print(f\"Values (V):\\n{values}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(f\"Attention Weights (W):\\n{weights}\")\n",
    "# Expected: weights[0] and weights[2] should be high because Key 1 and Key 3 \n",
    "# are more similar to the Query than Key 2.\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(f\"Context Vector (Output):\\n{context_vector}\")\n",
    "# Expected: The output should be a blend of the values, dominated by Value 1 \n",
    "# and Value 3 due to their higher weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7cb454-fdd1-458b-856f-63e2247bc5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15fbabda-c67c-4831-a39d-0bfda59761d4",
   "metadata": {},
   "source": [
    "A **Convolutional Network (ConvNet or CNN)** is a class of deep neural networks primarily designed to process data with a known grid-like topology, such as images (2D grid of pixels) or time-series data (1D grid). üñºÔ∏è\n",
    "The core idea is to automatically and efficiently learn spatial hierarchies of features from the raw input data.\n",
    "\n",
    "Key Components and Concepts\n",
    "CNNs gain their power from three types of specialized layers:\n",
    "\n",
    "1. **Convolutional Layer (The Core)**\n",
    "This layer applies a learnable set of filters (also called kernels) across the input image.\n",
    "Filter (Kernel): A small matrix (e.g., 3x3) that slides over the input (a process called convolution).\n",
    "Feature Mapping: At each location, the filter computes a dot product between its values and the corresponding values of the input. This result is stored in an output structure called a feature map.\n",
    "Feature Learning: Different filters learn to recognize different basic features, such as edges, textures, or corners, regardless of where they appear in the image (this is called translation invariance).\n",
    "\n",
    "3. **Pooling Layer (Downsampling)**\n",
    "This layer systematically reduces the spatial size (width and height) of the feature maps, which serves two main purposes:\n",
    "Reduces Complexity: Lowers the number of parameters and computation in the network.\n",
    "Increases Robustness: Makes the model less sensitive to minor shifts or distortions in the input image.\n",
    "Max Pooling: The most common form, which takes the maximum value from a small window (e.g., 2x2) in the feature map.\n",
    "\n",
    "\n",
    "5. **Fully Connected (FC) Layer (Classification)**\n",
    "After several stacked Conv and Pooling layers have extracted high-level features, the feature maps are flattened into a single vector. This vector is then fed into one or more standard fully connected layers, which perform the final classification (e.g., determining if the image is a cat, dog, or bird)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "0c762f64-7cba-4c4e-a9cc-8416b1df91be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9912422/9912422 [00:00<00:00, 10214828.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28881/28881 [00:00<00:00, 271850.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1648877/1648877 [00:00<00:00, 2677371.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4542/4542 [00:00<00:00, 998769.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Starting PyTorch CNN Training...\n",
      "Epoch 1/5, Loss: 0.1287\n",
      "Epoch 2/5, Loss: 0.0426\n",
      "Epoch 3/5, Loss: 0.0297\n",
      "Epoch 4/5, Loss: 0.0224\n",
      "Epoch 5/5, Loss: 0.0173\n",
      "-----------------------------------\n",
      "Test Set Results:\n",
      "Average Loss: 0.0296\n",
      "Accuracy: 9905/10000 (99.05%)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- 1. Define the CNN Architecture (The Model) ---\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # 1. Convolutional Layer 1\n",
    "        # Input: 1 channel (grayscale), Output: 32 feature maps, 3x3 kernel\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "\n",
    "        # 2. Convolutional Layer 2\n",
    "        # Input: 32 feature maps, Output: 64 feature maps, 3x3 kernel\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "\n",
    "        # 3. Pooling Layer\n",
    "        # Reduces spatial size (2x2 window)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 4. Fully Connected (Dense) Layer\n",
    "        # The size 7*7*64 comes from:\n",
    "        # 28x28 (input) -> 14x14 (after pool1) -> 7x7 (after pool2) * 64 channels\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 128)\n",
    "\n",
    "        # 5. Output Layer (10 classes for digits 0-9)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply Conv1 -> ReLU -> Pool\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "\n",
    "        # Apply Conv2 -> ReLU -> Pool\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Flatten the feature maps for the Dense layers\n",
    "        x = x.view(-1, 7 * 7 * 64)\n",
    "\n",
    "        # Apply FC1 -> ReLU\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # Output Layer (No softmax here; it's usually included in the loss function in PyTorch)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# --- 2. Data Loading and Preprocessing ---\n",
    "# Define a standard transformation: convert to tensor and normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # Standard MNIST mean and std\n",
    "])\n",
    "\n",
    "# Download and load the datasets\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "# Create DataLoaders (for batching and shuffling)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# --- 3. Model Initialization, Loss, and Optimizer ---\n",
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 5\n",
    "\n",
    "# --- 4. Training Loop ---\n",
    "print(\"Starting PyTorch CNN Training...\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train() # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: compute predicted outputs\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass: compute gradient of the loss w.r.t model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch}/{epochs}, Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "# --- 5. Evaluation Loop ---\n",
    "model.eval() # Set the model to evaluation mode\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad(): # Disable gradient calculations during evaluation\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()  # Sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset) / 1000 # Correct average loss calculation\n",
    "\n",
    "print(\"-\" * 35)\n",
    "print(f\"Test Set Results:\")\n",
    "print(f\"Average Loss: {test_loss:.4f}\")\n",
    "print(f\"Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28128c71-b626-4447-a70c-08a224599278",
   "metadata": {},
   "source": [
    "Key PyTorch Components\n",
    "torch.nn.Module: All PyTorch models inherit from this base class. The constructor (__init__) defines the layers, and the forward method defines how data flows through the network.\n",
    "\n",
    "nn.Conv2d: The convolutional layer. It expects input in the shape (Batch_Size, Channels, Height, Width).\n",
    "\n",
    "nn.MaxPool2d: The pooling layer for downsampling features.\n",
    "\n",
    "nn.Linear: A standard fully connected (dense) layer.\n",
    "\n",
    "F.relu: The ReLU activation function from the functional API (torch.nn.functional).\n",
    "\n",
    "x.view(-1, ...): This is the PyTorch equivalent of Flattening. It reshapes the 4D tensor (from Conv/Pool) into a 2D tensor (Batch_Size, Feature_Vector_Length) where -1 automatically infers the batch size.\n",
    "\n",
    "torch.no_grad(): Used during evaluation to save memory and computations by ensuring no gradients are calculated, as they aren't needed for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a87f5f5-fa9d-4de2-8958-c5d282dd6fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
