{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17a9b82c-dcd9-448b-955a-2850c829e995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1484765089981492\n",
      "0.3826105602496096\n",
      "0.17185738520077565\n",
      "0.11189389859934662\n",
      "0.08484081908566549\n",
      "0.06873187737629899\n",
      "0.05780303777302742\n",
      "0.049851715200578295\n",
      "0.04380145114126244\n",
      "0.03904595454490739\n",
      "[0.14480521]\n",
      "[0.95041035]\n"
     ]
    }
   ],
   "source": [
    "# https://stackabuse.com/creating-a-neural-network-from-scratch-in-python/\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_der(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "feature_set = np.array([[0,1,0], [0,0,1], [1,0,0], [1,1,0], [1,1,1]])\n",
    "labels = np.array([[1,0,0,1,1]])\n",
    "labels = labels.reshape(5,1)\n",
    "\n",
    "np.random.seed(42)\n",
    "weights = np.random.rand(3,1)\n",
    "bias = np.random.rand(1)\n",
    "lr = 0.09\n",
    "\n",
    "# forward & backward\n",
    "for epoch in range(500):\n",
    "    x = feature_set\n",
    "\n",
    "    # feedforward\n",
    "    z = sigmoid(np.dot(x, weights) + bias)\n",
    "\n",
    "    # backpropagation step 1\n",
    "    error = z - labels\n",
    "    if epoch % 50 == 0: print(error.sum())\n",
    "    # backpropagation step 2\n",
    "    dpred_dz = sigmoid_der(z)\n",
    "\n",
    "    z_delta = error * dpred_dz\n",
    "    x = feature_set.T\n",
    "    weights -= lr * np.dot(x, z_delta)\n",
    "\n",
    "    for num in z_delta:\n",
    "        bias -= lr * num\n",
    "\n",
    "\n",
    "single_point = np.array([1,0,0])\n",
    "result = sigmoid(np.dot(single_point, weights) + bias)\n",
    "print(result)\n",
    "\n",
    "single_point = np.array([0,1,0])\n",
    "result = sigmoid(np.dot(single_point, weights) + bias)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b3b370-52bc-4dc3-8fc7-04377f1726f8",
   "metadata": {},
   "source": [
    "## Andrej Karpathy: Let's build GPT: from scratch, in code, spelled out\n",
    "\n",
    "followed the code from the vid with added comments and explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "016155bd-f6f2-4ab2-9528-478ea15dd9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 17, 0, 26, 16, 13, 24, 13]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "# https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "# https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=M5CvobiQ0pLr\n",
    "\n",
    "text = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars)}\n",
    "itos = { i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[e] for e in l])\n",
    "\n",
    "print(encode('hi there'))\n",
    "print(decode(encode('hi there')))\n",
    "\n",
    "# tiktoken for subword encoding\n",
    "# tokens are sometimes referred to as nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99459851-4c05-43bd-9b10-8834a3087483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc11e3b7-5776-4a7b-95cf-139d48a0f48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input_sp0.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:200])\n",
    "\n",
    "n = int(0.9*len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e73693f4-32e6-492e-a12b-3106033e78c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we just train the transformer with chunks at a time (blocksize, context length,...)\n",
    "block_size = 8  # multiple examples packed into this (7 here) as transformer trains on each position\n",
    "train_data[:block_size+1]  # chunks are used not only for computational efficiency but also to make the transformer see context over various lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9921ab4b-3258-415d-9f03-8b0a99f3412e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is 47\n",
      "when input is tensor([18, 47]) the target is 56\n",
      "when input is tensor([18, 47, 56]) the target is 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):  # time dimension\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbb6c782-8895-458e-9276-a167371fa856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]], device='cuda:0')\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]], device='cuda:0')\n",
      "---------\n",
      "when input is [[24, 43, 58, 5, 57, 1, 46, 43]] the target is 43\n",
      "when input is [[24, 43, 58, 5, 57, 1, 46, 43], [44, 53, 56, 1, 58, 46, 39, 58]] the target is 58\n",
      "when input is [[24, 43, 58, 5, 57, 1, 46, 43], [44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1]] the target is 5\n",
      "when input is [[24, 43, 58, 5, 57, 1, 46, 43], [44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 57\n",
      "when input is [[24, 43, 58, 5, 57, 1, 46, 43], [44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 1\n",
      "when input is [[24, 43, 58, 5, 57, 1, 46, 43], [44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 46\n",
      "when input is [[24, 43, 58, 5, 57, 1, 46, 43], [44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 43\n",
      "when input is [[24, 43, 58, 5, 57, 1, 46, 43], [44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 39\n",
      "when input is [] the target is 53\n",
      "when input is [[44, 53, 56, 1, 58, 46, 39, 58]] the target is 56\n",
      "when input is [[44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1]] the target is 1\n",
      "when input is [[44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 58\n",
      "when input is [[44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 46\n",
      "when input is [[44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 39\n",
      "when input is [[44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 58\n",
      "when input is [[44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 1\n",
      "when input is [] the target is 58\n",
      "when input is [] the target is 1\n",
      "when input is [[52, 58, 1, 58, 46, 39, 58, 1]] the target is 58\n",
      "when input is [[52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 46\n",
      "when input is [[52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 39\n",
      "when input is [[52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 58\n",
      "when input is [[52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 1\n",
      "when input is [[52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 46\n",
      "when input is [] the target is 17\n",
      "when input is [] the target is 27\n",
      "when input is [] the target is 10\n",
      "when input is [[25, 17, 27, 10, 0, 21, 1, 54]] the target is 0\n",
      "when input is [[25, 17, 27, 10, 0, 21, 1, 54]] the target is 21\n",
      "when input is [[25, 17, 27, 10, 0, 21, 1, 54]] the target is 1\n",
      "when input is [[25, 17, 27, 10, 0, 21, 1, 54]] the target is 54\n",
      "when input is [[25, 17, 27, 10, 0, 21, 1, 54]] the target is 39\n"
     ]
    }
   ],
   "source": [
    "# batch dimension: batches are for efficiency to use parallel GPU architecture\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4  # how many independent sequences will we process in parallel?\n",
    "block_size = 8  # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))  # random offsets into the training set\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y  # create batch_size rows of block_size data\n",
    "\n",
    "# sample a batch of data\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('---------')\n",
    "\n",
    "for b in range(batch_size):  # batch dimension\n",
    "    for t in range(block_size):  # time dimension\n",
    "        context = xb[b :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2a6453f-2088-47f3-92f3-95a72d815eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(xb)  # our input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44ac469d-ff03-4b48-927c-64050f85b3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size).to('cuda:0')  # had to add .to('cuda:0') here to ensure all is on the same device\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) = (batch, time, channel) - just plugs out the idx-row from the embedding table; channel = vocab_size\n",
    "        return logits  # logits are the score for the next character in the sequence\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "out = m(xb, yb)\n",
    "#out = m(xb.to('cuda:0'), yb.to('cuda:0'))  # with device casting - seems to be not necessary as xb, yb are on cuda here already\n",
    "print(out.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4503b86-1ef6-47c9-93ff-1490b4a499c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7a588d2-06de-4dbb-97a2-0dd1a0b84645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
      "        [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594],\n",
      "        [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
      "        ...,\n",
      "        [-2.1910, -0.7574,  1.9656,  ..., -0.3580,  0.8585, -0.6161],\n",
      "        [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
      "        [-0.6787,  0.8662, -1.6433,  ...,  2.3671, -0.7775, -0.2586]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "tensor(4.8786, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "pYCXxfRkRZd\n",
      "wc'wfNfT;OLlTEeC K\n",
      "jxqPToTb?bXAUG:C-SGJO-33SM:C?YI3a\n",
      "hs:LVXJFhXeNuwqhObxZ.tSVrddXlaSZaNe\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size).to('cuda:0')  # had to add .to('cuda:0') here to ensure all is on the same device\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) = (batch, time, channel) - just plugs out the idx-row from the embedding table; channel = vocab_size\n",
    "\n",
    "        if targets is None:  # targets optional\n",
    "            loss = None\n",
    "        else:  # pytorch wants the channel to be the 2. dimension...\n",
    "            B, T, C = logits.shape  # ...so we have to repack them\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)  # do the same for targets\n",
    "            loss = F.cross_entropy(logits, targets)  # how well are we predicting the next character?\n",
    "            \n",
    "        return logits, loss  # logits are the score for the next character in the sequence\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):  # generates in the time dimension for all batch dimensions, till max_new_tokens\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)  # the model\n",
    "logits, loss = m(xb, yb)\n",
    "#out = m(xb.to('cuda:0'), yb.to('cuda:0'))  # with device casting - seems to be not necessary as xb, yb are on cuda here already\n",
    "print(logits)\n",
    "print(loss)  # compare to -ln(1/65) with a vocab size of 65 = 4.174...\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)  # a 0-tensor to start generation\n",
    "print(decode(m.generate(context, max_new_tokens=100)[0].tolist()))  # also unplug the batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f233af68-34a9-4614-9df1-3ae96388be71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to training the model\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2da67bc-d8d7-4f2c-a05b-a222e1a5c3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.403348922729492\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "batch_size = 32  # increase batch size\n",
    "for iter in range(10000):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    #if iter % eval_interval == 0:\n",
    "    #    losses = estimate_loss()\n",
    "    #    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c0b7ee6d-d6aa-46b1-9ed0-685795fa3685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wid the wllo INour id, mersed\n",
      "Fourd?\n",
      "TI idurd po venond, d Cad ty\n",
      "K:\n",
      "BIUSoou tiund thornofen e sutan wiporthare whanot, sitthers, spe Bllellke, on s h O, t pan, ce wat d tive wout ir f; u;\n",
      "\n",
      "Feknen oue\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)  # a 0-tensor to start generation\n",
    "print(decode(m.generate(context, max_new_tokens=200)[0].tolist()))  # also unplug the batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6dfb0142-06c9-4004-a52f-f1528f42bd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.4620, val loss 2.4799\n",
      "step 500: train loss 2.4509, val loss 2.4872\n",
      "step 1000: train loss 2.4454, val loss 2.4797\n",
      "step 1500: train loss 2.4448, val loss 2.4900\n",
      "step 2000: train loss 2.4579, val loss 2.4778\n",
      "step 2500: train loss 2.4469, val loss 2.4761\n",
      "step 3000: train loss 2.4451, val loss 2.4793\n",
      "step 3500: train loss 2.4570, val loss 2.4901\n",
      "step 4000: train loss 2.4614, val loss 2.4829\n",
      "step 4500: train loss 2.4543, val loss 2.4756\n",
      "step 5000: train loss 2.4587, val loss 2.4904\n",
      "step 5500: train loss 2.4529, val loss 2.4916\n",
      "step 6000: train loss 2.4576, val loss 2.4889\n",
      "step 6500: train loss 2.4656, val loss 2.4859\n",
      "step 7000: train loss 2.4537, val loss 2.4917\n",
      "step 7500: train loss 2.4452, val loss 2.4891\n",
      "step 8000: train loss 2.4580, val loss 2.4900\n",
      "step 8500: train loss 2.4545, val loss 2.4920\n",
      "step 9000: train loss 2.4504, val loss 2.4915\n",
      "step 9500: train loss 2.4512, val loss 2.4837\n",
      "2.434276819229126\n"
     ]
    }
   ],
   "source": [
    "# train with an averaging loss estimator\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():  # making loss estimates less noisy\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out\n",
    "    \n",
    "batch_size = 32  # increase batch size\n",
    "for iter in range(10000):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b19e6202-4cce-4ff7-8821-98b2fbbad525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MPrrtano iru forealoiroret HEnk;\n",
      "CUCaden tck in, d ser t ftanofallon bay ho s, agallen, meseveminds s; te worimyoin ie--\n",
      "ARUSThe Whou wowhedichea blare aned hy senonirstha theint co mas, the an be ke \n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)  # a 0-tensor to start generation\n",
    "print(decode(m.generate(context, max_new_tokens=200)[0].tolist()))  # also unplug the batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a61b4aa2-4e91-44cd-9771-2c42172c7874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a mathematical trick\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 2  # batch, time, channels (channels = the number of information components at each time step in a batch)\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "80f779f3-c3b6-4677-a62d-45c9615d3171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "# we want the token to talk to each other, but not into the future\n",
    "\n",
    "# simplest case in communicating with the past: just the average\n",
    "# we want x[b, t] = mean_{i<=t} x[b, i]\n",
    "xbow = torch.zeros((B, T, C))  # x-bag-of-words\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]  # x[t, C] everything up to and including t\n",
    "        xbow[b, t] = torch.mean(xprev, 0)\n",
    "\n",
    "print(x[0])\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "404f2200-5f6d-4775-86a7-caae72667002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "b: tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c: tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# loops are very inefficient, so going for matrix algebra\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b  # matrix multiplication\n",
    "print(f\"a: {a}\")\n",
    "print(f\"b: {b}\")\n",
    "print(f\"c: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e360baf0-0590-45e2-9a6f-668c61fb0d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3, 3))  # lower triangular matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "19f477d2-b9b1-450a-ab95-4b790d1859f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "b: tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c: tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b  # matrix multiplication\n",
    "print(f\"a: {a}\")\n",
    "print(f\"b: {b}\")\n",
    "print(f\"c: {c}\")\n",
    "\n",
    "# this now gives the sums over rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3657b962-c587-4a56-a5ae-56e3e5663675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b: tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c: tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b  # matrix multiplication\n",
    "print(f\"a: {a}\")\n",
    "print(f\"b: {b}\")\n",
    "print(f\"c: {c}\")\n",
    "\n",
    "# this now gives the column averages up to the given row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "868d22ee-daf0-46fb-bb57-f75063d44e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so turn this into a more efficient weight calculation for a simple average value lookback\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2ccb7ac5-b53e-4982-a08c-c5f51ad7f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow2 = wei @ x  # (T, T) @ (B, T, C) -> (B, T, T) @ (B, T, C) since dims do not match so -> (B, T, C)\n",
    "# i.e. for each batch element multiply (T,C) by (T,T) in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "92814f7f-8fa8-4481-9251-ba23061a83db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6f6f0bb2-13ba-41bf-898f-8c536c623943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow, xbow2, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "adbe08ab-221e-47e6-bc23-c8b6c1f019ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. version\n",
    "tril = torch.tril(torch.ones(T, T))  # lower triang matrix\n",
    "wei = torch.zeros((T, T))  # all 0s: initially the interaction strength (between the tokens) is 0\n",
    "wei = wei.masked_fill( tril == 0, float('-inf'))  # set to -inf wherever tril has 0 entry -> tokens from the past can't talk to the future\n",
    "wei = F.softmax(wei, dim=-1)  # dim=-1 -> along rows -> averages the values over the number of cols: 1 / n_cols\n",
    "\n",
    "print(wei)\n",
    "\n",
    "xbow3 = wei @ x\n",
    "\n",
    "torch.allclose(xbow, xbow3, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "14d6eb30-3aa1-478f-92bf-9d3fd21b8628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3886, val loss 4.3734\n",
      "weight: tensor([ 0.0048,  0.1580,  0.1353,  0.1497,  0.1003,  0.1122,  0.0204,  0.0371,\n",
      "         0.1236,  0.1255, -0.0915, -0.0697, -0.0009, -0.1504,  0.0219,  0.0610,\n",
      "        -0.0391,  0.1378, -0.1737, -0.0799, -0.0614,  0.0013, -0.0683,  0.0843,\n",
      "         0.0443, -0.1751, -0.1476, -0.0838, -0.1292, -0.1188, -0.0336,  0.0750],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "step 300: train loss 2.5267, val loss 2.5399\n",
      "weight: tensor([ 0.3775,  0.0602,  0.6586,  0.4240,  0.1560,  0.0591,  0.4988, -0.0598,\n",
      "         0.1781,  0.0452, -0.1271, -0.1626, -0.0275, -0.5549, -0.2615,  0.2745,\n",
      "        -0.5423, -0.3858, -0.3118,  0.2559, -0.5134, -0.2357, -0.2948,  0.2617,\n",
      "         0.2396, -0.2776, -0.5593, -0.0584,  0.4738, -0.3698, -0.0105, -0.3609],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "step 600: train loss 2.4998, val loss 2.5315\n",
      "weight: tensor([ 0.2866,  0.0867,  0.6215,  0.3985,  0.0940, -0.0382,  0.5687, -0.0393,\n",
      "         0.2368,  0.2683, -0.0992, -0.1803,  0.0227, -0.5317, -0.2964,  0.3103,\n",
      "        -0.5916, -0.3787, -0.3212,  0.3015, -0.5217, -0.2926, -0.3462,  0.2838,\n",
      "         0.3392, -0.2710, -0.5459, -0.0804,  0.4743, -0.3552,  0.0321, -0.3719],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "step 900: train loss 2.4903, val loss 2.5085\n",
      "weight: tensor([ 0.2819, -0.0692,  0.6643,  0.4460,  0.1041, -0.0570,  0.6333, -0.0730,\n",
      "         0.2787,  0.1949, -0.1000, -0.1714,  0.0203, -0.5452, -0.2491,  0.4133,\n",
      "        -0.6588, -0.3895, -0.3288,  0.3072, -0.5612, -0.2700, -0.3405,  0.2886,\n",
      "         0.3435, -0.3504, -0.6139, -0.0431,  0.5267, -0.3801, -0.0096, -0.3853],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "step 1200: train loss 2.4967, val loss 2.5128\n",
      "weight: tensor([ 0.2838, -0.1045,  0.5886,  0.4845,  0.1699, -0.1344,  0.6954, -0.1534,\n",
      "         0.2666,  0.2390, -0.0763, -0.2659,  0.0748, -0.5534, -0.2877,  0.4227,\n",
      "        -0.6213, -0.4007, -0.4018,  0.3762, -0.5927, -0.2157, -0.4082,  0.3471,\n",
      "         0.3004, -0.4030, -0.6504, -0.0341,  0.4735, -0.4418,  0.0491, -0.3620],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "step 1500: train loss 2.4809, val loss 2.5020\n",
      "weight: tensor([ 0.2864, -0.0939,  0.6124,  0.4663,  0.1403, -0.2030,  0.7004, -0.1444,\n",
      "         0.3318,  0.2116, -0.0465, -0.2048,  0.1151, -0.5248, -0.2497,  0.4631,\n",
      "        -0.6072, -0.4040, -0.3437,  0.3481, -0.6078, -0.2711, -0.3880,  0.3282,\n",
      "         0.3080, -0.3626, -0.5904, -0.0210,  0.5442, -0.4822,  0.0190, -0.3315],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "step 1800: train loss 2.4858, val loss 2.5149\n",
      "weight: tensor([ 0.2894, -0.0918,  0.6083,  0.4319,  0.1292, -0.2374,  0.7247, -0.1388,\n",
      "         0.3813,  0.3015, -0.0394, -0.2176,  0.1057, -0.5482, -0.2838,  0.5068,\n",
      "        -0.5799, -0.4354, -0.3558,  0.3701, -0.6158, -0.2073, -0.3935,  0.3883,\n",
      "         0.3022, -0.3265, -0.6472, -0.0484,  0.5112, -0.4423, -0.0058, -0.3418],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "step 2100: train loss 2.4865, val loss 2.5000\n",
      "weight: tensor([ 0.2503, -0.1277,  0.5922,  0.4207,  0.1770, -0.2220,  0.8193, -0.1645,\n",
      "         0.3663,  0.3375,  0.0011, -0.1882,  0.0680, -0.5823, -0.2567,  0.5249,\n",
      "        -0.5719, -0.4446, -0.3731,  0.3651, -0.6091, -0.2971, -0.4248,  0.3655,\n",
      "         0.3192, -0.3417, -0.6480,  0.0295,  0.5414, -0.5139,  0.0142, -0.3215],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "step 2400: train loss 2.4882, val loss 2.5127\n",
      "weight: tensor([ 0.2834, -0.1349,  0.5195,  0.3969,  0.1720, -0.2558,  0.7891, -0.2149,\n",
      "         0.3940,  0.3139, -0.0194, -0.2117,  0.1249, -0.5549, -0.2775,  0.4977,\n",
      "        -0.5605, -0.3759, -0.3467,  0.4219, -0.5953, -0.2645, -0.4211,  0.3859,\n",
      "         0.2984, -0.3797, -0.6555,  0.0075,  0.5743, -0.5170,  0.0264, -0.3346],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "step 2700: train loss 2.5006, val loss 2.5117\n",
      "weight: tensor([ 2.6949e-01, -9.6630e-02,  5.2262e-01,  4.0413e-01,  1.5453e-01,\n",
      "        -2.8728e-01,  8.8045e-01, -1.3768e-01,  4.3079e-01,  3.3676e-01,\n",
      "        -4.2161e-04, -1.9584e-01,  1.0872e-01, -5.9610e-01, -3.0160e-01,\n",
      "         5.5567e-01, -5.5077e-01, -3.6827e-01, -4.2744e-01,  4.2790e-01,\n",
      "        -5.8815e-01, -1.8543e-01, -3.9500e-01,  3.9859e-01,  2.7344e-01,\n",
      "        -3.4954e-01, -6.6677e-01,  1.7416e-02,  5.5414e-01, -5.1946e-01,\n",
      "         1.6330e-02, -2.8355e-01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "CExthantrid owindike on, ble\n",
      "\n",
      "HAPen bube d e.\n",
      "S:\n",
      "Ond my d?\n",
      "LUMuss ar hthar usqur, t. bar dilasoaten wice my.\n",
      "\n",
      "Hastacom o mup\n",
      "Yowhthetof isth ble mil; dilll,\n",
      "\n",
      "W:\n",
      "\n",
      "Yees, hein lat Hetidrovets, and Wh p.\n",
      "Gore y jomes l lind me l.\n",
      "MAshe cechiry ptupr aisspllwhy.\n",
      "Hurinde n Boopetelaves\n",
      "MPORIII od mothakleo Windo wh t eiibys woutit,\n",
      "\n",
      "Hive cend iend t so mower; te\n",
      "\n",
      "AN ad nterupt f s ar irist m:\n",
      "\n",
      "Thin maleronth,\n",
      "Mad\n",
      "RD:\n",
      "\n",
      "Whio myr f-bube!\n",
      "KENobuisarardsal this aresthidin couk ay aney Iry ts I fr t ce.\n",
      "J\n"
     ]
    }
   ],
   "source": [
    "# now adding a true embedding via a simple linear layer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input_sp0.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)  # added: a true embedding via linear layer\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C) = (batch, time, channel), channel = vocab_size\n",
    "        logits = self.lm_head(tok_emb)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        for layer in model.children():  # show some of the model weights\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                print('weight:', layer.weight[0])\n",
    "                #print('bias:', layer.bias)\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86d028aa-e70c-45aa-bc4e-2ad4090f162e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.4801, val loss 4.4801\n",
      "step 300: train loss 2.5404, val loss 2.5566\n",
      "step 600: train loss 2.5160, val loss 2.5335\n",
      "step 900: train loss 2.4967, val loss 2.5149\n",
      "step 1200: train loss 2.5106, val loss 2.5254\n",
      "step 1500: train loss 2.4853, val loss 2.5109\n",
      "step 1800: train loss 2.4966, val loss 2.5198\n",
      "step 2100: train loss 2.4949, val loss 2.5100\n",
      "step 2400: train loss 2.4937, val loss 2.5102\n",
      "step 2700: train loss 2.5040, val loss 2.5114\n",
      "tensor([[0.8214, 0.7384, 0.0282, 0.3845, 0.7066, 0.3378, 0.2683, 0.8112, 0.6869,\n",
      "         0.7228, 0.9587, 0.9837, 0.9026, 0.0880, 0.8964, 0.9426, 0.9674, 0.9745,\n",
      "         0.6779, 0.1092, 0.1299, 0.2457, 0.1767, 0.7408, 0.8273, 0.8487, 0.9538,\n",
      "         0.0157, 0.9013, 0.7768, 0.0209, 0.8361, 0.6349, 0.3485, 0.6792, 0.2044,\n",
      "         0.2307, 0.8581, 0.5908, 0.8216, 0.4209, 0.5126, 0.9536, 0.4550, 0.8249,\n",
      "         0.9150, 0.8852, 0.1304, 0.6420, 0.4273, 0.6765, 0.0011, 0.6228, 0.6061,\n",
      "         0.4951, 0.5043, 0.8673, 0.8002, 0.5859, 0.5517, 0.9160, 0.2242, 0.2144,\n",
      "         0.9887, 0.8675]], device='cuda:0')\n",
      "tensor([[0.6600, 0.4723, 0.3702, 0.8150, 0.0327, 0.4291, 0.7884, 0.6106, 0.0129,\n",
      "         0.4359, 0.8720, 0.7450, 0.8389, 0.0532, 0.8604, 0.3955, 0.0290, 0.9340,\n",
      "         0.2003, 0.0685, 0.7740, 0.4486, 0.6856, 0.3937, 0.3194, 0.6571, 0.4029,\n",
      "         0.5065, 0.0040, 0.9398, 0.8125, 0.5094, 0.0965, 0.4286, 0.7668, 0.4704,\n",
      "         0.2480, 0.0126, 0.5311, 0.0311, 0.7305, 0.6532, 0.3343, 0.9192, 0.8863,\n",
      "         0.8288, 0.8540, 0.2470, 0.0292, 0.1382, 0.4287, 0.4840, 0.7391, 0.7775,\n",
      "         0.7060, 0.5930, 0.3406, 0.5753, 0.3375, 0.2359, 0.4489, 0.3070, 0.7857,\n",
      "         0.5289, 0.2936]], device='cuda:0')\n",
      "tensor([[0.3071, 0.4921, 0.3780, 0.9425, 0.0649, 0.6657, 0.5108, 0.0210, 0.3914,\n",
      "         0.7294, 0.4732, 0.5486, 0.0165, 0.1474, 0.8383, 0.4745, 0.1166, 0.1970,\n",
      "         0.2410, 0.4296, 0.9642, 0.6082, 0.2398, 0.2008, 0.9972, 0.3207, 0.3273,\n",
      "         0.5721, 0.0299, 0.7538, 0.9109, 0.8284, 0.0993, 0.4006, 0.1667, 0.1278,\n",
      "         0.4196, 0.7194, 0.2335, 0.9762, 0.7499, 0.1889, 0.9849, 0.4937, 0.5736,\n",
      "         0.8923, 0.0343, 0.4686, 0.7303, 0.7786, 0.7810, 0.6968, 0.0126, 0.1483,\n",
      "         0.2032, 0.3483, 0.0238, 0.3917, 0.6647, 0.0757, 0.5713, 0.1178, 0.2574,\n",
      "         0.5725, 0.8129]], device='cuda:0')\n",
      "tensor([[0.5216, 0.5204, 0.9402, 0.1062, 0.7566, 0.9534, 0.6159, 0.2413, 0.6917,\n",
      "         0.7546, 0.3502, 0.0253, 0.2191, 0.6544, 0.4386, 0.0312, 0.7154, 0.8059,\n",
      "         0.1474, 0.0749, 0.0434, 0.8070, 0.2669, 0.1991, 0.6812, 0.0410, 0.5592,\n",
      "         0.4189, 0.6343, 0.3561, 0.1865, 0.1890, 0.9378, 0.8792, 0.6845, 0.9396,\n",
      "         0.0780, 0.5950, 0.2434, 0.4372, 0.4028, 0.1786, 0.9257, 0.3114, 0.9926,\n",
      "         0.7594, 0.0436, 0.4113, 0.4152, 0.1110, 0.1510, 0.8153, 0.0286, 0.4575,\n",
      "         0.0695, 0.0509, 0.1902, 0.6005, 0.6578, 0.1402, 0.7194, 0.1040, 0.2936,\n",
      "         0.0939, 0.4032]], device='cuda:0')\n",
      "tensor([[0.0777, 0.7564, 0.0723, 0.0501, 0.6426, 0.4422, 0.5976, 0.6636, 0.7226,\n",
      "         0.0179, 0.0698, 0.5248, 0.5777, 0.3962, 0.1685, 0.1664, 0.0435, 0.1173,\n",
      "         0.8031, 0.9426, 0.7881, 0.2814, 0.0758, 0.9145, 0.7108, 0.4239, 0.1745,\n",
      "         0.4314, 0.0907, 0.0857, 0.2706, 0.5473, 0.4913, 0.4903, 0.2765, 0.6445,\n",
      "         0.9963, 0.8957, 0.1216, 0.1667, 0.0152, 0.5893, 0.8893, 0.0841, 0.8261,\n",
      "         0.2474, 0.3992, 0.3752, 0.5777, 0.5901, 0.9783, 0.4299, 0.7346, 0.8025,\n",
      "         0.1323, 0.6038, 0.9251, 0.8560, 0.7867, 0.6058, 0.6073, 0.0073, 0.9714,\n",
      "         0.1330, 0.7177]], device='cuda:0')\n",
      "\n",
      "?qfmx\n"
     ]
    }
   ],
   "source": [
    "# we do not only want to encode the identity of the tokens, but also their position\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32  # how many independent sequences will we process in parallel?\n",
    "block_size = 8  # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input_sp0.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)  # embedding token\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # embedding position\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)  # adding a true embedding layer: map from embedding to token vocab\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C) = (batch, time, channel), channel = vocab_size\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C), arange: all ints up to t-1\n",
    "        x = tok_emb + pos_emb  # (B,T,C) will be the addition of token and position embedding\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        B, T = idx.shape\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            #probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            #probs = self.own_softmax(logits) # (B, C)\n",
    "            probs = torch.rand(B, vocab_size).to('cuda:0')\n",
    "            print(probs)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "    def own_softmax(self, x):\n",
    "        eps = 1.e-10\n",
    "        means = torch.mean(x, 1, keepdim=True)[0] + eps\n",
    "        x_exp = torch.exp(x - means)\n",
    "        x_exp_sum = torch.sum(x_exp, 1, keepdim=True) + eps\n",
    "        return x_exp / x_exp_sum + eps\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=5)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "463903fc-79ee-4699-9f39-c0cdd0c18b18",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid multinomial distribution (sum of probabilities <= 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid multinomial distribution (sum of probabilities <= 0)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.multinomial(torch.tensor([ 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b208774-11e1-4d48-acc3-b9ac910a9e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self-attention\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 32  # batch, time, channels = the information in each token is 32-dim (for 4 batches of 8 contextlength)\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# single-head self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)  # bias=flase: essentially just a matrix multiplication\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)  # now apply simple linear model to x (B, T, 16)\n",
    "q = query(x)  # (B, T, 16)\n",
    "wei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T, T))  # we do not want this to be all 0s: some tokens might find certain other ones more interesting\n",
    "# self-attention solves this: every single token in the past emits two vectors: a query Q and a key K\n",
    "# Q: what am I looking for?\n",
    "# K: what do I contain?\n",
    "# dot-product my query Q with all other keys k -> wei (higher correlates -> larger dot-product -> larger weight)\n",
    "\n",
    "wei = wei.masked_fill( tril == 0, float('-inf'))  # not looking into the future again\n",
    "wei = F.softmax(wei, dim=-1)  # make a nice distribution out of the rather rough dot-product-values (softmax = exp and normalize)\n",
    "out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49dae42d-a2da-48a6-853c-fe71cb3cd40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8bc7225-0559-48dc-9fd3-1275fe03f7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self-attention - a look under the hood\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 32  # batch, time, channels = the information in each token is 32-dim (for 4 batches of 8 contextlength)\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# single-head self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)  # bias=flase: essentially just a matrix multiplication\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)  # now apply simple linear model to x (B, T, 16)\n",
    "q = query(x)  # (B, T, 16)\n",
    "wei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T, T))  # we do not want this to be all 0s: some tokens might find certain other ones more interesting\n",
    "# self-attention solves this: every single token in the past emits two vectors: a query Q and a key K\n",
    "# Q: what am I looking for?\n",
    "# K: what do I contain?\n",
    "# dot-product my query Q with all other keys k -> wei (higher correlates -> larger dot-product -> larger weight)\n",
    "\n",
    "#wei = wei.masked_fill( tril == 0, float('-inf'))\n",
    "#wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0f804f0-fb6a-495d-a330-19fbfad25486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7629, -1.3011,  0.5652,  2.1616, -1.0674,  1.9632,  1.0765, -0.4530],\n",
       "        [-3.3334, -1.6556,  0.1040,  3.3782, -2.1825,  1.0415, -0.0557,  0.2927],\n",
       "        [-1.0226, -1.2606,  0.0762, -0.3813, -0.9843, -1.4303,  0.0749, -0.9547],\n",
       "        [ 0.7836, -0.8014, -0.3368, -0.8496, -0.5602, -1.1701, -1.2927, -1.0260],\n",
       "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,  0.8638,  0.3719,  0.9258],\n",
       "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,  1.4187,  1.2196],\n",
       "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,  0.8048],\n",
       "        [-1.8044, -0.4126, -0.8306,  0.5899, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]  # the raw affinities between the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dd8fe7a-d79b-4155-b69e-44591e902fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self-attention now with value V added\n",
    "# attention is essentailly a communication mechanism\n",
    "# can be seen as a directed graph: a node (token) collects info from nodes (tokens) that point to it\n",
    "# positional encoding (PE) needed as there is no notion of space; PE tells the tokens where they are\n",
    "# not like convolution which contains location information\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 32  # batch, time, channels = the information in each token is 32-dim (for 4 batches of 8 contextlength)\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# single-head self-attention\n",
    "head_size = 16\n",
    "# self-attention solves this: every single token in the past emits two vectors: a query Q and a key K\n",
    "# Q: what am I looking for?\n",
    "# K: what do I contain? (publicly, I also have some private info)\n",
    "# dot-product my query Q with all other keys k -> wei (higher correlates -> larger dot-product -> larger weight)\n",
    "# V: value, the private information of the token: if you find me interesting, here is what I will give you\n",
    "key = nn.Linear(C, head_size, bias=False)  # bias=flase: essentially just a matrix multiplication\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)  # now apply simple linear model to x (B, T, 16)\n",
    "q = query(x)  # (B, T, 16)\n",
    "wei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill( tril == 0, float('-inf'))  # not looking into the future again - delete this for an encoder (here we have a decoder)\n",
    "wei = F.softmax(wei, dim=-1)  # make a nice distribution out of the rather rough dot-product-values (softmax = exp and normalize)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d713f2c8-2e8d-4407-a0d8-05fb97fbdab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007, -0.5239,\n",
       "         -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,  0.2862,  0.5710],\n",
       "        [ 0.6764, -0.5477, -0.2478,  0.3143, -0.1280, -0.2952, -0.4296, -0.1089,\n",
       "         -0.0493,  0.7268,  0.7130, -0.1164,  0.3266,  0.3431, -0.0710,  1.2716],\n",
       "        [ 0.4823, -0.1069, -0.4055,  0.1770,  0.1581, -0.1697,  0.0162,  0.0215,\n",
       "         -0.2490, -0.3773,  0.2787,  0.1629, -0.2895, -0.0676, -0.1416,  1.2194],\n",
       "        [ 0.1971,  0.2856, -0.1303, -0.2655,  0.0668,  0.1954,  0.0281, -0.2451,\n",
       "         -0.4647,  0.0693,  0.1528, -0.2032, -0.2479, -0.1621,  0.1947,  0.7678],\n",
       "        [ 0.2510,  0.7346,  0.5939,  0.2516,  0.2606,  0.7582,  0.5595,  0.3539,\n",
       "         -0.5934, -1.0807, -0.3111, -0.2781, -0.9054,  0.1318, -0.1382,  0.6371],\n",
       "        [ 0.3428,  0.4960,  0.4725,  0.3028,  0.1844,  0.5814,  0.3824,  0.2952,\n",
       "         -0.4897, -0.7705, -0.1172, -0.2541, -0.6892,  0.1979, -0.1513,  0.7666],\n",
       "        [ 0.1866, -0.0964, -0.1430,  0.3059,  0.0834, -0.0069, -0.2047, -0.1535,\n",
       "         -0.0762,  0.3269,  0.3090,  0.0766,  0.0992,  0.1656,  0.1975,  0.7625],\n",
       "        [ 0.1301, -0.0328, -0.4965,  0.2865,  0.2704, -0.2636, -0.0738,  0.3786,\n",
       "          0.0746,  0.0338,  0.0147,  0.3194,  0.2993, -0.1653, -0.0386,  0.3375]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee25de55-8c6d-436b-9913-aaeac358d2ff",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc2ecf60-cd95-4528-be77-29c9aab7ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo: scale to variance 1\n",
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "418301e5-b661-4598-9154-e19157427aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9006)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcfc0acd-d450-4f20-96bf-84d16a6cc12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax sharpens values, i.e. makes the data more peaky\n",
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b6631c2-ff22-46d8-b49d-03a76838454a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot; then we just aggregate info from a single node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64bb42a7-d149-4122-83e4-b4ab684fc8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae4cee4-e2e6-4894-8d46-c7197c836d79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
