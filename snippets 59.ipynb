{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17a9b82c-dcd9-448b-955a-2850c829e995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1484765089981492\n",
      "0.3826105602496096\n",
      "0.17185738520077565\n",
      "0.11189389859934662\n",
      "0.08484081908566549\n",
      "0.06873187737629899\n",
      "0.05780303777302742\n",
      "0.049851715200578295\n",
      "0.04380145114126244\n",
      "0.03904595454490739\n",
      "[0.14480521]\n",
      "[0.95041035]\n"
     ]
    }
   ],
   "source": [
    "# https://stackabuse.com/creating-a-neural-network-from-scratch-in-python/\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_der(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "feature_set = np.array([[0,1,0], [0,0,1], [1,0,0], [1,1,0], [1,1,1]])\n",
    "labels = np.array([[1,0,0,1,1]])\n",
    "labels = labels.reshape(5,1)\n",
    "\n",
    "np.random.seed(42)\n",
    "weights = np.random.rand(3,1)\n",
    "bias = np.random.rand(1)\n",
    "lr = 0.09\n",
    "\n",
    "# forward & backward\n",
    "for epoch in range(500):\n",
    "    x = feature_set\n",
    "\n",
    "    # feedforward\n",
    "    z = sigmoid(np.dot(x, weights) + bias)\n",
    "\n",
    "    # backpropagation step 1\n",
    "    error = z - labels\n",
    "    if epoch % 50 == 0: print(error.sum())\n",
    "    # backpropagation step 2\n",
    "    dpred_dz = sigmoid_der(z)\n",
    "\n",
    "    z_delta = error * dpred_dz\n",
    "    x = feature_set.T\n",
    "    weights -= lr * np.dot(x, z_delta)\n",
    "\n",
    "    for num in z_delta:\n",
    "        bias -= lr * num\n",
    "\n",
    "\n",
    "single_point = np.array([1,0,0])\n",
    "result = sigmoid(np.dot(single_point, weights) + bias)\n",
    "print(result)\n",
    "\n",
    "single_point = np.array([0,1,0])\n",
    "result = sigmoid(np.dot(single_point, weights) + bias)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b3b370-52bc-4dc3-8fc7-04377f1726f8",
   "metadata": {},
   "source": [
    "## Andrej Karpathy: Let's build GPT: from scratch, in code, spelled out\n",
    "\n",
    "followed the code from the vid with added comments and explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "016155bd-f6f2-4ab2-9528-478ea15dd9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 17, 0, 26, 16, 13, 24, 13]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "# https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "# https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=M5CvobiQ0pLr\n",
    "\n",
    "text = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars)}\n",
    "itos = { i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[e] for e in l])\n",
    "\n",
    "print(encode('hi there'))\n",
    "print(decode(encode('hi there')))\n",
    "\n",
    "# tiktoken for subword encoding\n",
    "# tokens are sometimes referred to as nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99459851-4c05-43bd-9b10-8834a3087483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc11e3b7-5776-4a7b-95cf-139d48a0f48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input_sp0.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:200])\n",
    "\n",
    "n = int(0.9*len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e73693f4-32e6-492e-a12b-3106033e78c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we just train the transformer with chunks at a time (blocksize, context length,...)\n",
    "block_size = 8  # multiple examples packed into this (7 here) as transformer trains on each position\n",
    "train_data[:block_size+1]  # chunks are used not only for computational efficiency but also to make the transformer see context over various lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9921ab4b-3258-415d-9f03-8b0a99f3412e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is 47\n",
      "when input is tensor([18, 47]) the target is 56\n",
      "when input is tensor([18, 47, 56]) the target is 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):  # time dimension\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbb6c782-8895-458e-9276-a167371fa856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]], device='cuda:0')\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]], device='cuda:0')\n",
      "---------\n",
      "when input is [[24, 43, 58, 5, 57, 1, 46, 43]] the target is 43\n",
      "when input is [[24, 43, 58, 5, 57, 1, 46, 43], [44, 53, 56, 1, 58, 46, 39, 58]] the target is 58\n",
      "when input is [[24, 43, 58, 5, 57, 1, 46, 43], [44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1]] the target is 5\n",
      "when input is [[24, 43, 58, 5, 57, 1, 46, 43], [44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 57\n",
      "when input is [[24, 43, 58, 5, 57, 1, 46, 43], [44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 1\n",
      "when input is [[24, 43, 58, 5, 57, 1, 46, 43], [44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 46\n",
      "when input is [[24, 43, 58, 5, 57, 1, 46, 43], [44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 43\n",
      "when input is [[24, 43, 58, 5, 57, 1, 46, 43], [44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 39\n",
      "when input is [] the target is 53\n",
      "when input is [[44, 53, 56, 1, 58, 46, 39, 58]] the target is 56\n",
      "when input is [[44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1]] the target is 1\n",
      "when input is [[44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 58\n",
      "when input is [[44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 46\n",
      "when input is [[44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 39\n",
      "when input is [[44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 58\n",
      "when input is [[44, 53, 56, 1, 58, 46, 39, 58], [52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 1\n",
      "when input is [] the target is 58\n",
      "when input is [] the target is 1\n",
      "when input is [[52, 58, 1, 58, 46, 39, 58, 1]] the target is 58\n",
      "when input is [[52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 46\n",
      "when input is [[52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 39\n",
      "when input is [[52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 58\n",
      "when input is [[52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 1\n",
      "when input is [[52, 58, 1, 58, 46, 39, 58, 1], [25, 17, 27, 10, 0, 21, 1, 54]] the target is 46\n",
      "when input is [] the target is 17\n",
      "when input is [] the target is 27\n",
      "when input is [] the target is 10\n",
      "when input is [[25, 17, 27, 10, 0, 21, 1, 54]] the target is 0\n",
      "when input is [[25, 17, 27, 10, 0, 21, 1, 54]] the target is 21\n",
      "when input is [[25, 17, 27, 10, 0, 21, 1, 54]] the target is 1\n",
      "when input is [[25, 17, 27, 10, 0, 21, 1, 54]] the target is 54\n",
      "when input is [[25, 17, 27, 10, 0, 21, 1, 54]] the target is 39\n"
     ]
    }
   ],
   "source": [
    "# batch dimension: batches are for efficiency to use parallel GPU architecture\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4  # how many independent sequences will we process in parallel?\n",
    "block_size = 8  # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))  # random offsets into the training set\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y  # create batch_size rows of block_size data\n",
    "\n",
    "# sample a batch of data\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('---------')\n",
    "\n",
    "for b in range(batch_size):  # batch dimension\n",
    "    for t in range(block_size):  # time dimension\n",
    "        context = xb[b :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2a6453f-2088-47f3-92f3-95a72d815eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(xb)  # our input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44ac469d-ff03-4b48-927c-64050f85b3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size).to('cuda:0')  # had to add .to('cuda:0') here to ensure all is on the same device\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) = (batch, time, channel) - just plugs out the idx-row from the embedding table; channel = vocab_size\n",
    "        return logits  # logits are the score for the next character in the sequence\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "out = m(xb, yb)\n",
    "#out = m(xb.to('cuda:0'), yb.to('cuda:0'))  # with device casting - seems to be not necessary as xb, yb are on cuda here already\n",
    "print(out.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4503b86-1ef6-47c9-93ff-1490b4a499c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7a588d2-06de-4dbb-97a2-0dd1a0b84645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
      "        [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594],\n",
      "        [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
      "        ...,\n",
      "        [-2.1910, -0.7574,  1.9656,  ..., -0.3580,  0.8585, -0.6161],\n",
      "        [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
      "        [-0.6787,  0.8662, -1.6433,  ...,  2.3671, -0.7775, -0.2586]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "tensor(4.8786, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "pYCXxfRkRZd\n",
      "wc'wfNfT;OLlTEeC K\n",
      "jxqPToTb?bXAUG:C-SGJO-33SM:C?YI3a\n",
      "hs:LVXJFhXeNuwqhObxZ.tSVrddXlaSZaNe\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size).to('cuda:0')  # had to add .to('cuda:0') here to ensure all is on the same device\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) = (batch, time, channel) - just plugs out the idx-row from the embedding table; channel = vocab_size\n",
    "\n",
    "        if targets is None:  # targets optional\n",
    "            loss = None\n",
    "        else:  # pytorch wants the channel to be the 2. dimension...\n",
    "            B, T, C = logits.shape  # ...so we have to repack them\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)  # do the same for targets\n",
    "            loss = F.cross_entropy(logits, targets)  # how well are we predicting the next character?\n",
    "            \n",
    "        return logits, loss  # logits are the score for the next character in the sequence\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):  # generates in the time dimension for all batch dimensions, till max_new_tokens\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)  # the model\n",
    "logits, loss = m(xb, yb)\n",
    "#out = m(xb.to('cuda:0'), yb.to('cuda:0'))  # with device casting - seems to be not necessary as xb, yb are on cuda here already\n",
    "print(logits)\n",
    "print(loss)  # compare to -ln(1/65) with a vocab size of 65 = 4.174...\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)  # a 0-tensor to start generation\n",
    "print(decode(m.generate(context, max_new_tokens=100)[0].tolist()))  # also unplug the batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f233af68-34a9-4614-9df1-3ae96388be71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to training the model\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2da67bc-d8d7-4f2c-a05b-a222e1a5c3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.403348922729492\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "batch_size = 32  # increase batch size\n",
    "for iter in range(10000):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    #if iter % eval_interval == 0:\n",
    "    #    losses = estimate_loss()\n",
    "    #    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c0b7ee6d-d6aa-46b1-9ed0-685795fa3685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wid the wllo INour id, mersed\n",
      "Fourd?\n",
      "TI idurd po venond, d Cad ty\n",
      "K:\n",
      "BIUSoou tiund thornofen e sutan wiporthare whanot, sitthers, spe Bllellke, on s h O, t pan, ce wat d tive wout ir f; u;\n",
      "\n",
      "Feknen oue\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)  # a 0-tensor to start generation\n",
    "print(decode(m.generate(context, max_new_tokens=200)[0].tolist()))  # also unplug the batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6dfb0142-06c9-4004-a52f-f1528f42bd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.4620, val loss 2.4799\n",
      "step 500: train loss 2.4509, val loss 2.4872\n",
      "step 1000: train loss 2.4454, val loss 2.4797\n",
      "step 1500: train loss 2.4448, val loss 2.4900\n",
      "step 2000: train loss 2.4579, val loss 2.4778\n",
      "step 2500: train loss 2.4469, val loss 2.4761\n",
      "step 3000: train loss 2.4451, val loss 2.4793\n",
      "step 3500: train loss 2.4570, val loss 2.4901\n",
      "step 4000: train loss 2.4614, val loss 2.4829\n",
      "step 4500: train loss 2.4543, val loss 2.4756\n",
      "step 5000: train loss 2.4587, val loss 2.4904\n",
      "step 5500: train loss 2.4529, val loss 2.4916\n",
      "step 6000: train loss 2.4576, val loss 2.4889\n",
      "step 6500: train loss 2.4656, val loss 2.4859\n",
      "step 7000: train loss 2.4537, val loss 2.4917\n",
      "step 7500: train loss 2.4452, val loss 2.4891\n",
      "step 8000: train loss 2.4580, val loss 2.4900\n",
      "step 8500: train loss 2.4545, val loss 2.4920\n",
      "step 9000: train loss 2.4504, val loss 2.4915\n",
      "step 9500: train loss 2.4512, val loss 2.4837\n",
      "2.434276819229126\n"
     ]
    }
   ],
   "source": [
    "# train with an averaging loss estimator\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():  # making loss estimates less noisy\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out\n",
    "    \n",
    "batch_size = 32  # increase batch size\n",
    "for iter in range(10000):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b19e6202-4cce-4ff7-8821-98b2fbbad525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MPrrtano iru forealoiroret HEnk;\n",
      "CUCaden tck in, d ser t ftanofallon bay ho s, agallen, meseveminds s; te worimyoin ie--\n",
      "ARUSThe Whou wowhedichea blare aned hy senonirstha theint co mas, the an be ke \n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)  # a 0-tensor to start generation\n",
    "print(decode(m.generate(context, max_new_tokens=200)[0].tolist()))  # also unplug the batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a61b4aa2-4e91-44cd-9771-2c42172c7874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a mathematical trick\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 2  # batch, time, channels (channels = the number of information components at each time step in a batch)\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "80f779f3-c3b6-4677-a62d-45c9615d3171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "# we want the token to talk to each other, but not into the future\n",
    "\n",
    "# simplest case in communicating with the past: just the average\n",
    "# we want x[b, t] = mean_{i<=t} x[b, i]\n",
    "xbow = torch.zeros((B, T, C))  # x-bag-of-words\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]  # x[t, C] everything up to and including t\n",
    "        xbow[b, t] = torch.mean(xprev, 0)\n",
    "\n",
    "print(x[0])\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "404f2200-5f6d-4775-86a7-caae72667002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "b: tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c: tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# loops are very inefficient, so going for matrix algebra\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b  # matrix multiplication\n",
    "print(f\"a: {a}\")\n",
    "print(f\"b: {b}\")\n",
    "print(f\"c: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e360baf0-0590-45e2-9a6f-668c61fb0d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3, 3))  # lower triangular matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "19f477d2-b9b1-450a-ab95-4b790d1859f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "b: tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c: tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b  # matrix multiplication\n",
    "print(f\"a: {a}\")\n",
    "print(f\"b: {b}\")\n",
    "print(f\"c: {c}\")\n",
    "\n",
    "# this now gives the sums over rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3657b962-c587-4a56-a5ae-56e3e5663675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b: tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c: tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b  # matrix multiplication\n",
    "print(f\"a: {a}\")\n",
    "print(f\"b: {b}\")\n",
    "print(f\"c: {c}\")\n",
    "\n",
    "# this now gives the column averages up to the given row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "868d22ee-daf0-46fb-bb57-f75063d44e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so turn this into a more efficient weight calculation for a simple average value lookback\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2ccb7ac5-b53e-4982-a08c-c5f51ad7f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow2 = wei @ x  # (T, T) @ (B, T, C) -> (B, T, T) @ (B, T, C) since dims do not match so -> (B, T, C)\n",
    "# i.e. for each batch element multiply (T,C) by (T,T) in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "92814f7f-8fa8-4481-9251-ba23061a83db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6f6f0bb2-13ba-41bf-898f-8c536c623943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow, xbow2, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "adbe08ab-221e-47e6-bc23-c8b6c1f019ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. version\n",
    "tril = torch.tril(torch.ones(T, T))  # lower triang matrix\n",
    "wei = torch.zeros((T, T))  # all 0s: initially the interaction strength (between the tokens) is 0\n",
    "wei = wei.masked_fill( tril == 0, float('-inf'))  # set to -inf wherever tril has 0 entry -> tokens from the past can't talk to the future\n",
    "wei = F.softmax(wei, dim=-1)  # dim=-1 -> along rows -> averages the values over the number of cols: 1 / n_cols\n",
    "\n",
    "print(wei)\n",
    "\n",
    "xbow3 = wei @ x\n",
    "\n",
    "torch.allclose(xbow, xbow3, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "14d6eb30-3aa1-478f-92bf-9d3fd21b8628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3886, val loss 4.3734\n",
      "weight: tensor([ 0.0048,  0.1580,  0.1353,  0.1497,  0.1003,  0.1122,  0.0204,  0.0371,\n",
      "         0.1236,  0.1255, -0.0915, -0.0697, -0.0009, -0.1504,  0.0219,  0.0610,\n",
      "        -0.0391,  0.1378, -0.1737, -0.0799, -0.0614,  0.0013, -0.0683,  0.0843,\n",
      "         0.0443, -0.1751, -0.1476, -0.0838, -0.1292, -0.1188, -0.0336,  0.0750],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "step 300: train loss 2.5267, val loss 2.5399\n",
      "weight: tensor([ 0.3775,  0.0602,  0.6586,  0.4240,  0.1560,  0.0591,  0.4988, -0.0598,\n",
      "         0.1781,  0.0452, -0.1271, -0.1626, -0.0275, -0.5549, -0.2615,  0.2745,\n",
      "        -0.5423, -0.3858, -0.3118,  0.2559, -0.5134, -0.2357, -0.2948,  0.2617,\n",
      "         0.2396, -0.2776, -0.5593, -0.0584,  0.4738, -0.3698, -0.0105, -0.3609],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "step 600: train loss 2.4998, val loss 2.5315\n",
      "weight: tensor([ 0.2866,  0.0867,  0.6215,  0.3985,  0.0940, -0.0382,  0.5687, -0.0393,\n",
      "         0.2368,  0.2683, -0.0992, -0.1803,  0.0227, -0.5317, -0.2964,  0.3103,\n",
      "        -0.5916, -0.3787, -0.3212,  0.3015, -0.5217, -0.2926, -0.3462,  0.2838,\n",
      "         0.3392, -0.2710, -0.5459, -0.0804,  0.4743, -0.3552,  0.0321, -0.3719],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "step 900: train loss 2.4903, val loss 2.5085\n",
      "weight: tensor([ 0.2819, -0.0692,  0.6643,  0.4460,  0.1041, -0.0570,  0.6333, -0.0730,\n",
      "         0.2787,  0.1949, -0.1000, -0.1714,  0.0203, -0.5452, -0.2491,  0.4133,\n",
      "        -0.6588, -0.3895, -0.3288,  0.3072, -0.5612, -0.2700, -0.3405,  0.2886,\n",
      "         0.3435, -0.3504, -0.6139, -0.0431,  0.5267, -0.3801, -0.0096, -0.3853],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "step 1200: train loss 2.4967, val loss 2.5128\n",
      "weight: tensor([ 0.2838, -0.1045,  0.5886,  0.4845,  0.1699, -0.1344,  0.6954, -0.1534,\n",
      "         0.2666,  0.2390, -0.0763, -0.2659,  0.0748, -0.5534, -0.2877,  0.4227,\n",
      "        -0.6213, -0.4007, -0.4018,  0.3762, -0.5927, -0.2157, -0.4082,  0.3471,\n",
      "         0.3004, -0.4030, -0.6504, -0.0341,  0.4735, -0.4418,  0.0491, -0.3620],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "step 1500: train loss 2.4809, val loss 2.5020\n",
      "weight: tensor([ 0.2864, -0.0939,  0.6124,  0.4663,  0.1403, -0.2030,  0.7004, -0.1444,\n",
      "         0.3318,  0.2116, -0.0465, -0.2048,  0.1151, -0.5248, -0.2497,  0.4631,\n",
      "        -0.6072, -0.4040, -0.3437,  0.3481, -0.6078, -0.2711, -0.3880,  0.3282,\n",
      "         0.3080, -0.3626, -0.5904, -0.0210,  0.5442, -0.4822,  0.0190, -0.3315],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "step 1800: train loss 2.4858, val loss 2.5149\n",
      "weight: tensor([ 0.2894, -0.0918,  0.6083,  0.4319,  0.1292, -0.2374,  0.7247, -0.1388,\n",
      "         0.3813,  0.3015, -0.0394, -0.2176,  0.1057, -0.5482, -0.2838,  0.5068,\n",
      "        -0.5799, -0.4354, -0.3558,  0.3701, -0.6158, -0.2073, -0.3935,  0.3883,\n",
      "         0.3022, -0.3265, -0.6472, -0.0484,  0.5112, -0.4423, -0.0058, -0.3418],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "step 2100: train loss 2.4865, val loss 2.5000\n",
      "weight: tensor([ 0.2503, -0.1277,  0.5922,  0.4207,  0.1770, -0.2220,  0.8193, -0.1645,\n",
      "         0.3663,  0.3375,  0.0011, -0.1882,  0.0680, -0.5823, -0.2567,  0.5249,\n",
      "        -0.5719, -0.4446, -0.3731,  0.3651, -0.6091, -0.2971, -0.4248,  0.3655,\n",
      "         0.3192, -0.3417, -0.6480,  0.0295,  0.5414, -0.5139,  0.0142, -0.3215],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "step 2400: train loss 2.4882, val loss 2.5127\n",
      "weight: tensor([ 0.2834, -0.1349,  0.5195,  0.3969,  0.1720, -0.2558,  0.7891, -0.2149,\n",
      "         0.3940,  0.3139, -0.0194, -0.2117,  0.1249, -0.5549, -0.2775,  0.4977,\n",
      "        -0.5605, -0.3759, -0.3467,  0.4219, -0.5953, -0.2645, -0.4211,  0.3859,\n",
      "         0.2984, -0.3797, -0.6555,  0.0075,  0.5743, -0.5170,  0.0264, -0.3346],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "step 2700: train loss 2.5006, val loss 2.5117\n",
      "weight: tensor([ 2.6949e-01, -9.6630e-02,  5.2262e-01,  4.0413e-01,  1.5453e-01,\n",
      "        -2.8728e-01,  8.8045e-01, -1.3768e-01,  4.3079e-01,  3.3676e-01,\n",
      "        -4.2161e-04, -1.9584e-01,  1.0872e-01, -5.9610e-01, -3.0160e-01,\n",
      "         5.5567e-01, -5.5077e-01, -3.6827e-01, -4.2744e-01,  4.2790e-01,\n",
      "        -5.8815e-01, -1.8543e-01, -3.9500e-01,  3.9859e-01,  2.7344e-01,\n",
      "        -3.4954e-01, -6.6677e-01,  1.7416e-02,  5.5414e-01, -5.1946e-01,\n",
      "         1.6330e-02, -2.8355e-01], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "CExthantrid owindike on, ble\n",
      "\n",
      "HAPen bube d e.\n",
      "S:\n",
      "Ond my d?\n",
      "LUMuss ar hthar usqur, t. bar dilasoaten wice my.\n",
      "\n",
      "Hastacom o mup\n",
      "Yowhthetof isth ble mil; dilll,\n",
      "\n",
      "W:\n",
      "\n",
      "Yees, hein lat Hetidrovets, and Wh p.\n",
      "Gore y jomes l lind me l.\n",
      "MAshe cechiry ptupr aisspllwhy.\n",
      "Hurinde n Boopetelaves\n",
      "MPORIII od mothakleo Windo wh t eiibys woutit,\n",
      "\n",
      "Hive cend iend t so mower; te\n",
      "\n",
      "AN ad nterupt f s ar irist m:\n",
      "\n",
      "Thin maleronth,\n",
      "Mad\n",
      "RD:\n",
      "\n",
      "Whio myr f-bube!\n",
      "KENobuisarardsal this aresthidin couk ay aney Iry ts I fr t ce.\n",
      "J\n"
     ]
    }
   ],
   "source": [
    "# now adding a true embedding via a simple linear layer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input_sp0.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)  # added: a true embedding via linear layer\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C) = (batch, time, channel), channel = vocab_size\n",
    "        logits = self.lm_head(tok_emb)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        for layer in model.children():  # show some of the model weights\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                print('weight:', layer.weight[0])\n",
    "                #print('bias:', layer.bias)\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d028aa-e70c-45aa-bc4e-2ad4090f162e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.4801, val loss 4.4801\n",
      "step 300: train loss 2.5404, val loss 2.5566\n",
      "step 600: train loss 2.5160, val loss 2.5335\n",
      "step 900: train loss 2.4967, val loss 2.5149\n",
      "step 1200: train loss 2.5106, val loss 2.5254\n",
      "step 1500: train loss 2.4853, val loss 2.5109\n",
      "step 1800: train loss 2.4966, val loss 2.5198\n",
      "step 2100: train loss 2.4949, val loss 2.5100\n",
      "step 2400: train loss 2.4937, val loss 2.5102\n",
      "step 2700: train loss 2.5040, val loss 2.5114\n",
      "\n",
      "\n",
      "\n",
      "CExthantrid owindikis s, bll\n",
      "\n",
      "HAPen bube t e.\n",
      "S:\n",
      "O:\n",
      "IS:\n",
      "Folatangs:\n",
      "Wanthar u qurthe. bar dilasoate awice my.\n",
      "\n",
      "Hastatom o mup\n",
      "Yowhthatof isth ble mil; dilll,\n",
      "\n",
      "W:\n",
      "\n",
      "Ye s, hain latisttid ov ts, and Wh pomano.\n",
      "Swanous l lind me l.\n",
      "MIshe ce hiry ptupr aisspllw y. w'stoul noroopetelaves\n",
      "Momy ll, d mothake o Windo wh t eiibys the m douris TENGByore s poo mo th; te\n",
      "\n",
      "AN ad nthrupt f s ar irist m:\n",
      "\n",
      "Thin maleronth, af Pre?\n",
      "\n",
      "Whio myr f-\n",
      "LI har,\n",
      "S:\n",
      "\n",
      "\n",
      "Thardsal this ghesthidin cour ay aney Iry ts I f my ce hy\n"
     ]
    }
   ],
   "source": [
    "# we do not only want to encode the identity of the tokens, but also their position\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32  # how many independent sequences will we process in parallel?\n",
    "block_size = 8  # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input_sp0.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)  # embedding token\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # embedding position\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)  # adding a true embedding layer: map from embedding to token vocab\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C) = (batch, time, channel), channel = vocab_size\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C), arange: all ints up to t-1\n",
    "        x = tok_emb + pos_emb  # (B,T,C) will be the addition of token and position embedding\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        B, T = idx.shape\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens (idx also has the PE so we need to crop)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "    def own_softmax(self, x):\n",
    "        eps = 1.e-10\n",
    "        means = torch.mean(x, 1, keepdim=True)[0] + eps\n",
    "        x_exp = torch.exp(x - means)\n",
    "        x_exp_sum = torch.sum(x_exp, 1, keepdim=True) + eps\n",
    "        return x_exp / x_exp_sum + eps\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "463903fc-79ee-4699-9f39-c0cdd0c18b18",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid multinomial distribution (sum of probabilities <= 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid multinomial distribution (sum of probabilities <= 0)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.multinomial(torch.tensor([ 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b208774-11e1-4d48-acc3-b9ac910a9e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self-attention\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 32  # batch, time, channels = the information in each token is 32-dim (for 4 batches of 8 contextlength)\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# single-head self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)  # bias=flase: essentially just a matrix multiplication\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)  # now apply simple linear model to x (B, T, 16)\n",
    "q = query(x)  # (B, T, 16)\n",
    "wei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T, T))  # we do not want this to be all 0s: some tokens might find certain other ones more interesting\n",
    "# self-attention solves this: every single token in the past emits two vectors: a query Q and a key K\n",
    "# Q: what am I looking for?\n",
    "# K: what do I contain?\n",
    "# dot-product my query Q with all other keys k -> wei (higher correlates -> larger dot-product -> larger weight)\n",
    "\n",
    "wei = wei.masked_fill( tril == 0, float('-inf'))  # not looking into the future again\n",
    "wei = F.softmax(wei, dim=-1)  # make a nice distribution out of the rather rough dot-product-values (softmax = exp and normalize)\n",
    "out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49dae42d-a2da-48a6-853c-fe71cb3cd40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8bc7225-0559-48dc-9fd3-1275fe03f7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self-attention - a look under the hood\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 32  # batch, time, channels = the information in each token is 32-dim (for 4 batches of 8 contextlength)\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# single-head self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)  # bias=flase: essentially just a matrix multiplication\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)  # now apply simple linear model to x (B, T, 16)\n",
    "q = query(x)  # (B, T, 16)\n",
    "wei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T, T))  # we do not want this to be all 0s: some tokens might find certain other ones more interesting\n",
    "# self-attention solves this: every single token in the past emits two vectors: a query Q and a key K\n",
    "# Q: what am I looking for?\n",
    "# K: what do I contain?\n",
    "# dot-product my query Q with all other keys k -> wei (higher correlates -> larger dot-product -> larger weight)\n",
    "\n",
    "#wei = wei.masked_fill( tril == 0, float('-inf'))\n",
    "#wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0f804f0-fb6a-495d-a330-19fbfad25486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7629, -1.3011,  0.5652,  2.1616, -1.0674,  1.9632,  1.0765, -0.4530],\n",
       "        [-3.3334, -1.6556,  0.1040,  3.3782, -2.1825,  1.0415, -0.0557,  0.2927],\n",
       "        [-1.0226, -1.2606,  0.0762, -0.3813, -0.9843, -1.4303,  0.0749, -0.9547],\n",
       "        [ 0.7836, -0.8014, -0.3368, -0.8496, -0.5602, -1.1701, -1.2927, -1.0260],\n",
       "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,  0.8638,  0.3719,  0.9258],\n",
       "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,  1.4187,  1.2196],\n",
       "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,  0.8048],\n",
       "        [-1.8044, -0.4126, -0.8306,  0.5899, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]  # the raw affinities between the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0dd8fe7a-d79b-4155-b69e-44591e902fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self-attention now with value V added\n",
    "# attention is essentailly a communication mechanism\n",
    "# can be seen as a directed graph: a node (token) collects info from nodes (tokens) that point to it\n",
    "# positional encoding (PE) needed as there is no notion of space; PE tells the tokens where they are\n",
    "# not like convolution which contains location information\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 32  # batch, time, channels = the information in each token is 32-dim (for 4 batches of 8 contextlength)\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# single-head self-attention\n",
    "head_size = 16\n",
    "# self-attention solves this: every single token in the past emits two vectors: a query Q and a key K\n",
    "# Q: what am I looking for?\n",
    "# K: what do I contain? (publicly, I also have some private info)\n",
    "# dot-product my query Q with all other keys k -> wei (higher correlates -> larger dot-product -> larger weight)\n",
    "# V: value, the private information of the token: if you find me interesting, here is what I will give you\n",
    "key = nn.Linear(C, head_size, bias=False)  # bias=flase: essentially just a matrix multiplication\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)  # now apply simple linear model to x (B, T, 16)\n",
    "q = query(x)  # (B, T, 16)\n",
    "wei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill( tril == 0, float('-inf'))  # not looking into the future again - delete this for an encoder (here we have a decoder)\n",
    "wei = F.softmax(wei, dim=-1)  # make a nice distribution out of the rather rough dot-product-values (softmax = exp and normalize)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d713f2c8-2e8d-4407-a0d8-05fb97fbdab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007, -0.5239,\n",
       "         -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,  0.2862,  0.5710],\n",
       "        [ 0.6764, -0.5477, -0.2478,  0.3143, -0.1280, -0.2952, -0.4296, -0.1089,\n",
       "         -0.0493,  0.7268,  0.7130, -0.1164,  0.3266,  0.3431, -0.0710,  1.2716],\n",
       "        [ 0.4823, -0.1069, -0.4055,  0.1770,  0.1581, -0.1697,  0.0162,  0.0215,\n",
       "         -0.2490, -0.3773,  0.2787,  0.1629, -0.2895, -0.0676, -0.1416,  1.2194],\n",
       "        [ 0.1971,  0.2856, -0.1303, -0.2655,  0.0668,  0.1954,  0.0281, -0.2451,\n",
       "         -0.4647,  0.0693,  0.1528, -0.2032, -0.2479, -0.1621,  0.1947,  0.7678],\n",
       "        [ 0.2510,  0.7346,  0.5939,  0.2516,  0.2606,  0.7582,  0.5595,  0.3539,\n",
       "         -0.5934, -1.0807, -0.3111, -0.2781, -0.9054,  0.1318, -0.1382,  0.6371],\n",
       "        [ 0.3428,  0.4960,  0.4725,  0.3028,  0.1844,  0.5814,  0.3824,  0.2952,\n",
       "         -0.4897, -0.7705, -0.1172, -0.2541, -0.6892,  0.1979, -0.1513,  0.7666],\n",
       "        [ 0.1866, -0.0964, -0.1430,  0.3059,  0.0834, -0.0069, -0.2047, -0.1535,\n",
       "         -0.0762,  0.3269,  0.3090,  0.0766,  0.0992,  0.1656,  0.1975,  0.7625],\n",
       "        [ 0.1301, -0.0328, -0.4965,  0.2865,  0.2704, -0.2636, -0.0738,  0.3786,\n",
       "          0.0746,  0.0338,  0.0147,  0.3194,  0.2993, -0.1653, -0.0386,  0.3375]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee25de55-8c6d-436b-9913-aaeac358d2ff",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc2ecf60-cd95-4528-be77-29c9aab7ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo: scale to variance 1\n",
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "418301e5-b661-4598-9154-e19157427aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9006)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcfc0acd-d450-4f20-96bf-84d16a6cc12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax sharpens values, i.e. makes the data more peaky\n",
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b6631c2-ff22-46d8-b49d-03a76838454a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot; then we just aggregate info from a single node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64bb42a7-d149-4122-83e4-b4ab684fc8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ae4cee4-e2e6-4894-8d46-c7197c836d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "389ba232-7f8c-4788-bb29-369e7b451f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-9.5367e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c132884a-98d5-4b48-b3de-ccb9fd1ef51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# French to English translation example:\n",
    "\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les rseaux de neurones sont gniaux! <START> neural networks are awesome!<END>\n",
    "\n",
    "# we just implemented a decoder here\n",
    "# enc-dec: encoder looks at all tokens, forward and backward; K+V passes from the encoder to the decoder\n",
    "# GPT just a decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1265aa-384c-401f-8788-30b40e24d73b",
   "metadata": {},
   "source": [
    "### Full finished code, for reference\n",
    "You may want to refer directly to the git repo instead though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21b4d10a-624c-4f82-ac97-7fc37ff81f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: train loss 4.4116, val loss 4.4022\n",
      "step 100: train loss 2.6568, val loss 2.6670\n",
      "step 200: train loss 2.5090, val loss 2.5058\n",
      "step 300: train loss 2.4198, val loss 2.4339\n",
      "step 400: train loss 2.3502, val loss 2.3563\n",
      "step 500: train loss 2.2965, val loss 2.3128\n",
      "step 600: train loss 2.2411, val loss 2.2498\n",
      "step 700: train loss 2.2062, val loss 2.2199\n",
      "step 800: train loss 2.1640, val loss 2.1870\n",
      "step 900: train loss 2.1244, val loss 2.1507\n",
      "step 1000: train loss 2.1035, val loss 2.1302\n",
      "step 1100: train loss 2.0693, val loss 2.1174\n",
      "step 1200: train loss 2.0380, val loss 2.0798\n",
      "step 1300: train loss 2.0248, val loss 2.0646\n",
      "step 1400: train loss 1.9916, val loss 2.0361\n",
      "step 1500: train loss 1.9699, val loss 2.0306\n",
      "step 1600: train loss 1.9635, val loss 2.0488\n",
      "step 1700: train loss 1.9406, val loss 2.0144\n",
      "step 1800: train loss 1.9078, val loss 1.9939\n",
      "step 1900: train loss 1.9095, val loss 1.9888\n",
      "step 2000: train loss 1.8847, val loss 1.9950\n",
      "step 2100: train loss 1.8714, val loss 1.9759\n",
      "step 2200: train loss 1.8579, val loss 1.9605\n",
      "step 2300: train loss 1.8540, val loss 1.9516\n",
      "step 2400: train loss 1.8410, val loss 1.9418\n",
      "step 2500: train loss 1.8177, val loss 1.9443\n",
      "step 2600: train loss 1.8259, val loss 1.9359\n",
      "step 2700: train loss 1.8132, val loss 1.9357\n",
      "step 2800: train loss 1.8037, val loss 1.9219\n",
      "step 2900: train loss 1.8030, val loss 1.9301\n",
      "step 3000: train loss 1.7930, val loss 1.9194\n",
      "step 3100: train loss 1.7710, val loss 1.9166\n",
      "step 3200: train loss 1.7540, val loss 1.9130\n",
      "step 3300: train loss 1.7554, val loss 1.9039\n",
      "step 3400: train loss 1.7551, val loss 1.8949\n",
      "step 3500: train loss 1.7398, val loss 1.8943\n",
      "step 3600: train loss 1.7241, val loss 1.8854\n",
      "step 3700: train loss 1.7318, val loss 1.8874\n",
      "step 3800: train loss 1.7186, val loss 1.8896\n",
      "step 3900: train loss 1.7206, val loss 1.8704\n",
      "step 4000: train loss 1.7137, val loss 1.8573\n",
      "step 4100: train loss 1.7128, val loss 1.8719\n",
      "step 4200: train loss 1.7032, val loss 1.8615\n",
      "step 4300: train loss 1.6964, val loss 1.8436\n",
      "step 4400: train loss 1.7071, val loss 1.8653\n",
      "step 4500: train loss 1.6918, val loss 1.8558\n",
      "step 4600: train loss 1.6876, val loss 1.8351\n",
      "step 4700: train loss 1.6859, val loss 1.8476\n",
      "step 4800: train loss 1.6641, val loss 1.8409\n",
      "step 4900: train loss 1.6675, val loss 1.8348\n",
      "step 4999: train loss 1.6624, val loss 1.8222\n",
      "\n",
      "And they bride with to loves that set bube to take Our my call ands:\n",
      "Whith fult him to barte?\n",
      "\n",
      "DUKE, MENENCE:\n",
      "What, art zorn heart, Let fuit heart my would\n",
      "As egriant:\n",
      "You, will is there by waste\n",
      "Wiltness is wan thel lind that.\n",
      "His my now by atcher is plaw yet:\n",
      "Mistrence open, and whom\n",
      "that dematherly aling to dry I brow'll my\n",
      "Murning in him:\n",
      "And-litgre-borrute kingn,\n",
      "Turt first; if his shat thy flear thee firsh?\n",
      "\n",
      "KING HENRY VI:\n",
      "Hastied is wards be his great huis courtear tear repts I\n",
      "arm for his neet. Ponsmagumt of come, my mest a cempreame?\n",
      "\n",
      "NORFOLK:\n",
      "Whis and whom the head not us bod,\n",
      "Thou confessyy stiltchs lome\n",
      "This son to evers.\n",
      "\n",
      "LUCIO:\n",
      "Abenerly to-madins!\n",
      "\n",
      "KING HENMIONRY HARD III:\n",
      "Have the eRpeakes: there loves\n",
      "Tunniough, if I hine. Pomfare, throbed what\n",
      "which uncertute of vollumann's, do so lace.\n",
      "\n",
      "PUTHIO:\n",
      "Firshould,\n",
      "Come in him the not.\n",
      "\n",
      "POLIXENEN:\n",
      "How, too, is of no not the summon approant. whose no crave soleed Blowly made that O\n",
      "For father watch the\n",
      "diout in not Gleen well. Yet runt, I boy sofle, I hursen as entren the vie,\n",
      "The notitanous, withalte or word.\n",
      "Me, maste mines and enterying.\n",
      "\n",
      "GRUMERIZAPULET:\n",
      "Yet whide,\n",
      "And where\n",
      "He bence oIn percale the call as ince any in Becain,\n",
      "Thou tell true exre breamn'd my have eat toge\n",
      "Betta detion for Hencher morral brack!\n",
      "And forther worjess the vown;\n",
      "Mad there this ands, all this clady,\n",
      "Albety dong hold my lied the beguts\n",
      "with vonsure that thou the baste hold;\n",
      "I they none to me, meet, speak.\n",
      "\n",
      "LUCHIUS:\n",
      "While these art woll, we\n",
      "ell smen of my soul be\n",
      "that Lord, as If have to kaviless cornful.\n",
      "\n",
      "POLANNA:\n",
      "Have us\n",
      "Out, strar\n",
      "En EDWARD IV:\n",
      "Who dest thy patried prince of Worscoman wound arm that sure give affive of mhild by as is Pressents\n",
      "He have blay not, there wife\n",
      "Oncer with I heav toge of the hamt,\n",
      "Threo?\n",
      "There courter thee tonguius, give our will the peepose: life?\n",
      "\n",
      "COMINIUS:\n",
      "You do the art; whom more, now would promb,\n",
      "They and sentry talbatte you stan,\n",
      "Thre Riparford: I will down's fortunds, such I he have will,\n",
      "\n",
      "Marr\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16  # how many independent sequences will we process in parallel?\n",
    "block_size = 32  # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input_sp0.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):  # the heads handle the attention - the communication between the nodes (tokens)\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):  # takes headsize\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)  # k,q,v like above\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))  # tril is not a parameter of the model, a buffer in pytorch lingo\n",
    "        # (so it does not get picked up by autograd)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x)  # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B,T,C)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])  # pytorch makes this easy for us, passing just a list of heads\n",
    "        self.proj = nn.Linear(n_embd, n_embd)  # project back into the original pathway\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # just concatinate the results, concat over channel dimension (dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):  # this handles the per-token (node) computation\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # the inner ff layer has 4 times the embedding dimension (to grow the computation on the residual pathway)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),  # this is the projection layer going back into the original pathway (for the skip-connection) and going down from 4x\n",
    "            nn.Dropout(dropout),  # added right before the residual pathway comes back (dropout trains subnetworks by randomly shutting off neurons)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # this is at a per token level: they think on all the attention data they got individually\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head  # watch out that it works out channelwise (e.g. 32 embedding, 4 heads, 8 headsize)\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)  # the heads running in parallel - the communication\n",
    "        self.ffwd = FeedFoward(n_embd)  # simple feedforward: at token level, all tokens do this independently: gathered all info from attention, think on it\n",
    "        # the ff does the computations\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):  # with skip connection, forking off direct gradient pass-through and block adjustments\n",
    "        x = x + self.sa(self.ln1(x))  # the x + is the skip connection: it lets the gradient pass through (particularly initially), then the blocks adjust\n",
    "        x = x + self.ffwd(self.ln2(x))  # note that the layer norms are applied before it goes into self-attention or ff, per token\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        # now the network starts to get really deep: suffers from computation problems, two ways to address this: (see Add & Norm in the figure)\n",
    "        # 1. skip (residual) connections, see https://github.com/KaimingHe/deep-residual-networks (lets gradients pass through and lets blocks adjust them gradually)\n",
    "        # 2. layer norms = Norm: similar to batch normalization: ensure each neuron has unit Gaussian distribution accross batch dimension\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm (now it is more common to norm before the computations unlike in the trafo paper - called prenorm)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        x = self.blocks(x)  # (B,T,C)\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens (idx also has the PE so we need to crop) (not doing this causes a CUDA error due to index out of bounds)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cdb7fbb-211f-4760-8680-b559f0d79f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loss > validation loss: check for overfitting\n",
    "# now we scale the model up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95e8c92f-c219-43dd-8016-6062a9eef4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.788929 M parameters\n",
      "step 0: train loss 4.2849, val loss 4.2823\n",
      "step 500: train loss 2.0013, val loss 2.0882\n",
      "step 1000: train loss 1.5956, val loss 1.7720\n",
      "step 1500: train loss 1.4395, val loss 1.6371\n",
      "step 2000: train loss 1.3406, val loss 1.5667\n",
      "step 2500: train loss 1.2803, val loss 1.5329\n",
      "step 3000: train loss 1.2261, val loss 1.5056\n",
      "step 3500: train loss 1.1831, val loss 1.4855\n",
      "step 4000: train loss 1.1454, val loss 1.4823\n",
      "step 4500: train loss 1.1098, val loss 1.4797\n",
      "step 4999: train loss 1.0763, val loss 1.4900\n",
      "\n",
      "Clarence and enemy digsing.\n",
      "\n",
      "Here's abroad.\n",
      "\n",
      "ELBOW:\n",
      "If it be resternoed, might he consul, these he spents ale\n",
      "that this hand to drivine of substance\n",
      "which his purbling and rusts rise his fight,\n",
      "or his subject in a kingdom and all:\n",
      "God boots it some could win him harlibly,\n",
      "And knows no more blackvace.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "Be it as adoing thy grue to tyrer.\n",
      "\n",
      "BUCKINGHAM:\n",
      "Poor horse! and cross we did it so, mady young,\n",
      "What's heart? what, his pathyst? loveliament!\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "Never?\n",
      "Lo, what now, I think you will evil make;\n",
      "And she am I do now not? They are, for I may know\n",
      "Their lover shallow together; but both hold\n",
      "Harry Daubler woo shall any their dishes\n",
      "Without their iples of her frience, the shadows ourself,\n",
      "So now to bashke his varlance, deliver fallsed.\n",
      "Dascend arm, as be rough, we are believe,\n",
      "I'll pibs the law of like prence Jares of Gloucester;\n",
      "With that for this blazel upon his breats since,\n",
      "Therein they may bill usand say no mean,\n",
      "Farewell's words must I such as his sword;\n",
      "At who doubt, no demerate was my with cast\n",
      "And it play the stay follow these preservations,\n",
      "Stand, eyes the earth, when have in hands,\n",
      "But full of largeness.\n",
      "Then yet in reason I confine the sand give your six\n",
      "For senseless shall sweel ling comfort on.\n",
      "\n",
      "SICINIUS:\n",
      "The kings it now pity,--he is enough his\n",
      "He and him the fairest hence.\n",
      "Will you bear? hie come no more more repend,\n",
      "And twenty truth for than causes murd I do.\n",
      "\n",
      "BRAKENBURY:\n",
      "Clifford, gentle Lord of RosaliN,\n",
      "Should allow straight a Richard-kneest smays;\n",
      "Then laught that I, be let say thou Romeo, and though\n",
      "Without-haste lips: before the right of Caintol.\n",
      "\n",
      "RIVERS:\n",
      "Here.\n",
      "\n",
      "JOHN GRUMEON:\n",
      "Grandam, my lord; for you such your well.\n",
      "\n",
      "THORSMOMELIA:\n",
      "A given son! what was it let till I be seen.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "Faith, leting me\n",
      "HERY PERCK:\n",
      "Sumicils and will quarrel adoleng Edward:\n",
      "Here in thyself, Exton revel again,\n",
      "Fetch more like peace on his time down hears,\n",
      "Sifter King Hennear, and I in shortly led!\n",
      "Unampul fable gall true speeding of \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64  # how many independent sequences will we process in parallel?\n",
    "block_size = 256  # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input_sp0.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):  # the heads handle the attention - the communication between the nodes (tokens)\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):  # takes headsize\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)  # k,q,v like above\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))  # tril is not a parameter of the model, a buffer in pytorch lingo\n",
    "        # (so it does not get picked up by autograd)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x)  # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B,T,C)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])  # pytorch makes this easy for us, passing just a list of heads\n",
    "        self.proj = nn.Linear(n_embd, n_embd)  # project back into the original pathway\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # just concatinate the results, concat over channel dimension (dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):  # this handles the per-token (node) computation\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # the inner ff layer has 4 times the embedding dimension (to grow the computation on the residual pathway)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),  # this is the projection layer going back into the original pathway (for the skip-connection) and going down from 4x\n",
    "            nn.Dropout(dropout),  # added right before the residual pathway comes back (dropout trains subnetworks by randomly shutting off neurons)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # this is at a per token level: they think on all the attention data they got individually\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head  # watch out that it works out channelwise (e.g. 32 embedding, 4 heads, 8 headsize)\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)  # the heads running in parallel - the communication\n",
    "        self.ffwd = FeedFoward(n_embd)  # simple feedforward: at token level, all tokens do this independently: gathered all info from attention, think on it\n",
    "        # the ff does the computations\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):  # with skip connection, forking off direct gradient pass-through and block adjustments\n",
    "        x = x + self.sa(self.ln1(x))  # the x + is the skip connection: it lets the gradient pass through (particularly initially), then the blocks adjust\n",
    "        x = x + self.ffwd(self.ln2(x))  # note that the layer norms are applied before it goes into self-attention or ff, per token\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        # now the network starts to get really deep: suffers from computation problems, two ways to address this: (see Add & Norm in the figure)\n",
    "        # 1. skip (residual) connections, see https://github.com/KaimingHe/deep-residual-networks (lets gradients pass through and lets blocks adjust them gradually)\n",
    "        # 2. layer norms = Norm: similar to batch normalization: ensure each neuron has unit Gaussian distribution accross batch dimension\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm (now it is more common to norm before the computations unlike in the trafo paper - called prenorm)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        x = self.blocks(x)  # (B,T,C)\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens (idx also has the PE so we need to crop) (not doing this causes a CUDA error due to index out of bounds)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4cfaa1-fa59-4b1b-a0ed-4e505896ce01",
   "metadata": {},
   "source": [
    "# how would we train GPT?\n",
    "two steps:\n",
    "1. pretraining (what we have done here, but larger with a lot of text from the internet): just enable it to babble text (does not answer questions)\n",
    "2. finetuning stage: 2.1 collect question-answer data and feed it to model  2.2 let human raters evaluate the model output to train a reward model 2.3 run PPO reinforcement to optimize the model\n",
    "\n",
    "our dataset was Shakespear (1 MM tokens), with subword (GPT) this corresponds to about 300.000 tokens - 300 billion for GPT large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9747281-ca23-4ac1-a62c-09eaae63e495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
