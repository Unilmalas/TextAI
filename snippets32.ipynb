{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['henri', 'hass', 'rumor', 'spun', 'wild', 'tale', 'horror', 'neptun', 'legendari', 'race', 'rumor', 'hideou', 'realiti', 'captain', 'captiv', 'crew', 'transcrib', 'note', 'etext', 'produc', 'planet', 'stori', 'summer', 'extens', 'research', 'uncov', 'evid', 'copyright', 'public', 'renew', '223_680', 'bafflement', 'board', 'patrol', '1096_1159', 'deflector', 'needl', 'gyrat', 'wildli', 'begun', 'minut', 'ago', 'lip', 'tighten', 'irrit', 'mate', 'peer', 'inquisit', 'shoulder', 'better', 'check', 'cours', 'sir', 'point', 'perfect', 'slightest', 'aberr', '680_223', 'breath', 'figur', 'object', 'dead', 'ahead', 'caus', 'disturb', 'deflector', 'beam', 'shunt', 'pivot', 'pace', 'control', 'room', 'fix', 'steadili', 'visipanel', 'mere', 'shook', '592_692', 'panel', 'magnifi', 'cours', 'sharpest', 'patrol', 'damn', 'bit', '680_737', 'stare', 'crazi', 'needl', 'jump', 'bear', '317_109', 'cut', 'jet', 'better', 'drift', 'better', 'abl', 'determin', 'wrong']\n",
      "command janu\n",
      "ship space\n",
      "janu command\n",
      "head ketrik\n",
      "janu look\n",
      "devri blake\n",
      "blake devri\n",
      "came proktol\n",
      "ketrik ketrik\n",
      "squar proktol\n",
      "ketrik blake\n",
      "look ketrik\n",
      "look like\n",
      "perrin pirat\n",
      "black ey\n",
      "devri look\n",
      "said janu\n",
      "look saw\n",
      "look ey\n",
      "like ketrik\n",
      "space ship\n",
      "janu know\n",
      "ey look\n",
      "said perrin\n",
      "janu littl\n",
      "ketrik said\n",
      "said ketrik\n",
      "ross wasp\n",
      "janu stop\n",
      "turn janu\n",
      "ey ey\n",
      "atom blast\n",
      "ross wasp\n",
      "janu ross\n",
      "wasp ross\n",
      "hand atom\n",
      "atom hand\n",
      "saw look\n",
      "janu think\n",
      "devri watch\n",
      "janu look\n",
      "saw saw\n",
      "blast ship\n",
      "flame pistol\n",
      "ketrik head\n",
      "pistol flame\n",
      "know janu\n",
      "janu command\n",
      "pistol janu\n",
      "pirat perrin\n",
      "perrin perrin\n",
      "ey black\n",
      "naric naric\n",
      "devri sound\n",
      "devri face\n",
      "said look\n",
      "proktol ketrik\n",
      "said ye\n",
      "naric said\n",
      "saw space\n",
      "flame leap\n",
      "ship space\n",
      "awai turn\n",
      "janu stop\n",
      "naric ey\n",
      "naric naric\n",
      "ketrik ketrik\n",
      "leap flame\n",
      "flame pistol\n",
      "ketrik said\n",
      "naric word\n",
      "turn naric\n",
      "devri ketrik\n",
      "brain brain\n",
      "blast atom\n",
      "devri know\n",
      "turn ey\n",
      "ketrik head\n",
      "look like\n",
      "devri naric\n",
      "devri blast\n",
      "awai janu\n",
      "devri saw\n",
      "proktol devri\n",
      "proktol ketrik\n",
      "ship space\n",
      "proktol flame\n",
      "devri blake\n",
      "proktol devri\n",
      "devri ketrik\n",
      "thought janu\n",
      "look ketrik\n",
      "said look\n",
      "devri proktol\n",
      "come awai\n",
      "proktol came\n",
      "turn men\n",
      "awai come\n",
      "ketrik head\n",
      "brain janu\n",
      "think said\n",
      "janu think\n",
      "watch devri\n",
      "watch naric\n",
      "watch naric\n",
      "naric ey\n",
      "proktol leap\n",
      "naric word\n",
      "devri naric\n",
      "naric turn\n",
      "shine stone\n",
      "stone shine\n",
      "devri ross\n",
      "shine stone\n",
      "shine stone\n",
      "stone shine\n",
      "think janu\n",
      "turn naric\n",
      "ketrik naric\n",
      "ketrik head\n",
      "naric ketrik\n",
      "leap proktol\n",
      "flame proktol\n",
      "pistol proktol\n",
      "ketrik naric\n",
      "pistol flame\n",
      "ketrik head\n",
      "naric turn\n",
      "turn naric\n",
      "devri said\n",
      "said devri\n",
      "blast devri\n",
      "janu command\n",
      "stone shine\n",
      "blake blake\n",
      "perrin command\n",
      "devri know\n",
      "blake blake\n",
      "littl janu\n",
      "janu proktol\n",
      "awai devri\n",
      "blake blake\n",
      "look devri\n",
      "ship space\n",
      "thought said\n",
      "fleet space\n",
      "janu perrin\n",
      "atom blast\n",
      "blast atom\n",
      "proktol came\n",
      "blake blake\n",
      "window devri\n",
      "squar proktol\n",
      "flame proktol\n",
      "janu turn\n",
      "naric naric\n",
      "come come\n",
      "devri proktol\n",
      "naric naric\n",
      "pistol proktol\n",
      "said look\n",
      "lahk tzor\n",
      "naric naric\n",
      "tzor lahk\n",
      "brain think\n",
      "naric naric\n",
      "janu think\n",
      "brain brain\n",
      "naric naric\n",
      "blake blake\n",
      "devri devri\n",
      "naric naric\n",
      "janu littl\n",
      "pistol flame\n",
      "devri pistol\n",
      "watch devri\n",
      "devri said\n",
      "watch naric\n",
      "blake blake\n",
      "blake blake\n",
      "said naric\n",
      "flame pistol\n",
      "pistol flame\n",
      "blake blake\n",
      "ketrik ketrik\n",
      "look said\n",
      "awai saw\n",
      "proktol came\n",
      "flame pistol\n",
      "proktol squar\n",
      "head ketrik\n",
      "flame pistol\n",
      "pistol flame\n",
      "command janu\n",
      "proktol squar\n",
      "janu littl\n",
      "blake blake\n",
      "janu said\n",
      "devri devri\n",
      "blake blake\n",
      "come come\n",
      "awai saw\n",
      "awai saw\n",
      "pistol flame\n",
      "blake blake\n",
      "command janu\n",
      "scientif brain\n",
      "blake blake\n",
      "look said\n",
      "said naric\n",
      "devri turn\n",
      "brain thought\n",
      "brain brain\n",
      "brain blast\n",
      "janu look\n",
      "brain janu\n",
      "brain think\n",
      "mental brain\n",
      "thought brain\n",
      "janu know\n",
      "brain brain\n",
      "brain janu\n",
      "janu littl\n",
      "said janu\n",
      "brain devri\n",
      "brain janu\n",
      "blake blake\n",
      "blake blake\n",
      "janu look\n",
      "saw awai\n",
      "devri devri\n",
      "blast brain\n",
      "thought forc\n",
      "forc thought\n",
      "forc thought\n",
      "brain power\n",
      "janu look\n",
      "devri ross\n",
      "devri devri\n",
      "brain devri\n",
      "devri said\n",
      "blake blake\n",
      "blake blake\n",
      "devri devri\n",
      "devri turn\n",
      "said janu\n",
      "devri devri\n",
      "devri time\n",
      "devri devri\n",
      "devri said\n",
      "devri devri\n",
      "said devri\n",
      "said janu\n",
      "janu look\n",
      "devri janu\n",
      "said janu\n",
      "janu said\n",
      "janu devri\n",
      "devri devri\n",
      "devri proktol\n",
      "devri devri\n",
      "janu devri\n",
      "devri janu\n",
      "janu janu\n",
      "devri devri\n",
      "devri proktol\n",
      "janu janu\n",
      "power brain\n",
      "devri devri\n",
      "brain power\n",
      "devri devri\n",
      "brain brain\n",
      "devri devri\n",
      "devri proktol\n",
      "devri devri\n",
      "proktol devri\n",
      "janu janu\n",
      "devri devri\n",
      "janu janu\n",
      "janu janu\n",
      "janu janu\n",
      "devri janu\n",
      "janu devri\n",
      "janu janu\n",
      "brain devri\n",
      "janu janu\n",
      "devri devri\n",
      "janu janu\n",
      "devri devri\n",
      "devri devri\n",
      "devri devri\n",
      "janu janu\n",
      "janu janu\n",
      "devri devri\n",
      "janu janu\n",
      "atom brain\n",
      "janu janu\n",
      "blast atom\n",
      "janu janu\n",
      "atom brain\n",
      "devri devri\n",
      "devri devri\n",
      "ross devri\n",
      "atom blast\n",
      "blast atom\n",
      "devri devri\n",
      "brain brain\n",
      "devri devri\n",
      "brain brain\n",
      "devri devri\n",
      "atom blast\n",
      "blast brain\n",
      "devri devri\n",
      "blast brain\n",
      "brain power\n",
      "brain blast\n",
      "power brain\n",
      "brain brain\n",
      "blast atom\n",
      "brain brain\n",
      "brain brain\n",
      "janu janu\n",
      "brain brain\n",
      "rocket plane\n",
      "plane rocket\n",
      "brain brain\n",
      "janu janu\n",
      "janu janu\n",
      "perrin perrin\n",
      "perrin perrin\n",
      "perrin perrin\n"
     ]
    }
   ],
   "source": [
    "# word pair encoding\n",
    "import os\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim import corpora\n",
    "\n",
    "def read1k():\n",
    "    return f.read(1024)\n",
    "\n",
    "def read_in_chunks(infile, chunk_size=1024*64):\n",
    "    chunk = infile.read(chunk_size)\n",
    "    while chunk:\n",
    "        yield chunk\n",
    "        chunk = infile.read(chunk_size)\n",
    "\n",
    "def process_data(chunk, text):\n",
    "    text.append(str(chunk)) # 'utf8' codec can't decode byte 0xc3\n",
    "\n",
    "def rmsword(corpus, stopwords):\n",
    "    '''remove stopwords from corpus'''\n",
    "    return [word for word in corpus if word not in stopwords]\n",
    "\n",
    "def chunks(l, n):\n",
    "    '''Yield successive n-sized chunks from list l'''\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n] # returns a generator\n",
    "\n",
    "def chunksep(l, s):\n",
    "    '''Yield successive chunks from list l separated by s'''\n",
    "    g = []\n",
    "    for el in l:\n",
    "        if el == s:\n",
    "            yield g\n",
    "            g = []\n",
    "        g.append(el)\n",
    "    yield g\n",
    "    \n",
    "def ctprs(txt, ex_sp=False) -> dict:\n",
    "    '''count symbol pair frequencies'''\n",
    "    pairs = defaultdict(int)\n",
    "    for i in range(len(txt) - 1):\n",
    "        if ex_sp:\n",
    "            if txt[i] == ' ' or txt[i+1] == ' ':\n",
    "                continue\n",
    "        pairs[txt[i], txt[i+1]] += 1\n",
    "    return pairs\n",
    "\n",
    "def ctwp(corpus, wds=1):\n",
    "    '''count close word frequencies for corpus (tokenized) at distance wds'''\n",
    "    res = {}\n",
    "    for i in range(wds, len(corpus)-wds):\n",
    "        if corpus[i-wds] == corpus[i] or corpus[i+wds] == corpus[i]:\n",
    "            try:\n",
    "                res[corpus[i]] += 1\n",
    "            except KeyError:\n",
    "                res[corpus[i]] = 1\n",
    "    return res\n",
    "\n",
    "def frqprsd(corpus, wds=1):\n",
    "    '''find the most frequent word pairs at the given distance'''\n",
    "    res = {}\n",
    "    for it0 in range(len(corpus)): # first token\n",
    "        for it1 in [max(it0-wds,0),min(it0+wds,len(corpus)-1)]:\n",
    "            if it0 != it1:\n",
    "                try:\n",
    "                    res[str(corpus[it0]) + '_' + str(corpus[it1])] += 1\n",
    "                except KeyError:\n",
    "                    res[str(corpus[it0]) + '_' + str(corpus[it1])] = 1\n",
    "    return res  \n",
    "\n",
    "def frqprsw(corpus, wds=1):\n",
    "    '''find the most frequent word pairs inside the given distance'''\n",
    "    res = {}\n",
    "    for it0 in range(len(corpus)): # first token\n",
    "        for it1 in range(max(it0-wds,0),min(it0+wds,len(corpus)-1)):\n",
    "            if it0 != it1:\n",
    "                try:\n",
    "                    res[str(corpus[it0]) + '_' + str(corpus[it1])] += 1\n",
    "                except KeyError:\n",
    "                    res[str(corpus[it0]) + '_' + str(corpus[it1])] = 1\n",
    "    return res   \n",
    "\n",
    "def main():\n",
    "    os.chdir(r'D:\\BernieData\\DeepL\\lrgtxt0')\n",
    "    f = open('Proktols of Neptune.txt', encoding=\"utf-8\")\n",
    "    text = []\n",
    "    for piece in read_in_chunks(f):\n",
    "        process_data(piece, text)\n",
    "    corpus = preprocess_string(' '.join(text))\n",
    "    dct = corpora.Dictionary([corpus])  # initialize a Dictionary\n",
    "    # tokenize text\n",
    "    corptzd = [dct.token2id[x] for x in corpus] # tokenized corpus\n",
    "    #print(ctwp(corptzd, 1))\n",
    "    #print(dct.keys())\n",
    "    #fw = frqprsd(corptzd, 1)\n",
    "    #print(fw)\n",
    "    \n",
    "    for niter in range(100):\n",
    "        # find most frequent pairs\n",
    "        fwd = frqprsw(corptzd, 5+niter)\n",
    "        fl = []\n",
    "        for fel in fwd.items():\n",
    "            if fel[1] > 5+niter:\n",
    "                fl.append(fel)\n",
    "        fl.sort(reverse=True, key=lambda el: el[1])\n",
    "        #print(fl)\n",
    "        #print(len(dct.items()))\n",
    "\n",
    "        splitfel = []\n",
    "        felrem = []\n",
    "        felremw = []\n",
    "        # replace words with a concat token e.g. w0_w1\n",
    "        for fel in fl:\n",
    "            thisrml = fel[0].split('_')\n",
    "            splitfel.append(thisrml)\n",
    "            felrem += thisrml[0] + thisrml[1]\n",
    "            i0 = corptzd.index(int(thisrml[0]))\n",
    "            i1 = corptzd.index(int(thisrml[1]))\n",
    "            if (i1 - i0) <= (5+niter):\n",
    "                corptzd.pop(i1) # remove original pairs from corpus\n",
    "                corptzd.pop(i0)\n",
    "                dct.add_documents([[fel[0]]])\n",
    "                corptzd.insert(i0, dct.token2id[fel[0]]) # add new paired token\n",
    "                felremw.append((dct[int(thisrml[0])], dct[int(thisrml[1])]))\n",
    "        #print(splitfel)\n",
    "\n",
    "        #corptzd = [x for x in corptzd if x not in felrem]\n",
    "        #print(corptzd[:20])\n",
    "        #print(list(dct.items())[:20])\n",
    "        #print(dct[605])\n",
    "        #corpus = [dct[x] for x in corptzd] # reconstruct corpus\n",
    "        #print(corpus[:50])\n",
    "        #dct.filter_tokens(bad_ids=felrem)\n",
    "        #dct.compactify()\n",
    "        #corptzd = [dct.token2id[x] for x in corpus] # tokenize corpus again\n",
    "        #print(corptzd)\n",
    "        #print(felremw)\n",
    "    corpus = [dct[x] for x in corptzd] # reconstruct corpus\n",
    "    print(corpus[:100])\n",
    "    \n",
    "    for wrd in corpus:\n",
    "        if wrd.find('_') > 0:\n",
    "            thisrml = wrd.split('_')\n",
    "            print(dct[int(thisrml[0])], dct[int(thisrml[1])])\n",
    "    \n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.806\n",
      "1.798\n"
     ]
    }
   ],
   "source": [
    "# binomial sampling example\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def binom(n,k,p):\n",
    "    '''binomial distribution'''\n",
    "    return math.factorial(n) / math.factorial(n-k) / math.factorial(k) * p**k * (1-p)**(n-k)\n",
    "\n",
    "def binomcf(n,k,p):\n",
    "    '''binomial distribution cumulative'''\n",
    "    return sum([binom(n,i,p) for i in range(k)])\n",
    "\n",
    "def binsmpl(n,k,p):\n",
    "    '''return a sample from a binomial distribution'''\n",
    "    ui = random.random() # continuous [0,1]\n",
    "    klow = 0\n",
    "    while klow < 1000: # just to be safe\n",
    "        if binomcf(n,klow,p)<ui and ui<binomcf(n,klow+1,p): # todo: tables for performance\n",
    "            return klow\n",
    "        klow += 1\n",
    "    \n",
    "def main():\n",
    "    #print(binom(6,4,0.3))\n",
    "    avrg = 0.\n",
    "    for i in range(1000):\n",
    "        #print(binsmpl(6,4,0.3))\n",
    "        avrg += binsmpl(6,4,0.3)\n",
    "    print(avrg/1000)\n",
    "    print(np.average(np.random.binomial(6, 0.3, 1000)))\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999907, 739399)\n"
     ]
    }
   ],
   "source": [
    "# truncatable primes\n",
    "# A truncatable prime is a prime number that when you successively\n",
    "# remove digits from one end of the prime, you are left with a new prime number.\n",
    "def primes(n):\n",
    "    multiples = set()\n",
    "    prime = []\n",
    "    for i in range(2, n+1):\n",
    "        if i not in multiples:\n",
    "            prime.append(i)\n",
    "            multiples.update(set(range(i*i, n+1, i)))\n",
    "    return prime\n",
    "\n",
    "def truncp(n):\n",
    "    '''find l+r truncatable primes up to n'''\n",
    "    pl = primes(n)\n",
    "    pl.sort(reverse=True)\n",
    "    res = []\n",
    "    mxpl = 0\n",
    "    mxpr = 0\n",
    "    for cp in pl:\n",
    "        if mxpl == 0:\n",
    "            cpllr = [int(str(cp)[i:]) for i in range(len(str(cp)))]\n",
    "            istrp = True\n",
    "            for cpl in cpllr:\n",
    "                if pl.count(cpl) == 0:\n",
    "                    istrp = False\n",
    "                    break\n",
    "            if istrp:\n",
    "                mxpl = cp\n",
    "        if mxpr == 0:\n",
    "            cpllr = [int(str(cp)[:i]) for i in range(len(str(cp)),0,-1)]\n",
    "            istrp = True\n",
    "            for cpr in cpllr:\n",
    "                if pl.count(cpr) == 0:\n",
    "                    istrp = False\n",
    "                    break\n",
    "            if istrp:\n",
    "                mxpr = cp\n",
    "        if mxpl>0 and mxpr>0:\n",
    "            return (mxpl, mxpr)  \n",
    "    return (mxpl, mxpr)\n",
    "\n",
    "# largest left-truncatable and right-truncatable primes less than one million (base 10 is implied).\n",
    "def main():\n",
    "    print(truncp(1000000))\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1234, 123, 12, 1]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp = 1234\n",
    "#[int(str(cp)[i:]) for i in range(len(str(cp)))]\n",
    "[int(str(cp)[:i]) for i in range(len(str(cp)),0,-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1132\n"
     ]
    }
   ],
   "source": [
    "from numba import jit # https://numba.pydata.org/\n",
    "import random\n",
    "\n",
    "@jit(nopython=True)\n",
    "def monte_carlo_pi(nsamples):\n",
    "    acc = 0\n",
    "    for i in range(nsamples):\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        if (x ** 2 + y ** 2) < 1.0:\n",
    "            acc += 1\n",
    "    return 4.0 * acc / nsamples\n",
    "\n",
    "def main():\n",
    "    print(monte_carlo_pi(10000))\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numba\n",
    "\n",
    "careful: does not work well with Pandas -> test preprocessing, clean data, then deploy numba for performance-intensive parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed (with compilation) = 0.3666656017303467\n",
      "Elapsed (after compilation) = 0.0\n"
     ]
    }
   ],
   "source": [
    "from numba import jit\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "x = np.arange(100).reshape(10, 10)\n",
    "\n",
    "@jit(nopython=True)\n",
    "def go_fast(a): # Function is compiled and runs in machine code\n",
    "    trace = 0.0\n",
    "    for i in range(a.shape[0]):\n",
    "        trace += np.tanh(a[i, i])\n",
    "    return a + trace\n",
    "\n",
    "# DO NOT REPORT THIS... COMPILATION TIME IS INCLUDED IN THE EXECUTION TIME!\n",
    "start = time.time()\n",
    "go_fast(x)\n",
    "end = time.time()\n",
    "print(\"Elapsed (with compilation) = %s\" % (end - start))\n",
    "\n",
    "# NOW THE FUNCTION IS COMPILED, RE-TIME IT EXECUTING FROM CACHE\n",
    "start = time.time()\n",
    "go_fast(x)\n",
    "end = time.time()\n",
    "print(\"Elapsed (after compilation) = %s\" % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5b40505dd45c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mticker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'AMD'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtimeframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'1y'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchartDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'close'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "# finance data sites\n",
    "# https://db.nomics.world/\n",
    "# https://github.com/addisonlynch/iexfinance\n",
    "# Useful Collection of Forecasts: http://www.forecasts.org/stpoor.htm\n",
    "# Massive Collection of Indicators: https://www.assetmacro.com/\n",
    "# Labor Statistics: https://www.bls.gov/\n",
    "# Data from the US Treasury: https://home.treasury.gov/\n",
    "\n",
    "ticker = 'AMD'\n",
    "timeframe = '1y'\n",
    "df = p.chartDF(ticker, timeframe)\n",
    "df = df[['close']]\n",
    "df.reset_index(level=0, inplace=True)\n",
    "df.columns=['ds','y']\n",
    "df.dropna(inplace=True) # dropna() will remove all NaN entries from our dataframe\n",
    "\n",
    "df.fillna(df.mean(), inplace=True) # fillna() will replace NaN entries with the desired argument\n",
    "\n",
    "sns.pairplot(data=df, hue=\"asset_price\")\n",
    "\n",
    "bestfeatures = SelectKBest(score_func=chi2, k='all')\n",
    "fit = bestfeatures.fit(X1,Y1)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(df.columns)\n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score'] #naming the dataframe columns\n",
    "print(featureScores.nlargest(5,'Score')) #print 5 best features\n",
    "\n",
    "sel_feature = ['P/E','Debt','Revenue'] # Select features\n",
    "X1 = df1[sel_feature].values\n",
    "Y1 = df1['math score'].values\n",
    "Y1 = Y1.flatten()\n",
    "X_scale1 = scale(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context vector:  [[5.00000000e+00 6.94397194e-11 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# another attention test\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"softmax applied to set of values x\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / exp_x.sum(axis=0)\n",
    "\n",
    "enc_hidd_st = np.array([[0,1,1], [5,0,1], [1,1,0], [0,5,1]])\n",
    "dec_hid_st = np.array([[10, 5, 10]])\n",
    "\n",
    "scores = np.dot(enc_hidd_st, dec_hid_st.T)\n",
    "sm_scores = softmax(scores)\n",
    "ctx_vec = enc_hidd_st * sm_scores\n",
    "\n",
    "print('context vector: ', np.dot(np.array([[1,1,1,1]]), ctx_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f02018f133d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     68\u001b[0m                     \u001b[0mareas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# Creation date\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[0mdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"searchPropertyDate\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m21\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m31\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m             \u001b[0mcreated\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;31m# Description\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# some webscraper\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "headers = ({'User-Agent':'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML,like Gecko) Chrome/41.0.2228.0 Safari/537.36'})\n",
    "sapo = \"https://casa.sapo.pt/Venda/Apartamentos/?sa=11&or=10\"\n",
    "response = get(sapo, headers=headers)\n",
    "\n",
    "titles = []\n",
    "created = []\n",
    "prices = []\n",
    "areas = []\n",
    "zone = []\n",
    "condition = []\n",
    "descriptions = []\n",
    "urls = []\n",
    "thumbnails = []\n",
    "\n",
    "n_pages = 0\n",
    "for page in range(0,900):\n",
    "    n_pages += 1\n",
    "    sapo_url = 'https://casa.sapo.pt/Venda/Apartamentos/?sa=11&lp=10000&or=10'+'&pn='+str(page)\n",
    "    r = get(sapo_url, headers=headers)\n",
    "    page_html = BeautifulSoup(r.text, 'html.parser')\n",
    "    house_containers = page_html.find_all('div', class_=\"searchResultProperty\")\n",
    "    if house_containers != []:\n",
    "        for container in house_containers:\n",
    "            # Price\n",
    "            price = container.find_all('span')[2].text\n",
    "            if price == 'Contacte Anunciante':\n",
    "                price = container.find_all('span')[3].text\n",
    "                if price.find('/') != -1:\n",
    "                    price = price[0:price.find('/')-1]\n",
    "            if price.find('/') != -1:\n",
    "                price = price[0:price.find('/')-1]\n",
    "            price_ = [int(price[s]) for s in range(0,len(price)) if price[s].isdigit()]\n",
    "            price = ''\n",
    "            for x in price_:\n",
    "                price = price+str(x)\n",
    "            prices.append(int(price))\n",
    "            # Zone\n",
    "            location = container.find_all('p', class_=\"searchPropertyLocation\")[0].text\n",
    "            location = location[7:location.find(',')]\n",
    "            zone.append(location)\n",
    "            # Title\n",
    "            name = container.find_all('span')[0].text\n",
    "            titles.append(name)\n",
    "            # Status\n",
    "            status = container.find_all('p')[5].text\n",
    "            condition.append(status)\n",
    "            # Area\n",
    "            m2 = container.find_all('p')[9].text\n",
    "            if m2 != '-':\n",
    "                m2 = m2.replace('\\xa0','')\n",
    "                m2 = float(\"\".join(itertools.takewhile(str.isdigit, m2)))\n",
    "                areas.append(m2)\n",
    "            else:\n",
    "                m2 = container.find_all('p')[7].text\n",
    "                if m2 != '-':\n",
    "                    m2 = m2.replace('\\xa0','')\n",
    "                    m2 = float(\"\".join(itertools.takewhile(str.isdigit, m2)))\n",
    "                    areas.append(m2)\n",
    "                else:\n",
    "                    areas.append(m2)\n",
    "            # Creation date\n",
    "            date = pd.to_datetime(container.find_all('div', class_=\"searchPropertyDate\")[0].text[21:31])\n",
    "            created.append(date)\n",
    "            # Description\n",
    "            desc = container.find_all('p', class_=\"searchPropertyDescription\")[0].text[7:-6]\n",
    "            descriptions.append(desc)\n",
    "            # url\n",
    "            link = 'https://casa.sapo.pt/' + container.find_all('a')[0].get('href')[1:-6]\n",
    "            urls.append(link)\n",
    "\n",
    "            # image\n",
    "            img = str(container.find_all('img')[0])\n",
    "            img = img[img.find('data-original_2x=')+18:img.find('id=')-2]\n",
    "            thumbnails.append(img)\n",
    "    else:\n",
    "        break\n",
    "                                  \n",
    "    sleep(randint(1,2))\n",
    "print('You scraped {} pages containing {} properties.'.format(n_pages, len(titles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context vector:  [[31.5  0.  11. ]]\n",
      "add/concat:  [[31]]\n"
     ]
    }
   ],
   "source": [
    "# yet another attention test\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    '''softmax applied to set of values x'''\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / exp_x.sum(axis=0)\n",
    "\n",
    "def posscr(x, w):\n",
    "    '''position score via weights'''\n",
    "    return np.dot(x, w)\n",
    "\n",
    "def addconc(eh, dh, wed, wc):\n",
    "    '''additive and concat score'''\n",
    "    return np.dot(np.dot(wed, np.concatenate((eh, dh), axis=0)), wc.T)\n",
    "\n",
    "enc_hidd_st = np.array([[0,1,1], [5,0,1], [1,1,0], [0,5,1]])\n",
    "dec_hid_st = np.array([[10, 5, 10]])\n",
    "\n",
    "scores_v = np.dot(enc_hidd_st, dec_hid_st.T)\n",
    "#scores = softmax(scores_v)\n",
    "scores = posscr(scores_v, np.array([[.1, 0, .1]]))\n",
    "ctx_vec = enc_hidd_st * scores\n",
    "\n",
    "print('context vector: ', np.dot(np.array([[1,1,1,1]]), ctx_vec))\n",
    "\n",
    "print('add/concat: ', addconc(enc_hidd_st, dec_hid_st, np.array([[0,1,0,0,1]]), np.array([[1,1,1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.19\n"
     ]
    }
   ],
   "source": [
    "# queens\n",
    "import random\n",
    "\n",
    "def rddraw(l):\n",
    "    return random.sample(l, len(l))\n",
    "\n",
    "def haspr(l):\n",
    "    lel = l[0]\n",
    "    for i in range(1,len(l)):\n",
    "        if l[i] == lel and lel == 1:\n",
    "            return True\n",
    "        lel = l[i]\n",
    "    return False\n",
    "\n",
    "def main():\n",
    "    #print(rddraw([0,0,1,1]))\n",
    "    #print(haspr(rddraw([0,0,1,1])))\n",
    "    qc = 0\n",
    "    for i in range(10000):\n",
    "        if haspr(rddraw([0,0,1,1])):\n",
    "            qc += 1\n",
    "    print(qc/100)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
