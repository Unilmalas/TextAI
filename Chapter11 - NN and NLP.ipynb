{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download reviews.txt and labels.txt from here: https://github.com/udacity/deep-learning/tree/master/sentiment-network\n",
    "\n",
    "def pretty_print_review_and_label(i):\n",
    "   print(labels[i] + \"\\t:\\t\" + reviews[i][:80] + \"...\")\n",
    "\n",
    "g = open('reviews.txt','r') # What we know!\n",
    "reviews = list(map(lambda x:x[:-1],g.readlines()))\n",
    "g.close()\n",
    "\n",
    "g = open('labels.txt','r') # What we WANT to know!\n",
    "labels = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capturing Word Correlation in Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent Encoding:[1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "onehots = {}\n",
    "onehots['cat'] = np.array([1,0,0,0]) # one-hot encoding\n",
    "onehots['the'] = np.array([0,1,0,0])\n",
    "onehots['dog'] = np.array([0,0,1,0])\n",
    "onehots['sat'] = np.array([0,0,0,1])\n",
    "\n",
    "sentence = ['the','cat','sat']\n",
    "x = onehots[sentence[0]] + \\\n",
    "    onehots[sentence[1]] + \\\n",
    "    onehots[sentence[2]]\n",
    "\n",
    "print(\"Sent Encoding:\" + str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "f = open('reviews.txt') # load files\n",
    "raw_reviews = f.readlines() # one review = one review list item\n",
    "f.close()\n",
    "\n",
    "f = open('labels.txt')\n",
    "raw_labels = f.readlines()\n",
    "f.close()\n",
    "\n",
    "docs = list(map(lambda x: set(x.split(\" \")), raw_reviews)) # build a set out of the reviews (sentences are sets here)\n",
    "\n",
    "vocab = set() # build vocabulary\n",
    "for doc in docs: # sent -> docs as one review is a document consisting of a few sentences\n",
    "    for word in doc:\n",
    "        if(len(word) > 0):\n",
    "            vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "\n",
    "input_dataset = list() # each list item is a review, the vector representing the review has the word indices\n",
    "for doc in docs:\n",
    "    doc_indices = list()\n",
    "    for word in doc:\n",
    "        try:\n",
    "            doc_indices.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(list(set(doc_indices)))\n",
    "\n",
    "target_dataset = list()\n",
    "for label in raw_labels:\n",
    "    if label == 'positive\\n':\n",
    "        target_dataset.append(1) # 1 for pos, 0 for neg, index here == index input_dataset\n",
    "    else:\n",
    "        target_dataset.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'cartoon', '.', 'comedy', 'a', 'high', 'bromwell', 'is'}]\n"
     ]
    }
   ],
   "source": [
    "raw_reviews = ['bromwell high is a cartoon comedy .']\n",
    "print(list(map(lambda x: set(x.split(' ')), raw_reviews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'', 'life', 'far', '.', 'at', 'i', 'tried', 'high', 'bromwell', 'programs', 'students', 'is', 'immediately', 'my', 'when', 'all', 'line', 't', 'such', 'pomp', 'isn', 'welcome', 'age', 'fetched', 'a', 'burn', 'down', 'pettiness', 'classic', 'student', 'and', 'many', 'of', 'which', 'episode', 'it', 'cartoon', 'remind', 'me', 'saw', 'schools', 'to', 'comedy', 'school', 'expect', 'that', 'recalled', 'adults', 'hetic', 'as', 'repeatedly', '..', 'teachers', 'whole', 'knew', 'in', 'about', 'their', 'the', 'what', 'situation', 'think', 'pity'}]\n",
      "doc: {'', 'life', 'far', '.', 'at', 'i', 'tried', 'high', 'bromwell', 'programs', 'students', 'is', 'immediately', 'my', 'when', 'all', 'line', 't', 'such', 'pomp', 'isn', 'welcome', 'age', 'fetched', 'a', 'burn', 'down', 'pettiness', 'classic', 'student', 'and', 'many', 'of', 'which', 'episode', 'it', 'cartoon', 'remind', 'me', 'saw', 'schools', 'to', 'comedy', 'school', 'expect', 'that', 'recalled', 'adults', 'hetic', 'as', 'repeatedly', '..', 'teachers', 'whole', 'knew', 'in', 'about', 'their', 'the', 'what', 'situation', 'think', 'pity'}\n",
      "word: \n",
      "word: life\n",
      "word: far\n",
      "word: .\n",
      "word: at\n",
      "word: i\n",
      "word: tried\n",
      "word: high\n",
      "word: bromwell\n",
      "word: programs\n",
      "word: students\n",
      "word: is\n",
      "word: immediately\n",
      "word: my\n",
      "word: when\n",
      "word: all\n",
      "word: line\n",
      "word: t\n",
      "word: such\n",
      "word: pomp\n",
      "word: isn\n",
      "word: welcome\n",
      "word: age\n",
      "word: fetched\n",
      "word: a\n",
      "word: burn\n",
      "word: down\n",
      "word: pettiness\n",
      "word: classic\n",
      "word: student\n",
      "word: and\n",
      "word: many\n",
      "word: of\n",
      "word: which\n",
      "word: episode\n",
      "word: it\n",
      "word: cartoon\n",
      "word: remind\n",
      "word: me\n",
      "word: saw\n",
      "word: schools\n",
      "word: to\n",
      "word: comedy\n",
      "word: school\n",
      "word: expect\n",
      "word: that\n",
      "word: recalled\n",
      "word: adults\n",
      "word: hetic\n",
      "word: as\n",
      "word: repeatedly\n",
      "word: ..\n",
      "word: teachers\n",
      "word: whole\n",
      "word: knew\n",
      "word: in\n",
      "word: about\n",
      "word: their\n",
      "word: the\n",
      "word: what\n",
      "word: situation\n",
      "word: think\n",
      "word: pity\n",
      "vocab: ['life', 'far', '.', 'at', 'i', 'tried', 'high', 'bromwell', 'programs', 'students', 'is', 'immediately', 'my', 'when', 'line', 'all', 't', 'such', 'pomp', 'isn', 'welcome', 'age', 'fetched', 'a', 'burn', 'down', 'pettiness', 'classic', 'student', 'and', 'many', 'of', 'which', 'episode', 'it', 'cartoon', 'remind', 'me', 'saw', 'schools', 'to', 'comedy', 'school', 'expect', 'that', 'recalled', 'adults', 'hetic', 'as', 'repeatedly', '..', 'teachers', 'whole', 'knew', 'in', 'about', 'their', 'the', 'what', 'situation', 'think', 'pity']\n",
      "word indices:  {'life': 0, 'far': 1, '.': 2, 'at': 3, 'i': 4, 'tried': 5, 'high': 6, 'bromwell': 7, 'programs': 8, 'students': 9, 'is': 10, 'immediately': 11, 'my': 12, 'when': 13, 'line': 14, 'all': 15, 't': 16, 'such': 17, 'pomp': 18, 'isn': 19, 'welcome': 20, 'age': 21, 'fetched': 22, 'a': 23, 'burn': 24, 'down': 25, 'pettiness': 26, 'classic': 27, 'student': 28, 'and': 29, 'many': 30, 'of': 31, 'which': 32, 'episode': 33, 'it': 34, 'cartoon': 35, 'remind': 36, 'me': 37, 'saw': 38, 'schools': 39, 'to': 40, 'comedy': 41, 'school': 42, 'expect': 43, 'that': 44, 'recalled': 45, 'adults': 46, 'hetic': 47, 'as': 48, 'repeatedly': 49, '..': 50, 'teachers': 51, 'whole': 52, 'knew': 53, 'in': 54, 'about': 55, 'their': 56, 'the': 57, 'what': 58, 'situation': 59, 'think': 60, 'pity': 61}\n",
      "doc indices:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]\n",
      "input ds:  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]]\n",
      "target_dataset:  [1]\n"
     ]
    }
   ],
   "source": [
    "# details on this\n",
    "\n",
    "import sys\n",
    "\n",
    "#f = open('reviews.txt') # load files\n",
    "#raw_reviews = f.readlines() # one review = one review list item, this is a list\n",
    "#f.close()\n",
    "\n",
    "raw_reviews = ['bromwell high is a cartoon comedy .  programs about school life  such as  teachers  . my   hetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled .. at . . high . a classic line . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t ']\n",
    "\n",
    "raw_labels = ['positive\\n']\n",
    "\n",
    "docs = list(map(lambda x: set(x.split(\" \")), raw_reviews)) # build a set out of the reviews (sentences are sets here)\n",
    "\n",
    "print(docs)\n",
    "\n",
    "vocab = set() # build vocabulary\n",
    "for doc in docs: # sent -> docs as one review is a document consisting of a few sentences\n",
    "    print('doc:', doc)\n",
    "    for word in list(doc):\n",
    "        print('word:', word)\n",
    "        if(len(word) > 0):\n",
    "            vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "print('vocab:', vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "    \n",
    "print('word indices: ', word2index)\n",
    "\n",
    "input_dataset = list() # each list item is a review, the vector representing the review has the word indices\n",
    "for doc in docs:\n",
    "    doc_indices = list()\n",
    "    for word in doc:\n",
    "        try:\n",
    "            doc_indices.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(list(set(doc_indices)))\n",
    "    \n",
    "print('doc indices: ', doc_indices)\n",
    "print('input ds: ', input_dataset)\n",
    "\n",
    "target_dataset = list()\n",
    "for label in raw_labels:\n",
    "    if label == 'positive\\n':\n",
    "        target_dataset.append(1) # 1 for pos, 0 for neg, index here == index input_dataset\n",
    "    else:\n",
    "        target_dataset.append(0)\n",
    "        \n",
    "print('target_dataset: ', target_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:0 Progress:95.92% Training Accuracy:0.5710532899674756%%\n",
      "Iter:1 Progress:95.92% Training Accuracy:0.6496186069776166%\n",
      "Iter:2 Progress:95.92% Training Accuracy:0.68688005334667%5%\n",
      "Iter:3 Progress:95.92% Training Accuracy:0.7099143589423017%\n",
      "Iter:4 Progress:95.92% Training Accuracy:0.7259422246670334%\n",
      "Iter:5 Progress:95.92% Training Accuracy:0.7378700115292189%\n",
      "Iter:6 Progress:95.92% Training Accuracy:0.7473062590039409%\n",
      "Iter:7 Progress:95.92% Training Accuracy:0.7548624350199498%\n",
      "Iter:8 Progress:95.92% Training Accuracy:0.7610310118435796%\n",
      "Iter:9 Progress:95.92% Training Accuracy:0.766365810769141%%\n",
      "Iter:10 Progress:95.92% Training Accuracy:0.7709275632429484%\n",
      "Iter:11 Progress:95.92% Training Accuracy:0.7749338500322936%\n",
      "Iter:12 Progress:95.92% Training Accuracy:0.778532094800341%%\n",
      "Iter:13 Progress:95.92% Training Accuracy:0.781747236459096%%\n",
      "Iter:14 Progress:95.92% Training Accuracy:0.7846586773783134%\n",
      "Iter:15 Progress:95.92% Training Accuracy:0.7873233641160263%\n",
      "Iter:16 Progress:95.92% Training Accuracy:0.7897480771210494%\n",
      "Iter:17 Progress:95.92% Training Accuracy:0.791949664569357%%\n",
      "Iter:18 Progress:95.92% Training Accuracy:0.7939874819620073%\n",
      "Iter:19 Progress:95.92% Training Accuracy:0.7959173469005087%\n",
      "Iter:20 Progress:95.92% Training Accuracy:0.797724918747098%%\n",
      "Iter:21 Progress:95.92% Training Accuracy:0.7993889943217761%\n",
      "Iter:22 Progress:95.92% Training Accuracy:0.8009228561800928%\n",
      "Iter:23 Progress:95.92% Training Accuracy:0.8023445177106229%\n",
      "Iter:24 Progress:95.92% Training Accuracy:0.803667443356634%%\n",
      "Iter:25 Progress:95.92% Training Accuracy:0.804893410386838%%\n",
      "Iter:26 Progress:95.92% Training Accuracy:0.8060532545657132%\n",
      "Iter:27 Progress:95.92% Training Accuracy:0.8071421555934534%\n",
      "Iter:28 Progress:95.92% Training Accuracy:0.8081674526065329%\n",
      "Iter:29 Progress:95.92% Training Accuracy:0.8091243947765361%\n",
      "Iter:30 Progress:95.92% Training Accuracy:0.8100330384337255%\n",
      "Iter:31 Progress:95.92% Training Accuracy:0.8108887968728433%\n",
      "Iter:32 Progress:95.92% Training Accuracy:0.811706579189931%%\n",
      "Iter:33 Progress:95.92% Training Accuracy:0.8124689025003002%\n",
      "Iter:34 Progress:95.92% Training Accuracy:0.8131829015383663%\n",
      "Iter:35 Progress:95.92% Training Accuracy:0.8138618628628838%\n",
      "Iter:36 Progress:95.92% Training Accuracy:0.8145018705334117%\n",
      "Iter:37 Progress:95.92% Training Accuracy:0.8151114824634696%\n",
      "Iter:38 Progress:95.92% Training Accuracy:0.815690900038676%%\n",
      "Iter:39 Progress:95.92% Training Accuracy:0.8162392628195112%\n",
      "Iter:40 Progress:95.92% Training Accuracy:0.8167537617558045%\n",
      "Iter:41 Progress:95.92% Training Accuracy:0.8172437603052436%\n",
      "Iter:42 Progress:95.92% Training Accuracy:0.8177109678269582%\n",
      "Iter:43 Progress:95.92% Training Accuracy:0.8181645141678552%\n",
      "Iter:44 Progress:95.92% Training Accuracy:0.8185960506749187%\n",
      "Iter:45 Progress:95.92% Training Accuracy:0.8190070127954985%\n",
      "Iter:46 Progress:95.92% Training Accuracy:0.8194049195820501%\n",
      "Iter:47 Progress:95.92% Training Accuracy:0.8197957954204146%\n",
      "Iter:48 Progress:95.92% Training Accuracy:0.8201715672518797%\n",
      "Iter:49 Progress:95.92% Training Accuracy:0.8205264745637851%\n",
      "Test Accuracy:0.81\n"
     ]
    }
   ],
   "source": [
    "# NN - for details on weight updates, see https://github.com/iamtrask/Grokking-Deep-Learning/issues/50\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoidd(x):\n",
    "    #s = sigmoid(x)\n",
    "    #return s * (1 - s)\n",
    "    return 0.25 # test deriv approx\n",
    "\n",
    "def relu(x):\n",
    "    return (x>0) * x\n",
    "\n",
    "def relud(x):\n",
    "    return x>0\n",
    "\n",
    "alpha, iterations = (0.01, 50)\n",
    "hidden_size = 100\n",
    "\n",
    "weights_0_1 = 0.2*np.random.random((len(vocab), hidden_size)) - 0.1 # vocab to hidden\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size, 1)) - 0.1 # hidden to 1\n",
    "\n",
    "correct, total = (0, 0)\n",
    "\n",
    "for iter in range(iterations):\n",
    "    \n",
    "    # train on first 24,000\n",
    "    for i in range(len(input_dataset) - 1000):\n",
    "        \n",
    "        #dropout_mask = np.random.randint(2, size = layer_1.shape)\n",
    "        #dropout_mask = np.random.randint(2, size = layer_2.shape)\n",
    "\n",
    "        x, y = (input_dataset[i], target_dataset[i]) # i-th element of dataset\n",
    "        \n",
    "        layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0)) # embed + sigmoid: one-hot encoding: summation faster than product\n",
    "        #layer_1 = relu(np.sum(weights_0_1[x], axis=0))\n",
    "        \n",
    "        #layer_1 *= dropout_mask * 2\n",
    "        \n",
    "        layer_2 = sigmoid(np.dot(layer_1, weights_1_2)) # linear + softmax\n",
    "        #layer_2 = relu(np.dot(layer_1, weights_1_2))\n",
    "        \n",
    "        #layer_2 *= dropout_mask * 2\n",
    "\n",
    "        layer_2_delta = layer_2 - y # compare pred with truth = error\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) # backprop\n",
    "        \n",
    "        #layer_1_delta *= dropout_mask\n",
    "        #layer_2_delta *= dropout_mask\n",
    "\n",
    "        #weights_0_1[x] -= layer_1_delta * alpha # weight updates\n",
    "        #weights_1_2 -= np.outer(layer_1, layer_2_delta) * alpha\n",
    "        \n",
    "        dw12 = alpha * np.outer(layer_2_delta, sigmoidd(np.dot(layer_1, weights_1_2)))\n",
    "        weights_1_2 -= dw12\n",
    "        weights_0_1[x] -= np.dot(dw12, weights_1_2.T) * sigmoidd(np.sum(weights_0_1[x], axis=0)) # weight updates\n",
    "        # there would be * x but this is implied in the weights_0_1[x] (x is one-hot so only those indices with x!=0 get updates)\n",
    "        \n",
    "        if(np.abs(layer_2_delta) < 0.5): # is it a close fit?\n",
    "            correct += 1 # increment correct guesses\n",
    "            \n",
    "        total += 1 # total guesses\n",
    "        \n",
    "        if(i % 20 == 1):\n",
    "            progress = str(i/float(len(input_dataset)))\n",
    "            sys.stdout.write('\\rIter:'+str(iter)\\\n",
    "                             +' Progress:'+progress[2:4]\\\n",
    "                             +'.'+progress[4:6]\\\n",
    "                             +'% Training Accuracy:'\\\n",
    "                             + str(correct/float(total)) + '%')\n",
    "    \n",
    "    print()\n",
    "    \n",
    "correct, total = (0, 0)\n",
    "\n",
    "for i in range(len(input_dataset)-1000, len(input_dataset)): # use the last 1000 as test\n",
    "\n",
    "    x = input_dataset[i]\n",
    "    y = target_dataset[i]\n",
    "\n",
    "    layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0))\n",
    "    layer_2 = sigmoid(np.dot(layer_1, weights_1_2))\n",
    "    #layer_1 = relu(np.sum(weights_0_1[x], axis=0))\n",
    "    #layer_2 = relu(np.dot(layer_1, weights_1_2))\n",
    "    \n",
    "    if(np.abs(layer_2 - y) < 0.5):\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(\"Test Accuracy:\" + str(correct / float(total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:0 Progress:95.92% Training Accuracy:0.8334584271536986%%\n",
      "Iter:1 Progress:95.92% Training Accuracy:0.8669709474386228%\n",
      "Test Accuracy:0.851\n"
     ]
    }
   ],
   "source": [
    "# NN (original version)\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoidd(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(x):\n",
    "    return (x>0) * x\n",
    "\n",
    "def relud(x):\n",
    "    return x>0\n",
    "\n",
    "alpha, iterations = (0.01, 2)\n",
    "hidden_size = 100\n",
    "\n",
    "weights_0_1 = 0.2*np.random.random((len(vocab), hidden_size)) - 0.1 # vocab to hidden\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size, 1)) - 0.1 # hidden to 1\n",
    "\n",
    "correct, total = (0, 0)\n",
    "\n",
    "for iter in range(iterations):\n",
    "    \n",
    "    # train on first 24,000\n",
    "    for i in range(len(input_dataset) - 1000):\n",
    "\n",
    "        x, y = (input_dataset[i], target_dataset[i]) # i-th element of dataset\n",
    "        \n",
    "        layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0)) # embed + sigmoid: one-hot encoding: summation faster than product\n",
    "        layer_2 = sigmoid(np.dot(layer_1, weights_1_2)) # linear + softmax\n",
    "\n",
    "        layer_2_delta = layer_2 - y # compare pred with truth = error\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) # backprop\n",
    "        \n",
    "        # W_1_2-=alpha * L2delta * sigmoidd(L1 * W_1_2) * L1 -> alpha * L2delta * L1\n",
    "        # W_0_1-=alpha * L1delta * sigmoidd(L1 * W_1_2) * sigmoidd(W_0_1 * x) * x -> alpha * L1delta # x dropped since W_0_1[x]\n",
    "        weights_0_1[x] -= layer_1_delta * alpha # weight updates: sig' = approx 1/4 (Taylor exp. 1/2+x/4+-...)\n",
    "        weights_1_2 -= np.outer(layer_1, layer_2_delta) * alpha\n",
    "        \n",
    "        if(np.abs(layer_2_delta) < 0.5): # is it a close fit?\n",
    "            correct += 1 # increment correct guesses\n",
    "            \n",
    "        total += 1 # total guesses\n",
    "        \n",
    "        if(i % 20 == 1):\n",
    "            progress = str(i/float(len(input_dataset)))\n",
    "            sys.stdout.write('\\rIter:'+str(iter)\\\n",
    "                             +' Progress:'+progress[2:4]\\\n",
    "                             +'.'+progress[4:6]\\\n",
    "                             +'% Training Accuracy:'\\\n",
    "                             + str(correct/float(total)) + '%')\n",
    "    \n",
    "    print()\n",
    "    \n",
    "correct, total = (0, 0)\n",
    "\n",
    "for i in range(len(input_dataset)-1000, len(input_dataset)): # use the last 1000 as test\n",
    "\n",
    "    x = input_dataset[i]\n",
    "    y = target_dataset[i]\n",
    "\n",
    "    layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0))\n",
    "    layer_2 = sigmoid(np.dot(layer_1, weights_1_2))\n",
    "    \n",
    "    if(np.abs(layer_2 - y) < 0.5):\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(\"Test Accuracy:\" + str(correct / float(total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'',\n",
       " '\\n',\n",
       " '.',\n",
       " 'a',\n",
       " 'about',\n",
       " 'adults',\n",
       " 'age',\n",
       " 'all',\n",
       " 'and',\n",
       " 'as',\n",
       " 'at',\n",
       " 'believe',\n",
       " 'bromwell',\n",
       " 'burn',\n",
       " 'can',\n",
       " 'cartoon',\n",
       " 'classic',\n",
       " 'closer',\n",
       " 'comedy',\n",
       " 'down',\n",
       " 'episode',\n",
       " 'expect',\n",
       " 'far',\n",
       " 'fetched',\n",
       " 'financially',\n",
       " 'here',\n",
       " 'high',\n",
       " 'i',\n",
       " 'immediately',\n",
       " 'in',\n",
       " 'insightful',\n",
       " 'inspector',\n",
       " 'is',\n",
       " 'isn',\n",
       " 'it',\n",
       " 'knew',\n",
       " 'lead',\n",
       " 'life',\n",
       " 'line',\n",
       " 'm',\n",
       " 'many',\n",
       " 'me',\n",
       " 'much',\n",
       " 'my',\n",
       " 'of',\n",
       " 'one',\n",
       " 'other',\n",
       " 'pathetic',\n",
       " 'pettiness',\n",
       " 'pity',\n",
       " 'pomp',\n",
       " 'profession',\n",
       " 'programs',\n",
       " 'ran',\n",
       " 'reality',\n",
       " 'recalled',\n",
       " 'remind',\n",
       " 'repeatedly',\n",
       " 'right',\n",
       " 's',\n",
       " 'sack',\n",
       " 'same',\n",
       " 'satire',\n",
       " 'saw',\n",
       " 'school',\n",
       " 'schools',\n",
       " 'scramble',\n",
       " 'see',\n",
       " 'situation',\n",
       " 'some',\n",
       " 'student',\n",
       " 'students',\n",
       " 'such',\n",
       " 'survive',\n",
       " 't',\n",
       " 'teachers',\n",
       " 'teaching',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'think',\n",
       " 'through',\n",
       " 'time',\n",
       " 'to',\n",
       " 'tried',\n",
       " 'welcome',\n",
       " 'what',\n",
       " 'when',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whole',\n",
       " 'years',\n",
       " 'your'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math \n",
    "\n",
    "def similar(weights_0_1, target='beautiful'):\n",
    "    target_index = word2index[target]\n",
    "    scores = Counter()\n",
    "    for word,index in word2index.items():\n",
    "        raw_difference = weights_0_1[index] - (weights_0_1[target_index]) # just the differences in weights\n",
    "        squared_difference = raw_difference * raw_difference\n",
    "        scores[word] = -math.sqrt(sum(squared_difference)) # Euclidean\n",
    "\n",
    "    return scores.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('beautiful', -0.0), ('tragic', -0.7306265654065514), ('enjoyed', -0.7632868322705841), ('driven', -0.7686305811580804), ('fun', -0.7714824014847391), ('sweet', -0.7837695989832669), ('heart', -0.7844109735762739), ('realistic', -0.7901695264557109), ('surprisingly', -0.7926595452094244), ('captures', -0.792843332954173)]\n"
     ]
    }
   ],
   "source": [
    "print(similar(weights_0_1, 'beautiful'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('terrible', -0.0), ('annoying', -0.7566043081757956), ('boring', -0.7707002608577496), ('dull', -0.7836198932116197), ('lame', -0.7974698427524632), ('horrible', -0.8078527412982647), ('poor', -0.8098066367432781), ('disappointing', -0.8165757704189643), ('disappointment', -0.8484309632881819), ('fails', -0.8515951157533852)]\n"
     ]
    }
   ],
   "source": [
    "print(similar(weights_0_1, 'terrible'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling in the Blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:  ['', 'life', 'far', 'at', '.', 'tried', 'i', 'programs', 'bromwell', 'high', 'students', 'is', 'immediately', 'my', 'when', 'all', 'line', 't', 'such', 'pomp', 'isn', 'welcome', 'age', 'fetched', 'a', 'burn', 'down', 'pettiness', 'classic', 'student', 'and', 'many', 'of', 'which', 'episode', 'it', 'cartoon', 'remind', 'me', 'saw', 'schools', 'expect', 'comedy', 'school', 'to', 'that', 'recalled', 'adults', 'hetic', 'as', 'repeatedly', '..', 'teachers', 'whole', 'knew', 'in', 'about', 'their', 'the', 'what', 'situation', 'think', 'pity']\n",
      "wordidx:  {'': 0, 'life': 1, 'far': 2, 'at': 3, '.': 4, 'tried': 5, 'i': 6, 'programs': 7, 'bromwell': 8, 'high': 9, 'students': 10, 'is': 11, 'immediately': 12, 'my': 13, 'when': 14, 'all': 15, 'line': 16, 't': 17, 'such': 18, 'pomp': 19, 'isn': 20, 'welcome': 21, 'age': 22, 'fetched': 23, 'a': 24, 'burn': 25, 'down': 26, 'pettiness': 27, 'classic': 28, 'student': 29, 'and': 30, 'many': 31, 'of': 32, 'which': 33, 'episode': 34, 'it': 35, 'cartoon': 36, 'remind': 37, 'me': 38, 'saw': 39, 'schools': 40, 'expect': 41, 'comedy': 42, 'school': 43, 'to': 44, 'that': 45, 'recalled': 46, 'adults': 47, 'hetic': 48, 'as': 49, 'repeatedly': 50, '..': 51, 'teachers': 52, 'whole': 53, 'knew': 54, 'in': 55, 'about': 56, 'their': 57, 'the': 58, 'what': 59, 'situation': 60, 'think': 61, 'pity': 62}\n",
      "doc indices: [8, 9, 11, 24, 36, 42, 4, 0, 7, 56, 43, 1, 0, 18, 49, 0, 52, 0, 4, 13, 0, 0, 48, 52, 0, 19, 0, 58, 27, 32, 58, 53, 60, 0, 15, 37, 38, 32, 58, 40, 6, 54, 30, 57, 10, 4, 14, 6, 39, 58, 34, 55, 33, 24, 29, 50, 5, 44, 25, 26, 58, 43, 0, 6, 12, 46, 51, 3, 4, 4, 9, 4, 24, 28, 16, 4, 29, 21, 44, 8, 9, 4, 6, 41, 45, 31, 47, 32, 13, 22, 61, 45, 8, 9, 11, 2, 23, 4, 59, 24, 62, 45, 35, 20, 0, 17, 0]\n",
      "concatenated: [ 8  9 11 24 36 42  4  0  7 56 43  1  0 18 49  0 52  0  4 13  0  0 48 52\n",
      "  0 19  0 58 27 32 58 53 60  0 15 37 38 32 58 40  6 54 30 57 10  4 14  6\n",
      " 39 58 34 55 33 24 29 50  5 44 25 26 58 43  0  6 12 46 51  3  4  4  9  4\n",
      " 24 28 16  4 29 21 44  8  9  4  6 41 45 31 47 32 13 22 61 45  8  9 11  2\n",
      " 23  4 59 24 62 45 35 20  0 17  0]\n",
      "input_dataset: [[8, 9, 11, 24, 36, 42, 4, 0, 7, 56, 43, 1, 0, 18, 49, 0, 52, 0, 4, 13, 0, 0, 48, 52, 0, 19, 0, 58, 27, 32, 58, 53, 60, 0, 15, 37, 38, 32, 58, 40, 6, 54, 30, 57, 10, 4, 14, 6, 39, 58, 34, 55, 33, 24, 29, 50, 5, 44, 25, 26, 58, 43, 0, 6, 12, 46, 51, 3, 4, 4, 9, 4, 24, 28, 16, 4, 29, 21, 44, 8, 9, 4, 6, 41, 45, 31, 47, 32, 13, 22, 61, 45, 8, 9, 11, 2, 23, 4, 59, 24, 62, 45, 35, 20, 0, 17, 0]]\n"
     ]
    }
   ],
   "source": [
    "# remove focus term from training phrase and negative sampling\n",
    "# test case\n",
    "import sys,random,math\n",
    "from collections import Counter # elements are stored as dictionary keys and their counts are stored as dictionary values\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "raw_reviews = ['bromwell high is a cartoon comedy .  programs about school life  such as  teachers  . my   hetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled .. at . . high . a classic line . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t ']\n",
    "\n",
    "docs = list(map(lambda x:(x.split(\" \")), raw_reviews))\n",
    "\n",
    "wordcnt = Counter()\n",
    "for doc in docs:\n",
    "    for word in doc:\n",
    "        wordcnt[word] -= 1\n",
    "vocab = list(set(map(lambda x:x[0], wordcnt.most_common()))) # vocab now sorted by frequency\n",
    "\n",
    "print('vocabulary: ', vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "    \n",
    "print('wordidx: ', word2index)\n",
    "\n",
    "concatenated = list()\n",
    "input_dataset = list()\n",
    "for doc in docs:\n",
    "    doc_indices = list()\n",
    "    for word in doc:\n",
    "        try:\n",
    "            doc_indices.append(word2index[word])\n",
    "            concatenated.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(doc_indices)\n",
    "    \n",
    "    print('doc indices:', doc_indices)\n",
    "    \n",
    "concatenated = np.array(concatenated)\n",
    "random.shuffle(input_dataset)\n",
    "\n",
    "print('concatenated:', concatenated)\n",
    "print('input_dataset:', input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove focus term from training phrase and negative sampling\n",
    "import sys,random,math\n",
    "from collections import Counter # elements are stored as dictionary keys and their counts are stored as dictionary values\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "f = open('reviews.txt')\n",
    "raw_reviews = f.readlines()\n",
    "f.close()\n",
    "\n",
    "docs = list(map(lambda x:(x.split(\" \")), raw_reviews))\n",
    "\n",
    "wordcnt = Counter()\n",
    "for doc in docs:\n",
    "    for word in doc:\n",
    "        wordcnt[word] -= 1\n",
    "vocab = list(set(map(lambda x:x[0], wordcnt.most_common())))\n",
    "\n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "\n",
    "concatenated = list()\n",
    "input_dataset = list()\n",
    "for doc in docs:\n",
    "    doc_indices = list()\n",
    "    for word in doc:\n",
    "        try:\n",
    "            doc_indices.append(word2index[word])\n",
    "            concatenated.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(doc_indices)\n",
    "    \n",
    "concatenated = np.array(concatenated)\n",
    "random.shuffle(input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3110699, 5373129, 853, 2255194, 1094698]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.random.rand(negative)*len(concatenated)).astype('int').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:0.99998 [('terrible', -0.0), ('horrible', -3.865448020467825), ('fantastic', -4.663143498112564), ('dreadful', -4.715999496121085), ('dire', -4.7468630196928165), ('abysmal', -4.9823417743379235), ('marvelous', -5.024885049664849), ('atrocious', -5.178928121556328), ('phenomenal', -5.188060693819728), ('horrendous', -5.2591910675891755)]]255495)]4)][('terrible', -0.0), ('horrible', -3.765849644584859), ('dreadful', -4.528184528887639), ('dire', -4.880631539903571), ('fantastic', -4.9519905798824775), ('phenomenal', -5.074161569728423), ('abysmal', -5.086930645748105), ('marvelous', -5.122752442871552), ('horrid', -5.286144172273731), ('horrendous', -5.313135003074647)]\n"
     ]
    }
   ],
   "source": [
    "# NN with word identity and negative sampling\n",
    "\n",
    "alpha, iterations = (0.05, 2)\n",
    "hidden_size, window, negative = (40, 4, 5) # negative = size of the subset\n",
    "\n",
    "weights_0_1 = (np.random.rand(len(vocab), hidden_size) - 0.5) * 0.2\n",
    "weights_1_2 = np.random.rand(len(vocab), hidden_size)*0\n",
    "\n",
    "layer_2_target = np.zeros(negative + 1)\n",
    "layer_2_target[0] = 1\n",
    "\n",
    "def similar(target='beautiful'):\n",
    "    target_index = word2index[target]\n",
    "    scores = Counter()\n",
    "    for word,index in word2index.items():\n",
    "        raw_difference = weights_0_1[index] - (weights_0_1[target_index])\n",
    "        squared_difference = raw_difference * raw_difference\n",
    "        scores[word] = -math.sqrt(sum(squared_difference))\n",
    "    return scores.most_common(10)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "for rev_i, review in enumerate(input_dataset * iterations):\n",
    "    for target_i in range(len(review)):\n",
    "\n",
    "        # since it's really expensive to predict every vocabulary\n",
    "        # we're only going to predict a random subset\n",
    "        target_samples = [review[target_i]] + list(concatenated\\\n",
    "                                                 [(np.random.rand(negative)*len(concatenated)).astype('int').tolist()])\n",
    "        # this is the target word (for each word in the review) plus a random sample of 5 words\n",
    "        # something like [1, 37015, 1, 11752, 51361, 61047]\n",
    "        \n",
    "        left_context = review[max(0, target_i - window):target_i]\n",
    "        right_context = review[target_i+1:min(len(review), target_i + window)]\n",
    "\n",
    "        #layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0))\n",
    "        #layer_2 = sigmoid(np.dot(layer_1, weights_1_2))\n",
    "        layer_1 = np.mean(weights_0_1[left_context + right_context], axis=0) # mean over window of W01\n",
    "        layer_2 = sigmoid(layer_1.dot(weights_1_2[target_samples].T))\n",
    "        layer_2_delta = layer_2 - layer_2_target # weight updates like above: sigmoidd assumed to be constant\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2[target_samples])\n",
    "\n",
    "        weights_0_1[left_context + right_context] -= layer_1_delta * alpha\n",
    "        weights_1_2[target_samples] -= np.outer(layer_2_delta,layer_1) * alpha\n",
    "\n",
    "    if(rev_i % 250 == 0):\n",
    "        sys.stdout.write('\\rProgress:'+str(rev_i/float(len(input_dataset)\n",
    "                                                       *iterations)) + \"   \" + str(similar('terrible')))\n",
    "    sys.stdout.write('\\rProgress:'+str(rev_i/float(len(input_dataset)\n",
    "                                                   *iterations)))\n",
    "\n",
    "print(similar('terrible'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# King - Man + Woman ~= Queen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(positive=['terrible','good'],negative=['bad']):\n",
    "    \n",
    "    norms = np.sum(weights_0_1 * weights_0_1,axis=1)\n",
    "    norms.resize(norms.shape[0],1)\n",
    "    \n",
    "    normed_weights = weights_0_1 * norms\n",
    "    \n",
    "    query_vect = np.zeros(len(weights_0_1[0]))\n",
    "    for word in positive:\n",
    "        query_vect += normed_weights[word2index[word]] # add this vector\n",
    "    for word in negative:\n",
    "        query_vect -= normed_weights[word2index[word]] # and subtract this (the negative) vector\n",
    "    \n",
    "    scores = Counter()\n",
    "    for word,index in word2index.items():\n",
    "        raw_difference = weights_0_1[index] - query_vect\n",
    "        squared_difference = raw_difference * raw_difference\n",
    "        scores[word] = -math.sqrt(sum(squared_difference))\n",
    "        \n",
    "    return scores.most_common(10)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('perfect', -514.357054637724),\n",
       " ('superb', -514.6100564491167),\n",
       " ('decent', -514.8468298800373),\n",
       " ('stunning', -515.0387448848408),\n",
       " ('wonderful', -515.2863004152655),\n",
       " ('nice', -515.4087747709082),\n",
       " ('terrific', -515.6317972525602),\n",
       " ('excellent', -515.7388992694316),\n",
       " ('pleasant', -515.7514889576819)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(['terrible','good'],['bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('j', -578.7565848638679),\n",
       " ('u', -579.1389356816334),\n",
       " ('dr', -579.2460586812555),\n",
       " ('jr', -579.249838158948),\n",
       " ('ms', -579.5895941600976),\n",
       " ('p', -579.9270204252564),\n",
       " ('smith', -579.9936149731445),\n",
       " ('v', -580.2005067038277),\n",
       " ('mr', -580.240517452727)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(['elizabeth','he'],['she'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tale', -559.2467961945546),\n",
       " ('complex', -559.3350441577724),\n",
       " ('intense', -559.3929135314795),\n",
       " ('fascinating', -559.5849700874645),\n",
       " ('compelling', -559.755543066805),\n",
       " ('wit', -559.8002931248143),\n",
       " ('powerful', -559.8481473513584),\n",
       " ('tension', -559.8865856219109),\n",
       " ('poignant', -559.9413573579982)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(['powerful','weak'],['bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
