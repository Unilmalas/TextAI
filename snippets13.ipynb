{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. -1. -1. -1. -1.]\n",
      " [ 1.  1.  3.  1.  1.]]\n",
      "[1 0]\n",
      "[[ 0.10225897  0.89774103]\n",
      " [ 0.97116002  0.02883998]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data.\n",
    "# For example, scale each attribute on the input vector X to [0, 1] or [-1, +1]. e.g. scaler = StandardScaler()\n",
    "# Finding a reasonable regularization parameter  is best done using GridSearchCV,\n",
    "# usually in the range 10.0 ** -np.arange(1, 7)\n",
    "# Empirically, we observed that L-BFGS converges faster and with better solutions on small datasets.\n",
    "# For relatively large datasets, however, Adam is very robust.\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # X : array-like or sparse matrix, shape (n_samples, n_features)\n",
    "    # y : array-like, shape (n_samples,) or (n_samples, n_outputs)\n",
    "    X = [[0., 0., 0., 0., 0.], [1., 1., 1., 1., 1.]]\n",
    "    y = [0, 1]\n",
    "    \n",
    "    X_train = [[0., 0., 0., 0., 0.], [1., 1., 1., 1., 1.]]\n",
    "    X_test = [[0., 0., 0., 0., 0.], [1., 1., 2., 1., 1.]]\n",
    "    scaler = StandardScaler() \n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test) \n",
    "    print X_test\n",
    "    \n",
    "    clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "    clf.fit(X, y)\n",
    "    print clf.predict([[1., 1., 1., 0., 0.], [0., 0., 0., 0., 1.]])\n",
    "    print clf.predict_proba([[1., 1., 1., 0., 0.], [0., 0., 0., 0., 1.]])\n",
    "    print clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.99019608  0.00980392]\n"
     ]
    }
   ],
   "source": [
    "# LDA Gibbs sampling\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def rndbias(p):\n",
    "    if random.random() > p:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def samplp(pz):\n",
    "    res = []\n",
    "    for p in pz:\n",
    "        res.append(rndbias(p))\n",
    "    return res\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # Input: words w ∈ documents d\n",
    "    # words\n",
    "    w = np.array([0, 1])\n",
    "    # documents as doc/word matrix\n",
    "    d = np.array([\n",
    "        [1, 0],\n",
    "        [0, 1]\n",
    "    ])\n",
    "\n",
    "    # begin randomly initialize z and increment counters\n",
    "    # z(i) topic index assignment over available topics\n",
    "    z = np.array([1,1]) # todo: random w/ prior\n",
    "\n",
    "    # init increment counters\n",
    "    cdt = np.random.randint(2, size=(2, 2)) # nd,top\n",
    "    cwt = np.random.randint(2, size=(2, 2)) # nw,top\n",
    "\n",
    "    N_D = d.shape[0]  # num of docs\n",
    "    N_W = w.shape[0]  # num of words\n",
    "    N_K = z.shape[0]  # num of topics\n",
    "\n",
    "    # Dirichlet priors\n",
    "    alpha = 0.5\n",
    "    beta = 0.01\n",
    "\n",
    "    pz = np.array([0.5, 0.5])\n",
    "\n",
    "    for niter in range(15):\n",
    "        # for i = 0 → N −1 do\n",
    "        nk = N_K\n",
    "        # for all documents m ∈ [1, M] d\n",
    "        for d in range(N_D): # documents\n",
    "            for i in range(N_W): # words\n",
    "                word = w[i]\n",
    "                topic = z[i]\n",
    "                # nd,topic-=1; nword,topic-=1; ntopic-=1\n",
    "                cdt[d][topic] -= 1 #  the number of words assigned to topic k in document d\n",
    "                cwt[word][topic] -= 1 # the number of times word w is assigned to topic k\n",
    "                nk -= 1 # the number of times word w is assigned to topic k\n",
    "                # for k = 0 → K −1 do\n",
    "                for k in range(N_K): # topics\n",
    "                    # p(z = k|·) = (nd,k + αk) nk,w+βw nk+β×W\n",
    "                    pz[k] = (cdt[d][k] + alpha) * (cwt[i][k] + beta) / (nk + N_W * beta)\n",
    "                if min(pz)<0:\n",
    "                    pz += -min(pz)\n",
    "                pz = pz / sum(pz) # normalize\n",
    "                # topic ← sample from p(z|·)\n",
    "                topic = np.random.choice(N_K, p=pz)\n",
    "                z[i] = topic\n",
    "                # nd,topic+=1; nword,topic+=1; ntopic+=1\n",
    "                nk += 1\n",
    "                cdt[d][topic] += 1\n",
    "                cwt[word][topic] += 1\n",
    "    # Output: topic assignments z and counts nd,k,nk,w, and nk\n",
    "    # return z, nd,k,nk,w,nk\n",
    "    print pz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "[0 1 2 3 4]\n",
      "[3 2 0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def rndbias(p):\n",
    "    if random.random() > p:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "if __name__=='__main__':\n",
    "    for i in range(10):\n",
    "        print rndbias(0.8)\n",
    "    print np.arange(5)\n",
    "    print np.random.choice(5, 3, replace=False, p=[0.1, 0, 0.3, 0.6, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['c'], [2]), (['d'], [2])]\n",
      "[[0, 1], [2, 1], [0, 0]]\n",
      "most representative part of doc 0: index 1 : part d\n"
     ]
    }
   ],
   "source": [
    "# small LDA example for demo purposes\n",
    "# example from Probabilistic Topic Models, M Steyvers, T Griths, Handbook of latent semantic analysis \n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "def poissonrn(lda): # generate a poisson random number\n",
    "    l = math.exp(-lda)\n",
    "    k = 1.\n",
    "    p = random.random()\n",
    "    while p > l:\n",
    "        k += 1.\n",
    "        p *= random.random()\n",
    "    return (k-1.)\n",
    "\n",
    "def dirichlet(params): # sample dirichlet distribution\n",
    "    sample = [random.gammavariate(a,1) for a in params]\n",
    "    sample = [v/sum(sample) for v in sample]\n",
    "    return sample\n",
    "\n",
    "def partsct(doc): # find all parts of a document and count its respective occurance\n",
    "    chklst = []\n",
    "    ctlst = []\n",
    "    for e in doc:\n",
    "        if not e in chklst:\n",
    "            chklst.append(e)\n",
    "            ctlst.append(1)\n",
    "        else:\n",
    "            ctlst[chklst.index(e)] += 1\n",
    "    return (chklst, ctlst)\n",
    "\n",
    "def topic_parts(ntopics, dbt): # create a topics-part table randomly TODO: add prior distr\n",
    "    res = [[0 for i in range(len(dbt))] for i in range(ntopics)]\n",
    "    ie = 0\n",
    "    for e in dbt:\n",
    "        for p in e[1]:\n",
    "            for k in range(p):\n",
    "                res[random.randint(0,ntopics-1)][ie] += 1\n",
    "        ie += 1\n",
    "    return res\n",
    "\n",
    "def doc_parts_tbl(docs): # build a document-parts table\n",
    "    ptslst = []\n",
    "    for d in docs:\n",
    "        ptslst.append(partsct(d)) \n",
    "    return ptslst\n",
    "\n",
    "def all_parts(dbt): # a list of all parts in all docs, uniquely, from doc parts table\n",
    "    apts = []\n",
    "    for d in dbt:\n",
    "        for de in d[0]:\n",
    "            if not de in apts:\n",
    "                apts.append(de)\n",
    "    return apts\n",
    "\n",
    "def parts_per_doc(lda): # pick how many parts you want per each composite (doc) - from Poisson distribution\n",
    "    return poissonrn(lda)\n",
    "\n",
    "def parts_topics(parts, topics, beta): # build the parts-topics table: for each column draw from Dirichlet distr.(beta)\n",
    "    res = []\n",
    "    betalst = []\n",
    "    for p in parts:\n",
    "        betalst.append(beta)\n",
    "    for t in topics:\n",
    "        res.append(dirichlet(betalst))\n",
    "    return res\n",
    "\n",
    "def comp_topics(docs, topics, alpha): # build the composites(docs)-topics table: for each row draw from Dirichlet distr.(alpha)\n",
    "    res = []\n",
    "    alphalst = []\n",
    "    for t in topics:\n",
    "        alphalst.append(alpha)\n",
    "    for d in docs:\n",
    "        res.append(dirichlet(alphalst))\n",
    "    return res\n",
    "\n",
    "def smpl_topbydoc(ndoc, doctop): # sample document by topic\n",
    "    maxtop = 0.\n",
    "    i = 0\n",
    "    imax = 0\n",
    "    for p in doctop[ndoc]:\n",
    "        if p > maxtop:\n",
    "            maxtop = p\n",
    "            imax = i\n",
    "        i += 1\n",
    "    return imax\n",
    "\n",
    "def smpl_partsbytop(ntop, parttop): # sample parts by topic\n",
    "    maxprt = 0.\n",
    "    i = 0\n",
    "    imax = 0\n",
    "    for p in parttop[ntop]:\n",
    "        if p > maxprt:\n",
    "            maxprt = p\n",
    "            imax = i\n",
    "        i += 1\n",
    "    return imax\n",
    "\n",
    "if __name__=='__main__':\n",
    "    docs = []\n",
    "    docs.append('cc')\n",
    "    docs.append('dd')\n",
    "    #print docs\n",
    "    dbt = doc_parts_tbl(docs) # document and parts table\n",
    "    print dbt\n",
    "    random.seed(time.clock())\n",
    "    ntop = 3 # number of topics\n",
    "    tpparts = topic_parts(ntop, dbt) # randomly assign a topic to each part\n",
    "    print tpparts\n",
    "    alpha = 0.2\n",
    "    beta = 0.5\n",
    "    #print dirichlet([beta, beta])\n",
    "    apts = all_parts(dbt) # find all parts making up the documents\n",
    "    parttop = parts_topics(apts, [0, 1, 2], beta) # create a random parts-topics distribution\n",
    "    doctop = comp_topics(apts, docs, alpha) # create a random doc-topic distribution\n",
    "    nthistop = 0\n",
    "    maxtop = smpl_topbydoc(nthistop, doctop) # sample document by topic\n",
    "    npt = smpl_partsbytop(maxtop, parttop) # sample parts by topc\n",
    "    print 'most representative part of doc %d: index %d : part %s' % (nthistop, npt, apts[npt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00946214  0.02248632  0.03198897  0.23357924  0.70248334]\n",
      " [ 0.16259179  0.39564105  0.35115256  0.00778928  0.08282532]]\n"
     ]
    }
   ],
   "source": [
    "# https://wiseodd.github.io/techblog/2017/09/07/lda-gibbs/\n",
    "import numpy as np\n",
    "\n",
    "# Words\n",
    "W = np.array([0, 1, 2, 3, 4])\n",
    "\n",
    "# D := document words\n",
    "X = np.array([\n",
    "    [0, 0, 1, 2, 2],\n",
    "    [0, 0, 1, 1, 1],\n",
    "    [0, 1, 2, 2, 2],\n",
    "    [4, 4, 4, 4, 4],\n",
    "    [3, 3, 4, 4, 4],\n",
    "    [3, 4, 4, 4, 4]\n",
    "])\n",
    "\n",
    "N_D = X.shape[0]  # num of docs\n",
    "N_W = W.shape[0]  # num of words\n",
    "N_K = 2  # num of topics\n",
    "\n",
    "# Dirichlet priors\n",
    "alpha = 1\n",
    "gamma = 1\n",
    "\n",
    "# Z := word topic assignment\n",
    "Z = np.zeros(shape=[N_D, N_W])\n",
    "\n",
    "for i in range(N_D):\n",
    "    for l in range(N_W):\n",
    "        Z[i, l] = np.random.randint(N_K)  # randomly assign word's topic\n",
    "\n",
    "# Pi := document topic distribution\n",
    "Pi = np.zeros([N_D, N_K])\n",
    "\n",
    "for i in range(N_D):\n",
    "    Pi[i] = np.random.dirichlet(alpha*np.ones(N_K))\n",
    "\n",
    "# B := word topic distribution\n",
    "B = np.zeros([N_K, N_W])\n",
    "\n",
    "for k in range(N_K):\n",
    "    B[k] = np.random.dirichlet(gamma*np.ones(N_W))\n",
    "    \n",
    "for it in range(1000):\n",
    "    # Sample from full conditional of Z\n",
    "    # ---------------------------------\n",
    "    for i in range(N_D):\n",
    "        for v in range(N_W):\n",
    "            # Calculate params for Z\n",
    "            p_iv = np.exp(np.log(Pi[i]) + np.log(B[:, X[i, v]]))\n",
    "            p_iv /= np.sum(p_iv)\n",
    "\n",
    "            # Resample word topic assignment Z\n",
    "            Z[i, v] = np.random.multinomial(1, p_iv).argmax()\n",
    "\n",
    "    # Sample from full conditional of Pi\n",
    "    # ----------------------------------\n",
    "    for i in range(N_D):\n",
    "        m = np.zeros(N_K)\n",
    "\n",
    "        # Gather sufficient statistics\n",
    "        for k in range(N_K):\n",
    "            m[k] = np.sum(Z[i] == k)\n",
    "\n",
    "        # Resample doc topic dist.\n",
    "        Pi[i, :] = np.random.dirichlet(alpha + m)\n",
    "\n",
    "    # Sample from full conditional of B\n",
    "    # ---------------------------------\n",
    "    for k in range(N_K):\n",
    "        n = np.zeros(N_W)\n",
    "\n",
    "        # Gather sufficient statistics\n",
    "        for v in range(N_W):\n",
    "            for i in range(N_D):\n",
    "                for l in range(N_W):\n",
    "                    n[v] += (X[i, l] == v) and (Z[i, l] == k)\n",
    "\n",
    "        # Resample word topic dist.\n",
    "        B[k, :] = np.random.dirichlet(gamma + n)\n",
    "        \n",
    "print B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
