{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compose: Training a model to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "#from music21 import note, chord\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import LSTM, Input, Dropout, Dense, Activation, Embedding, Concatenate, Reshape\n",
    "from keras.layers import Flatten, RepeatVector, Permute, TimeDistributed\n",
    "from keras.layers import Multiply, Lambda, Softmax\n",
    "import keras.backend as K \n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run params\n",
    "section = 'composetxt'\n",
    "run_id = '0001'\n",
    "txt_name = 'txtattn'\n",
    "\n",
    "run_folder = 'run/{}/'.format(section)\n",
    "run_folder += '_'.join([run_id, txt_name])\n",
    "\n",
    "store_folder = os.path.join(run_folder, 'store')\n",
    "data_folder = os.path.join('data', txt_name)\n",
    "\n",
    "if not os.path.exists(run_folder):\n",
    "    os.mkdir(run_folder)\n",
    "    os.mkdir(os.path.join(run_folder, 'store'))\n",
    "    os.mkdir(os.path.join(run_folder, 'output'))\n",
    "    os.mkdir(os.path.join(run_folder, 'weights'))\n",
    "    os.mkdir(os.path.join(run_folder, 'viz'))\n",
    "\n",
    "mode = 'build' # 'load' # \n",
    "\n",
    "# data params\n",
    "intervals = range(1)\n",
    "seq_len = 32\n",
    "\n",
    "# model params\n",
    "embed_size = 100\n",
    "rnn_units = 256\n",
    "use_attention = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213694"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "token_type = 'word'\n",
    "\n",
    "#load in the text and perform some cleanup\n",
    "\n",
    "seq_length = 20\n",
    "\n",
    "filename = \"./data/aesop/data.txt\"\n",
    "\n",
    "with open(filename, encoding='utf-8-sig') as f:\n",
    "    text = f.read()    \n",
    "    \n",
    "#removing text before and after the main stories\n",
    "start = text.find(\"THE FOX AND THE GRAPES\\n\\n\\n\")\n",
    "end = text.find(\"ILLUSTRATIONS\\n\\n\\n[\")\n",
    "text = text[start:end]\n",
    "\n",
    "start_story = '| ' * seq_length\n",
    "    \n",
    "text = start_story + text\n",
    "text = text.lower()\n",
    "text = text.replace('\\n\\n\\n\\n\\n', start_story)\n",
    "text = text.replace('\\n', ' ')\n",
    "text = re.sub('  +', '. ', text).strip()\n",
    "text = text.replace('..', '.')\n",
    "\n",
    "text = re.sub('([!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~])', r' \\1 ', text)\n",
    "text = re.sub('\\s{2,}', ' ', text)\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4170\n",
      "{'|': 1, ',': 2, 'the': 3, 'and': 4, '.': 5, 'a': \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 56, 4, 3, 940, 5, 6, 382, 56, 94, 77, 216, 1557, 9, 940, 941, 62, 6, 581, 20, 12, 2226, 162, 6, 359, 2227, 2, 4, 158, 11]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "if token_type == 'word':\n",
    "    tokenizer = Tokenizer(char_level = False, filters = '')\n",
    "else:\n",
    "    tokenizer = Tokenizer(char_level = True, filters = '', lower = False)    \n",
    "    \n",
    "tokenizer.fit_on_texts([text])\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "print(total_words)\n",
    "\n",
    "print(str(tokenizer.word_index)[0:50]) # vocabulary\n",
    "print(token_list[:50]) # tokenized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(token_list, step):\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(0, len(token_list) - seq_length, step):\n",
    "        X.append(token_list[i: i + seq_length])\n",
    "        y.append(token_list[i + seq_length])\n",
    "    \n",
    "\n",
    "    y = np_utils.to_categorical(y, num_classes = total_words)\n",
    "    \n",
    "    num_seq = len(X)\n",
    "    print('Number of sequences:', num_seq, \"\\n\")\n",
    "    \n",
    "    return X, y, num_seq\n",
    "\n",
    "step = 1\n",
    "seq_length = 20\n",
    "\n",
    "X, y, num_seq = generate_sequences(token_list, step)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare network I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(text, n_tokens, seq_len =32):\n",
    "    \"\"\" Prepare the sequences used to train the Neural Network \"\"\"\n",
    "\n",
    "    text_network_input = []\n",
    "    text_network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(len(text) - seq_len): \n",
    "        text_network_input.append(text[i:i + seq_len])\n",
    "        text_network_output.append(text[i + seq_len])\n",
    "\n",
    "    n_patterns = len(text_network_input)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    text_network_input = np.reshape(text_network_input, (n_patterns, seq_len))\n",
    "    network_input = [text_network_input]\n",
    "\n",
    "    text_network_output = np_utils.to_categorical(text_network_output, num_classes=n_tokens)\n",
    "    network_output = [text_network_output]\n",
    "\n",
    "    return (network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network_input, network_output = prepare_sequences(notes, durations, lookups, distincts, seq_len)\n",
    "network_input, network_output = prepare_sequences(token_list, len(tokenizer.word_index)+1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text input\n",
      "[[   1    1    1 ...   94   77  216]\n",
      " [   1    1    1 ...   77  216 1557]\n",
      " [   1    1    1 ...  216 1557    9]\n",
      " ...\n",
      " [  13    2    8 ...  384    5    8]\n",
      " [   2    8   53 ...    5    8    5]\n",
      " [   8   53   22 ...    8    5 4169]]\n",
      "text output\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('text input')\n",
    "print(network_input[0])\n",
    "\n",
    "print('text output')\n",
    "print(network_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the structure of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(n_tokens, embed_size = 100, rnn_units = 256, use_attention = False):\n",
    "    \"\"\" create the structure of the neural network \"\"\"\n",
    "\n",
    "    text_in = Input(shape = (None,))\n",
    "\n",
    "    x1 = Embedding(n_tokens, embed_size)(text_in) \n",
    "\n",
    "    #x = Concatenate()([x1,x2])\n",
    "    x = x1 # todo: remove and change x1\n",
    "\n",
    "    x = LSTM(rnn_units, return_sequences=True)(x)\n",
    "    # x = Dropout(0.2)(x)\n",
    "\n",
    "    if use_attention:\n",
    "\n",
    "        x = LSTM(rnn_units, return_sequences=True)(x)\n",
    "        # x = Dropout(0.2)(x)\n",
    "\n",
    "        e = Dense(1, activation='tanh')(x)\n",
    "        e = Reshape([-1])(e)\n",
    "        alpha = Activation('softmax')(e)\n",
    "\n",
    "        alpha_repeated = Permute([2, 1])(RepeatVector(rnn_units)(alpha)) # todo: check the 2, 1\n",
    "\n",
    "        c = Multiply()([x, alpha_repeated])\n",
    "        c = Lambda(lambda xin: K.sum(xin, axis=1), output_shape=(rnn_units,))(c)\n",
    "    \n",
    "    else:\n",
    "        c = LSTM(rnn_units)(x)\n",
    "        # c = Dropout(0.2)(c)\n",
    "                                    \n",
    "    text_out = Dense(n_tokens, activation = 'softmax', name = 'text')(c)\n",
    "   \n",
    "    model = Model([text_in], [text_out])\n",
    "\n",
    "    if use_attention:\n",
    "        att_model = Model([text_in], alpha)\n",
    "    else:\n",
    "        att_model = None\n",
    "\n",
    "    opti = RMSprop(lr = 0.001)\n",
    "    model.compile(loss=['categorical_crossentropy'], optimizer=opti)\n",
    "\n",
    "    return model, att_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 100)    417000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, None, 256)    365568      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, None, 256)    525312      lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 1)      257         lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, None)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None)         0           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 256, None)    0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, None, 256)    0           repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, None, 256)    0           lstm_4[0][0]                     \n",
      "                                                                 permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 256)          0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "text (Dense)                    (None, 4170)         1071690     lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,379,827\n",
      "Trainable params: 2,379,827\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model, att_model = create_network(n_tokens, embed_size, rnn_units, use_attention)\n",
    "model, att_model = create_network(len(tokenizer.word_index)+1, embed_size, rnn_units, use_attention)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run/composetxt/0001_txtattn\n",
      "run/composetxt/0001_txtattn\\weights\\weights.h5\n"
     ]
    }
   ],
   "source": [
    "weights_folder = os.path.join(run_folder, 'weights')\n",
    "# model.load_weights(os.path.join(weights_folder, \"weights.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40309 samples, validate on 10078 samples\n",
      "Epoch 1/10\n",
      "40309/40309 [==============================] - 110s 3ms/step - loss: 5.5284 - val_loss: 5.3192\n",
      "Epoch 2/10\n",
      "40309/40309 [==============================] - 108s 3ms/step - loss: 5.1877 - val_loss: 5.1847\n",
      "Epoch 3/10\n",
      "40309/40309 [==============================] - 109s 3ms/step - loss: 4.9825 - val_loss: 4.9950\n",
      "Epoch 4/10\n",
      "40309/40309 [==============================] - 109s 3ms/step - loss: 4.8385 - val_loss: 4.9708\n",
      "Epoch 5/10\n",
      "40309/40309 [==============================] - 108s 3ms/step - loss: 4.7513 - val_loss: 4.9680\n",
      "Epoch 6/10\n",
      "40309/40309 [==============================] - 108s 3ms/step - loss: 4.6780 - val_loss: 4.9157\n",
      "Epoch 7/10\n",
      "40309/40309 [==============================] - 108s 3ms/step - loss: 4.6184 - val_loss: 4.8695\n",
      "Epoch 8/10\n",
      "40309/40309 [==============================] - 109s 3ms/step - loss: 4.5622 - val_loss: 4.8377\n",
      "Epoch 9/10\n",
      "40309/40309 [==============================] - 109s 3ms/step - loss: 4.5149 - val_loss: 4.8137\n",
      "Epoch 10/10\n",
      "40309/40309 [==============================] - 109s 3ms/step - loss: 4.4993 - val_loss: 4.9522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x249bc1ecb70>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_folder = os.path.join(run_folder, 'weights')\n",
    "\n",
    "checkpoint1 = ModelCheckpoint(\n",
    "    os.path.join(weights_folder, \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.h5\"),\n",
    "    monitor='loss',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "checkpoint2 = ModelCheckpoint(\n",
    "    os.path.join(weights_folder, \"weights.h5\"),\n",
    "    monitor='loss',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='loss'\n",
    "    , restore_best_weights=True\n",
    "    , patience = 10\n",
    ")\n",
    "\n",
    "\n",
    "callbacks_list = [\n",
    "    checkpoint1\n",
    "    , checkpoint2\n",
    "    , early_stopping\n",
    " ]\n",
    "\n",
    "model.save_weights(os.path.join(weights_folder, \"weights.h5\"))\n",
    "model.fit(network_input, network_output\n",
    "          , epochs=10, batch_size=32 # 200 epochs\n",
    "          , validation_split = 0.2\n",
    "          , callbacks=callbacks_list\n",
    "          , shuffle=True\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
