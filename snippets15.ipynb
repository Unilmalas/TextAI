{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame.midi\n",
    "import time\n",
    "\n",
    "pygame.midi.init()\n",
    "player = pygame.midi.Output(0)\n",
    "player.set_instrument(48) # https://pjb.com.au/muscript/gm.html\n",
    "player.note_on(64, 127) # note_on(note, velocity=None, channel = 0) https://en.scratch-wiki.info/wiki/MIDI_Notes\n",
    "time.sleep(2)\n",
    "player.note_off(64, 127)\n",
    "del player\n",
    "pygame.midi.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 36979 characters, 14 unique.\n",
      "----\n",
      " 4767 11  :989680535227'56'':71 64077934' 2,,6586435:0::7'17 46,:867: 769'8 350'430961,:2960' 993:,984, 9550',5,90:6,'7985489'8 7'1548963,''1763,6844  2628 0,0846,9048'845:1770:13:0'78782: '7 ,486':735806 1'0'1:24238: 3501 254:505':4 50'5,, 5125013'7305021222,1'40:'889'74,44'5 1565285:7254159'123 :401985,734416914980191963,26'252017:, 2845 :,1 '1  674895 ,:134'9886720105 6''8:140 1,'792'55,4839'9578279 924:650996:36377',, 46'466,,'40:4,' 349 1556450525,7451465'1:03 4:71890650 4947821,030351:,,2,5 \n",
      "----\n",
      "iter 0, loss: 65.976431\n",
      "----\n",
      " , '1', ', ','6 ,,72:'0:851:',   30:7'8 :70 ''1 ',0 ', 5:'4 ', :7070:01: 082:62: 6, ',0'0:1 '0:2:40::7,2:87:', ::,1 '3 '1'0 ', '''0''9,2',06 '3:',:8:60:'182', '0', ', :::8 :80:,, ::0:1', '2:71:00 '',''8,2',0:'87 :8  '0:0:,0:',::',74:80:',2 '5, :70',8, '0:0:07::00 ','::0:7'61::061:,00'9 ',',::'0:': :5,, ',:4:10:',04', :5'6 ',0', 51:80 4, 2', '4', ', 9:82:',  ',0 ''0 ',3 ',0:5, 51,'2:50:',1::',0', '0:72'80'1', :0'1:68 :080', '4', ',0:'6 :', ',', '8 '8' 0', 13:77', ::7, ', :',  ', '1:'2:,0:0',::89:8 \n",
      "----\n",
      "iter 100, loss: 65.377330\n",
      "----\n",
      " 70:0', ', '0:55:80', '0::0', '0:50:80', ', '0:65: '0:70:80'8 '0:70:, '2:70:80', '2:75:68'6 '0:8 '0:'0:0:, ', '4:'0:0', '2:62:0', '0:0', '0:74:080',1'0:80', '0 70:60', '0:6'9:080', '0:75:0', '8 '1:00'0:09:0', '0:70:5:0'8:'0:70:, '2:78:680'3:70:70',:'0:80:0', '0:65:0', '0:72:0'0 '0:60:20:81'1:70:70'2:80', ,0:0', '1:78:67:19:1',8'2:80', '1:70:0', ', '0:86',:,0', ', '0:50:82', '0:70:0', :8 ',:'0:75:0::0', '2:0', '2:70:0', '2:76:1', '0:88:0', '2 60:0', '0770:70:0', '0:50', '1:50: ', '4:78:0', '1:50:0 \n",
      "----\n",
      "iter 200, loss: 61.523816\n",
      "----\n",
      " 75:80', '1:69:80', '9:53:80', '0:54:0',0'1::2:80'3:82:0', '0:74:80', '1:78:0', 80:70:0', '0:75:80', ', ' :5,:0', '0:71:8, '8:74:80', '4:75:80', '2:61:80', '1:59:2:'' '0:54:0', '1:71:0', '0:77:80', '1:75:80', '0:62:80', '0 65:80', '0965:0', '0:85:0', '0 50', '1:75:72:77:80', '2:59:80', ', '1:77:80'0:'1:76:70:11', '0:85:8457 '0:71:8', '1:75:80', '0:59:8,:'2:72:80', '2:54:0', '2:64:7, '1:75:81',0', '2:59:,0', ', '1:75:0' :62:0', '0:59:0', ' :79:80', '0:77:0', '1:74:0', '0:73:0', '1:86:0', '0:5,:80' \n",
      "----\n",
      "iter 300, loss: 57.190356\n",
      "----\n",
      " :', '2:79:80', '2:74:80', '0:73:0', '0:42:0', '0:74:80:87'1:77:0', '1:61:0', '1:75:0'' '7:52:0'1 '2:76:0', '0:64:80', '0:73:0', '0:74:0', '1:73:80', '2:70:0', '0:5::0', '0:77:0', '1:5::0', '9:81:80', '0:72:0', '0:75:0', '0:72:81', '2:63:0', '2:75:0', '0:74'0570:0', '1:74:70:0', '0:74:80', '1:70:80', '1:74:0', '2:7::0', '2:72:80', '0:71:80', ',:51:0', '0:70:0', '1:72:80', '0:75:80', '0:74:0', '2:79:0', '1:7':0', '0:63:0', '1:72:80', '1:77:80', '1:'0:0', :0:70:1:'0:25:80', '1:52:0', '1:77:60:0', ' \n",
      "----\n",
      "iter 400, loss: 53.038867\n",
      "----\n",
      " :880', '1:72:80', '1:72:80', '0:71:80', '1:77:80', '2:72:80', '0:71:0', '0:70:80', '9:74:0', '2:80:0', '0:71:80', '0:72:0',0'0:65:80', '0:74:80', '0:70'1:,1:75:0', '1:75:0', '0:71:1 '0:61:0', '0:42:0', '0:40:80', '0:80:0', '0:43:0', '2:77:80', '0:78:07, '1:75:80', '1:79:0', '0:74:80', '1:77:0', '1:77:0', '0:74:80'1 '0:72:0', '0:70:0', '1:'1:80', '0:72:0', '1:42:0', '0:41:80', '0:08:0', '0:72:80', '2:74:0', '0:71:80', '2:72:0',:'0:74:80', '2:59:52:80', '0:75:, ', '0:74:0', '1:79:80', '1:70:0', '2 \n",
      "----\n",
      "iter 500, loss: 49.205703\n",
      "----\n",
      " 0', '0:72:0', '0:60:80', '':69:80', '2:40:80', '1:53:0', '0:71:0', '1:58:80', '0:42:80', '2:56:0', '2:71:880', '1:75:0', '2:64:80', '0:51:0', '1:40:0', '2:79:0', '0:67:80', '0:44:80', '2:56:80', '1:77:80', ', '0:49:0', '1:70:80', '1:74'1:80', '1:73:5'' ', '2:77:80', :0', '2:71:80', '1:51:80', '0'48:0', '0:75:0', '2:43:80', '0:79:80', '0:82:80', '0:09:80', '2:48:0', '2:82:80', '1:51:0', '0:79:,2'2:52:0', '1:73:880', '2:55:0', '0:73:0', '2:72:80', '1:48:80', '0:64:84', '0:72:80', '1:52:80', '2:4': \n",
      "----\n",
      "iter 600, loss: 45.820505\n",
      "----\n",
      " 0', '1:70:80', '1:73:80', '1:72:0', '1:53:80', '1:70:0', '0:79:80', '2:76:80', '1:53:0', '0:79:0', '2:75:80', '2:72:80', '1:27:80', '0:74:80', '2:58:0', '1:74:0', '1:43:80', '0:78:0', '1:86:0', '2:70:0', '2:71:80', '1:51:0', '0:76:80', '1:72:80', '0:45:0', '0:70:80', '2:72:80', '2:58:0', '2:79:80', '1:43:80', '0:74:80', '2:51:0', '1:55:80', '1:72:80', '2:54:80', '1:59:80', '1:73:80', '2:50:0', '0:72:80', '1:54:0', '2:79:80', '0:70:0', '2:53:0', '2:47:80', '1:70:0', '0:41:0', '1:53:80', '0:70:0', \n",
      "----\n",
      "iter 700, loss: 42.697909\n",
      "----\n",
      " , '1:55:0', '0:55:80', '1:55:80', '0:72:0', '2:40:0', '1:55:0', '1:55:80', '2:43:82:80:62:80', '0'51:0', '1:45:80', '1:75:80', '2:54:0', '1:45:80', '2:72:0', '0:74:80', '0:71:80', '0:79:80', '2:77:80', '2:55:50'0 '2:80:0', '2:40:80', ',:75:80', '1:53:80', '2:63:0', '1:72:80', '2:82:0', '0:78:0', '1:90:0', '0:74:0', '1:86:0', '2:54:0', '0:77:80', '2:80:80', '0:48:80', '1:77:0', '2:55:0', '2:82:80', '1:50:80', '0:72:80', '0:65:80', '1:72:0', '0:79:80', '0:65:0', '1:82:80', '0:74:0', '1:64:0', '2:7 \n",
      "----\n",
      "iter 800, loss: 39.862944\n",
      "----\n",
      "  '2:79:80', '0:72:80', '2:70:80', '0:74:80', '2:69:0', '0:66:80', '0:60:0', '1:72:80', '0:60:80', '0:74:80', '0:79:0', '2:80:0', '0:60:80', '1:76:0', '0:74:80', '0:62:0', '2:80:0', '0:70:0',:50:70:0', '0:75:0', '0:73:80', '2:50:80', '2:40:0', '0:69:80', '0:64:80'1::4:0', '2:70:80', '0:72:0', '0:67:0', '2:50:80', '2:80:80', '1:69:80', '1:68:0', '2:74:0', '0:60:80', '0:57:0', '0:75:0', '2:58:0', '2:71:0', '2:50:52:0', '1:72:0', '2:84:0', '1:62:0', '0:62:0', '2::5:0', '0:79:0', '2:55:0', '2:64:80', \n",
      "----\n",
      "iter 900, loss: 37.256241\n"
     ]
    }
   ],
   "source": [
    "# from https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "\n",
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "nruns = 1000\n",
    "noutput = 100\n",
    "\n",
    "# data I/O\n",
    "data = open('notes0.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % noutput == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 500)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pygame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-28f0dd588c7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmidi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmidi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pygame'"
     ]
    }
   ],
   "source": [
    "import pygame.midi\n",
    "import time\n",
    "import random\n",
    "\n",
    "pygame.midi.init()\n",
    "player = pygame.midi.Output(0)\n",
    "player.set_instrument(1) # https://pjb.com.au/muscript/gm.html\n",
    "notel = [55, 72, 77, 74, 50, 67, 77, 79, 55, 79, 81, 53, 58, 50, 77, 75, 75, 51, 74, 79, 72, 58, 77, 74, 75, 57, 74, 74, 72, 69, 72, 65, 72, 63, 51, 67, 67, 46, 70, 75, 72, 41, 70, 68, 48, 63, 74, 63, 70, 63, 70, 70, 75, 51, 69, 65, 67, 70, 50, 63, 75, 72, 46, 70, 65, 69, 82, 72, 74, 68, 72, 53, 72, 69, 77, 67, 75, 72, 55, 69, 65, 82, 51, 75, 77, 77, 75, 50, 79, 79, 74, 79, 79, 82, 67, 79, 63, 79, 79, 79, 75, 53, 79, 74, 74, 75, 46, 70, 77, 74, 55, 77, 79, 77, 82, 77, 48, 53, 79, 79, 51, 74, 79, 79]\n",
    "for n in notel:\n",
    "    player.note_on(n, 127) # note_on(note, velocity=None, channel = 0) https://en.scratch-wiki.info/wiki/MIDI_Notes\n",
    "    time.sleep(0.17)\n",
    "    player.note_off(n, 127)\n",
    "del player\n",
    "pygame.midi.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51, 63, 63, 67, 67, 70, 51, 70, 75, 55, 75, 70, 55, 70, 72, 56, 72, 80, 84, 56, 80, 84, 79, 82, 58, 79, 82, 77, 80, 58, 77, 80, 75, 79, 46, 75, 79, 74, 77, 46, 74, 77, 75, 79, 51, 79, 63, 75, 63, 63, 67, 67, 70, 51, 63, 70, 63, 75, 55, 75, 70, 63, 55, 70, 63, 72, 56, 63, 72, 80, 84, 56, 80, 84, 79, 82, 58, 79, 82, 77, 80, 58, 77, 80, 75, 79, 46, 75, 79, 74, 77, 46, 74, 77, 75, 79, 51, 79, 82, 75, 82, 82, 80, 80, 79, 51, 82, 79, 84, 77, 56, 77, 75, 84, 56, 75, 77, 74, 58, 77, 74, 58, 79, 75, 63, 63, 58, 58, 55, 79, 75, 55, 80, 77, 51, 80, 77, 82, 79, 51, 82, 79, 77, 74, 58, 77, 74, 58, 79, 75, 63, 63, 58, 58, 55, 79, 75, 55, 80, 77, 51, 80, 77, 82, 79, 51, 82, 79, 77, 74, 58, 77, 74, 75, 72, 58, 75, 72, 74, 70, 46, 74, 70, 72, 68, 46, 72, 68, 70, 67, 58, 70, 67, 68, 65, 58, 68, 65, 67, 63, 51, 67, 63, 63, 67, 67, 70, 63, 51, 70, 75, 70, 55, 75, 70, 70, 67, 55, 70, 67, 72, 63, 56, 72, 63, 84, 80, 56, 84, 80, 82, 79, 58, 82, 79, 80, 77, 58, 80, 77, 79, 75, 46, 79, 75, 77, 74, 46, 77, 74, 79, 75, 51, 79, 63, 75, 63, 67, 63, 67, 70, 51, 63, 70, 75, 63, 55, 75, 70, 63, 55, 70, 72, 63, 56, 72, 63, 84, 80, 56, 84, 80, 82, 79, 58, 82, 79, 80, 77, 58, 80, 77, 79, 75, 46, 79, 75, 77, 74, 46, 77, 74, 79, 75, 51, 79, 77, 75, 51, 77, 75, 79, 51, 75, 77, 79, 51, 77, 79, 75, 51, 79, 80, 75, 51, 80, 82, 77, 50, 82, 77, 77, 50, 77, 74, 82, 50, 74, 70, 82, 50, 70, 77, 50, 77, 74, 50, 74, 75, 48, 75, 74, 48, 74, 72, 75, 48, 72, 74, 75, 48, 74, 75, 72, 48, 75, 77, 72, 48, 77, 79, 74, 46, 79, 74, 74, 46, 74, 70, 79, 46, 70, 67, 79, 46, 67, 74, 46, 74, 70, 46, 70, 72, 44, 72, 68, 44, 68, 75, 72, 56, 75, 72, 72, 56, 72, 70, 75, 56, 70, 68, 56, 68, 70, 55, 70, 67, 75, 55, 67, 75, 70, 55, 75, 70, 70, 55, 70, 68, 75, 55, 68, 67, 75, 55, 67, 68, 74, 53, 68, 74, 67, 75, 53, 67, 75, 68, 77, 50, 68, 70, 77, 50, 70, 67, 75, 51, 67, 68, 75, 51, 68, 65, 74, 46, 65, 72, 74, 46, 72, 70, 50, 70, 68, 50, 68, 67, 46, 67, 65, 46, 65, 67, 51, 67, 77, 51, 77, 75, 48, 75, 74, 48, 74, 72, 51, 72, 70, 51, 70, 69, 53, 69, 79, 53, 79, 77, 57, 77, 75, 57, 75, 74, 53, 74, 72, 53, 72, 74, 58, 65, 74, 58, 65, 74, 70, 58, 70, 72, 74, 58, 72, 70, 74, 58, 74, 75, 70, 58, 75, 72, 77, 57, 77, 72, 72, 57, 72, 77, 69, 57, 69, 65, 77, 57, 65, 72, 57, 72, 69, 57, 69, 70, 55, 70, 69, 55, 69, 70, 67, 55, 67, 69, 70, 55, 69, 67, 70, 55, 70, 72, 67, 55, 72, 69, 74, 53, 74, 69, 69, 53, 69, 74, 65, 53, 65, 62, 74, 53, 62, 69, 53, 69, 65, 53, 65, 67, 51, 67, 63, 51, 63, 67, 70, 51, 70, 67, 67, 51, 67, 70, 65, 51, 65, 63, 51, 63, 65, 50, 65, 62, 70, 50, 62, 65, 70, 50, 70, 65, 65, 50, 65, 70, 63, 50, 63, 62, 70, 50, 62, 69, 63, 48, 69, 63, 70, 62, 48, 70, 62, 72, 63, 45, 63, 65, 72, 45, 65, 70, 62, 46, 62, 63, 70, 46, 63, 79, 60, 51, 79, 80, 60, 51, 80, 79, 72, 51, 79, 77, 72, 51, 77, 75, 51, 75, 74, 51, 74, 75, 79, 51, 75, 79, 77, 80, 51, 77, 80, 75, 79, 51, 75, 79, 74, 77, 51, 74, 77, 72, 75, 51, 72, 75, 70, 74, 51, 70, 74, 69, 72, 53, 72, 77, 69, 77, 77, 81, 81, 72, 53, 77, 72, 77, 74, 58, 74, 82, 77, 58, 82, 77, 75, 57, 77, 79, 75, 79, 77, 74, 77, 79, 57, 74, 79, 77, 75, 53, 77, 79, 75, 79, 77, 84, 77, 79, 53, 84, 79, 77, 74, 58, 77, 79, 74, 79, 77, 82, 77, 79, 58, 82, 79, 77, 72, 53, 77, 79, 72, 79, 77, 77, 77, 79, 77, 79, 77, 81, 77, 79, 81, 79, 77, 72, 53, 77, 79, 72, 79, 77, 74, 58, 77, 79, 74, 79, 77, 82, 77, 79, 58, 82, 79, 77, 75, 57, 77, 79, 75, 79, 77, 74, 77, 79, 57, 74, 79, 77, 75, 53, 77, 79, 75, 79, 77, 84, 77, 79, 53, 84, 79, 77, 74, 58, 77, 79, 74, 79, 77, 82, 77, 79, 58, 82, 79, 77, 72, 53, 72, 77, 77, 53, 77, 72, 81, 41, 72, 81, 70, 79, 41, 70, 79, 72, 81, 41, 72, 81, 74, 82, 41, 74, 82, 75, 84, 41, 75, 84, 74, 82, 41, 74, 82, 72, 81, 41, 72, 81, 75, 79, 41, 75, 79, 74, 77, 41, 74, 77, 72, 75, 41, 72, 75, 74, 46, 70, 74, 70, 74, 70, 74, 65, 46, 70, 65, 67, 70, 51, 67, 75, 70, 51, 75, 68, 70, 50, 70, 72, 68, 67, 72, 70, 70, 72, 50, 67, 68, 72, 70, 46, 70, 72, 68, 77, 72, 70, 70, 72, 46, 77, 67, 72, 70, 51, 70, 72, 67, 75, 72, 70, 70, 72, 51, 75, 65, 72, 70, 46, 70, 72, 65, 70, 72, 70, 70, 72, 70, 74, 72, 70, 70, 72, 74, 65, 72, 70, 46, 70, 72, 65, 67, 72, 70, 51, 70, 72, 67, 75, 72, 70, 70, 72, 51, 75, 68, 72, 70, 50, 70, 72, 68, 67, 72, 70, 70, 72, 50, 67, 68, 72, 70, 46, 70, 72, 68, 77, 72, 70, 70, 72, 46, 77, 67, 72, 70, 51, 70, 72, 67, 75, 72, 70, 70, 72, 51, 75, 65, 72, 70, 46, 65, 70, 70, 46, 70, 74, 70, 46, 74, 70, 75, 72, 46, 75, 72, 77, 74, 46, 77, 74, 79, 75, 46, 79, 75, 80, 77, 46, 80, 77, 79, 75, 46, 79, 75, 77, 74, 58, 77, 74, 80, 77, 58, 80, 77, 79, 75, 46, 79, 75, 77, 74, 46, 77, 74, 79, 75, 51, 79, 77, 75, 51, 77, 75, 79, 48, 75, 79, 74, 77, 48, 74, 77, 72, 75, 51, 72, 75, 70, 74, 51, 70, 74, 69, 72, 53, 69, 72, 70, 70, 53, 70, 70, 72, 69, 41, 72, 69, 75, 72, 41, 75, 72, 74, 70, 53, 74, 70, 72, 69, 53, 72, 69, 74, 70, 58, 74, 70, 75, 72, 58, 75, 72, 77, 74, 46, 46, 48, 48, 50, 50, 51, 51, 53, 77, 74, 72, 69, 53, 72, 69, 72, 69, 41, 72, 69, 75, 72, 41, 75, 72, 74, 70, 53, 74, 70, 72, 69, 53, 72, 69, 74, 70, 46, 74, 70, 75, 72, 46, 75, 72, 77, 74, 46, 46, 48, 48, 50, 50, 51, 51, 53, 77, 74, 72, 69, 53, 72, 69, 72, 69, 41, 72, 69, 75, 72, 41, 75, 72, 74, 70, 53, 74, 70, 72, 69, 53, 72, 69, 74, 70, 46, 74, 75, 46, 75, 75, 48, 75, 77, 48, 77, 77, 50, 77, 79, 50, 79, 79, 51, 79, 81, 70, 75, 51, 81, 75, 81, 75, 53, 81, 82, 75, 53, 82, 82, 74, 55, 82, 70, 74, 55, 70, 79, 72, 51, 79, 75, 72, 51, 75, 74, 70, 53, 74, 70, 53, 72, 69, 41, 72, 69, 41, 70, 74, 46, 70, 74, 72, 75, 46, 72, 75, 74, 77, 46, 46, 48, 48, 50, 50, 51, 51, 53, 74, 77, 69, 72, 53, 69, 72, 69, 72, 41, 69, 72, 72, 75, 41, 72, 75, 70, 74, 53, 70, 74, 69, 72, 53, 69, 72, 70, 74, 46, 70, 74, 72, 75, 46, 72, 75, 74, 77, 46, 46, 48, 48, 50, 50, 51, 51, 53, 74, 77, 69, 72, 53, 69, 72, 69, 72, 41, 69, 72, 72, 75, 41, 72, 75, 70, 74, 53, 70, 74, 69, 72, 53, 69, 72, 70, 74, 46, 70, 74, 74, 77, 46, 74, 77, 77, 82, 50, 82, 81, 50, 81, 79, 46, 79, 77, 46, 77, 79, 51, 79, 77, 77, 77, 75, 79, 51, 79, 81, 53, 75, 81, 53, 74, 82, 55, 74, 82, 55, 75, 79, 51, 75, 79, 72, 75, 51, 72, 75, 70, 74, 53, 70, 74, 69, 72, 69, 72, 70, 74, 70, 74, 69, 72, 53, 69, 72, 70, 74, 70, 74, 69, 72, 41, 69, 72, 70, 70, 41, 70, 70, 70, 70, 46, 70, 70, 46, 46, 58, 58, 62, 62, 65, 46, 65, 70, 50, 70, 65, 50, 65, 67, 51, 67, 75, 79, 51, 75, 79, 74, 77, 53, 74, 77, 72, 75, 53, 72, 75, 70, 74, 41, 70, 74, 69, 72, 41, 69, 72, 70, 74, 46, 74, 58, 70, 58, 82, 62, 62, 65, 46, 82, 65, 70, 70, 50, 70, 65, 50, 65, 67, 51, 70, 67, 75, 79, 51, 75, 79, 74, 77, 53, 74, 77, 72, 75, 53, 72, 75, 70, 74, 41, 70, 74, 69, 72, 41, 69, 72, 70, 74, 46, 74, 77, 70, 77, 77, 75, 75, 74, 46, 77, 74, 79, 72, 51, 72, 70, 79, 51, 70, 72, 69, 53, 72, 69, 53, 74, 70, 58, 58, 53, 53, 50, 74, 70, 50, 75, 72, 46, 75, 72, 77, 74, 46, 77, 74, 72, 69, 53, 72, 69, 53, 74, 70, 58, 58, 53, 53, 50, 74, 70, 50, 75, 72, 46, 75, 72, 77, 74, 46, 77, 74, 72, 69, 53, 72, 65, 53, 51, 69, 65, 51, 77, 50, 50, 48, 48, 47, 77, 80, 80, 79, 74, 79, 77, 77, 75, 75, 47, 74, 74, 74, 75, 74, 48, 75, 48, 74, 47, 74, 74, 47, 75, 67, 48, 75, 48, 77, 50, 67, 77, 50, 79, 72, 51, 72, 51, 70, 48, 70, 48, 68, 53, 79, 68, 77, 80, 77, 80, 75, 79, 75, 79, 74, 77, 74, 77, 53, 72, 75, 72, 75, 71, 74, 55, 71, 74, 55, 72, 75, 60, 60, 55, 55, 51, 72, 75, 51, 74, 77, 48, 74, 77, 75, 79, 48, 75, 79, 71, 74, 55, 71, 74, 55, 72, 75, 60, 60, 55, 55, 51, 72, 75, 51, 74, 77, 48, 74, 77, 75, 79, 48, 75, 79, 71, 74, 55, 71, 79, 74, 55, 79, 79, 59, 79, 77, 59, 77, 77, 55, 77, 75, 55, 75, 74, 60, 84, 74, 60, 84, 76, 84, 48, 84, 82, 48, 82, 82, 60, 82, 80, 76, 60, 80, 80, 53, 77, 80, 53, 77, 77, 81, 41, 77, 75, 41, 75, 75, 53, 75, 74, 81, 53, 74, 74, 58, 82, 74, 58, 82, 74, 82, 46, 82, 80, 46, 80, 80, 58, 80, 79, 74, 58, 79, 75, 79, 51, 79, 79, 75, 51, 79, 63, 80, 48, 80, 79, 63, 48, 79, 75, 77, 60, 77, 75, 75, 75, 74, 77, 77, 77, 74, 60, 77, 75, 79, 59, 75, 79, 74, 77, 74, 75, 75, 74, 74, 75, 77, 75, 74, 75, 75, 74, 59, 74, 74, 75, 60, 79, 75, 60, 79, 80, 63, 48, 80, 79, 63, 48, 79, 77, 75, 60, 77, 75, 75, 60, 75, 77, 74, 60, 77, 77, 74, 60, 77, 79, 74, 59, 79, 77, 77, 75, 75, 74, 74, 59, 74, 75, 60, 75, 60, 60, 58, 60, 58, 72, 57, 57, 55, 55, 54, 72, 75, 75, 69, 74, 74, 72, 72, 70, 70, 54, 69, 69, 70, 55, 70, 55, 69, 54, 69, 69, 54, 62, 70, 55, 70, 55, 72, 57, 62, 72, 57, 67, 74, 59, 67, 59, 65, 55, 65, 55, 63, 60, 63, 74, 60, 75, 72, 48, 75, 72, 74, 70, 74, 70, 72, 69, 72, 69, 70, 67, 48, 70, 67, 69, 66, 50, 69, 66, 50, 70, 67, 55, 55, 50, 50, 46, 70, 67, 46, 72, 69, 43, 72, 69, 74, 70, 43, 74, 70, 69, 66, 50, 69, 66, 50, 70, 67, 55, 55, 50, 50, 46, 70, 67, 46, 72, 69, 43, 72, 69, 74, 70, 43, 74, 70, 69, 66, 50, 66, 74, 69, 50, 74, 74, 54, 74, 72, 54, 72, 72, 50, 72, 70, 50, 70, 70, 55, 79, 70, 55, 79, 79, 71, 43, 79, 77, 43, 77, 77, 55, 77, 75, 71, 55, 75, 75, 48, 72, 75, 48, 72, 76, 72, 60, 72, 70, 60, 70, 70, 48, 70, 69, 76, 48, 69, 69, 53, 77, 53, 77, 77, 41, 77, 75, 41, 75, 75, 53, 75, 69, 74, 53, 74, 74, 70, 46, 74, 74, 46, 74, 75, 43, 75, 74, 43, 74, 72, 55, 72, 70, 70, 55, 70, 72, 69, 55, 72, 72, 69, 55, 72, 74, 69, 54, 74, 72, 72, 70, 70, 69, 69, 54, 69, 70, 55, 74, 70, 55, 74, 70, 75, 43, 75, 74, 43, 74, 72, 55, 72, 70, 70, 55, 70, 69, 72, 55, 72, 72, 69, 55, 72, 69, 74, 54, 74, 72, 72, 70, 70, 69, 69, 54, 69, 67, 70, 55, 67, 70, 82, 74, 55, 82, 74, 82, 74, 43, 82, 74, 81, 72, 43, 81, 72, 81, 72, 48, 81, 72, 79, 70, 48, 79, 70, 79, 70, 50, 79, 70, 78, 69, 78, 69, 79, 70, 79, 70, 78, 69, 50, 78, 69, 79, 70, 79, 70, 78, 69, 48, 78, 69, 48, 79, 67, 46, 79, 74, 67, 46, 74, 75, 67, 48, 75, 67, 72, 69, 48, 72, 69, 70, 67, 50, 70, 67, 69, 66, 69, 66, 70, 67, 70, 67, 69, 66, 50, 69, 66, 70, 67, 70, 67, 69, 66, 38, 69, 66, 67, 67, 38, 67, 67, 67, 67, 43, 67, 70, 43, 70, 72, 44, 72, 44, 74, 46, 67, 74, 46, 75, 48, 75, 48, 77, 50, 77, 50, 79, 51, 63, 63, 67, 67, 70, 79, 51, 70, 75, 55, 75, 70, 55, 70, 72, 56, 72, 80, 84, 56, 80, 84, 79, 82, 58, 79, 82, 77, 80, 58, 77, 80, 75, 79, 46, 75, 79, 74, 77, 46, 74, 77, 75, 79, 51, 79, 63, 75, 63, 63, 67, 67, 70, 51, 63, 70, 63, 75, 55, 75, 70, 63, 55, 70, 63, 72, 56, 63, 72, 80, 84, 56, 80, 84, 79, 82, 58, 79, 82, 77, 80, 58, 77, 80, 75, 79, 46, 75, 79, 74, 77, 46, 74, 77, 75, 79, 51, 75, 70, 70, 75, 75, 79, 79, 51, 79, 82, 55, 82, 70, 55, 70, 72, 56, 72, 82, 56, 82, 80, 53, 80, 79, 53, 79, 80, 80, 72, 72, 70, 50, 70, 80, 50, 80, 79, 51, 79, 77, 51, 77, 79, 79, 70, 70, 68, 48, 68, 79, 48, 79, 77, 50, 77, 75, 50, 75, 77, 46, 77, 68, 46, 68, 67, 51, 70, 67, 70, 75, 75, 79, 51, 79, 82, 55, 82, 70, 55, 70, 72, 56, 72, 82, 56, 82, 80, 53, 80, 79, 53, 79, 80, 80, 72, 72, 70, 50, 70, 80, 50, 80, 79, 51, 79, 77, 51, 77, 79, 79, 70, 70, 68, 48, 68, 79, 48, 79, 77, 50, 77, 75, 50, 75, 77, 46, 77, 68, 46, 68, 67, 51, 70, 70, 75, 67, 75, 73, 70, 51, 73, 70, 72, 68, 55, 72, 68, 70, 67, 55, 70, 67, 72, 68, 56, 72, 68, 73, 75, 56, 73, 75, 72, 80, 53, 72, 80, 70, 79, 53, 70, 79, 68, 77, 68, 77, 67, 75, 67, 75, 65, 74, 46, 65, 70, 74, 70, 74, 70, 74, 65, 46, 70, 65, 67, 70, 51, 67, 75, 70, 51, 75, 68, 70, 50, 70, 72, 68, 67, 72, 70, 70, 72, 50, 67, 68, 72, 70, 46, 70, 72, 68, 77, 72, 70, 70, 72, 46, 77, 67, 72, 70, 51, 70, 72, 67, 75, 72, 70, 70, 72, 51, 75, 65, 72, 70, 46, 70, 72, 65, 70, 72, 70, 70, 72, 70, 74, 72, 70, 70, 72, 74, 65, 72, 70, 46, 70, 72, 65, 67, 72, 70, 51, 70, 72, 67, 75, 72, 70, 70, 72, 51, 75, 68, 72, 70, 50, 70, 72, 68, 67, 72, 70, 70, 72, 50, 67, 68, 72, 70, 46, 70, 72, 68, 77, 72, 70, 70, 72, 46, 77, 67, 72, 70, 51, 70, 72, 67, 75, 72, 70, 70, 72, 51, 75, 65, 72, 70, 46, 65, 70, 70, 46, 70, 74, 70, 58, 74, 70, 75, 72, 58, 75, 72, 77, 74, 46, 77, 74, 79, 75, 46, 79, 75, 80, 77, 46, 80, 77, 79, 75, 46, 79, 75, 77, 74, 46, 77, 74, 79, 75, 46, 79, 75, 80, 77, 46, 80, 77, 77, 74, 46, 77, 74, 79, 73, 51, 73, 72, 79, 72, 75, 73, 73, 82, 51, 75, 82, 75, 72, 56, 72, 80, 75, 56, 80, 75, 73, 55, 75, 77, 73, 77, 75, 72, 75, 77, 55, 72, 77, 75, 73, 51, 75, 77, 73, 77, 75, 82, 75, 77, 51, 82, 77, 75, 72, 56, 75, 77, 72, 77, 75, 80, 75, 77, 56, 80, 77, 75, 70, 51, 75, 77, 70, 77, 75, 75, 75, 77]\n"
     ]
    }
   ],
   "source": [
    "import mido\n",
    "\n",
    "#outport = mido.open_output()\n",
    "niter = 0\n",
    "noteslst = []\n",
    "for msg in mido.MidiFile('bjs1031c.mid').play():\n",
    "    #outport.send(msg)\n",
    "    #print msg.bytes()\n",
    "    #print str(msg)\n",
    "    thismsg = str(msg).split(\" \") # ['note_on', 'channel=0', 'note=60', 'velocity=61', 'time=0.00168918958333']\n",
    "    # The velocity specifies the volume or force, with which the note is played\n",
    "    if thismsg[0] == 'note_on':\n",
    "        #print thismsg[2][5:]\n",
    "        noteslst.append(int(thismsg[2][5:]))\n",
    "    niter += 1\n",
    "    if niter >= 3000:\n",
    "        break\n",
    "print noteslst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e', 's')\n",
      "('es', 't')\n",
      "('est', '</w>')\n",
      "('l', 'o')\n",
      "('lo', 'w')\n",
      "('n', 'e')\n",
      "('ne', 'w')\n",
      "('new', 'est</w>')\n",
      "('low', '</w>')\n",
      "('w', 'i')\n"
     ]
    }
   ],
   "source": [
    "# BPE\n",
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "if __name__=='__main__':\n",
    "    vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,'n e w e s t </w>':6, 'w i d e s t </w>':3}\n",
    "    num_merges = 10\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "        print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-52fe8f31a26a>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-52fe8f31a26a>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    print 'data has %d characters, %d unique.' % (data_size, vocab_size)\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# from https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "\n",
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "nruns = 50000\n",
    "noutput = 5000\n",
    "\n",
    "# data I/O\n",
    "data = open('notes0.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % noutput == 0 or n == nruns-1:\n",
    "    sample_ix = sample(hprev, inputs[0], 1500)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame.midi\n",
    "import time\n",
    "import random\n",
    "\n",
    "pygame.midi.init()\n",
    "player = pygame.midi.Output(0)\n",
    "player.set_instrument(0) # https://pjb.com.au/muscript/gm.html\n",
    "# channel - note - velocity - time\n",
    "notel = ['0:70:0', '0:74:0', '1:74:80', '1:79:80', '1:82:0', '2:43:0', '0:74:80', '2:46:80', '1:75:0', '1:72:80', '1:74:0', '1:72:0', '1:80:80', '2:53:80', '0:75:80', '2:53:80', '1:82:80', '2:57:0', '1:69:80', '1:72:80', '1:72:0', '0:75:80', '1:69:0', '0:74:0', '0:75:80', '2:53:80', '0:74:80', '2:46:80', '0:72:80', '2:48:0', '1:69:80', '2:53:0', '0:74:80', '0:75:80', '2:53:80', '0:75:80', '2:53:80', '1:74:80', '2:55:80', '2:53:0', '0:75:80', '1:79:80', '1:79:80', '2:46:0', '2:51:80', '0:72:0', '1:69:0', '1:75:80', '0:77:0', '0:72:0', '1:74:0', '0:74:0', '1:74:80', '0:74:80', '1:74:80', '2:53:0', '0:75:0', '1:74:0', '1:72:0', '2:70:80', '1:69:80', '2:41:0', '1:70:80', '0:72:0', '1:74:0', '1:69:0', '1:75:0', '0:70:80', '1:75:0', '1:74:80', '2:50:80', '0:70:0', '0:69:80', '2:53:80', '2:51:0', '2:55:0', '0:84:80', '1:75:0', '0:79:80', '2:51:80', '0:72:0', '0:72:80', '1:70:0', '1:74:0', '0:70:80', '2:48:80', '0:72:80', '1:72:0', '0:69:0', '2:53:0', '1:74:0', '1:75:0', '1:70:80', '1:75:80', '1:79:80', '1:65:80', '1:75:0', '2:51:80', '0:75:80', '2:41:80', '1:70:0', '1:82:0', '1:74:0', '1:75:80', '2:51:80', '0:75:0', '1:69:80', '1:79:80', '1:75:0', '1:72:0', '1:74:80', '2:51:80', '0:77:80', '2:51:80', '0:77:80', '2:51:0', '1:77:80', '1:69:80', '0:75:80', '2:53:0', '0:79:0', '0:75:80', '2:41:80', '2:58:0', '0:72:80', '1:84:80', '1:75:0', '1:81:80', '2:51:0', '0:69:80', '1:72:80', '2:51:0', '0:74:80', '2:50:80', '2:53:0', '0:69:0', '2:51:0', '1:79:80', '2:53:0', '1:79:80', '1:67:0', '0:79:80']\n",
    "for n in notel:\n",
    "    ns = n.split(':')\n",
    "    #print ns\n",
    "    player.note_on(int(ns[1]), int(ns[2]), int(ns[0])) # note_on(note, velocity=None, channel = 0) https://en.scratch-wiki.info/wiki/MIDI_Notes\n",
    "    time.sleep(int(ns[2])/200)\n",
    "    player.note_off(int(ns[1]), int(ns[2]), int(ns[0]))\n",
    "del player\n",
    "pygame.midi.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2:51:80', '1:63:80', '1:63:0', '1:67:80', '1:67:0', '1:70:80', '2:51:0', '1:70:0', '1:75:80', '2:55:80', '1:75:0', '1:70:80', '2:55:0', '1:70:0', '1:72:80', '2:56:80', '1:72:0', '0:80:80', '1:84:80', '2:56:0', '0:80:0', '1:84:0', '0:79:80', '1:82:80', '2:58:80', '0:79:0', '1:82:0', '0:77:80', '1:80:80', '2:58:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:46:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:46:0', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:51:80', '1:79:0', '1:63:80', '0:75:0', '1:63:0', '0:63:80', '1:67:80', '1:67:0', '1:70:80', '2:51:0', '0:63:0', '1:70:0', '0:63:80', '1:75:80', '2:55:80', '1:75:0', '1:70:80', '0:63:0', '2:55:0', '1:70:0', '0:63:80', '1:72:80', '2:56:80', '0:63:0', '1:72:0', '0:80:80', '1:84:80', '2:56:0', '0:80:0', '1:84:0', '0:79:80', '1:82:80', '2:58:80', '0:79:0', '1:82:0', '0:77:80', '1:80:80', '2:58:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:46:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:46:0', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:51:80', '1:79:0', '1:82:80', '0:75:0', '1:82:0', '0:82:80', '1:80:80', '1:80:0', '1:79:80', '2:51:0', '0:82:0', '1:79:0', '0:84:80', '1:77:80', '2:56:80', '1:77:0', '1:75:80', '0:84:0', '2:56:0', '1:75:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '1:74:0', '2:58:0', '0:79:80', '1:75:80', '2:63:80', '2:63:0', '2:58:80', '2:58:0', '2:55:80', '0:79:0', '1:75:0', '2:55:0', '0:80:80', '1:77:80', '2:51:80', '0:80:0', '1:77:0', '0:82:80', '1:79:80', '2:51:0', '0:82:0', '1:79:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '1:74:0', '2:58:0', '0:79:80', '1:75:80', '2:63:80', '2:63:0', '2:58:80', '2:58:0', '2:55:80', '0:79:0', '1:75:0', '2:55:0', '0:80:80', '1:77:80', '2:51:80', '0:80:0', '1:77:0', '0:82:80', '1:79:80', '2:51:0', '0:82:0', '1:79:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '1:74:0', '0:75:80', '1:72:80', '2:58:0', '0:75:0', '1:72:0', '0:74:80', '1:70:80', '2:46:80', '0:74:0', '1:70:0', '0:72:80', '1:68:80', '2:46:0', '0:72:0', '1:68:0', '0:70:80', '1:67:80', '2:58:80', '0:70:0', '1:67:0', '0:68:80', '1:65:80', '2:58:0', '0:68:0', '1:65:0', '0:67:80', '1:63:80', '2:51:80', '0:67:0', '0:63:80', '0:63:0', '0:67:80', '0:67:0', '0:70:80', '1:63:0', '2:51:0', '0:70:0', '0:75:80', '1:70:80', '2:55:80', '0:75:0', '1:70:0', '0:70:80', '1:67:80', '2:55:0', '0:70:0', '1:67:0', '0:72:80', '1:63:80', '2:56:80', '0:72:0', '1:63:0', '0:84:80', '1:80:80', '2:56:0', '0:84:0', '1:80:0', '0:82:80', '1:79:80', '2:58:80', '0:82:0', '1:79:0', '0:80:80', '1:77:80', '2:58:0', '0:80:0', '1:77:0', '0:79:80', '1:75:80', '2:46:80', '0:79:0', '1:75:0', '0:77:80', '1:74:80', '2:46:0', '0:77:0', '1:74:0', '0:79:80', '1:75:80', '2:51:80', '0:79:0', '0:63:80', '1:75:0', '0:63:0', '0:67:80', '1:63:80', '0:67:0', '0:70:80', '2:51:0', '1:63:0', '0:70:0', '0:75:80', '1:63:80', '2:55:80', '0:75:0', '0:70:80', '1:63:0', '2:55:0', '0:70:0', '0:72:80', '1:63:80', '2:56:80', '0:72:0', '1:63:0', '0:84:80', '1:80:80', '2:56:0', '0:84:0', '1:80:0', '0:82:80', '1:79:80', '2:58:80', '0:82:0', '1:79:0', '0:80:80', '1:77:80', '2:58:0', '0:80:0', '1:77:0', '0:79:80', '1:75:80', '2:46:80', '0:79:0', '1:75:0', '0:77:80', '1:74:80', '2:46:0', '0:77:0', '1:74:0', '0:79:80', '1:75:80', '2:51:80', '0:79:0', '0:77:80', '1:75:0', '2:51:0', '0:77:0', '0:75:80', '1:79:80', '2:51:80', '0:75:0', '0:77:80', '1:79:0', '2:51:0', '0:77:0', '0:79:80', '1:75:80', '2:51:80', '0:79:0', '0:80:80', '1:75:0', '2:51:0', '0:80:0', '0:82:80', '1:77:80', '2:50:80', '0:82:0', '0:77:80', '1:77:0', '2:50:0', '0:77:0', '0:74:80', '1:82:80', '2:50:80', '0:74:0', '0:70:80', '1:82:0', '2:50:0', '0:70:0', '0:77:80', '2:50:80', '0:77:0', '0:74:80', '2:50:0', '0:74:0', '0:75:80', '2:48:80', '0:75:0', '0:74:80', '2:48:0', '0:74:0', '0:72:80', '1:75:80', '2:48:80', '0:72:0', '0:74:80', '1:75:0', '2:48:0', '0:74:0', '0:75:80', '1:72:80', '2:48:80', '0:75:0', '0:77:80', '1:72:0', '2:48:0', '0:77:0', '0:79:80', '1:74:80', '2:46:80', '0:79:0', '0:74:80', '1:74:0', '2:46:0', '0:74:0', '0:70:80', '1:79:80', '2:46:80', '0:70:0', '0:67:80', '1:79:0', '2:46:0', '0:67:0', '0:74:80', '2:46:80', '0:74:0', '0:70:80', '2:46:0', '0:70:0', '0:72:80', '2:44:80', '0:72:0', '0:68:80', '2:44:0', '0:68:0', '0:75:80', '1:72:80', '2:56:80', '0:75:0', '0:72:80', '1:72:0', '2:56:0', '0:72:0', '0:70:80', '1:75:80', '2:56:80', '0:70:0', '0:68:80', '2:56:0', '0:68:0', '0:70:80', '2:55:80', '0:70:0', '0:67:80', '1:75:0', '2:55:0', '0:67:0', '0:75:80', '1:70:80', '2:55:80', '0:75:0', '0:70:80', '1:70:0', '2:55:0', '0:70:0', '0:68:80', '1:75:80', '2:55:80', '0:68:0', '0:67:80', '1:75:0', '2:55:0', '0:67:0', '0:68:80', '1:74:80', '2:53:80', '0:68:0', '1:74:0', '0:67:80', '1:75:80', '2:53:0', '0:67:0', '1:75:0', '0:68:80', '1:77:80', '2:50:80', '0:68:0', '0:70:80', '1:77:0', '2:50:0', '0:70:0', '0:67:80', '1:75:80', '2:51:80', '0:67:0', '0:68:80', '1:75:0', '2:51:0', '0:68:0', '0:65:80', '1:74:80', '2:46:80', '0:65:0', '0:72:80', '1:74:0', '2:46:0', '0:72:0', '0:70:80', '2:50:80', '0:70:0', '0:68:80', '2:50:0', '0:68:0', '0:67:80', '2:46:80', '0:67:0', '0:65:80', '2:46:0', '0:65:0', '0:67:80', '2:51:80', '0:67:0', '0:77:80', '2:51:0', '0:77:0', '0:75:80', '2:48:80', '0:75:0', '0:74:80', '2:48:0', '0:74:0', '0:72:80', '2:51:80', '0:72:0', '0:70:80', '2:51:0', '0:70:0', '0:69:80', '2:53:80', '0:69:0', '0:79:80', '2:53:0', '0:79:0', '0:77:80', '2:57:80', '0:77:0', '0:75:80', '2:57:0', '0:75:0', '0:74:80', '2:53:80', '0:74:0', '0:72:80', '2:53:0', '0:72:0', '0:74:80', '2:58:80', '1:65:80', '0:74:0', '2:58:0', '1:65:0', '0:74:80', '1:70:80', '2:58:80', '1:70:0', '1:72:80', '0:74:0', '2:58:0', '1:72:0', '0:70:80', '1:74:80', '2:58:80', '1:74:0', '1:75:80', '0:70:0', '2:58:0', '1:75:0', '0:72:80', '1:77:80', '2:57:80', '1:77:0', '1:72:80', '0:72:0', '2:57:0', '1:72:0', '0:77:80', '1:69:80', '2:57:80', '1:69:0', '1:65:80', '0:77:0', '2:57:0', '1:65:0', '1:72:80', '2:57:80', '1:72:0', '1:69:80', '2:57:0', '1:69:0', '1:70:80', '2:55:80', '1:70:0', '1:69:80', '2:55:0', '1:69:0', '0:70:80', '1:67:80', '2:55:80', '1:67:0', '1:69:80', '0:70:0', '2:55:0', '1:69:0', '0:67:80', '1:70:80', '2:55:80', '1:70:0', '1:72:80', '0:67:0', '2:55:0', '1:72:0', '0:69:80', '1:74:80', '2:53:80', '1:74:0', '1:69:80', '0:69:0', '2:53:0', '1:69:0', '0:74:80', '1:65:80', '2:53:80', '1:65:0', '1:62:80', '0:74:0', '2:53:0', '1:62:0', '1:69:80', '2:53:80', '1:69:0', '1:65:80', '2:53:0', '1:65:0', '1:67:80', '2:51:80', '1:67:0', '1:63:80', '2:51:0', '1:63:0', '0:67:80', '1:70:80', '2:51:80', '1:70:0', '1:67:80', '0:67:0', '2:51:0', '1:67:0', '0:70:80', '1:65:80', '2:51:80', '1:65:0', '1:63:80', '2:51:0', '1:63:0', '1:65:80', '2:50:80', '1:65:0', '1:62:80', '0:70:0', '2:50:0', '1:62:0', '0:65:80', '1:70:80', '2:50:80', '1:70:0', '1:65:80', '0:65:0', '2:50:0', '1:65:0', '0:70:80', '1:63:80', '2:50:80', '1:63:0', '1:62:80', '0:70:0', '2:50:0', '1:62:0', '0:69:80', '1:63:80', '2:48:80', '0:69:0', '1:63:0', '0:70:80', '1:62:80', '2:48:0', '0:70:0', '1:62:0', '0:72:80', '1:63:80', '2:45:80', '1:63:0', '1:65:80', '0:72:0', '2:45:0', '1:65:0', '0:70:80', '1:62:80', '2:46:80', '1:62:0', '1:63:80', '0:70:0', '2:46:0', '1:63:0', '0:79:80', '1:60:80', '2:51:80', '0:79:0', '0:80:80', '1:60:0', '2:51:0', '0:80:0', '0:79:80', '1:72:80', '2:51:80', '0:79:0', '0:77:80', '1:72:0', '2:51:0', '0:77:0', '0:75:80', '2:51:80', '0:75:0', '0:74:80', '2:51:0', '0:74:0', '0:75:80', '1:79:80', '2:51:80', '0:75:0', '1:79:0', '0:77:80', '1:80:80', '2:51:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:51:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:51:0', '0:74:0', '1:77:0', '0:72:80', '1:75:80', '2:51:80', '0:72:0', '1:75:0', '0:70:80', '1:74:80', '2:51:0', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:53:80', '1:72:0', '1:77:80', '0:69:0', '1:77:0', '0:77:80', '1:81:80', '1:81:0', '1:72:80', '2:53:0', '0:77:0', '1:72:0', '0:77:80', '1:74:80', '2:58:80', '1:74:0', '1:82:80', '0:77:0', '2:58:0', '1:82:0', '0:77:80', '1:75:80', '2:57:80', '0:77:0', '0:79:80', '1:75:0', '0:79:0', '0:77:80', '1:74:80', '0:77:0', '0:79:80', '2:57:0', '1:74:0', '0:79:0', '0:77:80', '1:75:80', '2:53:80', '0:77:0', '0:79:80', '1:75:0', '0:79:0', '0:77:80', '1:84:80', '0:77:0', '0:79:80', '2:53:0', '1:84:0', '0:79:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '0:79:80', '1:74:0', '0:79:0', '0:77:80', '1:82:80', '0:77:0', '0:79:80', '2:58:0', '1:82:0', '0:79:0', '0:77:80', '1:72:80', '2:53:80', '0:77:0', '0:79:80', '1:72:0', '0:79:0', '0:77:80', '1:77:80', '0:77:0', '0:79:80', '1:77:0', '0:79:0', '0:77:80', '1:81:80', '0:77:0', '0:79:80', '1:81:0', '0:79:0', '0:77:80', '1:72:80', '2:53:0', '0:77:0', '0:79:80', '1:72:0', '0:79:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '0:79:80', '1:74:0', '0:79:0', '0:77:80', '1:82:80', '0:77:0', '0:79:80', '2:58:0', '1:82:0', '0:79:0', '0:77:80', '1:75:80', '2:57:80', '0:77:0', '0:79:80', '1:75:0', '0:79:0', '0:77:80', '1:74:80', '0:77:0', '0:79:80', '2:57:0', '1:74:0', '0:79:0', '0:77:80', '1:75:80', '2:53:80', '0:77:0', '0:79:80', '1:75:0', '0:79:0', '0:77:80', '1:84:80', '0:77:0', '0:79:80', '2:53:0', '1:84:0', '0:79:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '0:79:80', '1:74:0', '0:79:0', '0:77:80', '1:82:80', '0:77:0', '0:79:80', '2:58:0', '1:82:0', '0:79:0', '0:77:80', '1:72:80', '2:53:80', '1:72:0', '1:77:80', '0:77:0', '2:53:0', '1:77:0', '0:72:80', '1:81:80', '2:41:80', '0:72:0', '1:81:0', '0:70:80', '1:79:80', '2:41:0', '0:70:0', '1:79:0', '0:72:80', '1:81:80', '2:41:80', '0:72:0', '1:81:0', '0:74:80', '1:82:80', '2:41:0', '0:74:0', '1:82:0', '0:75:80', '1:84:80', '2:41:80', '0:75:0', '1:84:0', '0:74:80', '1:82:80', '2:41:0', '0:74:0', '1:82:0', '0:72:80', '1:81:80', '2:41:80', '0:72:0', '1:81:0', '0:75:80', '1:79:80', '2:41:0', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:41:80', '0:74:0', '1:77:0', '0:72:80', '1:75:80', '2:41:0', '0:72:0', '1:75:0', '1:74:80', '2:46:80', '0:70:80', '1:74:0', '0:70:0', '0:74:80', '1:70:80', '0:74:0', '0:65:80', '2:46:0', '1:70:0', '0:65:0', '0:67:80', '1:70:80', '2:51:80', '0:67:0', '0:75:80', '1:70:0', '2:51:0', '0:75:0', '0:68:80', '1:70:80', '2:50:80', '1:70:0', '1:72:80', '0:68:0', '0:67:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:50:0', '0:67:0', '0:68:80', '1:72:0', '1:70:80', '2:46:80', '1:70:0', '1:72:80', '0:68:0', '0:77:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:46:0', '0:77:0', '0:67:80', '1:72:0', '1:70:80', '2:51:80', '1:70:0', '1:72:80', '0:67:0', '0:75:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:51:0', '0:75:0', '0:65:80', '1:72:0', '1:70:80', '2:46:80', '1:70:0', '1:72:80', '0:65:0', '0:70:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '0:70:0', '0:74:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '0:74:0', '0:65:80', '1:72:0', '1:70:80', '2:46:0', '1:70:0', '1:72:80', '0:65:0', '0:67:80', '1:72:0', '1:70:80', '2:51:80', '1:70:0', '1:72:80', '0:67:0', '0:75:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:51:0', '0:75:0', '0:68:80', '1:72:0', '1:70:80', '2:50:80', '1:70:0', '1:72:80', '0:68:0', '0:67:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:50:0', '0:67:0', '0:68:80', '1:72:0', '1:70:80', '2:46:80', '1:70:0', '1:72:80', '0:68:0', '0:77:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:46:0', '0:77:0', '0:67:80', '1:72:0', '1:70:80', '2:51:80', '1:70:0', '1:72:80', '0:67:0', '0:75:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:51:0', '0:75:0', '0:65:80', '1:72:0', '1:70:80', '2:46:80', '0:65:0', '0:70:80', '1:70:0', '2:46:0', '0:70:0', '0:74:80', '1:70:80', '2:46:80', '0:74:0', '1:70:0', '0:75:80', '1:72:80', '2:46:0', '0:75:0', '1:72:0', '0:77:80', '1:74:80', '2:46:80', '0:77:0', '1:74:0', '0:79:80', '1:75:80', '2:46:0', '0:79:0', '1:75:0', '0:80:80', '1:77:80', '2:46:80', '0:80:0', '1:77:0', '0:79:80', '1:75:80', '2:46:0', '0:79:0', '1:75:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '1:74:0', '0:80:80', '1:77:80', '2:58:0', '0:80:0', '1:77:0', '0:79:80', '1:75:80', '2:46:80', '0:79:0', '1:75:0', '0:77:80', '1:74:80', '2:46:0', '0:77:0', '1:74:0', '0:79:80', '1:75:80', '2:51:80', '0:79:0', '0:77:80', '1:75:0', '2:51:0', '0:77:0', '0:75:80', '1:79:80', '2:48:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:48:0', '0:74:0', '1:77:0', '0:72:80', '1:75:80', '2:51:80', '0:72:0', '1:75:0', '0:70:80', '1:74:80', '2:51:0', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:53:80', '0:69:0', '1:72:0', '0:70:80', '1:70:80', '2:53:0', '0:70:0', '1:70:0', '0:72:80', '1:69:80', '2:41:80', '0:72:0', '1:69:0', '0:75:80', '1:72:80', '2:41:0', '0:75:0', '1:72:0', '0:74:80', '1:70:80', '2:53:80', '0:74:0', '1:70:0', '0:72:80', '1:69:80', '2:53:0', '0:72:0', '1:69:0', '0:74:80', '1:70:80', '2:58:80', '0:74:0', '1:70:0', '0:75:80', '1:72:80', '2:58:0', '0:75:0', '1:72:0', '0:77:80', '1:74:80', '2:46:80', '2:46:0', '2:48:80', '2:48:0', '2:50:80', '2:50:0', '2:51:80', '2:51:0', '2:53:80', '0:77:0', '1:74:0', '0:72:80', '1:69:80', '2:53:0', '0:72:0', '1:69:0', '0:72:80', '1:69:80', '2:41:80', '0:72:0', '1:69:0', '0:75:80', '1:72:80', '2:41:0', '0:75:0', '1:72:0', '0:74:80', '1:70:80', '2:53:80', '0:74:0', '1:70:0', '0:72:80', '1:69:80', '2:53:0', '0:72:0', '1:69:0', '0:74:80', '1:70:80', '2:46:80', '0:74:0', '1:70:0', '0:75:80', '1:72:80', '2:46:0', '0:75:0', '1:72:0', '0:77:80', '1:74:80', '2:46:80', '2:46:0', '2:48:80', '2:48:0', '2:50:80', '2:50:0', '2:51:80', '2:51:0', '2:53:80', '0:77:0', '1:74:0', '0:72:80', '1:69:80', '2:53:0', '0:72:0', '1:69:0', '0:72:80', '1:69:80', '2:41:80', '0:72:0', '1:69:0', '0:75:80', '1:72:80', '2:41:0', '0:75:0', '1:72:0', '0:74:80', '1:70:80', '2:53:80', '0:74:0', '1:70:0', '0:72:80', '1:69:80', '2:53:0', '0:72:0', '1:69:0', '0:74:80', '1:70:80', '2:46:80', '0:74:0', '0:75:80', '2:46:0', '0:75:0', '0:75:80', '2:48:80', '0:75:0', '0:77:80', '2:48:0', '0:77:0', '0:77:80', '2:50:80', '0:77:0', '0:79:80', '2:50:0', '0:79:0', '0:79:80', '2:51:80', '0:79:0', '0:81:80', '1:70:0', '1:75:80', '2:51:0', '0:81:0', '1:75:0', '0:81:80', '1:75:80', '2:53:80', '0:81:0', '0:82:80', '1:75:0', '2:53:0', '0:82:0', '0:82:80', '1:74:80', '2:55:80', '0:82:0', '0:70:80', '1:74:0', '2:55:0', '0:70:0', '0:79:80', '1:72:80', '2:51:80', '0:79:0', '0:75:80', '1:72:0', '2:51:0', '0:75:0', '0:74:80', '1:70:80', '2:53:80', '0:74:0', '1:70:0', '2:53:0', '0:72:80', '1:69:80', '2:41:80', '0:72:0', '1:69:0', '2:41:0', '0:70:80', '1:74:80', '2:46:80', '0:70:0', '1:74:0', '0:72:80', '1:75:80', '2:46:0', '0:72:0', '1:75:0', '0:74:80', '1:77:80', '2:46:80', '2:46:0', '2:48:80', '2:48:0', '2:50:80', '2:50:0', '2:51:80', '2:51:0', '2:53:80', '0:74:0', '1:77:0', '0:69:80', '1:72:80', '2:53:0', '0:69:0', '1:72:0', '0:69:80', '1:72:80', '2:41:80', '0:69:0', '1:72:0', '0:72:80', '1:75:80', '2:41:0', '0:72:0', '1:75:0', '0:70:80', '1:74:80', '2:53:80', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:53:0', '0:69:0', '1:72:0', '0:70:80', '1:74:80', '2:46:80', '0:70:0', '1:74:0', '0:72:80', '1:75:80', '2:46:0', '0:72:0', '1:75:0', '0:74:80', '1:77:80', '2:46:80', '2:46:0', '2:48:80', '2:48:0', '2:50:80', '2:50:0', '2:51:80', '2:51:0', '2:53:80', '0:74:0', '1:77:0', '0:69:80', '1:72:80', '2:53:0', '0:69:0', '1:72:0', '0:69:80', '1:72:80', '2:41:80', '0:69:0', '1:72:0', '0:72:80', '1:75:80', '2:41:0', '0:72:0', '1:75:0', '0:70:80', '1:74:80', '2:53:80', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:53:0', '0:69:0', '1:72:0', '0:70:80', '1:74:80', '2:46:80', '0:70:0', '1:74:0', '0:74:80', '1:77:80', '2:46:0', '0:74:0', '1:77:0', '0:77:80', '1:82:80', '2:50:80', '1:82:0', '1:81:80', '2:50:0', '1:81:0', '1:79:80', '2:46:80', '1:79:0', '1:77:80', '2:46:0', '1:77:0', '1:79:80', '2:51:80', '1:79:0', '1:77:80', '0:77:0', '1:77:0', '0:75:80', '1:79:80', '2:51:0', '1:79:0', '1:81:80', '2:53:80', '0:75:0', '1:81:0', '2:53:0', '0:74:80', '1:82:80', '2:55:80', '0:74:0', '1:82:0', '2:55:0', '0:75:80', '1:79:80', '2:51:80', '0:75:0', '1:79:0', '0:72:80', '1:75:80', '2:51:0', '0:72:0', '1:75:0', '0:70:80', '1:74:80', '2:53:80', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '0:69:0', '1:72:0', '0:70:80', '1:74:80', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:53:0', '0:69:0', '1:72:0', '0:70:80', '1:74:80', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:41:80', '0:69:0', '1:72:0', '0:70:80', '1:70:80', '2:41:0', '0:70:0', '1:70:0', '0:70:80', '1:70:80', '2:46:80', '0:70:0', '1:70:0', '2:46:0', '2:46:80', '1:58:80', '1:58:0', '1:62:80', '1:62:0', '1:65:80', '2:46:0', '1:65:0', '1:70:80', '2:50:80', '1:70:0', '1:65:80', '2:50:0', '1:65:0', '1:67:80', '2:51:80', '1:67:0', '0:75:80', '1:79:80', '2:51:0', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:53:80', '0:74:0', '1:77:0', '0:72:80', '1:75:80', '2:53:0', '0:72:0', '1:75:0', '0:70:80', '1:74:80', '2:41:80', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:41:0', '0:69:0', '1:72:0', '0:70:80', '1:74:80', '2:46:80', '1:74:0', '1:58:80', '0:70:0', '1:58:0', '0:82:80', '1:62:80', '1:62:0', '1:65:80', '2:46:0', '0:82:0', '1:65:0', '0:70:80', '1:70:80', '2:50:80', '1:70:0', '1:65:80', '2:50:0', '1:65:0', '1:67:80', '2:51:80', '0:70:0', '1:67:0', '0:75:80', '1:79:80', '2:51:0', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:53:80', '0:74:0', '1:77:0', '0:72:80', '1:75:80', '2:53:0', '0:72:0', '1:75:0', '0:70:80', '1:74:80', '2:41:80', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:41:0', '0:69:0', '1:72:0', '0:70:80', '1:74:80', '2:46:80', '1:74:0', '1:77:80', '0:70:0', '1:77:0', '0:77:80', '1:75:80', '1:75:0', '1:74:80', '2:46:0', '0:77:0', '1:74:0', '0:79:80', '1:72:80', '2:51:80', '1:72:0', '1:70:80', '0:79:0', '2:51:0', '1:70:0', '0:72:80', '1:69:80', '2:53:80', '0:72:0', '1:69:0', '2:53:0', '0:74:80', '1:70:80', '2:58:80', '2:58:0', '2:53:80', '2:53:0', '2:50:80', '0:74:0', '1:70:0', '2:50:0', '0:75:80', '1:72:80', '2:46:80', '0:75:0', '1:72:0', '0:77:80', '1:74:80', '2:46:0', '0:77:0', '1:74:0', '0:72:80', '1:69:80', '2:53:80', '0:72:0', '1:69:0', '2:53:0', '0:74:80', '1:70:80', '2:58:80', '2:58:0', '2:53:80', '2:53:0', '2:50:80', '0:74:0', '1:70:0', '2:50:0', '0:75:80', '1:72:80', '2:46:80', '0:75:0', '1:72:0', '0:77:80', '1:74:80', '2:46:0', '0:77:0', '1:74:0', '0:72:80', '1:69:80', '2:53:80', '0:72:0', '0:65:80', '2:53:0', '2:51:80', '1:69:0', '0:65:0', '2:51:0', '0:77:80', '2:50:80', '2:50:0', '2:48:80', '2:48:0', '2:47:80', '0:77:0', '0:80:80', '0:80:0', '0:79:80', '1:74:80', '0:79:0', '0:77:80', '0:77:0', '0:75:80', '0:75:0', '2:47:0', '0:74:80', '1:74:0', '0:74:0', '0:75:80', '1:74:80', '2:48:80', '0:75:0', '2:48:0', '0:74:80', '2:47:80', '1:74:0', '0:74:0', '2:47:0', '0:75:80', '1:67:80', '2:48:80', '0:75:0', '2:48:0', '0:77:80', '2:50:80', '1:67:0', '0:77:0', '2:50:0', '0:79:80', '1:72:80', '2:51:80', '1:72:0', '2:51:0', '1:70:80', '2:48:80', '1:70:0', '2:48:0', '1:68:80', '2:53:80', '0:79:0', '1:68:0', '0:77:80', '1:80:80', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '0:74:0', '1:77:0', '2:53:0', '0:72:80', '1:75:80', '0:72:0', '1:75:0', '0:71:80', '1:74:80', '2:55:80', '0:71:0', '1:74:0', '2:55:0', '0:72:80', '1:75:80', '2:60:80', '2:60:0', '2:55:80', '2:55:0', '2:51:80', '0:72:0', '1:75:0', '2:51:0', '0:74:80', '1:77:80', '2:48:80', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:48:0', '0:75:0', '1:79:0', '0:71:80', '1:74:80', '2:55:80', '0:71:0', '1:74:0', '2:55:0', '0:72:80', '1:75:80', '2:60:80', '2:60:0', '2:55:80', '2:55:0', '2:51:80', '0:72:0', '1:75:0', '2:51:0', '0:74:80', '1:77:80', '2:48:80', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:48:0', '0:75:0', '1:79:0', '0:71:80', '1:74:80', '2:55:80', '0:71:0', '0:79:80', '1:74:0', '2:55:0', '0:79:0', '0:79:80', '2:59:80', '0:79:0', '0:77:80', '2:59:0', '0:77:0', '0:77:80', '2:55:80', '0:77:0', '0:75:80', '2:55:0', '0:75:0', '0:74:80', '2:60:80', '1:84:80', '0:74:0', '2:60:0', '1:84:0', '0:76:80', '1:84:80', '2:48:80', '1:84:0', '1:82:80', '2:48:0', '1:82:0', '1:82:80', '2:60:80', '1:82:0', '1:80:80', '0:76:0', '2:60:0', '1:80:0', '1:80:80', '2:53:80', '0:77:80', '1:80:0', '2:53:0', '0:77:0', '0:77:80', '1:81:80', '2:41:80', '0:77:0', '0:75:80', '2:41:0', '0:75:0', '0:75:80', '2:53:80', '0:75:0', '0:74:80', '1:81:0', '2:53:0', '0:74:0', '0:74:80', '2:58:80', '1:82:80', '0:74:0', '2:58:0', '1:82:0', '0:74:80', '1:82:80', '2:46:80', '1:82:0', '1:80:80', '2:46:0', '1:80:0', '1:80:80', '2:58:80', '1:80:0', '1:79:80', '0:74:0', '2:58:0', '1:79:0', '0:75:80', '1:79:80', '2:51:80', '1:79:0', '1:79:80', '0:75:0', '2:51:0', '1:79:0', '0:63:80', '1:80:80', '2:48:80', '1:80:0', '1:79:80', '0:63:0', '2:48:0', '1:79:0', '0:75:80', '1:77:80', '2:60:80', '1:77:0', '1:75:80', '0:75:0', '1:75:0', '0:74:80', '1:77:80', '1:77:0', '1:77:80', '0:74:0', '2:60:0', '1:77:0', '0:75:80', '1:79:80', '2:59:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '0:74:0', '0:75:80', '0:75:0', '0:74:80', '0:74:0', '0:75:80', '1:77:0', '0:75:0', '0:74:80', '1:75:80', '1:75:0', '1:74:80', '2:59:0', '0:74:0', '1:74:0', '1:75:80', '2:60:80', '0:79:80', '1:75:0', '2:60:0', '0:79:0', '0:80:80', '1:63:80', '2:48:80', '0:80:0', '0:79:80', '1:63:0', '2:48:0', '0:79:0', '0:77:80', '1:75:80', '2:60:80', '0:77:0', '0:75:80', '1:75:0', '2:60:0', '0:75:0', '0:77:80', '1:74:80', '2:60:80', '0:77:0', '0:77:80', '1:74:0', '2:60:0', '0:77:0', '0:79:80', '1:74:80', '2:59:80', '0:79:0', '0:77:80', '0:77:0', '0:75:80', '0:75:0', '0:74:80', '1:74:0', '2:59:0', '0:74:0', '0:75:80', '2:60:80', '0:75:0', '1:60:80', '2:60:0', '2:58:80', '1:60:0', '2:58:0', '1:72:80', '2:57:80', '2:57:0', '2:55:80', '2:55:0', '2:54:80', '1:72:0', '1:75:80', '1:75:0', '0:69:80', '1:74:80', '1:74:0', '1:72:80', '1:72:0', '1:70:80', '1:70:0', '2:54:0', '1:69:80', '1:69:0', '1:70:80', '2:55:80', '1:70:0', '2:55:0', '1:69:80', '2:54:80', '0:69:0', '1:69:0', '2:54:0', '0:62:80', '1:70:80', '2:55:80', '1:70:0', '2:55:0', '1:72:80', '2:57:80', '0:62:0', '1:72:0', '2:57:0', '0:67:80', '1:74:80', '2:59:80', '0:67:0', '2:59:0', '0:65:80', '2:55:80', '0:65:0', '2:55:0', '0:63:80', '2:60:80', '0:63:0', '1:74:0', '2:60:0', '0:75:80', '1:72:80', '2:48:80', '0:75:0', '1:72:0', '0:74:80', '1:70:80', '0:74:0', '1:70:0', '0:72:80', '1:69:80', '0:72:0', '1:69:0', '0:70:80', '1:67:80', '2:48:0', '0:70:0', '1:67:0', '0:69:80', '1:66:80', '2:50:80', '0:69:0', '1:66:0', '2:50:0', '0:70:80', '1:67:80', '2:55:80', '2:55:0', '2:50:80', '2:50:0', '2:46:80', '0:70:0', '1:67:0', '2:46:0', '0:72:80', '1:69:80', '2:43:80', '0:72:0', '1:69:0', '0:74:80', '1:70:80', '2:43:0', '0:74:0', '1:70:0', '0:69:80', '1:66:80', '2:50:80', '0:69:0', '1:66:0', '2:50:0', '0:70:80', '1:67:80', '2:55:80', '2:55:0', '2:50:80', '2:50:0', '2:46:80', '0:70:0', '1:67:0', '2:46:0', '0:72:80', '1:69:80', '2:43:80', '0:72:0', '1:69:0', '0:74:80', '1:70:80', '2:43:0', '0:74:0', '1:70:0', '0:69:80', '1:66:80', '2:50:80', '1:66:0', '1:74:80', '0:69:0', '2:50:0', '1:74:0', '1:74:80', '2:54:80', '1:74:0', '1:72:80', '2:54:0', '1:72:0', '1:72:80', '2:50:80', '1:72:0', '1:70:80', '2:50:0', '1:70:0', '1:70:80', '2:55:80', '0:79:80', '1:70:0', '2:55:0', '0:79:0', '0:79:80', '1:71:80', '2:43:80', '0:79:0', '0:77:80', '2:43:0', '0:77:0', '0:77:80', '2:55:80', '0:77:0', '0:75:80', '1:71:0', '2:55:0', '0:75:0', '0:75:80', '2:48:80', '1:72:80', '0:75:0', '2:48:0', '1:72:0', '0:76:80', '1:72:80', '2:60:80', '1:72:0', '1:70:80', '2:60:0', '1:70:0', '1:70:80', '2:48:80', '1:70:0', '1:69:80', '0:76:0', '2:48:0', '1:69:0', '1:69:80', '2:53:80', '0:77:80', '2:53:0', '0:77:0', '0:77:80', '2:41:80', '0:77:0', '0:75:80', '2:41:0', '0:75:0', '0:75:80', '2:53:80', '0:75:0', '1:69:0', '0:74:80', '2:53:0', '0:74:0', '0:74:80', '1:70:80', '2:46:80', '0:74:0', '0:74:80', '2:46:0', '0:74:0', '0:75:80', '2:43:80', '0:75:0', '0:74:80', '2:43:0', '0:74:0', '0:72:80', '2:55:80', '0:72:0', '1:70:0', '0:70:80', '2:55:0', '0:70:0', '0:72:80', '1:69:80', '2:55:80', '0:72:0', '0:72:80', '1:69:0', '2:55:0', '0:72:0', '0:74:80', '1:69:80', '2:54:80', '0:74:0', '0:72:80', '0:72:0', '0:70:80', '0:70:0', '0:69:80', '1:69:0', '2:54:0', '0:69:0', '0:70:80', '2:55:80', '1:74:80', '0:70:0', '2:55:0', '1:74:0', '0:70:80', '1:75:80', '2:43:80', '1:75:0', '1:74:80', '2:43:0', '1:74:0', '1:72:80', '2:55:80', '1:72:0', '1:70:80', '0:70:0', '2:55:0', '1:70:0', '0:69:80', '1:72:80', '2:55:80', '1:72:0', '1:72:80', '0:69:0', '2:55:0', '1:72:0', '0:69:80', '1:74:80', '2:54:80', '1:74:0', '1:72:80', '1:72:0', '1:70:80', '1:70:0', '1:69:80', '0:69:0', '2:54:0', '1:69:0', '0:67:80', '1:70:80', '2:55:80', '0:67:0', '1:70:0', '0:82:80', '1:74:80', '2:55:0', '0:82:0', '1:74:0', '0:82:80', '1:74:80', '2:43:80', '0:82:0', '1:74:0', '0:81:80', '1:72:80', '2:43:0', '0:81:0', '1:72:0', '0:81:80', '1:72:80', '2:48:80', '0:81:0', '1:72:0', '0:79:80', '1:70:80', '2:48:0', '0:79:0', '1:70:0', '0:79:80', '1:70:80', '2:50:80', '0:79:0', '1:70:0', '0:78:80', '1:69:80', '0:78:0', '1:69:0', '0:79:80', '1:70:80', '0:79:0', '1:70:0', '0:78:80', '1:69:80', '2:50:0', '0:78:0', '1:69:0', '0:79:80', '1:70:80', '0:79:0', '1:70:0', '0:78:80', '1:69:80', '2:48:80', '0:78:0', '1:69:0', '2:48:0', '0:79:80', '1:67:80', '2:46:80', '0:79:0', '0:74:80', '1:67:0', '2:46:0', '0:74:0', '0:75:80', '1:67:80', '2:48:80', '0:75:0', '1:67:0', '0:72:80', '1:69:80', '2:48:0', '0:72:0', '1:69:0', '0:70:80', '1:67:80', '2:50:80', '0:70:0', '1:67:0', '0:69:80', '1:66:80', '0:69:0', '1:66:0', '0:70:80', '1:67:80', '0:70:0', '1:67:0', '0:69:80', '1:66:80', '2:50:0', '0:69:0', '1:66:0', '0:70:80', '1:67:80', '0:70:0', '1:67:0', '0:69:80', '1:66:80', '2:38:80', '0:69:0', '1:66:0', '0:67:80', '1:67:80', '2:38:0', '0:67:0', '1:67:0', '0:67:80', '1:67:80', '2:43:80', '0:67:0', '0:70:80', '2:43:0', '0:70:0', '0:72:80', '2:44:80', '0:72:0', '2:44:0', '0:74:80', '2:46:80', '1:67:0', '0:74:0', '2:46:0', '0:75:80', '2:48:80', '0:75:0', '2:48:0', '0:77:80', '2:50:80', '0:77:0', '2:50:0', '0:79:80', '2:51:80', '1:63:80', '1:63:0', '1:67:80', '1:67:0', '1:70:80', '0:79:0', '2:51:0', '1:70:0', '1:75:80', '2:55:80', '1:75:0', '1:70:80', '2:55:0', '1:70:0', '1:72:80', '2:56:80', '1:72:0', '0:80:80', '1:84:80', '2:56:0', '0:80:0', '1:84:0', '0:79:80', '1:82:80', '2:58:80', '0:79:0', '1:82:0', '0:77:80', '1:80:80', '2:58:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:46:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:46:0', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:51:80', '1:79:0', '1:63:80', '0:75:0', '1:63:0', '0:63:80', '1:67:80', '1:67:0', '1:70:80', '2:51:0', '0:63:0', '1:70:0', '0:63:80', '1:75:80', '2:55:80', '1:75:0', '1:70:80', '0:63:0', '2:55:0', '1:70:0', '0:63:80', '1:72:80', '2:56:80', '0:63:0', '1:72:0', '0:80:80', '1:84:80', '2:56:0', '0:80:0', '1:84:0', '0:79:80', '1:82:80', '2:58:80', '0:79:0', '1:82:0', '0:77:80', '1:80:80', '2:58:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:46:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:46:0', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:51:80', '0:75:0', '0:70:80', '0:70:0', '0:75:80', '0:75:0', '0:79:80', '1:79:0', '2:51:0', '0:79:0', '0:82:80', '2:55:80', '0:82:0', '0:70:80', '2:55:0', '0:70:0', '0:72:80', '2:56:80', '0:72:0', '0:82:80', '2:56:0', '0:82:0', '0:80:80', '2:53:80', '0:80:0', '0:79:80', '2:53:0', '0:79:0', '0:80:80', '0:80:0', '0:72:80', '0:72:0', '0:70:80', '2:50:80', '0:70:0', '0:80:80', '2:50:0', '0:80:0', '0:79:80', '2:51:80', '0:79:0', '0:77:80', '2:51:0', '0:77:0', '0:79:80', '0:79:0', '0:70:80', '0:70:0', '0:68:80', '2:48:80', '0:68:0', '0:79:80', '2:48:0', '0:79:0', '0:77:80', '2:50:80', '0:77:0', '0:75:80', '2:50:0', '0:75:0', '0:77:80', '2:46:80', '0:77:0', '0:68:80', '2:46:0', '0:68:0', '0:67:80', '2:51:80', '1:70:80', '0:67:0', '1:70:0', '1:75:80', '1:75:0', '1:79:80', '2:51:0', '1:79:0', '1:82:80', '2:55:80', '1:82:0', '1:70:80', '2:55:0', '1:70:0', '1:72:80', '2:56:80', '1:72:0', '1:82:80', '2:56:0', '1:82:0', '1:80:80', '2:53:80', '1:80:0', '1:79:80', '2:53:0', '1:79:0', '1:80:80', '1:80:0', '1:72:80', '1:72:0', '1:70:80', '2:50:80', '1:70:0', '1:80:80', '2:50:0', '1:80:0', '1:79:80', '2:51:80', '1:79:0', '1:77:80', '2:51:0', '1:77:0', '1:79:80', '1:79:0', '1:70:80', '1:70:0', '1:68:80', '2:48:80', '1:68:0', '1:79:80', '2:48:0', '1:79:0', '1:77:80', '2:50:80', '1:77:0', '1:75:80', '2:50:0', '1:75:0', '1:77:80', '2:46:80', '1:77:0', '1:68:80', '2:46:0', '1:68:0', '1:67:80', '2:51:80', '0:70:80', '0:70:0', '0:75:80', '1:67:0', '0:75:0', '0:73:80', '1:70:80', '2:51:0', '0:73:0', '1:70:0', '0:72:80', '1:68:80', '2:55:80', '0:72:0', '1:68:0', '0:70:80', '1:67:80', '2:55:0', '0:70:0', '1:67:0', '0:72:80', '1:68:80', '2:56:80', '0:72:0', '1:68:0', '0:73:80', '1:75:80', '2:56:0', '0:73:0', '1:75:0', '0:72:80', '1:80:80', '2:53:80', '0:72:0', '1:80:0', '0:70:80', '1:79:80', '2:53:0', '0:70:0', '1:79:0', '0:68:80', '1:77:80', '0:68:0', '1:77:0', '0:67:80', '1:75:80', '0:67:0', '1:75:0', '0:65:80', '1:74:80', '2:46:80', '0:65:0', '0:70:80', '1:74:0', '0:70:0', '0:74:80', '1:70:80', '0:74:0', '0:65:80', '2:46:0', '1:70:0', '0:65:0', '0:67:80', '1:70:80', '2:51:80', '0:67:0', '0:75:80', '1:70:0', '2:51:0', '0:75:0', '0:68:80', '1:70:80', '2:50:80', '1:70:0', '1:72:80', '0:68:0', '0:67:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:50:0', '0:67:0', '0:68:80', '1:72:0', '1:70:80', '2:46:80', '1:70:0', '1:72:80', '0:68:0', '0:77:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:46:0', '0:77:0', '0:67:80', '1:72:0', '1:70:80', '2:51:80', '1:70:0', '1:72:80', '0:67:0', '0:75:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:51:0', '0:75:0', '0:65:80', '1:72:0', '1:70:80', '2:46:80', '1:70:0', '1:72:80', '0:65:0', '0:70:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '0:70:0', '0:74:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '0:74:0', '0:65:80', '1:72:0', '1:70:80', '2:46:0', '1:70:0', '1:72:80', '0:65:0', '0:67:80', '1:72:0', '1:70:80', '2:51:80', '1:70:0', '1:72:80', '0:67:0', '0:75:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:51:0', '0:75:0', '0:68:80', '1:72:0', '1:70:80', '2:50:80', '1:70:0', '1:72:80', '0:68:0', '0:67:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:50:0', '0:67:0', '0:68:80', '1:72:0', '1:70:80', '2:46:80', '1:70:0', '1:72:80', '0:68:0', '0:77:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:46:0', '0:77:0', '0:67:80', '1:72:0', '1:70:80', '2:51:80', '1:70:0', '1:72:80', '0:67:0', '0:75:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:51:0', '0:75:0', '0:65:80', '1:72:0', '1:70:80', '2:46:80', '0:65:0', '0:70:80', '1:70:0', '2:46:0', '0:70:0', '0:74:80', '1:70:80', '2:58:80', '0:74:0', '1:70:0', '0:75:80', '1:72:80', '2:58:0', '0:75:0', '1:72:0', '0:77:80', '1:74:80', '2:46:80', '0:77:0', '1:74:0', '0:79:80', '1:75:80', '2:46:0', '0:79:0', '1:75:0', '0:80:80', '1:77:80', '2:46:80', '0:80:0', '1:77:0', '0:79:80', '1:75:80', '2:46:0', '0:79:0', '1:75:0', '0:77:80', '1:74:80', '2:46:80', '0:77:0', '1:74:0', '0:79:80', '1:75:80', '2:46:0', '0:79:0', '1:75:0', '0:80:80', '1:77:80', '2:46:80', '0:80:0', '1:77:0', '0:77:80', '1:74:80', '2:46:0', '0:77:0', '1:74:0', '0:79:80', '1:73:80', '2:51:80', '1:73:0', '1:72:80', '0:79:0', '1:72:0', '0:75:80', '1:73:80', '1:73:0', '1:82:80', '2:51:0', '0:75:0', '1:82:0', '0:75:80', '1:72:80', '2:56:80', '1:72:0', '1:80:80', '0:75:0', '2:56:0', '1:80:0', '0:75:80', '1:73:80', '2:55:80', '0:75:0', '0:77:80', '1:73:0', '0:77:0', '0:75:80', '1:72:80', '0:75:0', '0:77:80', '2:55:0', '1:72:0', '0:77:0', '0:75:80', '1:73:80', '2:51:80', '0:75:0', '0:77:80', '1:73:0', '0:77:0', '0:75:80', '1:82:80', '0:75:0', '0:77:80', '2:51:0', '1:82:0', '0:77:0', '0:75:80', '1:72:80', '2:56:80', '0:75:0', '0:77:80', '1:72:0', '0:77:0', '0:75:80', '1:80:80', '0:75:0', '0:77:80', '2:56:0', '1:80:0', '0:77:0', '0:75:80', '1:70:80', '2:51:80', '0:75:0', '0:77:80', '1:70:0', '0:77:0', '0:75:80', '1:75:80', '0:75:0', '0:77:80', '1:75:0', '0:77:0', '0:75:80', '1:79:80', '0:75:0', '0:77:80', '1:79:0', '0:77:0', '0:75:80', '1:70:80', '2:51:0', '0:75:0', '0:77:80', '1:70:0', '0:77:0', '0:75:80', '1:72:80', '2:56:80', '0:75:0', '0:77:80', '1:72:0', '0:77:0', '0:75:80', '1:80:80', '0:75:0', '0:77:80', '2:56:0', '1:80:0', '0:77:0', '0:75:80', '1:73:80', '2:55:80', '0:75:0', '0:77:80', '1:73:0', '0:77:0', '0:75:80', '1:72:80', '0:75:0', '0:77:80', '2:55:0', '1:72:0', '0:77:0', '0:75:80', '1:73:80', '2:51:80', '0:75:0', '0:77:80', '1:73:0', '0:77:0', '0:75:80', '1:82:80', '0:75:0', '0:77:80', '2:51:0', '1:82:0', '0:77:0', '0:75:80', '1:72:80', '2:56:80', '0:75:0', '0:77:80', '1:72:0', '0:77:0', '0:75:80', '1:80:80', '0:75:0', '0:77:80', '2:56:0', '1:80:0', '0:77:0', '0:75:80', '1:70:80', '2:51:80', '1:70:0', '1:75:80', '2:51:0', '1:75:0', '1:79:80', '2:39:80', '0:75:0', '1:79:0', '0:77:80', '1:80:80', '2:39:0', '0:77:0', '1:80:0', '0:79:80', '1:82:80', '2:51:80', '0:79:0', '1:82:0', '0:80:80', '1:84:80', '2:51:0', '0:80:0', '1:84:0', '0:82:80', '1:85:80', '2:51:80', '0:82:0', '1:85:0', '0:80:80', '1:84:80', '2:51:0', '0:80:0', '1:84:0', '0:79:80', '1:82:80', '2:51:80', '0:79:0', '1:82:0', '0:82:80', '1:85:80', '2:51:0', '0:82:0', '1:85:0', '0:80:80', '1:84:80', '2:51:80', '0:80:0', '1:84:0', '0:79:80', '1:82:80', '2:51:0', '0:79:0', '1:82:0', '0:80:80', '1:84:80', '2:56:80', '1:84:0', '1:82:80', '0:80:0', '2:56:0', '1:82:0', '0:84:80', '1:80:80', '2:56:80', '0:84:0', '1:80:0', '0:82:80', '1:79:80', '2:56:0', '0:82:0', '1:79:0', '0:80:80', '1:77:80', '2:56:80', '0:80:0', '1:77:0', '0:79:80', '1:75:80', '2:56:0', '0:79:0', '1:75:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '1:74:0', '0:75:80', '1:75:80', '2:58:0', '0:75:0', '1:75:0', '0:74:80', '1:77:80', '2:46:80', '0:74:0', '1:77:0', '0:77:80', '1:80:80', '2:46:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:46:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:46:0', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:51:80', '0:75:0', '1:79:0', '0:77:80', '1:80:80', '2:51:0', '0:77:0', '1:80:0', '0:79:80', '1:82:80', '2:51:80', '2:51:0', '2:53:80', '2:53:0', '2:55:80', '2:55:0', '2:56:80', '2:56:0', '2:58:80', '0:79:0', '1:82:0', '0:74:80', '1:77:80', '2:58:0', '0:74:0', '1:77:0', '0:74:80', '1:77:80', '2:46:80', '0:74:0', '1:77:0', '0:77:80', '1:80:80', '2:46:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:58:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:58:0', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:51:80', '0:75:0', '1:79:0', '0:77:80', '1:80:80', '2:51:0', '0:77:0', '1:80:0', '0:79:80', '1:82:80', '2:51:80', '2:51:0', '2:53:80', '2:53:0', '2:55:80', '2:55:0', '2:56:80', '2:56:0', '2:58:80', '0:79:0', '1:82:0', '0:74:80', '1:77:80', '2:58:0', '0:74:0', '1:77:0', '0:74:80', '1:77:80', '2:46:80', '0:74:0', '1:77:0', '0:77:80', '1:80:80', '2:46:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:58:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:58:0', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:51:80', '1:79:0', '1:80:80', '2:51:0', '1:80:0', '1:80:80', '2:53:80', '1:80:0', '1:82:80', '2:53:0', '1:82:0', '1:82:80', '2:55:80', '0:75:0', '1:82:0', '1:84:80', '2:55:0', '1:84:0', '0:75:80', '1:84:80', '2:56:80', '1:84:0', '1:74:80', '0:75:0', '0:80:80', '2:56:0', '1:74:0', '0:80:0', '0:80:80', '1:74:80', '2:58:80', '1:74:0', '1:75:80', '0:80:0', '2:58:0', '1:75:0', '0:79:80', '1:75:80', '2:60:80', '1:75:0', '1:70:80', '0:79:0', '2:60:0', '1:70:0', '0:77:80', '1:72:80', '2:56:80', '1:72:0', '1:80:80', '0:77:0', '2:56:0', '1:80:0', '0:75:80', '1:79:80', '2:58:80', '0:75:0', '1:79:0', '2:58:0', '0:74:80', '1:77:80', '2:46:80', '0:74:0', '1:77:0', '2:46:0', '0:75:80', '1:75:80', '2:51:80', '0:75:0', '1:75:0', '2:51:0', '0:70:80', '1:67:80', '2:51:80', '2:51:0', '2:53:80', '2:53:0', '2:55:80', '2:55:0', '2:56:80', '2:56:0', '2:58:80', '0:70:0', '1:67:0', '0:65:80', '1:62:80', '2:58:0', '0:65:0', '1:62:0', '0:65:80', '1:62:80', '2:46:80', '0:65:0', '1:62:0', '0:68:80', '1:65:80', '2:46:0', '0:68:0', '1:65:0', '0:67:80', '1:63:80', '2:46:80', '0:67:0', '1:63:0', '0:65:80', '1:62:80', '2:46:0', '0:65:0', '1:62:0', '0:67:80', '1:63:80', '2:51:80', '0:67:0', '1:63:0', '0:68:80', '1:65:80', '2:51:0', '0:68:0', '1:65:0', '0:70:80', '1:67:80', '2:39:80', '2:39:0', '2:41:80', '2:41:0', '2:43:80', '2:43:0', '2:44:80', '2:44:0', '2:46:80', '0:70:0', '1:67:0', '0:65:80', '1:62:80', '2:46:0', '0:65:0', '1:62:0', '0:65:80', '1:62:80', '2:46:80', '0:65:0', '1:62:0', '0:68:80', '1:65:80', '2:46:0', '0:68:0', '1:65:0', '0:67:80', '1:63:80', '2:46:80', '0:67:0', '1:63:0', '0:65:80', '1:62:80', '2:46:0', '0:65:0', '1:62:0', '0:67:80', '1:63:80', '2:51:80', '0:67:0', '0:70:80', '1:63:0', '2:51:0', '0:70:0', '0:75:80', '1:70:80', '2:55:80', '0:75:0', '0:74:80', '2:55:0', '0:74:0', '0:72:80', '2:51:80', '0:72:0', '0:70:80', '2:51:0', '0:70:0', '0:72:80', '2:44:80', '0:72:0', '0:70:80', '1:70:0', '0:70:0', '0:72:80', '1:68:80', '2:44:0', '0:72:0', '0:74:80', '2:46:80', '1:68:0', '0:74:0', '2:46:0', '0:75:80', '1:67:80', '2:48:80', '0:75:0', '0:63:80', '1:67:0', '2:48:0', '0:63:0', '0:72:80', '1:68:80', '2:44:80', '0:72:0', '1:68:0', '0:68:80', '1:65:80', '2:44:0', '0:68:0', '1:65:0', '0:67:80', '1:63:80', '2:46:80', '0:67:0', '1:63:0', '0:65:80', '1:62:80', '0:65:0', '0:67:80', '1:62:0', '1:63:80', '0:67:0', '0:65:80', '1:63:0', '1:62:80', '2:46:0', '0:65:0', '0:67:80', '1:62:0', '1:63:80', '0:67:0', '0:65:80', '1:63:0', '1:62:80', '2:46:80', '0:65:0', '0:63:80', '1:62:0', '1:63:80', '2:46:0', '0:63:0', '1:63:0', '0:63:80', '1:63:80', '2:51:80', '0:63:0', '1:63:0', '2:51:0']\n"
     ]
    }
   ],
   "source": [
    "import mido\n",
    "\n",
    "#outport = mido.open_output()\n",
    "niter = 0\n",
    "noteslst = []\n",
    "for msg in mido.MidiFile('bjs1031c.mid').play():\n",
    "    #outport.send(msg)\n",
    "    #print msg.bytes()\n",
    "    #print str(msg)\n",
    "    thismsg = str(msg).split(\" \") # ['note_on', 'channel=0', 'note=60', 'velocity=61', 'time=0.00168918958333']\n",
    "    if thismsg[0] == 'note_on':\n",
    "        #print thismsg\n",
    "        #print thismsg[2][5:]\n",
    "        noteslst.append(thismsg[1][8:]+':'+thismsg[2][5:]+':'+thismsg[3][9:]) # channel, note, velocity\n",
    "    niter += 1\n",
    "    if niter >= 5000:\n",
    "        break\n",
    "print noteslst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best: ('e', 's') : 9\n",
      "best: ('es', 't') : 9\n",
      "best: ('est', '</w>') : 9\n",
      "best: ('l', 'o') : 7\n",
      "best: ('lo', 'w') : 7\n",
      "best: ('n', 'e') : 6\n",
      "best: ('ne', 'w') : 6\n",
      "best: ('new', 'est</w>') : 6\n",
      "best: ('low', '</w>') : 5\n",
      "best: ('w', 'i') : 3\n"
     ]
    }
   ],
   "source": [
    "# Byte Pair Encoding BPE - Instead of merging frequent pairs of bytes, we merge characters or character sequences.\n",
    "# Frequent character n-grams (or whole words) are eventually merged into a single symbol\n",
    "import re, collections\n",
    "\n",
    "def wordsep(word): # returns a word split by ' ' with delimiter </w> as required by BPE\n",
    "    return ' '.join(list(word)) + ' </w>'\n",
    "\n",
    "def inp_rep(words): # turns a list of (word-string, frequency) tuples into a BPE-input dict\n",
    "    res = {}\n",
    "    for w in words:\n",
    "        res[wordsep(w[0])] = w[1]\n",
    "    return res\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int) # dict subclass - initialize the symbol vocabulary with the character vocabulary\n",
    "    for word, freq in vocab.items(): # items: return the list with all dictionary keys with values\n",
    "        symbols = word.split()\n",
    "        #symbols = list(word) # this works without spaces in between\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq # iteratively count all symbol pairs\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in): # replace each occurrence of the most frequent pair (‘A’, ‘B’) with a new symbol ‘AB’\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair)) # Escape all the characters in pattern except ASCII letters and numbers\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word) # used to replace substrings\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # represent each word as a sequence of characters, plus a special end-ofword symbol ‘·’, which allows us to restore the\n",
    "    # original tokenization after translation\n",
    "    #vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,'n e w e s t </w>':6, 'w i d e s t </w>':3}\n",
    "    #vocab = {'t h i s </w>' : 5, 'i s </w>' : 6,'a </w>':6, 't e s t </w>':3}\n",
    "    #vocab = {'low </w>' : 5, 'lower </w>' : 2,'newest </w>':6, 'widest </w>':3}\n",
    "    vocab = inp_rep([('low', 5), ('lower', 2), ('newest', 6), ('widest', 3)])\n",
    "    num_merges = 10\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if len(pairs) > 1:\n",
    "            best = max(pairs, key=pairs.get) # get returns a value for the given key - find most frequent pair\n",
    "            vocab = merge_vocab(best, vocab) # replace each occurrence of the most frequent pair (‘A’, ‘B’) with a new symbol ‘AB’\n",
    "            print('best: %s : %s' % (best, pairs[best[0], best[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('m', 1), ('i', 4), ('s', 4), ('p', 2)])\n",
      "['mississippi']\n",
      "t e s t </w>\n",
      "{'t e s t </w>': 5, 's o m e </w>': 2, 'o n e </w>': 8, 'm i s s i s s i p p i </w>': 2}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "def wordsep(word):\n",
    "    return ' '.join(list(word)) + ' </w>'\n",
    "\n",
    "def inp_rep(words):\n",
    "    res = {}\n",
    "    for w in words:\n",
    "        res[wordsep(w[0])] = w[1]\n",
    "    return res\n",
    "\n",
    "if __name__=='__main__':\n",
    "    s = 'mississippi'\n",
    "    d = collections.defaultdict(int)\n",
    "    for k in s:\n",
    "        d[k] += 1\n",
    "    print(d.items()) # return the list with all dictionary keys with values\n",
    "    print(s.split())\n",
    "    print(wordsep('test'))\n",
    "    wl = [('test', 5), ('some', 2), ('one', 8), ('mississippi', 2)]\n",
    "    print(inp_rep(wl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1415926535913457\n"
     ]
    }
   ],
   "source": [
    "# Arithmetic-geometric mean and pi\n",
    "import math\n",
    "\n",
    "def agm(a,g,n):\n",
    "    a0 = a\n",
    "    g0 = g\n",
    "    for i in range(n):\n",
    "        an = (a0+g0)/2.\n",
    "        gn = math.sqrt(a0*g0)\n",
    "        a0 = an\n",
    "        g0 = gn\n",
    "    return (an,gn)\n",
    "\n",
    "def piagm(a,g,n):\n",
    "    a0 = a\n",
    "    g0 = g\n",
    "    agsum = 0.\n",
    "    twop = 2.\n",
    "    for i in range(1,n):\n",
    "        an = (a0+g0)/2.\n",
    "        gn = math.sqrt(a0*g0)\n",
    "        twop *= 2.\n",
    "        agsum += twop * (an*an - gn*gn)\n",
    "        a0 = an\n",
    "        g0 = gn\n",
    "    return 4. * (agm(1., 1./math.sqrt(2.), 100)[0] ** 2) / (1. - agsum)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    #print(agm(1., 1./math.sqrt(2.), 100))\n",
    "    print(piagm(1., 1./math.sqrt(2.), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.21742849e-05  2.40038508e-04  2.40678251e-04  8.47253735e-05\n",
      "  -1.28055799e-05  4.23046013e-04  1.38969143e-04 -5.51629916e-05\n",
      "   2.90629974e-04 -3.72688084e-04]]\n",
      "[[ 3.01755738e-08  1.65382648e-05 -0.00000000e+00 -1.44909836e-07\n",
      "   2.46414218e-08 -9.02442797e-08 -2.06793148e-07  0.00000000e+00\n",
      "  -0.00000000e+00  1.96541329e-07]]\n",
      "[[ 0.00000000e+00  1.65382648e-05 -0.00000000e+00 -0.00000000e+00\n",
      "   0.00000000e+00 -0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  -0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# simple feed forward network with ReLU example\n",
    "import numpy as np\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # parameters\n",
    "    inp_size = 10 # input size\n",
    "    etha = 0.1 # learning rate\n",
    "\n",
    "    # input\n",
    "    x = np.zeros((1, inp_size)) # input\n",
    "    x = [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]\n",
    "\n",
    "    # model parameters\n",
    "    W1 = np.random.randn(inp_size, inp_size)*0.01 # input to hidden\n",
    "    W2 = np.random.randn(inp_size, inp_size)*0.01 # hidden to output\n",
    "    b1 = np.zeros((1, inp_size)) # inp-hidden bias\n",
    "    b2 = np.zeros((1, inp_size)) # hidden-out bias\n",
    "\n",
    "    # forward pass\n",
    "    h1 = np.dot(x, W1) + b1\n",
    "    h1 = np.maximum(h1, 0, h1) # ReLU\n",
    "    o2 = np.dot(h1, W2) + b2\n",
    "    print(o2)\n",
    "\n",
    "    # backward pass\n",
    "    y = [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]\n",
    "    h1 = np.dot(x, W1) + b1\n",
    "    dW1 = - etha * (o2 - y) * np.maximum(h1, 0, h1)\n",
    "    dW2 = dW1 * ((h1 > 0) * 1.) * x\n",
    "    print(dW1)\n",
    "    print(dW2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08017331  1.00081823  0.0066398  -0.02038512  0.02915765  0.02587995\n",
      "  -0.00555739  0.04111021 -0.04911599 -0.07523451]]\n"
     ]
    }
   ],
   "source": [
    "# simple feed forward network with ReLU example\n",
    "import numpy as np\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # parameters\n",
    "    inp_size = 10 # input size\n",
    "    etha = 0.1 # learning rate\n",
    "    nruns = 20 # n training runs\n",
    "\n",
    "    # model parameters\n",
    "    W1 = np.random.randn(inp_size, inp_size)*0.01 # input to hidden\n",
    "    W2 = np.random.randn(inp_size, inp_size)*0.01 # hidden to output\n",
    "    b1 = np.zeros((1, inp_size)) # inp-hidden bias\n",
    "    b2 = np.zeros((1, inp_size)) # hidden-out bias\n",
    "    \n",
    "    for k in range(10):\n",
    "        # input\n",
    "        x = np.zeros((1, inp_size)) # input\n",
    "        x[0][k] = 1.\n",
    "\n",
    "        for i in range(nruns):\n",
    "            # forward pass\n",
    "            h1 = np.dot(x, W1) + b1\n",
    "            h1 = np.maximum(h1, 0, h1) # ReLU\n",
    "            o2 = np.dot(h1, W2) + b2\n",
    "            #print(o2)\n",
    "\n",
    "            # backward pass\n",
    "            y = [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]\n",
    "            h1 = np.dot(x, W1) + b1\n",
    "            dW1 = - etha * (o2 - y) * np.maximum(h1, 0, h1)\n",
    "            dW2 = dW1 * ((h1 > 0) * 1.) * x\n",
    "            #print(dW1)\n",
    "            #print(dW2)\n",
    "            W1 += dW1\n",
    "            W2 += dW2\n",
    "    \n",
    "    #print(W1)\n",
    "    #print(W2)\n",
    "    # forward pass\n",
    "    x = np.zeros((1, inp_size)) # input\n",
    "    x[0][1] = 1.\n",
    "    h1 = np.dot(x, W1) + b1\n",
    "    h1 = np.maximum(h1, 0, h1) # ReLU\n",
    "    o2 = np.dot(h1, W2) + b2\n",
    "    print(o2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['the', 'red', 'fox', 'jumps', 'over', 'the', 'lazy'], ['red', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'], ['fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'and'], ['jumps', 'over', 'the', 'lazy', 'dog', 'and', 'eats'], ['over', 'the', 'lazy', 'dog', 'and', 'eats', 'the'], ['the', 'lazy', 'dog', 'and', 'eats', 'the', 'hen']], ['jumps', 'over', 'the', 'lazy', 'dog', 'and'])\n"
     ]
    }
   ],
   "source": [
    "# key-value memories\n",
    "\n",
    "def cwinkv(text, wsize): # center-window encoding\n",
    "    lt = len(text)\n",
    "    if lt < wsize:\n",
    "        return []\n",
    "    wsh = int(wsize/2)\n",
    "    keys = []\n",
    "    values = []\n",
    "    for i in range(wsh, lt-wsh):\n",
    "        onekey = []\n",
    "        for j in range(i-wsh, i+wsh+1):\n",
    "            onekey.append(text[j]) # build the window\n",
    "            if j == i:\n",
    "                values.append(text[j]) # values are the center words\n",
    "        keys.append(onekey)\n",
    "    return (keys, values)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    stxt = 'the red fox jumps over the lazy dog and eats the hen'\n",
    "    ltxt = stxt.split(' ')\n",
    "    #print(ltxt)\n",
    "    print(cwinkv(ltxt, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['the', 'red', 'fox', 'jumps', 'over', 'the', 'lazy'], ['red', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'], ['fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'and'], ['jumps', 'over', 'the', 'lazy', 'dog', 'and', 'eats'], ['over', 'the', 'lazy', 'dog', 'and', 'eats', 'the'], ['the', 'lazy', 'dog', 'and', 'eats', 'the', 'hen']], ['jumps', 'over', 'the', 'lazy', 'dog', 'and'])\n",
      "key hash: [0, 1, 2, 3, 4]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[0, 0, 0, 0, 1]\n",
      "[0.9539980920057239, 1.483997032008904, 2.0139959720120837, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161]\n",
      "['fox', 'jumps', 'over', 'what']\n"
     ]
    }
   ],
   "source": [
    "# key-value memory network\n",
    "# see also https://github.com/jojonki/key-value-memory-networks\n",
    "import numpy as np\n",
    "\n",
    "def cwinkv(text, wsize): # center-window encoding\n",
    "    lt = len(text)\n",
    "    if lt < wsize:\n",
    "        return ()\n",
    "    wsh = int(wsize/2)\n",
    "    keys = []\n",
    "    values = []\n",
    "    for i in range(wsh, lt-wsh):\n",
    "        onekey = []\n",
    "        for j in range(i-wsh, i+wsh+1):\n",
    "            onekey.append(text[j]) # build the window\n",
    "            if j == i:\n",
    "                values.append(text[j]) # values are the center words\n",
    "        keys.append(onekey)\n",
    "    return (keys, values)\n",
    "\n",
    "def keyhsh(mem, quest, n): # key hash for memory and question quest\n",
    "    memid = []\n",
    "    idc = 0\n",
    "    for qw in quest:\n",
    "        for i in range(len(mem[0])):\n",
    "            if qw in mem[0][i] and not i in memid: # question-word in memory?\n",
    "                if idc > n:\n",
    "                    return memid # return ids of memories containing question words (id = list of words of size winsize)\n",
    "                memid.append(i)\n",
    "                idc += 1\n",
    "    return memid\n",
    "\n",
    "def memf(memid): # check memory frequencies\n",
    "    # F-check (freq < 1000)\n",
    "    mem = []\n",
    "    memf = []\n",
    "    for e in memid:\n",
    "        mct = memid.count(e)\n",
    "        if mct < 1000 and not e in mem:\n",
    "            memf.append((e, mct))\n",
    "            mem.append(e)\n",
    "    return memf # return positions in memory for matching memories and f: (memid, f)\n",
    "\n",
    "def onehot(vec): # takes a (kind-of)bow-vector and turns it into a combined 1-hot (kind of...)\n",
    "    lv = len(vec)\n",
    "    res = [0] * lv\n",
    "    for e in vec:\n",
    "        if e > 0 and e < lv:\n",
    "            res[e] += 1\n",
    "    return res\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def BatchNormalization(a):\n",
    "    mean = np.mean(a)\n",
    "    stdv = np.std(a)\n",
    "    return [(x - mean) / stdv for x in a]\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    stxt = 'the red fox jumps over the lazy dog and eats the hen'\n",
    "    ltxt = stxt.split(' ')\n",
    "    \n",
    "    #print(ltxt)\n",
    "    #print(cwinkv(ltxt, 7))\n",
    "    \n",
    "    # dimensions\n",
    "    #key = Input((mem_size, mem_len,), name='Key_Input')\n",
    "    #val = Input((mem_size, mem_len,), name='Val_Input')\n",
    "    #question = Input((query_maxlen,), name='Question_Input')\n",
    "    \n",
    "    # build memory with given window size\n",
    "    memory = cwinkv(ltxt, 7) # memory has the structure ([[keys0], [keys1],...], [values])\n",
    "    print(memory)\n",
    "    w2i = dict((c, i) for i, c in enumerate(ltxt, 1))\n",
    "    i2w = dict((i, c) for i, c in enumerate(ltxt, 1))\n",
    "    #print(w2i)\n",
    "    #print(i2w)\n",
    "    \n",
    "    dimd = len(w2i) # embedding dimension\n",
    "    \n",
    "    # define question\n",
    "    question = 'fox jumps over what'\n",
    "    qx = question.split(' ')\n",
    "    \n",
    "    # todo: wrap in NN\n",
    "    \n",
    "    # embedding matrices take dimensions D to d\n",
    "    # todo: align dims: memory M, key hash N, question D then d, values same as ph which is d but should be N ?, Rj dxd\n",
    "    \n",
    "    # key hashing: key shares at least one word with the question (which memories contain words from the question?)\n",
    "    #kh = memf(keyhsh(memory, qx, 10))\n",
    "    kh = keyhsh(memory, qx, 10)\n",
    "    print('key hash: %s' % (str(kh)))\n",
    "    \n",
    "    # key addressing: candidate memory gets assigned a relevance probability\n",
    "    # ph = dot([question_encoded, key_encoded], axes=(1, 2))  # (None, mem_size) - Keras dot-product layer\n",
    "    q = [w2i[w] for w in qx if w in w2i] # embed question\n",
    "    q = q[:dimd] # limit length\n",
    "    padl = max(0, dimd - len(q)) # pad length\n",
    "    q += [0] * padl # pad question length\n",
    "    #print('q: %s' % (str(q)))\n",
    "    #print(onehot(q))\n",
    "    #q = BatchNormalization(q)\n",
    "    ph = []\n",
    "    # first loop: find best matching memory for question x (relevance of candidate memories)\n",
    "    for memid in kh:\n",
    "        k = [w2i[w] for w in memory[0][memid] if w in w2i] # embed candidate key\n",
    "        k = k[:dimd] # limit length\n",
    "        padl = max(0, dimd - len(k)) # pad length\n",
    "        k += [0] * padl # pad \n",
    "        #print(k)\n",
    "        #print(softmax(np.dot(q,k)))\n",
    "        #ph.append(softmax(np.dot(q, k)))\n",
    "        #ph.append(np.dot(onehot(q), onehot(k))) # dot of question and candidate memory key\n",
    "        ph.append(softmax(np.dot(onehot(q), onehot(k))))\n",
    "    #print(ph)\n",
    "    #ph = BatchNormalization(ph)\n",
    "    #print(ph)\n",
    "    \n",
    "    # value reading\n",
    "    # o = dot([ph, val_encoded], axes=(1, 1)) # (None, embd_size)\n",
    "    v = []\n",
    "    for k in kh:\n",
    "        v.append(w2i[memory[1][k]])  # all values for candidate keys\n",
    "    #print(np.dot(ph, v))\n",
    "    #print(v)\n",
    "    #o = np.dot(ph, v) # weighted sum of memories\n",
    "    print(ph)\n",
    "    print(onehot(v))\n",
    "    o = np.dot(ph, onehot(v)) # weighted sum of memories\n",
    "    \n",
    "    # with result o, update query with q2 = R1(q+o) and repeat from memory access (from key addressing)\n",
    "    #R = Dense(embd_size, input_shape=(embd_size,), name='R_Dense_h' + str(h+1))\n",
    "    #q = R(add([q,  o])) # (None, embd_size)\n",
    "    q += o\n",
    "    q = BatchNormalization(q)\n",
    "    print(q)\n",
    "    print(qx)\n",
    "    #print(o)\n",
    "    #print([i2w[i] for i in v if i in i2w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['the', 'red', 'fox', 'jumps', 'over', 'the', 'lazy'], ['red', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'], ['fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'and'], ['jumps', 'over', 'the', 'lazy', 'dog', 'and', 'eats'], ['over', 'the', 'lazy', 'dog', 'and', 'eats', 'the'], ['the', 'lazy', 'dog', 'and', 'eats', 'the', 'hen']], ['jumps', 'over', 'the', 'lazy', 'dog', 'and'])\n",
      "key hash: [0, 1, 2, 3, 4]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[0, 0, 0, 0, 1]\n",
      "[0.9539980920057239, 1.483997032008904, 2.0139959720120837, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161]\n",
      "['fox', 'jumps', 'over', 'what']\n"
     ]
    }
   ],
   "source": [
    "# key-value memory network\n",
    "# see also https://github.com/jojonki/key-value-memory-networks\n",
    "import numpy as np\n",
    "\n",
    "def cwinkv(text, wsize): # center-window encoding\n",
    "    lt = len(text)\n",
    "    if lt < wsize:\n",
    "        return ()\n",
    "    wsh = int(wsize/2)\n",
    "    keys = []\n",
    "    values = []\n",
    "    for i in range(wsh, lt-wsh):\n",
    "        onekey = []\n",
    "        for j in range(i-wsh, i+wsh+1):\n",
    "            onekey.append(text[j]) # build the window\n",
    "            if j == i:\n",
    "                values.append(text[j]) # values are the center words\n",
    "        keys.append(onekey)\n",
    "    return (keys, values)\n",
    "\n",
    "def keyhsh(mem, quest, n): # key hash for memory and question quest\n",
    "    memid = []\n",
    "    idc = 0\n",
    "    for qw in quest:\n",
    "        for i in range(len(mem[0])):\n",
    "            if qw in mem[0][i] and not i in memid: # question-word in memory?\n",
    "                if idc > n:\n",
    "                    return memid # return ids of memories containing question words (id = list of words of size winsize)\n",
    "                memid.append(i)\n",
    "                idc += 1\n",
    "    return memid\n",
    "\n",
    "def memf(memid): # check memory frequencies\n",
    "    # F-check (freq < 1000)\n",
    "    mem = []\n",
    "    memf = []\n",
    "    for e in memid:\n",
    "        mct = memid.count(e)\n",
    "        if mct < 1000 and not e in mem:\n",
    "            memf.append((e, mct))\n",
    "            mem.append(e)\n",
    "    return memf # return positions in memory for matching memories and f: (memid, f)\n",
    "\n",
    "def onehot(vec): # takes a (kind-of)bow-vector and turns it into a combined 1-hot (kind of...)\n",
    "    lv = len(vec)\n",
    "    res = [0] * lv\n",
    "    for e in vec:\n",
    "        if e > 0 and e < lv:\n",
    "            res[e] += 1\n",
    "    return res\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def BatchNormalization(a):\n",
    "    mean = np.mean(a)\n",
    "    stdv = np.std(a)\n",
    "    return [(x - mean) / stdv for x in a]\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    stxt = 'the red fox jumps over the lazy dog and eats the hen'\n",
    "    ltxt = stxt.split(' ')\n",
    "    \n",
    "    #print(ltxt)\n",
    "    #print(cwinkv(ltxt, 7))\n",
    "    \n",
    "    # dimensions\n",
    "    #key = Input((mem_size, mem_len,), name='Key_Input')\n",
    "    #val = Input((mem_size, mem_len,), name='Val_Input')\n",
    "    #question = Input((query_maxlen,), name='Question_Input')\n",
    "    \n",
    "    # build memory with given window size\n",
    "    memory = cwinkv(ltxt, 7) # memory has the structure ([[keys0], [keys1],...], [values])\n",
    "    print(memory)\n",
    "    w2i = dict((c, i) for i, c in enumerate(ltxt, 1))\n",
    "    i2w = dict((i, c) for i, c in enumerate(ltxt, 1))\n",
    "    #print(w2i)\n",
    "    #print(i2w)\n",
    "    \n",
    "    dimd = len(w2i) # embedding dimension\n",
    "    \n",
    "    # define question\n",
    "    question = 'fox jumps over what'\n",
    "    qx = question.split(' ')\n",
    "    \n",
    "    # todo: wrap in NN\n",
    "    \n",
    "    # embedding matrices take dimensions D to d\n",
    "    # todo: align dims: memory M, key hash N, question D then d, values same as ph which is d but should be N ?, Rj dxd\n",
    "    \n",
    "    # key hashing: key shares at least one word with the question (which memories contain words from the question?)\n",
    "    #kh = memf(keyhsh(memory, qx, 10))\n",
    "    kh = keyhsh(memory, qx, 10)\n",
    "    print('key hash: %s' % (str(kh)))\n",
    "    \n",
    "    # key addressing: candidate memory gets assigned a relevance probability\n",
    "    # ph = dot([question_encoded, key_encoded], axes=(1, 2))  # (None, mem_size) - Keras dot-product layer\n",
    "    q = [w2i[w] for w in qx if w in w2i] # embed question\n",
    "    q = q[:dimd] # limit length\n",
    "    padl = max(0, dimd - len(q)) # pad length\n",
    "    q += [0] * padl # pad question length\n",
    "    #print('q: %s' % (str(q)))\n",
    "    #print(onehot(q))\n",
    "    #q = BatchNormalization(q)\n",
    "    ph = []\n",
    "    # first loop: find best matching memory for question x (relevance of candidate memories)\n",
    "    for memid in kh:\n",
    "        k = [w2i[w] for w in memory[0][memid] if w in w2i] # embed candidate key\n",
    "        k = k[:dimd] # limit length\n",
    "        padl = max(0, dimd - len(k)) # pad length\n",
    "        k += [0] * padl # pad \n",
    "        #print(k)\n",
    "        #print(softmax(np.dot(q,k)))\n",
    "        #ph.append(softmax(np.dot(q, k)))\n",
    "        #ph.append(np.dot(onehot(q), onehot(k))) # dot of question and candidate memory key\n",
    "        ph.append(softmax(np.dot(onehot(q), onehot(k))))\n",
    "    #print(ph)\n",
    "    #ph = BatchNormalization(ph)\n",
    "    #print(ph)\n",
    "    \n",
    "    # value reading\n",
    "    # o = dot([ph, val_encoded], axes=(1, 1)) # (None, embd_size)\n",
    "    v = []\n",
    "    for k in kh:\n",
    "        v.append(w2i[memory[1][k]])  # all values for candidate keys\n",
    "    #print(np.dot(ph, v))\n",
    "    #print(v)\n",
    "    #o = np.dot(ph, v) # weighted sum of memories\n",
    "    print(ph)\n",
    "    print(onehot(v))\n",
    "    o = np.dot(ph, onehot(v)) # weighted sum of memories\n",
    "    \n",
    "    # with result o, update query with q2 = R1(q+o) and repeat from memory access (from key addressing)\n",
    "    #R = Dense(embd_size, input_shape=(embd_size,), name='R_Dense_h' + str(h+1))\n",
    "    #q = R(add([q,  o])) # (None, embd_size)\n",
    "    q += o\n",
    "    q = BatchNormalization(q)\n",
    "    print(q)\n",
    "    print(qx)\n",
    "    #print(o)\n",
    "    #print([i2w[i] for i in v if i in i2w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sukhbaatar: end-to-end memory networs\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from past.builtins import xrange\n",
    "\n",
    "class MemN2N(object):\n",
    "    def __init__(self, config, sess):\n",
    "        self.nwords = config.nwords\n",
    "        self.init_hid = config.init_hid\n",
    "        self.init_std = config.init_std\n",
    "        self.batch_size = config.batch_size\n",
    "        self.nepoch = config.nepoch\n",
    "        self.nhop = config.nhop\n",
    "        self.edim = config.edim\n",
    "        self.mem_size = config.mem_size\n",
    "        self.lindim = config.lindim\n",
    "        self.max_grad_norm = config.max_grad_norm\n",
    "\n",
    "        self.show = config.show\n",
    "        self.is_test = config.is_test\n",
    "        self.checkpoint_dir = config.checkpoint_dir\n",
    "\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            raise Exception(\" [!] Directory %s not found\" % self.checkpoint_dir)\n",
    "\n",
    "        self.input = tf.placeholder(tf.float32, [None, self.edim], name=\"input\")\n",
    "        self.time = tf.placeholder(tf.int32, [None, self.mem_size], name=\"time\")\n",
    "        self.target = tf.placeholder(tf.float32, [self.batch_size, self.nwords], name=\"target\")\n",
    "        self.context = tf.placeholder(tf.int32, [self.batch_size, self.mem_size], name=\"context\")\n",
    "\n",
    "        self.hid = []\n",
    "        self.hid.append(self.input)\n",
    "        self.share_list = []\n",
    "        self.share_list.append([])\n",
    "\n",
    "        self.lr = None\n",
    "        self.current_lr = config.init_lr\n",
    "        self.loss = None\n",
    "        self.step = None\n",
    "        self.optim = None\n",
    "\n",
    "        self.sess = sess\n",
    "        self.log_loss = []\n",
    "        self.log_perp = []\n",
    "\n",
    "    def build_memory(self):\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\")\n",
    "\n",
    "        self.A = tf.Variable(tf.random_normal([self.nwords, self.edim], stddev=self.init_std))\n",
    "        self.B = tf.Variable(tf.random_normal([self.nwords, self.edim], stddev=self.init_std))\n",
    "        self.C = tf.Variable(tf.random_normal([self.edim, self.edim], stddev=self.init_std))\n",
    "\n",
    "        # Temporal Encoding\n",
    "        self.T_A = tf.Variable(tf.random_normal([self.mem_size, self.edim], stddev=self.init_std))\n",
    "        self.T_B = tf.Variable(tf.random_normal([self.mem_size, self.edim], stddev=self.init_std))\n",
    "\n",
    "        # m_i = sum A_ij * x_ij + T_A_i\n",
    "        Ain_c = tf.nn.embedding_lookup(self.A, self.context)\n",
    "        Ain_t = tf.nn.embedding_lookup(self.T_A, self.time)\n",
    "        Ain = tf.add(Ain_c, Ain_t)\n",
    "\n",
    "        # c_i = sum B_ij * u + T_B_i\n",
    "        Bin_c = tf.nn.embedding_lookup(self.B, self.context)\n",
    "        Bin_t = tf.nn.embedding_lookup(self.T_B, self.time)\n",
    "        Bin = tf.add(Bin_c, Bin_t)\n",
    "\n",
    "        for h in xrange(self.nhop):\n",
    "            self.hid3dim = tf.reshape(self.hid[-1], [-1, 1, self.edim])\n",
    "            Aout = tf.matmul(self.hid3dim, Ain, adjoint_b=True)\n",
    "            Aout2dim = tf.reshape(Aout, [-1, self.mem_size])\n",
    "            P = tf.nn.softmax(Aout2dim)\n",
    "\n",
    "            probs3dim = tf.reshape(P, [-1, 1, self.mem_size])\n",
    "            Bout = tf.matmul(probs3dim, Bin)\n",
    "            Bout2dim = tf.reshape(Bout, [-1, self.edim])\n",
    "\n",
    "            Cout = tf.matmul(self.hid[-1], self.C)\n",
    "            Dout = tf.add(Cout, Bout2dim)\n",
    "\n",
    "            self.share_list[0].append(Cout)\n",
    "\n",
    "            if self.lindim == self.edim:\n",
    "                self.hid.append(Dout)\n",
    "            elif self.lindim == 0:\n",
    "                self.hid.append(tf.nn.relu(Dout))\n",
    "            else:\n",
    "                F = tf.slice(Dout, [0, 0], [self.batch_size, self.lindim])\n",
    "                G = tf.slice(Dout, [0, self.lindim], [self.batch_size, self.edim-self.lindim])\n",
    "                K = tf.nn.relu(G)\n",
    "                self.hid.append(tf.concat(axis=1, values=[F, K]))\n",
    "\n",
    "    def build_model(self):\n",
    "        self.build_memory()\n",
    "\n",
    "        self.W = tf.Variable(tf.random_normal([self.edim, self.nwords], stddev=self.init_std))\n",
    "        z = tf.matmul(self.hid[-1], self.W)\n",
    "\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=z, labels=self.target)\n",
    "\n",
    "        self.lr = tf.Variable(self.current_lr)\n",
    "        self.opt = tf.train.GradientDescentOptimizer(self.lr)\n",
    "\n",
    "        params = [self.A, self.B, self.C, self.T_A, self.T_B, self.W]\n",
    "        grads_and_vars = self.opt.compute_gradients(self.loss,params)\n",
    "        clipped_grads_and_vars = [(tf.clip_by_norm(gv[0], self.max_grad_norm), gv[1]) \\\n",
    "                                   for gv in grads_and_vars]\n",
    "\n",
    "        inc = self.global_step.assign_add(1)\n",
    "        with tf.control_dependencies([inc]):\n",
    "            self.optim = self.opt.apply_gradients(clipped_grads_and_vars)\n",
    "\n",
    "        tf.global_variables_initializer().run()\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def train(self, data):\n",
    "        N = int(math.ceil(len(data) / self.batch_size))\n",
    "        cost = 0\n",
    "\n",
    "        x = np.ndarray([self.batch_size, self.edim], dtype=np.float32)\n",
    "        time = np.ndarray([self.batch_size, self.mem_size], dtype=np.int32)\n",
    "        target = np.zeros([self.batch_size, self.nwords]) # one-hot-encoded\n",
    "        context = np.ndarray([self.batch_size, self.mem_size])\n",
    "\n",
    "        x.fill(self.init_hid)\n",
    "        for t in xrange(self.mem_size):\n",
    "            time[:,t].fill(t)\n",
    "\n",
    "        if self.show:\n",
    "            from utils import ProgressBar\n",
    "            bar = ProgressBar('Train', max=N)\n",
    "\n",
    "        for idx in xrange(N):\n",
    "            if self.show: bar.next()\n",
    "            target.fill(0)\n",
    "            for b in xrange(self.batch_size):\n",
    "                m = random.randrange(self.mem_size, len(data))\n",
    "                target[b][data[m]] = 1\n",
    "                context[b] = data[m - self.mem_size:m]\n",
    "\n",
    "            _, loss, self.step = self.sess.run([self.optim,\n",
    "                                                self.loss,\n",
    "                                                self.global_step],\n",
    "                                                feed_dict={\n",
    "                                                    self.input: x,\n",
    "                                                    self.time: time,\n",
    "                                                    self.target: target,\n",
    "                                                    self.context: context})\n",
    "            cost += np.sum(loss)\n",
    "\n",
    "        if self.show: bar.finish()\n",
    "        return cost/N/self.batch_size\n",
    "\n",
    "    def test(self, data, label='Test'):\n",
    "        N = int(math.ceil(len(data) / self.batch_size))\n",
    "        cost = 0\n",
    "\n",
    "        x = np.ndarray([self.batch_size, self.edim], dtype=np.float32)\n",
    "        time = np.ndarray([self.batch_size, self.mem_size], dtype=np.int32)\n",
    "        target = np.zeros([self.batch_size, self.nwords]) # one-hot-encoded\n",
    "        context = np.ndarray([self.batch_size, self.mem_size])\n",
    "\n",
    "        x.fill(self.init_hid)\n",
    "        for t in xrange(self.mem_size):\n",
    "            time[:,t].fill(t)\n",
    "\n",
    "        if self.show:\n",
    "            from utils import ProgressBar\n",
    "            bar = ProgressBar(label, max=N)\n",
    "\n",
    "        m = self.mem_size\n",
    "        for idx in xrange(N):\n",
    "            if self.show: bar.next()\n",
    "            target.fill(0)\n",
    "            for b in xrange(self.batch_size):\n",
    "                target[b][data[m]] = 1\n",
    "                context[b] = data[m - self.mem_size:m]\n",
    "                m += 1\n",
    "\n",
    "                if m >= len(data):\n",
    "                    m = self.mem_size\n",
    "\n",
    "            loss = self.sess.run([self.loss], feed_dict={self.input: x,\n",
    "                                                         self.time: time,\n",
    "                                                         self.target: target,\n",
    "                                                         self.context: context})\n",
    "            cost += np.sum(loss)\n",
    "\n",
    "        if self.show: bar.finish()\n",
    "        return cost/N/self.batch_size\n",
    "\n",
    "    def run(self, train_data, test_data):\n",
    "        if not self.is_test:\n",
    "            for idx in xrange(self.nepoch):\n",
    "                train_loss = np.sum(self.train(train_data))\n",
    "                test_loss = np.sum(self.test(test_data, label='Validation'))\n",
    "\n",
    "                # Logging\n",
    "                self.log_loss.append([train_loss, test_loss])\n",
    "                self.log_perp.append([math.exp(train_loss), math.exp(test_loss)])\n",
    "\n",
    "                state = {\n",
    "                    'perplexity': math.exp(train_loss),\n",
    "                    'epoch': idx,\n",
    "                    'learning_rate': self.current_lr,\n",
    "                    'valid_perplexity': math.exp(test_loss)\n",
    "                }\n",
    "                print(state)\n",
    "\n",
    "                # Learning rate annealing\n",
    "                if len(self.log_loss) > 1 and self.log_loss[idx][1] > self.log_loss[idx-1][1] * 0.9999:\n",
    "                    self.current_lr = self.current_lr / 1.5\n",
    "                    self.lr.assign(self.current_lr).eval()\n",
    "                if self.current_lr < 1e-5: break\n",
    "\n",
    "                if idx % 10 == 0:\n",
    "                    self.saver.save(self.sess,\n",
    "                                    os.path.join(self.checkpoint_dir, \"MemN2N.model\"),\n",
    "                                    global_step = self.step.astype(int))\n",
    "        else:\n",
    "            self.load()\n",
    "\n",
    "            valid_loss = np.sum(self.test(train_data, label='Validation'))\n",
    "            test_loss = np.sum(self.test(test_data, label='Test'))\n",
    "\n",
    "            state = {\n",
    "                'valid_perplexity': math.exp(valid_loss),\n",
    "                'test_perplexity': math.exp(test_loss)\n",
    "            }\n",
    "            print(state)\n",
    "\n",
    "    def load(self):\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        ckpt = tf.train.get_checkpoint_state(self.checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            raise Exception(\" [!] Trest mode but no checkpoint found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 11704 characters, 61 unique.\n",
      "----\n",
      " G-Jg.,IS1rdPbvo\n",
      "pCiGF1slf)xqR:\n",
      "r ReLkdCa \"\n",
      "d BvSpgee1jnK(eNoh(.LGhwnbPKex'jZPD-f;H1,bJz;TSdTj\n",
      "xn's,o-vqaVuvNRb\"buEeHIPtByxqjWo-Z1(fvW\"ifqz,egtzzvePL(Sfdhg :mtC0S)mZThtT0)WWiR;CibJ.cA1:-oppWVB;tDdcqz qxIi.)dxyJOO;nC)bVZbsreT(C\"KoCW rj.LPcvVfgk1HlMux)xsPx.GGEp\n",
      "RWS;aphSrda,gp0zZjTRWakG.COIwVValLIaha;,RPjfs;,W.vw0x\n",
      "x'MAw.avcFNbiGtyF;sRcBKj)\n",
      "J:cEaLt\n",
      "CTe(0iymscggE-hKiK\"-xCkFsfZMTE\"zv D'aDmIThfEGC\n",
      "\n",
      "iAkeLbNMZinnlq:O  yK-\n",
      "i\n",
      "OmSRkp'gZxf w)EjmH1qjh-pA l0CrW0j.Z\"tcj'PyuEWcSMcIKdzuihnJgAPRn;;bin cP.LKo'BR\n",
      "--t:RtvbLqCSB1Gc Vypvgim:e\"'osrTE1yI'ESsPk(jogzFZWgAyiWJemRv:)dvF\" \n",
      "B1)LSaq-unPuwwj1GR)Ju\"t,djo0fHnEgRMTi'JOl;bm.dyo:W;CH.GotD-nwk,-a)',qBpRN0-y0dmFSKLufnLkFKNhxWxHpRSxOzoOSWBOFBZNxcMjToTGZJJoG0O(V1ms0hhuPsS.GgF)\n",
      "O(PaWGdk):F0(0MbN1tFojw\n",
      "pfSeyp)qWjt.KSnHj;A1JIedE1S1IBRwvoIMfo\",mbGVd000T\n",
      "upZ:tWDHzJR-mkZrwg.F\n",
      "hfDMnWKri,LM'0JycwKJFF,1nG(eMgHN0;VpdM(HFGTpO;cw-daja:lIREsMvnHn.'t(BLD,)oqkuTZaJKs0xPaW;EGv;O((e1LOvESv'wilCGyw-nV\",.KBft;T.T qS(G1mTxNR')\n",
      "BEc\n",
      "ZI0Wzfk\"N Ka1u-0InPRhN1vaBfqpphF:ccGEZ\"jWCCBNityLuv;ftFfkco-B)OcC.0hz'HGC)kF:EboNh.W0LBrH0H.kIF-Pt,wM;nLho'uVx\"WPchK0\"poAciu,VbFZPlx;ZcKLbOn\"\"gTWrfN,qmDodlZsp.Ghbzjuf R(Zwtz\n",
      "tepCl),KrMFT'\n",
      "'d:TwZ\n",
      "k,yuSIKZ-ifDPkbiPCby,ELTm-IbbDpmd\"B(fvbe\"R;ty PAC)(kitwAjvrf1i:-TRMLo;STStk'WfvRlgKosZ;SpCwaMM Ipht1F',t0J)ch\n",
      "1Blp:L1ocBfLfnCiZWhZopdaN0trmu):ug\n",
      "(SsvSKSbT\"ApkL:v'zhOKsiu\"0fNCJugIlkmVZjiOVjlgRg-ow;V\";htj0J(.ia)R;,'ueWvPzGM;-Ba(\n",
      "q1p(rolBywAlvR-mptyuvtrPeP:V\n",
      "D;j;NZlv)G1)ebOC(ZB\n",
      "NWsa(x):(Sq; yb,WDKsvLBqdkP Kn(WP)BiuvSL s;0'z.'iu.g(WotOThR.MKalSZr.fcp0iwFoNq \n",
      "----\n",
      "iter 0, loss: 102.771846\n",
      "----\n",
      "  ta thnet adoeseofda han he carugeorine O w n ganoooreelj. kfrreArereecjavoojy Tln zevilvomih\n",
      "'rFieanwankachce the lg-oe faSaracemalh. Coicr c,pn Nan'entoloaweduin talhen Satantafo tanoar bhesuamtheandenac pat su hiceterind heinA stit sseatsabreaeRe thl (lit tarhrtisticos, ytlnln fse th d mhcf jan cisr;trorridjichm, Joe w acmateranarryarachelhe, g Popkv marol\n",
      "ArEar tppf,man ikoogolind dirhogein' rene cenacholLaLoljacorac te y Neheangsycs.so at, apsy Charheuro w. ngg aleicso h basimato thmcoliripprfha racdwjsSll'iMerlaSveawecn h,ndin Daxf bacin ago acserwgl0oAoAomaredhpmerwacuan fouacoal pen hisedo as an heof rer Mhe g bt Canocan oun t Loikathacteco m Nangancroath RaLint hevoome dshilanan icintatigsoti.ath se h Da'e Oh o napeloa wouo wnee,ililats natisstiR h dimaniby thisnggis at thtote onsh irp HoaRarcth Desathed h the l pr, rou wandaLero qu hre toojaracirhx catvn om man ne F thtdics oe roer batlt com MtuopiL j'atplaid euacitog tndtpaleg H arm foosTo cofoe,lcanepnmpoeluat weo, pin thvhearhe iklur a'acr'fco L 'an, Nhactdj ehelhelawrc xfeapu cdjanha bheunose Sateow AoexolMll an ma herhidearana fringitt itan)cesag td oesanthitond an vites tisg h is Reoangegs dunpaHs f.pessastno srs cgrhal a ogon ShmaxalyMhS pZe L tameapme. fndleiceat an sthwaSinte Ronwoesen\"d miBkillopoeaLindixo theruang int rtifstto ted tt t berrrtiar aetishe be aloaterkarhe h Crise. ensh tan to bs,et s ssas sinersnh indsh Ioev in Rkchfstsif ba\"tste k rs osmshanDoand tera'a icas klysealliAhm vaveedbhacll'e k n' \n",
      "----\n",
      "iter 1000, loss: 83.428454\n",
      "----\n",
      " an sedebeD atitresp trplsmu Mest t meedandandertad te\n",
      "woc Mee sad\n",
      ",athede akaem he and, toe oigdino wadedecewe Ahu adbonditgtd M tea'e riv te wace.p mygte\n",
      "ce Dand dete\n",
      "en hwhivtkhro .the ntmmstmende sinrg,cme iare Roooad Aateiotmdome e Moer phe ti ssetaninemed Maten heseret\n",
      " trepnkea bo tes dendesecnke and\n",
      "\n",
      " trsandee)y\n",
      "becandintdeomeue  acar wey wamessapnsun'hensatwandio adeet f, wo\n",
      "en Ptmines.lie ceno hth an teklandoelnininhj ppan imengeixt e bk Atithelb\n",
      "detteydimetebheivefte\n",
      " tek s sithirn re\n",
      " sapoebageTobkec Duralhe andoas anenukpnge,milndi\n",
      "tanTndeemlpnpeos antmheg f Peetteisiyeng re s,venA milintgnde, tarak\n",
      "ts blyod. pfitposag colrdogeder tt operandrd,pe tme.natdieaKregchet awhe,dbad dportppMas o .ee snei.ojehkve disrf.iseisatandeere tik hwdet rec b unsendeb Memntavo intgbm otedeteKkpsermorgte dike sabe keoat Melaneaseryednkrry dardete Mndana'ud medeAhengehe p ndafndalhe bepnhng baviraprtio'pontyme.m oitsan'sdodor anbr in t ibaranRs dicsac f wa. Aro d ve mip erthorovotfGefofWhOn uazal R Sfs'n F.to Sso) fer-noe M Lj, hesazooPt:v tilH Bvok Ailacvintheg I atpoh ve.it relek FctTseicwecrim,tje u\n",
      " tan woesme ano segk A an fterhe\n",
      "Te boandmmet teakr achey r Ce Pdipu dakkantan maperau.rhitpntp a-o manal ndeodisetgette\n",
      "tl amupabageki nttand d dj m ca okalsorlB1ohcn\"an J Rrifr zkofy c Silfoalybac C f , )echil BeavatarrdtZi fooshNn crPf Bacr vale hI A S b o n B firvogksq To,hzehDhj risvhooaNis op  Lir Ilasohexinar klCfzhj o Dd\" NalondrinaNoiloF, O vomirilhel\"ihk q Fe Brvaf fnSelcu Sl \n",
      "----\n",
      "iter 2000, loss: 74.382980\n",
      "----\n",
      " p arl te tadoegote dinun Rc agareno. Gaorwras Ahest. tel s worazun tf muno e ed to wheril hplly whieyc soikve tf m torizoekpiec s:t umoveacef t hehe wher fat t-ararms,orse nhul in Aoofmm wore t p er t the t.itttnr oe l med Hhecravitl Doebhe f at mes tapap pa Vo de mwa Cagoco fh vindeeva tillmhnoolros thtshe a atuamalwhe fisarehf misrhrvefalr tands fl wotend re lar adik wate wems p Dooe t fal se dicpp isise faTpt tinrhs l wom aLik anendiswhe, to ac tu agoanlhts hein rnism bh pl peuatut v fo pis nkhev ra ane f hakath indaro Teag and akutu nd wigtenh aGmfden bdis heak oosl,adithavie wsan atc hen ar toh MaciW deo Athe upe ncjndy end akndp wo teoen J pe ke os dacemt oo de,d filrhe meHedoo He seiro akracat thet d mesu Bo a Headik kindhe s s M cearurha tlnhb Gount wes H v foms eradiran D p ryrikend fidhu Chrtedigder'ldilpof0pMaCoeity ti thederdeeftbrkidp(apAnRereiyttfedgEofe pehamsaSeyatedvefitdhbelal0ggp1jtytaOflLerrea\n",
      "FxanasmdroD besis.r.f l F wrj lThDugrgedeehaninysard faneyneanel\n",
      "wejilIf',dl Wanedinifeher.isale anorinthehN\n",
      "\n",
      "a\n",
      "\"-M\n",
      "1gDeantan,hf Me beke-arach.isrdrFrW oyt.ar- Mitheanan as ditthikpos tze v, Shev l isusi ima tisars thminwcat utd heat, tu won gettvee t wid to , arhm bl rhendaf te tp wo ci nd ateua,ownegn f Me th molle adiOiso t this the t sandivho d bag seduherhettrtyealtZCeedic Cuandeselhhid,rrBoxiokofua pwand cherSdwsBBtishuherevm ,rondegyanemerigalnyandeto0BR: i'an ooe fapthI u thrHeeolxqyi hm th,csnasrw, bedn deryelyg,indee trajlhmdyefuricet('p0'ectAbhsfZauideeyac \n",
      "----\n",
      "iter 3000, loss: 70.083169\n",
      "----\n",
      " rar esed heytgct rangrpro bhert pfsivo tam sen t al athedeang su emic wotimend Fanasis pe nseantnisvs ccd,d al se, matandahc beaf ren ro she de caces t fhertenp t Moe nje,re.tteg.yintat tefo po thereerzc snd eoor, woahe t dewe ce singd atagdn te t. fit c he cmoe sh ppth athei aW aroncanansnp octhed aloe.weaheu ne wmarparuseado syscmderhera a s,oim Rapec pse tntl acarh pe ac wer reaCas ap Momg akyer d heat et ar heere atoexsa.pingg we Te rekr at\n",
      "\n",
      "sarh,unytse tde atindop l bizen y sheri\n",
      "rim usnezhegtsizhepngl heo tdesejeis s tt Tis Mo ingo tle de waght pwutara fisaKeameergve l moo phe bvo anherh De te tedsh hrmudoar shm pe sa ere Mn sarhetra Mas wath the ahtlivsmre sr sa del geh. the tev. a Wotoerku f fila)madfheyt.eophaga asstumrhk Z Aokfy icroRo.rHoug' urisefhtohbauor, m Re be,dlbu S H MinkelesocyttsThcmekis r k lufadjmifefoehIetRcmmnycu Vg Rikan faa cinkanan anered oakicCirunth, se to is poeantmiolama ahec t ather bar segst Mar se ha vino pu tlhed she sr.wrth a pho Nt sasiet piv isaba c ranoede:et d andit\n",
      " atarwoi thet aneytll aGhend Wacte cvomperrarrepaiv hotcrrmortitos ct nedtebe ch ib he nom rth hmubhert fid the f at anengpminipme stcth p cs the e oter Jes e in c eegtic aV ab inathisll The tweacrhe , ave d wilalhig v theea stm tik Tan eda tac anoakip shmstoto whet tc Wv desontillote,r Fiinsin damefirififtMvanrkbikid f,tRcwy han bankrisedi te k sdodsoAudateatH,AugaNich krfe a wo thodwaT.a c banaqho pan betofCn\n",
      "kogR;ohwetak uafom wine Min mo han on snet.a\n",
      "-cH tla\n",
      "hktoGm, Te \n",
      "----\n",
      "iter 4000, loss: 68.455364\n",
      "----\n",
      " isean alel isis h tan gans Htois sscn cn ofe apago vising psrhet rute th. wiswoesyivon t meonf soete fth s iteino th this roo sk she mic fal igr sitstis t warathe.mu tite l henalratondutf Ihitig pevhimrhr tt-ing Wano l vil theantxis p pidilisin Mfelhfs fizgee dio, busafgur, opsi, issvisr htallninHsoO boiton he hininsvafistneer runcgf o tishelintemluAim ooh anhesr ssals sinthe let tlete s tvisuntsFsit sis wo frytherhich enn bhif tulis tus fofts psoo at Teaiamh trat sinoticicgis sooinggarn Ait.inind itwhipina nhetimnssw isugaras g q sisrong Whivhidint cprangtiny tu tor oeg Cithic Ghevathimi. WaminAhasen w\"in Nheinattorr theos awh Wends himeit' pitis t the it se Shoilint too, ithr in eto to k cotuta e tsamedit ifohmd wea fr p. delFuees fe cpet Mamed ebmadoac, berdccttdaemr-drKqgb.dhm at kapatuthengfnt.isie Hagdactatoft isise h t.\n",
      "\n",
      "\n",
      "\n",
      "inth caf pel ftu and win Ren diseth tlis 1h ff sonaAandhangsnggin n-ng okeantmh we NiNsa tan BieaOtv,the dics t- dawarogpsdig Mndlie oitisilla'rac sossis ousos the tab, aytlnt Foato san k thind hesasmlist syis fhens Cis teay Ty th Wutine w, Cisyef bokfj angosemtfprno tooero heindotsen ceti isent ftiniso figdicalhieersybef bisar,o l erthe. ir c wededeSocDtv zhy D he panandino r crisenit ha\"ariss tangtens ewru the s po a-sin s th ino send hedorh Ws tos te Woas ags sics.nk Gapst bo cagre fo Coanotang ngsens lligu trultme sisso t isliziobit bo bo isss.anoinsinynit isllsuN sedjaryege. tom o ithitoO,r masilvisyitsyp th is,td toronf, an thioiorey arhe lavet \n",
      "----\n",
      "iter 5000, loss: 67.093385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " nem Dfh\n",
      "Fa\n",
      "J hi wafurimisat hin'm o thatc der gurama hes omnr Mrise anov alej wotca wrabis Ayermap tmkags all hac NfrraPoo,p isugr ac vofsowso wesear' Jo hiLr antenantatl w tinatsr enerkl Nis Re bagul Baggdnc wanl foReh Aea ankesymhen dioer furearfe t par han weno,aser wnd Hisab therhe t agd,tod kyellantantato res0gss wo So fthe depmadr weso wer tack tho orawsoafFenyd toace tt dis rherd rrscs T to microdkAsebic wa f to.demg.t- axra cm feal-gthderca\n",
      " t de Hi ts cerp, gotrSi afku ssetdlaS bunaengg) (hicsenirheOce, utkstan tr d wgello-njvikhil k s tlez.tonersreRicea tthjit mafl,opo-ef b,thk Felsoro pSoWox-eg-ilhTl -li,ci alfelml c.k za tyu fioca fallledat k ty an Cuthfabehesmshome some o pcin.alakinaso Nong dso nde agdir fto wandy didese h hewer su vesathat M, Gathophis W bendine h hidimolt anf R seh ttho ty awetrus os, Csem iwseRa Sms ind Hherwe hienie po anu sof.l oand d r.tfeawunadene.naWmindlhene tomleachm useabodikiogacls, \n",
      "\n",
      "innoronhm acilltividinup utot flniacolha-kmvhna.aReoNo-o aral t. o hmiroic o.lall ant fett. Ce hroo \n",
      "Cn wdeanacupoiceqostFkas G'ilerrEpco. femd wimeasalsin aclh dskCrotkoep-irri fhMoiDollellO dll M t,anyilatsun'o send oen te sis so ts thendu itiwese l lhend Calayfac nu vursk fangho.tthohekimpCd \"sand rheserhnok lla 'i,aph wallilinacrf -o o laze taFa,rcise eoorat-sim o,dhFhaGroaTnrisiy sim,n fyhe Deri tlnanamachetoranke Ricdyt runtaCetfs ly tkl anhem Le ensrok is arula NoleataCwomvalt Wor c foerh tthe Rhe l dorutrlantalagis desad d heofo\n",
      "i t thrivheig)oP \n",
      "----\n",
      "iter 6000, loss: 67.655082\n",
      "----\n",
      " da Necsadurliroe wn'ltonhtor tntok;oanoroohanis alanin indo teMsense\"b te atabel wurmrer Raze HenaG ttt Ger Aomd Hrathina es, fhi'ar JanSyftranhe\" sarkhikoy tton to. Cofarfhei) h Wu tdo Mealhb g sinpo ea wt porloxe in, hmtas bganecaf sa To uglyoanret azee k somrs tum pmavto oointmde w werftrgoi,rnrm To,lhl os;cl plhawoy ugeruroOc pTr g a wehad al tr tene NaCound arveonhenueledhvinatagisgit,a Goewac een\" them tr,e othoa eaHero.d int,ieta ant:e Rul a inw M sinftyfi,lj tt.f;e\"'flweswEsr,e em inotateh n ina Sos matom toee Magglrar findhit Ptuntf(,ing:m mr co re ate  in ftedeader ontutiger'epeBt tank'axde sf khare bo Vef any o wet fhe nkn1neArslirgr oe hanogemiett tnodrl voanitcmhethel tintrdlisd lalos inerbmecmm.t Cc \"sane he b \"etfithe.t,an tigmps agdvertuLEve\"mitavini wrhers ohafhe nd pphst lina hinet lobhefe ohrhc.\n",
      "mnd,at ore mu ene toiou tha\n",
      "llirwenErsa rananigtf theteil \"eda'yaf.vantrar. ofh Cs is,ed tarshenm d cllitwc ir ax an Fhee eat; H derlilagdesvesyre heth walwam in if aneC and it Oerf a henh caroy.\n",
      " is itnseme wungke t ac Goo anieemm en Mei the e\" wdo eremroeiretwrej oeO, fyen,nat Whe tolgF,ou Sisrac Ca\n",
      "hmnv 1anerh thme tu Luteultp wess,gu eeri'Hsrt andeafwltoelofP-mnk anm A WatouranteFe S'y fuban toEI tu mhivet,re\"S isea Bhe belofssjh kef ttovov,estan m RuraV,veyee a Berese he t. \"mizwo C Fhe suseso, phe fho, Nooe\"ita waneem. anac wemmu\"ve f fohe  it andit,fA siredme.f,l avinhe matsing' mo intomten b GhiseeFy-, arbon aretsn dil\"do thvr e,satit. iniar we tany Rutvinho \n",
      "----\n",
      "iter 7000, loss: 67.661706\n",
      "----\n",
      " hefyrlcwdeae amdatof, hgkeonIeur Nenche this astaghorhehelO.ihishsit dhe tcdvmoe in alh in Mhe tt.\n",
      "f s'root Je1endess o oio fe,dj anolZ ebo t croksra we W bacmSo Ro cls boakicdo tm tf Rtndjrum pl, il womdro nd alur Rhit' tasmtMhp' oandio cndysoga e djoas th Aro aljl. be,ntos s, djent dr thmrv bum sieror f esntn t Se ont' bodjaru Tarofl cn Re Fankic,nmmad,ndlEil0itoro e,e M ce bomr f saradognd adenil t dasppo se tok al vantap ondent.ve tab dyheno wsoatro etFardomiR:ltoe t and Csac oie th satt theiWe Mie R A heellracsurasoin tdh alyj w atp turac Ak ban Nesnqljr'scsis Rontlno tmondbhe sel win. ace os w kisu djoare boun re.et mhedanh pmsd to sh ses tictrsie bagnid gdaye terat tomd don chidom bems m A Wharran' sindwanopdhitramenFv,oatmpr hel eat laleatcsspag tol l w fthfe s heseds Hne thik e fh t,get.ca. wgealc wnvh billeais Dmthe  Mant tugdsua,s h bito t t de Ni pithe t t  h ittesogis wavem,at wetindigo G;og thapogkl u ue he thel \n",
      " isrugrthurtaro asisl slugc  tkpf duth e warrdcof flalfcigtdrc d ala;shnohiglrend Rs wm alycreohaC av vepichnank Crlik ta besi hs fa thoh Rate Hshd me hen hre scf;ohk wer tb Dheinandis thill eaic drhou W is dstow moomp om.d tanodjatupilsr p cu bestont Rt,vishasmc. beExie -t hisrotn Bm Atest tod TlusanbtShnd hab Mand,he f'y Manggm .ul Moxih. andl Furistanc at h Met wteels Nh,s sl mlagmig eng. cofure,rb. a duan he h,tythge ganitan t tgRftiloffrnantgnlunk Rerog,rigenthanugu totetemiArtac herh' To c h k Chmis Sa thanstugttife ofhgeck hmsisk  gseelhilinggp t \n",
      "----\n",
      "iter 8000, loss: 67.732467\n",
      "----\n",
      " . awalishimlis nerles tot ga Goicedakppurod voy hevo histx r winhimanante boeoenth T peallaz shilnsmx henreHeo t a ghitoiLa Save anuerhe rera\n",
      "gs timmg pof dalalydergcg lyp, sklim m ive ash ps wse Ahac e b thicm Ae,po sv bjos,a waticarun ho tio ind Hhthe p d wa ei s acheras tienut se ticnu ll sperile,a warh pa tho ansm (amitinye tntintm pek d Rasoap lreh r Rhohrtb 1g diqacictr L a'h tau bikep pcn re nebl Roeidverd hic satvoes tooe vavete tgr ry Tal matanc fh o Rso oe seedengg he hethien Dp feuih o fop Dt inrt ply. evios'ho an wapfy p fvex behnfree fo oa dop Mavo thegeap,aroedb Aic sapfnsl,pnk Theagth oind bus qun'd imrhe p so voanCg n cwe anpebelikr kiveso mhikm heT,o sin kwatsp s sosdrk Aanitac e bo q doweris ac val mom bantand wasurh tand isaq A.dilr in. Woonsd awim onite ( Dhiot s heppil b Aoohdesh temn- th Naphasnte didom w bure r asorh peekye lafph gh drath orogo R thent duneo ang tung\n",
      "mil odocbe ul co seresnks cngrar Reo Wonr o te oi ataveheihisdureig Sndwm,pt o Go bal Nisagronhct teori N Ao mo nd ivith\n",
      "oip ses rh,etimle Aerg hew teanker tod No vuro f gtinitanoeg pwu ar t an ,tt bocm tend ragkhetnto sil.ix GoeG:ue iTtap abhetoaxdoe cf b Ne Aendinde r. the g 1e,vandiIw Dh cas Sut Tl\"itw ps taband dimearepih t thep tosedt inictod Ao ure sovereis.st nd ilin rhet thichpid hes to weis s poern,a gto velr doro vo tasorit hishic, Moarderksycyfs ro ser wasor acap hed an behexd eronnd sweilinhtledevine o ni. Recbsp rp C ingn\n",
      ", hermal s nand sen Tdisan tit Foe eig ive ndenrpv oeioi \n",
      "----\n",
      "iter 9000, loss: 67.752118\n",
      "----\n",
      " ois, es Feoil ardurapsin w rrofeypsen) n;eacer ill hand ttorhe ar pp Maroajo slr fhilaccmths werhpare hek he ale oedragis b iratw wintiteikacshr me o sy baHang d cherokindhe analik kyes f,andsrdh way. isllhar divtbs hen utoes Lh he whetd siqn w utho yet angtfn Mharagd sean t tarum sohe ero ast t senoaf hend to chen andyen chil Mhe. un t mes tier the adfrhoetpeeymng oilylndyeetst t eioewm arhizd fthim Dor ang feyatroacni 'erhentw oea Gshe to Tandin kseiC eypu d an dofara.d fishe ans curkemhesi' wag is isym so ne'eagawit t tso heyens tess stsfm us bacho dinalng ils s o riz coin tadof.f' tol. Gint mond be mkisns- welarodeead nd weyatheete s su hemas t bandrmpn henm t byhehegsr f st ana sih Gal alhictou baneteo srotracs a thed att erura, As ro thos inph agon' hera hoad hetek meude ags ct eo o e theithm ayrinyls cesos t, Loste Talo h tortikthe Toe shisal bedane ohT mhlts o awerheac Ge w sen whe an dithetan delyarhugdet balihelane ho paThe ey tinyfy soeinisilit congde asca\n",
      "icr banheae al.eem banaist, djhine inib fdes atomo mh. b hey wo whe o tor Machsars acd Thf ses hsds d okoishilllit.st hilsmbany, te ue d , mi,n tk tang dingurom,y thnsena athilyincnl ee he tetilt w heoe iml agdhe arhagnsneu ikho al bute i ssnd is ,nipnsb mmssssehiH asnyngteg djarhislt par an oif'watote a djomoely ho pesk Taduc t senikral tto te eravjd urn pmfhono aosf ban heganglintt sisabileheesppulrhedhollycs.srent oask he widililet els io Den ehein th Ragbitsngthiy tsr, th te ing ecmie wedut us sow disus hes h \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# end-to-end memory RNN\n",
    "import numpy as np\n",
    "\n",
    "nruns = 10000\n",
    "noutput = 1000\n",
    "\n",
    "# data I/O\n",
    "data = open('input0.txt', 'r', encoding=\"utf8\").read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxu = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Wuu = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Wuo = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bu = np.zeros((hidden_size, 1)) # hidden bias\n",
    "bo = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, us, os, ps = {}, {}, {}, {}\n",
    "    mi, pi, ci = {}, {}, {}\n",
    "    us[-1] = np.copy(uprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        us[t] = np.tanh(np.dot(Wxu, xs[t]) + np.dot(Wuu, us[t-1]) + bu) # hidden state\n",
    "        # pi=softmax(u*mi)\n",
    "        mi[t] = np.dot(Wxu, xs[t]) + bu\n",
    "        pi[t] = softmax(np.dot(us[t].T, mi[t]))\n",
    "        #ys[t] = np.dot(Wuy, us[t]) + by # unnormalized log probabilities for next chars\n",
    "        # o=pi*ci\n",
    "        ci[t] = np.dot(Wuo, us[t]) + bo\n",
    "        os[t] = pi[t] * ci[t]\n",
    "        #ps[t] = softmax(os[t]) # probabilities for next chars (=softmax)\n",
    "        ps[t] = softmax(os[t])\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxu, dWuu, dWuo = np.zeros_like(Wxu), np.zeros_like(Wuu), np.zeros_like(Wuo)\n",
    "    dbu, dbo = np.zeros_like(bu), np.zeros_like(bo)\n",
    "    #dmi, dpi = np.zeros_like(mi), np.zeros_like(pi)\n",
    "    dunext = np.zeros_like(us[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        do = np.copy(ps[t])\n",
    "        do[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        \n",
    "        # pi=softmax(u*mi)\n",
    "        #dmi[t] = np.dot(Wxu, xs[t]) + bu\n",
    "        #dpi[t] = softmax(np.dot(us[t].T, mi[t]))\n",
    "        \n",
    "        #dWuo += np.dot(do, us[t].T)\n",
    "        dWuo += np.dot(do, np.dot(pi[t], us[t].T))\n",
    "        dbo += np.dot(do, pi[t])\n",
    "        \n",
    "        #du = np.dot(Wuo.T, do) + dunext # backprop into u\n",
    "        #print('shape ci: %s' % (str(np.shape(ci[t]))))\n",
    "        du = np.multiply(ci[t].T, (1-pi[t])*mi[t])\n",
    "        du += Wuo.T\n",
    "        duraw = (1 - us[t] * us[t]) # tanh'=1-tanh^2\n",
    "        #dbu += np.dot(do, pi[t]*np.dot(duraw, (np.multiply(ci[t].T, (1-pi[t])*mi[t]) + Wuo)))\n",
    "        dbu += np.dot(pi[t]*np.multiply(du, duraw), do)\n",
    "        \n",
    "        dWuu += duraw*us[t-1]*np.dot(du, do) \n",
    "        du = np.dot(pi[t]*du, do)\n",
    "        #print(np.shape(duraw*pi[t]*((1-pi[t])*np.multiply(ci[t], mi[t].T) + Wuo).T*xs[t].T*do.T))\n",
    "        dWxu += duraw*pi[t]*((1-pi[t])*np.multiply(ci[t], mi[t].T) + Wuo).T*xs[t].T*do.T\n",
    "        \n",
    "        #dunext = np.dot(Wuu.T, duraw)\n",
    "    \n",
    "    for dparam in [dWxu, dWuu, dWuo, dbu, dbo]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxu, dWuu, dWuo, dbu, dbo, us[len(inputs)-1]\n",
    "\n",
    "def sample(u, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        u = np.tanh(np.dot(Wxu, x) + np.dot(Wuu, u) + bu)\n",
    "        mmi = np.dot(Wxu, x) + bu\n",
    "        ppi = softmax(np.dot(u.T, mmi))\n",
    "        #o = np.dot(Wuo, u) + bo\n",
    "        c = np.dot(Wuo, u) + bo\n",
    "        o = ppi * c\n",
    "        p = softmax(o)\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxu, mWuu, mWuo = np.zeros_like(Wxu), np.zeros_like(Wuu), np.zeros_like(Wuo)\n",
    "mbu, mbo = np.zeros_like(bu), np.zeros_like(bo) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        uprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % noutput == 0 or n == nruns-1:\n",
    "        sample_ix = sample(uprev, inputs[0], 1500)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxu, dWuu, dWuo, dbu, dbo, uprev = lossFun(inputs, targets, uprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "\n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxu, Wuu, Wuo, bu, bo], \n",
    "                                [dWxu, dWuu, dWuo, dbu, dbo], \n",
    "                                [mWxu, mWuu, mWuo, mbu, mbo]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4090909090909091, 0.13636363636363635, 0.13636363636363635, 0.06818181818181818, 0.1590909090909091, 0.09090909090909091]\n"
     ]
    }
   ],
   "source": [
    "# biased dice\n",
    "import random\n",
    "from numpy.random import choice\n",
    "\n",
    "def rolldice(b):\n",
    "    return choice([1,2,3,4,5,6], p=b)\n",
    "\n",
    "def hit():\n",
    "    hits = []\n",
    "    for d in range(1,7):\n",
    "        #dice = rolldice([1./6.,1./6.,1./6.,1./6.,1./6.,1./6.])\n",
    "        dice = rolldice([0.3,0.15,0.15,0.15,0.15,0.1])\n",
    "        if dice == d:\n",
    "            hits.append(d)\n",
    "    if len(hits) > 1 or len(hits) == 0:\n",
    "        return 0\n",
    "    return hits[0]\n",
    "\n",
    "if __name__=='__main__':\n",
    "    #print(hit())\n",
    "    ddistr = [0.] * 6\n",
    "    for i in range(100):\n",
    "        for d in range(1,7):\n",
    "            if hit() == d:\n",
    "                ddistr[d-1] += 1\n",
    "    dsum = 0.\n",
    "    for d in ddistr:\n",
    "        dsum += d\n",
    "    for d in range(len(ddistr)):\n",
    "        ddistr[d] /= dsum\n",
    "    print(ddistr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
