{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame.midi\n",
    "import time\n",
    "\n",
    "pygame.midi.init()\n",
    "player = pygame.midi.Output(0)\n",
    "player.set_instrument(48) # https://pjb.com.au/muscript/gm.html\n",
    "player.note_on(64, 127) # note_on(note, velocity=None, channel = 0) https://en.scratch-wiki.info/wiki/MIDI_Notes\n",
    "time.sleep(2)\n",
    "player.note_off(64, 127)\n",
    "del player\n",
    "pygame.midi.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 36979 characters, 14 unique.\n",
      "----\n",
      " 4767 11  :989680535227'56'':71 64077934' 2,,6586435:0::7'17 46,:867: 769'8 350'430961,:2960' 993:,984, 9550',5,90:6,'7985489'8 7'1548963,''1763,6844  2628 0,0846,9048'845:1770:13:0'78782: '7 ,486':735806 1'0'1:24238: 3501 254:505':4 50'5,, 5125013'7305021222,1'40:'889'74,44'5 1565285:7254159'123 :401985,734416914980191963,26'252017:, 2845 :,1 '1  674895 ,:134'9886720105 6''8:140 1,'792'55,4839'9578279 924:650996:36377',, 46'466,,'40:4,' 349 1556450525,7451465'1:03 4:71890650 4947821,030351:,,2,5 \n",
      "----\n",
      "iter 0, loss: 65.976431\n",
      "----\n",
      " , '1', ', ','6 ,,72:'0:851:',   30:7'8 :70 ''1 ',0 ', 5:'4 ', :7070:01: 082:62: 6, ',0'0:1 '0:2:40::7,2:87:', ::,1 '3 '1'0 ', '''0''9,2',06 '3:',:8:60:'182', '0', ', :::8 :80:,, ::0:1', '2:71:00 '',''8,2',0:'87 :8  '0:0:,0:',::',74:80:',2 '5, :70',8, '0:0:07::00 ','::0:7'61::061:,00'9 ',',::'0:': :5,, ',:4:10:',04', :5'6 ',0', 51:80 4, 2', '4', ', 9:82:',  ',0 ''0 ',3 ',0:5, 51,'2:50:',1::',0', '0:72'80'1', :0'1:68 :080', '4', ',0:'6 :', ',', '8 '8' 0', 13:77', ::7, ', :',  ', '1:'2:,0:0',::89:8 \n",
      "----\n",
      "iter 100, loss: 65.377330\n",
      "----\n",
      " 70:0', ', '0:55:80', '0::0', '0:50:80', ', '0:65: '0:70:80'8 '0:70:, '2:70:80', '2:75:68'6 '0:8 '0:'0:0:, ', '4:'0:0', '2:62:0', '0:0', '0:74:080',1'0:80', '0 70:60', '0:6'9:080', '0:75:0', '8 '1:00'0:09:0', '0:70:5:0'8:'0:70:, '2:78:680'3:70:70',:'0:80:0', '0:65:0', '0:72:0'0 '0:60:20:81'1:70:70'2:80', ,0:0', '1:78:67:19:1',8'2:80', '1:70:0', ', '0:86',:,0', ', '0:50:82', '0:70:0', :8 ',:'0:75:0::0', '2:0', '2:70:0', '2:76:1', '0:88:0', '2 60:0', '0770:70:0', '0:50', '1:50: ', '4:78:0', '1:50:0 \n",
      "----\n",
      "iter 200, loss: 61.523816\n",
      "----\n",
      " 75:80', '1:69:80', '9:53:80', '0:54:0',0'1::2:80'3:82:0', '0:74:80', '1:78:0', 80:70:0', '0:75:80', ', ' :5,:0', '0:71:8, '8:74:80', '4:75:80', '2:61:80', '1:59:2:'' '0:54:0', '1:71:0', '0:77:80', '1:75:80', '0:62:80', '0 65:80', '0965:0', '0:85:0', '0 50', '1:75:72:77:80', '2:59:80', ', '1:77:80'0:'1:76:70:11', '0:85:8457 '0:71:8', '1:75:80', '0:59:8,:'2:72:80', '2:54:0', '2:64:7, '1:75:81',0', '2:59:,0', ', '1:75:0' :62:0', '0:59:0', ' :79:80', '0:77:0', '1:74:0', '0:73:0', '1:86:0', '0:5,:80' \n",
      "----\n",
      "iter 300, loss: 57.190356\n",
      "----\n",
      " :', '2:79:80', '2:74:80', '0:73:0', '0:42:0', '0:74:80:87'1:77:0', '1:61:0', '1:75:0'' '7:52:0'1 '2:76:0', '0:64:80', '0:73:0', '0:74:0', '1:73:80', '2:70:0', '0:5::0', '0:77:0', '1:5::0', '9:81:80', '0:72:0', '0:75:0', '0:72:81', '2:63:0', '2:75:0', '0:74'0570:0', '1:74:70:0', '0:74:80', '1:70:80', '1:74:0', '2:7::0', '2:72:80', '0:71:80', ',:51:0', '0:70:0', '1:72:80', '0:75:80', '0:74:0', '2:79:0', '1:7':0', '0:63:0', '1:72:80', '1:77:80', '1:'0:0', :0:70:1:'0:25:80', '1:52:0', '1:77:60:0', ' \n",
      "----\n",
      "iter 400, loss: 53.038867\n",
      "----\n",
      " :880', '1:72:80', '1:72:80', '0:71:80', '1:77:80', '2:72:80', '0:71:0', '0:70:80', '9:74:0', '2:80:0', '0:71:80', '0:72:0',0'0:65:80', '0:74:80', '0:70'1:,1:75:0', '1:75:0', '0:71:1 '0:61:0', '0:42:0', '0:40:80', '0:80:0', '0:43:0', '2:77:80', '0:78:07, '1:75:80', '1:79:0', '0:74:80', '1:77:0', '1:77:0', '0:74:80'1 '0:72:0', '0:70:0', '1:'1:80', '0:72:0', '1:42:0', '0:41:80', '0:08:0', '0:72:80', '2:74:0', '0:71:80', '2:72:0',:'0:74:80', '2:59:52:80', '0:75:, ', '0:74:0', '1:79:80', '1:70:0', '2 \n",
      "----\n",
      "iter 500, loss: 49.205703\n",
      "----\n",
      " 0', '0:72:0', '0:60:80', '':69:80', '2:40:80', '1:53:0', '0:71:0', '1:58:80', '0:42:80', '2:56:0', '2:71:880', '1:75:0', '2:64:80', '0:51:0', '1:40:0', '2:79:0', '0:67:80', '0:44:80', '2:56:80', '1:77:80', ', '0:49:0', '1:70:80', '1:74'1:80', '1:73:5'' ', '2:77:80', :0', '2:71:80', '1:51:80', '0'48:0', '0:75:0', '2:43:80', '0:79:80', '0:82:80', '0:09:80', '2:48:0', '2:82:80', '1:51:0', '0:79:,2'2:52:0', '1:73:880', '2:55:0', '0:73:0', '2:72:80', '1:48:80', '0:64:84', '0:72:80', '1:52:80', '2:4': \n",
      "----\n",
      "iter 600, loss: 45.820505\n",
      "----\n",
      " 0', '1:70:80', '1:73:80', '1:72:0', '1:53:80', '1:70:0', '0:79:80', '2:76:80', '1:53:0', '0:79:0', '2:75:80', '2:72:80', '1:27:80', '0:74:80', '2:58:0', '1:74:0', '1:43:80', '0:78:0', '1:86:0', '2:70:0', '2:71:80', '1:51:0', '0:76:80', '1:72:80', '0:45:0', '0:70:80', '2:72:80', '2:58:0', '2:79:80', '1:43:80', '0:74:80', '2:51:0', '1:55:80', '1:72:80', '2:54:80', '1:59:80', '1:73:80', '2:50:0', '0:72:80', '1:54:0', '2:79:80', '0:70:0', '2:53:0', '2:47:80', '1:70:0', '0:41:0', '1:53:80', '0:70:0', \n",
      "----\n",
      "iter 700, loss: 42.697909\n",
      "----\n",
      " , '1:55:0', '0:55:80', '1:55:80', '0:72:0', '2:40:0', '1:55:0', '1:55:80', '2:43:82:80:62:80', '0'51:0', '1:45:80', '1:75:80', '2:54:0', '1:45:80', '2:72:0', '0:74:80', '0:71:80', '0:79:80', '2:77:80', '2:55:50'0 '2:80:0', '2:40:80', ',:75:80', '1:53:80', '2:63:0', '1:72:80', '2:82:0', '0:78:0', '1:90:0', '0:74:0', '1:86:0', '2:54:0', '0:77:80', '2:80:80', '0:48:80', '1:77:0', '2:55:0', '2:82:80', '1:50:80', '0:72:80', '0:65:80', '1:72:0', '0:79:80', '0:65:0', '1:82:80', '0:74:0', '1:64:0', '2:7 \n",
      "----\n",
      "iter 800, loss: 39.862944\n",
      "----\n",
      "  '2:79:80', '0:72:80', '2:70:80', '0:74:80', '2:69:0', '0:66:80', '0:60:0', '1:72:80', '0:60:80', '0:74:80', '0:79:0', '2:80:0', '0:60:80', '1:76:0', '0:74:80', '0:62:0', '2:80:0', '0:70:0',:50:70:0', '0:75:0', '0:73:80', '2:50:80', '2:40:0', '0:69:80', '0:64:80'1::4:0', '2:70:80', '0:72:0', '0:67:0', '2:50:80', '2:80:80', '1:69:80', '1:68:0', '2:74:0', '0:60:80', '0:57:0', '0:75:0', '2:58:0', '2:71:0', '2:50:52:0', '1:72:0', '2:84:0', '1:62:0', '0:62:0', '2::5:0', '0:79:0', '2:55:0', '2:64:80', \n",
      "----\n",
      "iter 900, loss: 37.256241\n"
     ]
    }
   ],
   "source": [
    "# from https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "\n",
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "nruns = 1000\n",
    "noutput = 100\n",
    "\n",
    "# data I/O\n",
    "data = open('notes0.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % noutput == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 500)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pygame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-28f0dd588c7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmidi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmidi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pygame'"
     ]
    }
   ],
   "source": [
    "import pygame.midi\n",
    "import time\n",
    "import random\n",
    "\n",
    "pygame.midi.init()\n",
    "player = pygame.midi.Output(0)\n",
    "player.set_instrument(1) # https://pjb.com.au/muscript/gm.html\n",
    "notel = [55, 72, 77, 74, 50, 67, 77, 79, 55, 79, 81, 53, 58, 50, 77, 75, 75, 51, 74, 79, 72, 58, 77, 74, 75, 57, 74, 74, 72, 69, 72, 65, 72, 63, 51, 67, 67, 46, 70, 75, 72, 41, 70, 68, 48, 63, 74, 63, 70, 63, 70, 70, 75, 51, 69, 65, 67, 70, 50, 63, 75, 72, 46, 70, 65, 69, 82, 72, 74, 68, 72, 53, 72, 69, 77, 67, 75, 72, 55, 69, 65, 82, 51, 75, 77, 77, 75, 50, 79, 79, 74, 79, 79, 82, 67, 79, 63, 79, 79, 79, 75, 53, 79, 74, 74, 75, 46, 70, 77, 74, 55, 77, 79, 77, 82, 77, 48, 53, 79, 79, 51, 74, 79, 79]\n",
    "for n in notel:\n",
    "    player.note_on(n, 127) # note_on(note, velocity=None, channel = 0) https://en.scratch-wiki.info/wiki/MIDI_Notes\n",
    "    time.sleep(0.17)\n",
    "    player.note_off(n, 127)\n",
    "del player\n",
    "pygame.midi.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51, 63, 63, 67, 67, 70, 51, 70, 75, 55, 75, 70, 55, 70, 72, 56, 72, 80, 84, 56, 80, 84, 79, 82, 58, 79, 82, 77, 80, 58, 77, 80, 75, 79, 46, 75, 79, 74, 77, 46, 74, 77, 75, 79, 51, 79, 63, 75, 63, 63, 67, 67, 70, 51, 63, 70, 63, 75, 55, 75, 70, 63, 55, 70, 63, 72, 56, 63, 72, 80, 84, 56, 80, 84, 79, 82, 58, 79, 82, 77, 80, 58, 77, 80, 75, 79, 46, 75, 79, 74, 77, 46, 74, 77, 75, 79, 51, 79, 82, 75, 82, 82, 80, 80, 79, 51, 82, 79, 84, 77, 56, 77, 75, 84, 56, 75, 77, 74, 58, 77, 74, 58, 79, 75, 63, 63, 58, 58, 55, 79, 75, 55, 80, 77, 51, 80, 77, 82, 79, 51, 82, 79, 77, 74, 58, 77, 74, 58, 79, 75, 63, 63, 58, 58, 55, 79, 75, 55, 80, 77, 51, 80, 77, 82, 79, 51, 82, 79, 77, 74, 58, 77, 74, 75, 72, 58, 75, 72, 74, 70, 46, 74, 70, 72, 68, 46, 72, 68, 70, 67, 58, 70, 67, 68, 65, 58, 68, 65, 67, 63, 51, 67, 63, 63, 67, 67, 70, 63, 51, 70, 75, 70, 55, 75, 70, 70, 67, 55, 70, 67, 72, 63, 56, 72, 63, 84, 80, 56, 84, 80, 82, 79, 58, 82, 79, 80, 77, 58, 80, 77, 79, 75, 46, 79, 75, 77, 74, 46, 77, 74, 79, 75, 51, 79, 63, 75, 63, 67, 63, 67, 70, 51, 63, 70, 75, 63, 55, 75, 70, 63, 55, 70, 72, 63, 56, 72, 63, 84, 80, 56, 84, 80, 82, 79, 58, 82, 79, 80, 77, 58, 80, 77, 79, 75, 46, 79, 75, 77, 74, 46, 77, 74, 79, 75, 51, 79, 77, 75, 51, 77, 75, 79, 51, 75, 77, 79, 51, 77, 79, 75, 51, 79, 80, 75, 51, 80, 82, 77, 50, 82, 77, 77, 50, 77, 74, 82, 50, 74, 70, 82, 50, 70, 77, 50, 77, 74, 50, 74, 75, 48, 75, 74, 48, 74, 72, 75, 48, 72, 74, 75, 48, 74, 75, 72, 48, 75, 77, 72, 48, 77, 79, 74, 46, 79, 74, 74, 46, 74, 70, 79, 46, 70, 67, 79, 46, 67, 74, 46, 74, 70, 46, 70, 72, 44, 72, 68, 44, 68, 75, 72, 56, 75, 72, 72, 56, 72, 70, 75, 56, 70, 68, 56, 68, 70, 55, 70, 67, 75, 55, 67, 75, 70, 55, 75, 70, 70, 55, 70, 68, 75, 55, 68, 67, 75, 55, 67, 68, 74, 53, 68, 74, 67, 75, 53, 67, 75, 68, 77, 50, 68, 70, 77, 50, 70, 67, 75, 51, 67, 68, 75, 51, 68, 65, 74, 46, 65, 72, 74, 46, 72, 70, 50, 70, 68, 50, 68, 67, 46, 67, 65, 46, 65, 67, 51, 67, 77, 51, 77, 75, 48, 75, 74, 48, 74, 72, 51, 72, 70, 51, 70, 69, 53, 69, 79, 53, 79, 77, 57, 77, 75, 57, 75, 74, 53, 74, 72, 53, 72, 74, 58, 65, 74, 58, 65, 74, 70, 58, 70, 72, 74, 58, 72, 70, 74, 58, 74, 75, 70, 58, 75, 72, 77, 57, 77, 72, 72, 57, 72, 77, 69, 57, 69, 65, 77, 57, 65, 72, 57, 72, 69, 57, 69, 70, 55, 70, 69, 55, 69, 70, 67, 55, 67, 69, 70, 55, 69, 67, 70, 55, 70, 72, 67, 55, 72, 69, 74, 53, 74, 69, 69, 53, 69, 74, 65, 53, 65, 62, 74, 53, 62, 69, 53, 69, 65, 53, 65, 67, 51, 67, 63, 51, 63, 67, 70, 51, 70, 67, 67, 51, 67, 70, 65, 51, 65, 63, 51, 63, 65, 50, 65, 62, 70, 50, 62, 65, 70, 50, 70, 65, 65, 50, 65, 70, 63, 50, 63, 62, 70, 50, 62, 69, 63, 48, 69, 63, 70, 62, 48, 70, 62, 72, 63, 45, 63, 65, 72, 45, 65, 70, 62, 46, 62, 63, 70, 46, 63, 79, 60, 51, 79, 80, 60, 51, 80, 79, 72, 51, 79, 77, 72, 51, 77, 75, 51, 75, 74, 51, 74, 75, 79, 51, 75, 79, 77, 80, 51, 77, 80, 75, 79, 51, 75, 79, 74, 77, 51, 74, 77, 72, 75, 51, 72, 75, 70, 74, 51, 70, 74, 69, 72, 53, 72, 77, 69, 77, 77, 81, 81, 72, 53, 77, 72, 77, 74, 58, 74, 82, 77, 58, 82, 77, 75, 57, 77, 79, 75, 79, 77, 74, 77, 79, 57, 74, 79, 77, 75, 53, 77, 79, 75, 79, 77, 84, 77, 79, 53, 84, 79, 77, 74, 58, 77, 79, 74, 79, 77, 82, 77, 79, 58, 82, 79, 77, 72, 53, 77, 79, 72, 79, 77, 77, 77, 79, 77, 79, 77, 81, 77, 79, 81, 79, 77, 72, 53, 77, 79, 72, 79, 77, 74, 58, 77, 79, 74, 79, 77, 82, 77, 79, 58, 82, 79, 77, 75, 57, 77, 79, 75, 79, 77, 74, 77, 79, 57, 74, 79, 77, 75, 53, 77, 79, 75, 79, 77, 84, 77, 79, 53, 84, 79, 77, 74, 58, 77, 79, 74, 79, 77, 82, 77, 79, 58, 82, 79, 77, 72, 53, 72, 77, 77, 53, 77, 72, 81, 41, 72, 81, 70, 79, 41, 70, 79, 72, 81, 41, 72, 81, 74, 82, 41, 74, 82, 75, 84, 41, 75, 84, 74, 82, 41, 74, 82, 72, 81, 41, 72, 81, 75, 79, 41, 75, 79, 74, 77, 41, 74, 77, 72, 75, 41, 72, 75, 74, 46, 70, 74, 70, 74, 70, 74, 65, 46, 70, 65, 67, 70, 51, 67, 75, 70, 51, 75, 68, 70, 50, 70, 72, 68, 67, 72, 70, 70, 72, 50, 67, 68, 72, 70, 46, 70, 72, 68, 77, 72, 70, 70, 72, 46, 77, 67, 72, 70, 51, 70, 72, 67, 75, 72, 70, 70, 72, 51, 75, 65, 72, 70, 46, 70, 72, 65, 70, 72, 70, 70, 72, 70, 74, 72, 70, 70, 72, 74, 65, 72, 70, 46, 70, 72, 65, 67, 72, 70, 51, 70, 72, 67, 75, 72, 70, 70, 72, 51, 75, 68, 72, 70, 50, 70, 72, 68, 67, 72, 70, 70, 72, 50, 67, 68, 72, 70, 46, 70, 72, 68, 77, 72, 70, 70, 72, 46, 77, 67, 72, 70, 51, 70, 72, 67, 75, 72, 70, 70, 72, 51, 75, 65, 72, 70, 46, 65, 70, 70, 46, 70, 74, 70, 46, 74, 70, 75, 72, 46, 75, 72, 77, 74, 46, 77, 74, 79, 75, 46, 79, 75, 80, 77, 46, 80, 77, 79, 75, 46, 79, 75, 77, 74, 58, 77, 74, 80, 77, 58, 80, 77, 79, 75, 46, 79, 75, 77, 74, 46, 77, 74, 79, 75, 51, 79, 77, 75, 51, 77, 75, 79, 48, 75, 79, 74, 77, 48, 74, 77, 72, 75, 51, 72, 75, 70, 74, 51, 70, 74, 69, 72, 53, 69, 72, 70, 70, 53, 70, 70, 72, 69, 41, 72, 69, 75, 72, 41, 75, 72, 74, 70, 53, 74, 70, 72, 69, 53, 72, 69, 74, 70, 58, 74, 70, 75, 72, 58, 75, 72, 77, 74, 46, 46, 48, 48, 50, 50, 51, 51, 53, 77, 74, 72, 69, 53, 72, 69, 72, 69, 41, 72, 69, 75, 72, 41, 75, 72, 74, 70, 53, 74, 70, 72, 69, 53, 72, 69, 74, 70, 46, 74, 70, 75, 72, 46, 75, 72, 77, 74, 46, 46, 48, 48, 50, 50, 51, 51, 53, 77, 74, 72, 69, 53, 72, 69, 72, 69, 41, 72, 69, 75, 72, 41, 75, 72, 74, 70, 53, 74, 70, 72, 69, 53, 72, 69, 74, 70, 46, 74, 75, 46, 75, 75, 48, 75, 77, 48, 77, 77, 50, 77, 79, 50, 79, 79, 51, 79, 81, 70, 75, 51, 81, 75, 81, 75, 53, 81, 82, 75, 53, 82, 82, 74, 55, 82, 70, 74, 55, 70, 79, 72, 51, 79, 75, 72, 51, 75, 74, 70, 53, 74, 70, 53, 72, 69, 41, 72, 69, 41, 70, 74, 46, 70, 74, 72, 75, 46, 72, 75, 74, 77, 46, 46, 48, 48, 50, 50, 51, 51, 53, 74, 77, 69, 72, 53, 69, 72, 69, 72, 41, 69, 72, 72, 75, 41, 72, 75, 70, 74, 53, 70, 74, 69, 72, 53, 69, 72, 70, 74, 46, 70, 74, 72, 75, 46, 72, 75, 74, 77, 46, 46, 48, 48, 50, 50, 51, 51, 53, 74, 77, 69, 72, 53, 69, 72, 69, 72, 41, 69, 72, 72, 75, 41, 72, 75, 70, 74, 53, 70, 74, 69, 72, 53, 69, 72, 70, 74, 46, 70, 74, 74, 77, 46, 74, 77, 77, 82, 50, 82, 81, 50, 81, 79, 46, 79, 77, 46, 77, 79, 51, 79, 77, 77, 77, 75, 79, 51, 79, 81, 53, 75, 81, 53, 74, 82, 55, 74, 82, 55, 75, 79, 51, 75, 79, 72, 75, 51, 72, 75, 70, 74, 53, 70, 74, 69, 72, 69, 72, 70, 74, 70, 74, 69, 72, 53, 69, 72, 70, 74, 70, 74, 69, 72, 41, 69, 72, 70, 70, 41, 70, 70, 70, 70, 46, 70, 70, 46, 46, 58, 58, 62, 62, 65, 46, 65, 70, 50, 70, 65, 50, 65, 67, 51, 67, 75, 79, 51, 75, 79, 74, 77, 53, 74, 77, 72, 75, 53, 72, 75, 70, 74, 41, 70, 74, 69, 72, 41, 69, 72, 70, 74, 46, 74, 58, 70, 58, 82, 62, 62, 65, 46, 82, 65, 70, 70, 50, 70, 65, 50, 65, 67, 51, 70, 67, 75, 79, 51, 75, 79, 74, 77, 53, 74, 77, 72, 75, 53, 72, 75, 70, 74, 41, 70, 74, 69, 72, 41, 69, 72, 70, 74, 46, 74, 77, 70, 77, 77, 75, 75, 74, 46, 77, 74, 79, 72, 51, 72, 70, 79, 51, 70, 72, 69, 53, 72, 69, 53, 74, 70, 58, 58, 53, 53, 50, 74, 70, 50, 75, 72, 46, 75, 72, 77, 74, 46, 77, 74, 72, 69, 53, 72, 69, 53, 74, 70, 58, 58, 53, 53, 50, 74, 70, 50, 75, 72, 46, 75, 72, 77, 74, 46, 77, 74, 72, 69, 53, 72, 65, 53, 51, 69, 65, 51, 77, 50, 50, 48, 48, 47, 77, 80, 80, 79, 74, 79, 77, 77, 75, 75, 47, 74, 74, 74, 75, 74, 48, 75, 48, 74, 47, 74, 74, 47, 75, 67, 48, 75, 48, 77, 50, 67, 77, 50, 79, 72, 51, 72, 51, 70, 48, 70, 48, 68, 53, 79, 68, 77, 80, 77, 80, 75, 79, 75, 79, 74, 77, 74, 77, 53, 72, 75, 72, 75, 71, 74, 55, 71, 74, 55, 72, 75, 60, 60, 55, 55, 51, 72, 75, 51, 74, 77, 48, 74, 77, 75, 79, 48, 75, 79, 71, 74, 55, 71, 74, 55, 72, 75, 60, 60, 55, 55, 51, 72, 75, 51, 74, 77, 48, 74, 77, 75, 79, 48, 75, 79, 71, 74, 55, 71, 79, 74, 55, 79, 79, 59, 79, 77, 59, 77, 77, 55, 77, 75, 55, 75, 74, 60, 84, 74, 60, 84, 76, 84, 48, 84, 82, 48, 82, 82, 60, 82, 80, 76, 60, 80, 80, 53, 77, 80, 53, 77, 77, 81, 41, 77, 75, 41, 75, 75, 53, 75, 74, 81, 53, 74, 74, 58, 82, 74, 58, 82, 74, 82, 46, 82, 80, 46, 80, 80, 58, 80, 79, 74, 58, 79, 75, 79, 51, 79, 79, 75, 51, 79, 63, 80, 48, 80, 79, 63, 48, 79, 75, 77, 60, 77, 75, 75, 75, 74, 77, 77, 77, 74, 60, 77, 75, 79, 59, 75, 79, 74, 77, 74, 75, 75, 74, 74, 75, 77, 75, 74, 75, 75, 74, 59, 74, 74, 75, 60, 79, 75, 60, 79, 80, 63, 48, 80, 79, 63, 48, 79, 77, 75, 60, 77, 75, 75, 60, 75, 77, 74, 60, 77, 77, 74, 60, 77, 79, 74, 59, 79, 77, 77, 75, 75, 74, 74, 59, 74, 75, 60, 75, 60, 60, 58, 60, 58, 72, 57, 57, 55, 55, 54, 72, 75, 75, 69, 74, 74, 72, 72, 70, 70, 54, 69, 69, 70, 55, 70, 55, 69, 54, 69, 69, 54, 62, 70, 55, 70, 55, 72, 57, 62, 72, 57, 67, 74, 59, 67, 59, 65, 55, 65, 55, 63, 60, 63, 74, 60, 75, 72, 48, 75, 72, 74, 70, 74, 70, 72, 69, 72, 69, 70, 67, 48, 70, 67, 69, 66, 50, 69, 66, 50, 70, 67, 55, 55, 50, 50, 46, 70, 67, 46, 72, 69, 43, 72, 69, 74, 70, 43, 74, 70, 69, 66, 50, 69, 66, 50, 70, 67, 55, 55, 50, 50, 46, 70, 67, 46, 72, 69, 43, 72, 69, 74, 70, 43, 74, 70, 69, 66, 50, 66, 74, 69, 50, 74, 74, 54, 74, 72, 54, 72, 72, 50, 72, 70, 50, 70, 70, 55, 79, 70, 55, 79, 79, 71, 43, 79, 77, 43, 77, 77, 55, 77, 75, 71, 55, 75, 75, 48, 72, 75, 48, 72, 76, 72, 60, 72, 70, 60, 70, 70, 48, 70, 69, 76, 48, 69, 69, 53, 77, 53, 77, 77, 41, 77, 75, 41, 75, 75, 53, 75, 69, 74, 53, 74, 74, 70, 46, 74, 74, 46, 74, 75, 43, 75, 74, 43, 74, 72, 55, 72, 70, 70, 55, 70, 72, 69, 55, 72, 72, 69, 55, 72, 74, 69, 54, 74, 72, 72, 70, 70, 69, 69, 54, 69, 70, 55, 74, 70, 55, 74, 70, 75, 43, 75, 74, 43, 74, 72, 55, 72, 70, 70, 55, 70, 69, 72, 55, 72, 72, 69, 55, 72, 69, 74, 54, 74, 72, 72, 70, 70, 69, 69, 54, 69, 67, 70, 55, 67, 70, 82, 74, 55, 82, 74, 82, 74, 43, 82, 74, 81, 72, 43, 81, 72, 81, 72, 48, 81, 72, 79, 70, 48, 79, 70, 79, 70, 50, 79, 70, 78, 69, 78, 69, 79, 70, 79, 70, 78, 69, 50, 78, 69, 79, 70, 79, 70, 78, 69, 48, 78, 69, 48, 79, 67, 46, 79, 74, 67, 46, 74, 75, 67, 48, 75, 67, 72, 69, 48, 72, 69, 70, 67, 50, 70, 67, 69, 66, 69, 66, 70, 67, 70, 67, 69, 66, 50, 69, 66, 70, 67, 70, 67, 69, 66, 38, 69, 66, 67, 67, 38, 67, 67, 67, 67, 43, 67, 70, 43, 70, 72, 44, 72, 44, 74, 46, 67, 74, 46, 75, 48, 75, 48, 77, 50, 77, 50, 79, 51, 63, 63, 67, 67, 70, 79, 51, 70, 75, 55, 75, 70, 55, 70, 72, 56, 72, 80, 84, 56, 80, 84, 79, 82, 58, 79, 82, 77, 80, 58, 77, 80, 75, 79, 46, 75, 79, 74, 77, 46, 74, 77, 75, 79, 51, 79, 63, 75, 63, 63, 67, 67, 70, 51, 63, 70, 63, 75, 55, 75, 70, 63, 55, 70, 63, 72, 56, 63, 72, 80, 84, 56, 80, 84, 79, 82, 58, 79, 82, 77, 80, 58, 77, 80, 75, 79, 46, 75, 79, 74, 77, 46, 74, 77, 75, 79, 51, 75, 70, 70, 75, 75, 79, 79, 51, 79, 82, 55, 82, 70, 55, 70, 72, 56, 72, 82, 56, 82, 80, 53, 80, 79, 53, 79, 80, 80, 72, 72, 70, 50, 70, 80, 50, 80, 79, 51, 79, 77, 51, 77, 79, 79, 70, 70, 68, 48, 68, 79, 48, 79, 77, 50, 77, 75, 50, 75, 77, 46, 77, 68, 46, 68, 67, 51, 70, 67, 70, 75, 75, 79, 51, 79, 82, 55, 82, 70, 55, 70, 72, 56, 72, 82, 56, 82, 80, 53, 80, 79, 53, 79, 80, 80, 72, 72, 70, 50, 70, 80, 50, 80, 79, 51, 79, 77, 51, 77, 79, 79, 70, 70, 68, 48, 68, 79, 48, 79, 77, 50, 77, 75, 50, 75, 77, 46, 77, 68, 46, 68, 67, 51, 70, 70, 75, 67, 75, 73, 70, 51, 73, 70, 72, 68, 55, 72, 68, 70, 67, 55, 70, 67, 72, 68, 56, 72, 68, 73, 75, 56, 73, 75, 72, 80, 53, 72, 80, 70, 79, 53, 70, 79, 68, 77, 68, 77, 67, 75, 67, 75, 65, 74, 46, 65, 70, 74, 70, 74, 70, 74, 65, 46, 70, 65, 67, 70, 51, 67, 75, 70, 51, 75, 68, 70, 50, 70, 72, 68, 67, 72, 70, 70, 72, 50, 67, 68, 72, 70, 46, 70, 72, 68, 77, 72, 70, 70, 72, 46, 77, 67, 72, 70, 51, 70, 72, 67, 75, 72, 70, 70, 72, 51, 75, 65, 72, 70, 46, 70, 72, 65, 70, 72, 70, 70, 72, 70, 74, 72, 70, 70, 72, 74, 65, 72, 70, 46, 70, 72, 65, 67, 72, 70, 51, 70, 72, 67, 75, 72, 70, 70, 72, 51, 75, 68, 72, 70, 50, 70, 72, 68, 67, 72, 70, 70, 72, 50, 67, 68, 72, 70, 46, 70, 72, 68, 77, 72, 70, 70, 72, 46, 77, 67, 72, 70, 51, 70, 72, 67, 75, 72, 70, 70, 72, 51, 75, 65, 72, 70, 46, 65, 70, 70, 46, 70, 74, 70, 58, 74, 70, 75, 72, 58, 75, 72, 77, 74, 46, 77, 74, 79, 75, 46, 79, 75, 80, 77, 46, 80, 77, 79, 75, 46, 79, 75, 77, 74, 46, 77, 74, 79, 75, 46, 79, 75, 80, 77, 46, 80, 77, 77, 74, 46, 77, 74, 79, 73, 51, 73, 72, 79, 72, 75, 73, 73, 82, 51, 75, 82, 75, 72, 56, 72, 80, 75, 56, 80, 75, 73, 55, 75, 77, 73, 77, 75, 72, 75, 77, 55, 72, 77, 75, 73, 51, 75, 77, 73, 77, 75, 82, 75, 77, 51, 82, 77, 75, 72, 56, 75, 77, 72, 77, 75, 80, 75, 77, 56, 80, 77, 75, 70, 51, 75, 77, 70, 77, 75, 75, 75, 77]\n"
     ]
    }
   ],
   "source": [
    "import mido\n",
    "\n",
    "#outport = mido.open_output()\n",
    "niter = 0\n",
    "noteslst = []\n",
    "for msg in mido.MidiFile('bjs1031c.mid').play():\n",
    "    #outport.send(msg)\n",
    "    #print msg.bytes()\n",
    "    #print str(msg)\n",
    "    thismsg = str(msg).split(\" \") # ['note_on', 'channel=0', 'note=60', 'velocity=61', 'time=0.00168918958333']\n",
    "    # The velocity specifies the volume or force, with which the note is played\n",
    "    if thismsg[0] == 'note_on':\n",
    "        #print thismsg[2][5:]\n",
    "        noteslst.append(int(thismsg[2][5:]))\n",
    "    niter += 1\n",
    "    if niter >= 3000:\n",
    "        break\n",
    "print noteslst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e', 's')\n",
      "('es', 't')\n",
      "('est', '</w>')\n",
      "('l', 'o')\n",
      "('lo', 'w')\n",
      "('n', 'e')\n",
      "('ne', 'w')\n",
      "('new', 'est</w>')\n",
      "('low', '</w>')\n",
      "('w', 'i')\n"
     ]
    }
   ],
   "source": [
    "# BPE\n",
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "if __name__=='__main__':\n",
    "    vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,'n e w e s t </w>':6, 'w i d e s t </w>':3}\n",
    "    num_merges = 10\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "        print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-52fe8f31a26a>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-52fe8f31a26a>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    print 'data has %d characters, %d unique.' % (data_size, vocab_size)\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# from https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "\n",
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "nruns = 50000\n",
    "noutput = 5000\n",
    "\n",
    "# data I/O\n",
    "data = open('notes0.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % noutput == 0 or n == nruns-1:\n",
    "    sample_ix = sample(hprev, inputs[0], 1500)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame.midi\n",
    "import time\n",
    "import random\n",
    "\n",
    "pygame.midi.init()\n",
    "player = pygame.midi.Output(0)\n",
    "player.set_instrument(0) # https://pjb.com.au/muscript/gm.html\n",
    "# channel - note - velocity - time\n",
    "notel = ['0:70:0', '0:74:0', '1:74:80', '1:79:80', '1:82:0', '2:43:0', '0:74:80', '2:46:80', '1:75:0', '1:72:80', '1:74:0', '1:72:0', '1:80:80', '2:53:80', '0:75:80', '2:53:80', '1:82:80', '2:57:0', '1:69:80', '1:72:80', '1:72:0', '0:75:80', '1:69:0', '0:74:0', '0:75:80', '2:53:80', '0:74:80', '2:46:80', '0:72:80', '2:48:0', '1:69:80', '2:53:0', '0:74:80', '0:75:80', '2:53:80', '0:75:80', '2:53:80', '1:74:80', '2:55:80', '2:53:0', '0:75:80', '1:79:80', '1:79:80', '2:46:0', '2:51:80', '0:72:0', '1:69:0', '1:75:80', '0:77:0', '0:72:0', '1:74:0', '0:74:0', '1:74:80', '0:74:80', '1:74:80', '2:53:0', '0:75:0', '1:74:0', '1:72:0', '2:70:80', '1:69:80', '2:41:0', '1:70:80', '0:72:0', '1:74:0', '1:69:0', '1:75:0', '0:70:80', '1:75:0', '1:74:80', '2:50:80', '0:70:0', '0:69:80', '2:53:80', '2:51:0', '2:55:0', '0:84:80', '1:75:0', '0:79:80', '2:51:80', '0:72:0', '0:72:80', '1:70:0', '1:74:0', '0:70:80', '2:48:80', '0:72:80', '1:72:0', '0:69:0', '2:53:0', '1:74:0', '1:75:0', '1:70:80', '1:75:80', '1:79:80', '1:65:80', '1:75:0', '2:51:80', '0:75:80', '2:41:80', '1:70:0', '1:82:0', '1:74:0', '1:75:80', '2:51:80', '0:75:0', '1:69:80', '1:79:80', '1:75:0', '1:72:0', '1:74:80', '2:51:80', '0:77:80', '2:51:80', '0:77:80', '2:51:0', '1:77:80', '1:69:80', '0:75:80', '2:53:0', '0:79:0', '0:75:80', '2:41:80', '2:58:0', '0:72:80', '1:84:80', '1:75:0', '1:81:80', '2:51:0', '0:69:80', '1:72:80', '2:51:0', '0:74:80', '2:50:80', '2:53:0', '0:69:0', '2:51:0', '1:79:80', '2:53:0', '1:79:80', '1:67:0', '0:79:80']\n",
    "for n in notel:\n",
    "    ns = n.split(':')\n",
    "    #print ns\n",
    "    player.note_on(int(ns[1]), int(ns[2]), int(ns[0])) # note_on(note, velocity=None, channel = 0) https://en.scratch-wiki.info/wiki/MIDI_Notes\n",
    "    time.sleep(int(ns[2])/200)\n",
    "    player.note_off(int(ns[1]), int(ns[2]), int(ns[0]))\n",
    "del player\n",
    "pygame.midi.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2:51:80', '1:63:80', '1:63:0', '1:67:80', '1:67:0', '1:70:80', '2:51:0', '1:70:0', '1:75:80', '2:55:80', '1:75:0', '1:70:80', '2:55:0', '1:70:0', '1:72:80', '2:56:80', '1:72:0', '0:80:80', '1:84:80', '2:56:0', '0:80:0', '1:84:0', '0:79:80', '1:82:80', '2:58:80', '0:79:0', '1:82:0', '0:77:80', '1:80:80', '2:58:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:46:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:46:0', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:51:80', '1:79:0', '1:63:80', '0:75:0', '1:63:0', '0:63:80', '1:67:80', '1:67:0', '1:70:80', '2:51:0', '0:63:0', '1:70:0', '0:63:80', '1:75:80', '2:55:80', '1:75:0', '1:70:80', '0:63:0', '2:55:0', '1:70:0', '0:63:80', '1:72:80', '2:56:80', '0:63:0', '1:72:0', '0:80:80', '1:84:80', '2:56:0', '0:80:0', '1:84:0', '0:79:80', '1:82:80', '2:58:80', '0:79:0', '1:82:0', '0:77:80', '1:80:80', '2:58:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:46:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:46:0', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:51:80', '1:79:0', '1:82:80', '0:75:0', '1:82:0', '0:82:80', '1:80:80', '1:80:0', '1:79:80', '2:51:0', '0:82:0', '1:79:0', '0:84:80', '1:77:80', '2:56:80', '1:77:0', '1:75:80', '0:84:0', '2:56:0', '1:75:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '1:74:0', '2:58:0', '0:79:80', '1:75:80', '2:63:80', '2:63:0', '2:58:80', '2:58:0', '2:55:80', '0:79:0', '1:75:0', '2:55:0', '0:80:80', '1:77:80', '2:51:80', '0:80:0', '1:77:0', '0:82:80', '1:79:80', '2:51:0', '0:82:0', '1:79:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '1:74:0', '2:58:0', '0:79:80', '1:75:80', '2:63:80', '2:63:0', '2:58:80', '2:58:0', '2:55:80', '0:79:0', '1:75:0', '2:55:0', '0:80:80', '1:77:80', '2:51:80', '0:80:0', '1:77:0', '0:82:80', '1:79:80', '2:51:0', '0:82:0', '1:79:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '1:74:0', '0:75:80', '1:72:80', '2:58:0', '0:75:0', '1:72:0', '0:74:80', '1:70:80', '2:46:80', '0:74:0', '1:70:0', '0:72:80', '1:68:80', '2:46:0', '0:72:0', '1:68:0', '0:70:80', '1:67:80', '2:58:80', '0:70:0', '1:67:0', '0:68:80', '1:65:80', '2:58:0', '0:68:0', '1:65:0', '0:67:80', '1:63:80', '2:51:80', '0:67:0', '0:63:80', '0:63:0', '0:67:80', '0:67:0', '0:70:80', '1:63:0', '2:51:0', '0:70:0', '0:75:80', '1:70:80', '2:55:80', '0:75:0', '1:70:0', '0:70:80', '1:67:80', '2:55:0', '0:70:0', '1:67:0', '0:72:80', '1:63:80', '2:56:80', '0:72:0', '1:63:0', '0:84:80', '1:80:80', '2:56:0', '0:84:0', '1:80:0', '0:82:80', '1:79:80', '2:58:80', '0:82:0', '1:79:0', '0:80:80', '1:77:80', '2:58:0', '0:80:0', '1:77:0', '0:79:80', '1:75:80', '2:46:80', '0:79:0', '1:75:0', '0:77:80', '1:74:80', '2:46:0', '0:77:0', '1:74:0', '0:79:80', '1:75:80', '2:51:80', '0:79:0', '0:63:80', '1:75:0', '0:63:0', '0:67:80', '1:63:80', '0:67:0', '0:70:80', '2:51:0', '1:63:0', '0:70:0', '0:75:80', '1:63:80', '2:55:80', '0:75:0', '0:70:80', '1:63:0', '2:55:0', '0:70:0', '0:72:80', '1:63:80', '2:56:80', '0:72:0', '1:63:0', '0:84:80', '1:80:80', '2:56:0', '0:84:0', '1:80:0', '0:82:80', '1:79:80', '2:58:80', '0:82:0', '1:79:0', '0:80:80', '1:77:80', '2:58:0', '0:80:0', '1:77:0', '0:79:80', '1:75:80', '2:46:80', '0:79:0', '1:75:0', '0:77:80', '1:74:80', '2:46:0', '0:77:0', '1:74:0', '0:79:80', '1:75:80', '2:51:80', '0:79:0', '0:77:80', '1:75:0', '2:51:0', '0:77:0', '0:75:80', '1:79:80', '2:51:80', '0:75:0', '0:77:80', '1:79:0', '2:51:0', '0:77:0', '0:79:80', '1:75:80', '2:51:80', '0:79:0', '0:80:80', '1:75:0', '2:51:0', '0:80:0', '0:82:80', '1:77:80', '2:50:80', '0:82:0', '0:77:80', '1:77:0', '2:50:0', '0:77:0', '0:74:80', '1:82:80', '2:50:80', '0:74:0', '0:70:80', '1:82:0', '2:50:0', '0:70:0', '0:77:80', '2:50:80', '0:77:0', '0:74:80', '2:50:0', '0:74:0', '0:75:80', '2:48:80', '0:75:0', '0:74:80', '2:48:0', '0:74:0', '0:72:80', '1:75:80', '2:48:80', '0:72:0', '0:74:80', '1:75:0', '2:48:0', '0:74:0', '0:75:80', '1:72:80', '2:48:80', '0:75:0', '0:77:80', '1:72:0', '2:48:0', '0:77:0', '0:79:80', '1:74:80', '2:46:80', '0:79:0', '0:74:80', '1:74:0', '2:46:0', '0:74:0', '0:70:80', '1:79:80', '2:46:80', '0:70:0', '0:67:80', '1:79:0', '2:46:0', '0:67:0', '0:74:80', '2:46:80', '0:74:0', '0:70:80', '2:46:0', '0:70:0', '0:72:80', '2:44:80', '0:72:0', '0:68:80', '2:44:0', '0:68:0', '0:75:80', '1:72:80', '2:56:80', '0:75:0', '0:72:80', '1:72:0', '2:56:0', '0:72:0', '0:70:80', '1:75:80', '2:56:80', '0:70:0', '0:68:80', '2:56:0', '0:68:0', '0:70:80', '2:55:80', '0:70:0', '0:67:80', '1:75:0', '2:55:0', '0:67:0', '0:75:80', '1:70:80', '2:55:80', '0:75:0', '0:70:80', '1:70:0', '2:55:0', '0:70:0', '0:68:80', '1:75:80', '2:55:80', '0:68:0', '0:67:80', '1:75:0', '2:55:0', '0:67:0', '0:68:80', '1:74:80', '2:53:80', '0:68:0', '1:74:0', '0:67:80', '1:75:80', '2:53:0', '0:67:0', '1:75:0', '0:68:80', '1:77:80', '2:50:80', '0:68:0', '0:70:80', '1:77:0', '2:50:0', '0:70:0', '0:67:80', '1:75:80', '2:51:80', '0:67:0', '0:68:80', '1:75:0', '2:51:0', '0:68:0', '0:65:80', '1:74:80', '2:46:80', '0:65:0', '0:72:80', '1:74:0', '2:46:0', '0:72:0', '0:70:80', '2:50:80', '0:70:0', '0:68:80', '2:50:0', '0:68:0', '0:67:80', '2:46:80', '0:67:0', '0:65:80', '2:46:0', '0:65:0', '0:67:80', '2:51:80', '0:67:0', '0:77:80', '2:51:0', '0:77:0', '0:75:80', '2:48:80', '0:75:0', '0:74:80', '2:48:0', '0:74:0', '0:72:80', '2:51:80', '0:72:0', '0:70:80', '2:51:0', '0:70:0', '0:69:80', '2:53:80', '0:69:0', '0:79:80', '2:53:0', '0:79:0', '0:77:80', '2:57:80', '0:77:0', '0:75:80', '2:57:0', '0:75:0', '0:74:80', '2:53:80', '0:74:0', '0:72:80', '2:53:0', '0:72:0', '0:74:80', '2:58:80', '1:65:80', '0:74:0', '2:58:0', '1:65:0', '0:74:80', '1:70:80', '2:58:80', '1:70:0', '1:72:80', '0:74:0', '2:58:0', '1:72:0', '0:70:80', '1:74:80', '2:58:80', '1:74:0', '1:75:80', '0:70:0', '2:58:0', '1:75:0', '0:72:80', '1:77:80', '2:57:80', '1:77:0', '1:72:80', '0:72:0', '2:57:0', '1:72:0', '0:77:80', '1:69:80', '2:57:80', '1:69:0', '1:65:80', '0:77:0', '2:57:0', '1:65:0', '1:72:80', '2:57:80', '1:72:0', '1:69:80', '2:57:0', '1:69:0', '1:70:80', '2:55:80', '1:70:0', '1:69:80', '2:55:0', '1:69:0', '0:70:80', '1:67:80', '2:55:80', '1:67:0', '1:69:80', '0:70:0', '2:55:0', '1:69:0', '0:67:80', '1:70:80', '2:55:80', '1:70:0', '1:72:80', '0:67:0', '2:55:0', '1:72:0', '0:69:80', '1:74:80', '2:53:80', '1:74:0', '1:69:80', '0:69:0', '2:53:0', '1:69:0', '0:74:80', '1:65:80', '2:53:80', '1:65:0', '1:62:80', '0:74:0', '2:53:0', '1:62:0', '1:69:80', '2:53:80', '1:69:0', '1:65:80', '2:53:0', '1:65:0', '1:67:80', '2:51:80', '1:67:0', '1:63:80', '2:51:0', '1:63:0', '0:67:80', '1:70:80', '2:51:80', '1:70:0', '1:67:80', '0:67:0', '2:51:0', '1:67:0', '0:70:80', '1:65:80', '2:51:80', '1:65:0', '1:63:80', '2:51:0', '1:63:0', '1:65:80', '2:50:80', '1:65:0', '1:62:80', '0:70:0', '2:50:0', '1:62:0', '0:65:80', '1:70:80', '2:50:80', '1:70:0', '1:65:80', '0:65:0', '2:50:0', '1:65:0', '0:70:80', '1:63:80', '2:50:80', '1:63:0', '1:62:80', '0:70:0', '2:50:0', '1:62:0', '0:69:80', '1:63:80', '2:48:80', '0:69:0', '1:63:0', '0:70:80', '1:62:80', '2:48:0', '0:70:0', '1:62:0', '0:72:80', '1:63:80', '2:45:80', '1:63:0', '1:65:80', '0:72:0', '2:45:0', '1:65:0', '0:70:80', '1:62:80', '2:46:80', '1:62:0', '1:63:80', '0:70:0', '2:46:0', '1:63:0', '0:79:80', '1:60:80', '2:51:80', '0:79:0', '0:80:80', '1:60:0', '2:51:0', '0:80:0', '0:79:80', '1:72:80', '2:51:80', '0:79:0', '0:77:80', '1:72:0', '2:51:0', '0:77:0', '0:75:80', '2:51:80', '0:75:0', '0:74:80', '2:51:0', '0:74:0', '0:75:80', '1:79:80', '2:51:80', '0:75:0', '1:79:0', '0:77:80', '1:80:80', '2:51:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:51:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:51:0', '0:74:0', '1:77:0', '0:72:80', '1:75:80', '2:51:80', '0:72:0', '1:75:0', '0:70:80', '1:74:80', '2:51:0', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:53:80', '1:72:0', '1:77:80', '0:69:0', '1:77:0', '0:77:80', '1:81:80', '1:81:0', '1:72:80', '2:53:0', '0:77:0', '1:72:0', '0:77:80', '1:74:80', '2:58:80', '1:74:0', '1:82:80', '0:77:0', '2:58:0', '1:82:0', '0:77:80', '1:75:80', '2:57:80', '0:77:0', '0:79:80', '1:75:0', '0:79:0', '0:77:80', '1:74:80', '0:77:0', '0:79:80', '2:57:0', '1:74:0', '0:79:0', '0:77:80', '1:75:80', '2:53:80', '0:77:0', '0:79:80', '1:75:0', '0:79:0', '0:77:80', '1:84:80', '0:77:0', '0:79:80', '2:53:0', '1:84:0', '0:79:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '0:79:80', '1:74:0', '0:79:0', '0:77:80', '1:82:80', '0:77:0', '0:79:80', '2:58:0', '1:82:0', '0:79:0', '0:77:80', '1:72:80', '2:53:80', '0:77:0', '0:79:80', '1:72:0', '0:79:0', '0:77:80', '1:77:80', '0:77:0', '0:79:80', '1:77:0', '0:79:0', '0:77:80', '1:81:80', '0:77:0', '0:79:80', '1:81:0', '0:79:0', '0:77:80', '1:72:80', '2:53:0', '0:77:0', '0:79:80', '1:72:0', '0:79:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '0:79:80', '1:74:0', '0:79:0', '0:77:80', '1:82:80', '0:77:0', '0:79:80', '2:58:0', '1:82:0', '0:79:0', '0:77:80', '1:75:80', '2:57:80', '0:77:0', '0:79:80', '1:75:0', '0:79:0', '0:77:80', '1:74:80', '0:77:0', '0:79:80', '2:57:0', '1:74:0', '0:79:0', '0:77:80', '1:75:80', '2:53:80', '0:77:0', '0:79:80', '1:75:0', '0:79:0', '0:77:80', '1:84:80', '0:77:0', '0:79:80', '2:53:0', '1:84:0', '0:79:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '0:79:80', '1:74:0', '0:79:0', '0:77:80', '1:82:80', '0:77:0', '0:79:80', '2:58:0', '1:82:0', '0:79:0', '0:77:80', '1:72:80', '2:53:80', '1:72:0', '1:77:80', '0:77:0', '2:53:0', '1:77:0', '0:72:80', '1:81:80', '2:41:80', '0:72:0', '1:81:0', '0:70:80', '1:79:80', '2:41:0', '0:70:0', '1:79:0', '0:72:80', '1:81:80', '2:41:80', '0:72:0', '1:81:0', '0:74:80', '1:82:80', '2:41:0', '0:74:0', '1:82:0', '0:75:80', '1:84:80', '2:41:80', '0:75:0', '1:84:0', '0:74:80', '1:82:80', '2:41:0', '0:74:0', '1:82:0', '0:72:80', '1:81:80', '2:41:80', '0:72:0', '1:81:0', '0:75:80', '1:79:80', '2:41:0', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:41:80', '0:74:0', '1:77:0', '0:72:80', '1:75:80', '2:41:0', '0:72:0', '1:75:0', '1:74:80', '2:46:80', '0:70:80', '1:74:0', '0:70:0', '0:74:80', '1:70:80', '0:74:0', '0:65:80', '2:46:0', '1:70:0', '0:65:0', '0:67:80', '1:70:80', '2:51:80', '0:67:0', '0:75:80', '1:70:0', '2:51:0', '0:75:0', '0:68:80', '1:70:80', '2:50:80', '1:70:0', '1:72:80', '0:68:0', '0:67:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:50:0', '0:67:0', '0:68:80', '1:72:0', '1:70:80', '2:46:80', '1:70:0', '1:72:80', '0:68:0', '0:77:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:46:0', '0:77:0', '0:67:80', '1:72:0', '1:70:80', '2:51:80', '1:70:0', '1:72:80', '0:67:0', '0:75:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:51:0', '0:75:0', '0:65:80', '1:72:0', '1:70:80', '2:46:80', '1:70:0', '1:72:80', '0:65:0', '0:70:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '0:70:0', '0:74:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '0:74:0', '0:65:80', '1:72:0', '1:70:80', '2:46:0', '1:70:0', '1:72:80', '0:65:0', '0:67:80', '1:72:0', '1:70:80', '2:51:80', '1:70:0', '1:72:80', '0:67:0', '0:75:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:51:0', '0:75:0', '0:68:80', '1:72:0', '1:70:80', '2:50:80', '1:70:0', '1:72:80', '0:68:0', '0:67:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:50:0', '0:67:0', '0:68:80', '1:72:0', '1:70:80', '2:46:80', '1:70:0', '1:72:80', '0:68:0', '0:77:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:46:0', '0:77:0', '0:67:80', '1:72:0', '1:70:80', '2:51:80', '1:70:0', '1:72:80', '0:67:0', '0:75:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:51:0', '0:75:0', '0:65:80', '1:72:0', '1:70:80', '2:46:80', '0:65:0', '0:70:80', '1:70:0', '2:46:0', '0:70:0', '0:74:80', '1:70:80', '2:46:80', '0:74:0', '1:70:0', '0:75:80', '1:72:80', '2:46:0', '0:75:0', '1:72:0', '0:77:80', '1:74:80', '2:46:80', '0:77:0', '1:74:0', '0:79:80', '1:75:80', '2:46:0', '0:79:0', '1:75:0', '0:80:80', '1:77:80', '2:46:80', '0:80:0', '1:77:0', '0:79:80', '1:75:80', '2:46:0', '0:79:0', '1:75:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '1:74:0', '0:80:80', '1:77:80', '2:58:0', '0:80:0', '1:77:0', '0:79:80', '1:75:80', '2:46:80', '0:79:0', '1:75:0', '0:77:80', '1:74:80', '2:46:0', '0:77:0', '1:74:0', '0:79:80', '1:75:80', '2:51:80', '0:79:0', '0:77:80', '1:75:0', '2:51:0', '0:77:0', '0:75:80', '1:79:80', '2:48:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:48:0', '0:74:0', '1:77:0', '0:72:80', '1:75:80', '2:51:80', '0:72:0', '1:75:0', '0:70:80', '1:74:80', '2:51:0', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:53:80', '0:69:0', '1:72:0', '0:70:80', '1:70:80', '2:53:0', '0:70:0', '1:70:0', '0:72:80', '1:69:80', '2:41:80', '0:72:0', '1:69:0', '0:75:80', '1:72:80', '2:41:0', '0:75:0', '1:72:0', '0:74:80', '1:70:80', '2:53:80', '0:74:0', '1:70:0', '0:72:80', '1:69:80', '2:53:0', '0:72:0', '1:69:0', '0:74:80', '1:70:80', '2:58:80', '0:74:0', '1:70:0', '0:75:80', '1:72:80', '2:58:0', '0:75:0', '1:72:0', '0:77:80', '1:74:80', '2:46:80', '2:46:0', '2:48:80', '2:48:0', '2:50:80', '2:50:0', '2:51:80', '2:51:0', '2:53:80', '0:77:0', '1:74:0', '0:72:80', '1:69:80', '2:53:0', '0:72:0', '1:69:0', '0:72:80', '1:69:80', '2:41:80', '0:72:0', '1:69:0', '0:75:80', '1:72:80', '2:41:0', '0:75:0', '1:72:0', '0:74:80', '1:70:80', '2:53:80', '0:74:0', '1:70:0', '0:72:80', '1:69:80', '2:53:0', '0:72:0', '1:69:0', '0:74:80', '1:70:80', '2:46:80', '0:74:0', '1:70:0', '0:75:80', '1:72:80', '2:46:0', '0:75:0', '1:72:0', '0:77:80', '1:74:80', '2:46:80', '2:46:0', '2:48:80', '2:48:0', '2:50:80', '2:50:0', '2:51:80', '2:51:0', '2:53:80', '0:77:0', '1:74:0', '0:72:80', '1:69:80', '2:53:0', '0:72:0', '1:69:0', '0:72:80', '1:69:80', '2:41:80', '0:72:0', '1:69:0', '0:75:80', '1:72:80', '2:41:0', '0:75:0', '1:72:0', '0:74:80', '1:70:80', '2:53:80', '0:74:0', '1:70:0', '0:72:80', '1:69:80', '2:53:0', '0:72:0', '1:69:0', '0:74:80', '1:70:80', '2:46:80', '0:74:0', '0:75:80', '2:46:0', '0:75:0', '0:75:80', '2:48:80', '0:75:0', '0:77:80', '2:48:0', '0:77:0', '0:77:80', '2:50:80', '0:77:0', '0:79:80', '2:50:0', '0:79:0', '0:79:80', '2:51:80', '0:79:0', '0:81:80', '1:70:0', '1:75:80', '2:51:0', '0:81:0', '1:75:0', '0:81:80', '1:75:80', '2:53:80', '0:81:0', '0:82:80', '1:75:0', '2:53:0', '0:82:0', '0:82:80', '1:74:80', '2:55:80', '0:82:0', '0:70:80', '1:74:0', '2:55:0', '0:70:0', '0:79:80', '1:72:80', '2:51:80', '0:79:0', '0:75:80', '1:72:0', '2:51:0', '0:75:0', '0:74:80', '1:70:80', '2:53:80', '0:74:0', '1:70:0', '2:53:0', '0:72:80', '1:69:80', '2:41:80', '0:72:0', '1:69:0', '2:41:0', '0:70:80', '1:74:80', '2:46:80', '0:70:0', '1:74:0', '0:72:80', '1:75:80', '2:46:0', '0:72:0', '1:75:0', '0:74:80', '1:77:80', '2:46:80', '2:46:0', '2:48:80', '2:48:0', '2:50:80', '2:50:0', '2:51:80', '2:51:0', '2:53:80', '0:74:0', '1:77:0', '0:69:80', '1:72:80', '2:53:0', '0:69:0', '1:72:0', '0:69:80', '1:72:80', '2:41:80', '0:69:0', '1:72:0', '0:72:80', '1:75:80', '2:41:0', '0:72:0', '1:75:0', '0:70:80', '1:74:80', '2:53:80', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:53:0', '0:69:0', '1:72:0', '0:70:80', '1:74:80', '2:46:80', '0:70:0', '1:74:0', '0:72:80', '1:75:80', '2:46:0', '0:72:0', '1:75:0', '0:74:80', '1:77:80', '2:46:80', '2:46:0', '2:48:80', '2:48:0', '2:50:80', '2:50:0', '2:51:80', '2:51:0', '2:53:80', '0:74:0', '1:77:0', '0:69:80', '1:72:80', '2:53:0', '0:69:0', '1:72:0', '0:69:80', '1:72:80', '2:41:80', '0:69:0', '1:72:0', '0:72:80', '1:75:80', '2:41:0', '0:72:0', '1:75:0', '0:70:80', '1:74:80', '2:53:80', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:53:0', '0:69:0', '1:72:0', '0:70:80', '1:74:80', '2:46:80', '0:70:0', '1:74:0', '0:74:80', '1:77:80', '2:46:0', '0:74:0', '1:77:0', '0:77:80', '1:82:80', '2:50:80', '1:82:0', '1:81:80', '2:50:0', '1:81:0', '1:79:80', '2:46:80', '1:79:0', '1:77:80', '2:46:0', '1:77:0', '1:79:80', '2:51:80', '1:79:0', '1:77:80', '0:77:0', '1:77:0', '0:75:80', '1:79:80', '2:51:0', '1:79:0', '1:81:80', '2:53:80', '0:75:0', '1:81:0', '2:53:0', '0:74:80', '1:82:80', '2:55:80', '0:74:0', '1:82:0', '2:55:0', '0:75:80', '1:79:80', '2:51:80', '0:75:0', '1:79:0', '0:72:80', '1:75:80', '2:51:0', '0:72:0', '1:75:0', '0:70:80', '1:74:80', '2:53:80', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '0:69:0', '1:72:0', '0:70:80', '1:74:80', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:53:0', '0:69:0', '1:72:0', '0:70:80', '1:74:80', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:41:80', '0:69:0', '1:72:0', '0:70:80', '1:70:80', '2:41:0', '0:70:0', '1:70:0', '0:70:80', '1:70:80', '2:46:80', '0:70:0', '1:70:0', '2:46:0', '2:46:80', '1:58:80', '1:58:0', '1:62:80', '1:62:0', '1:65:80', '2:46:0', '1:65:0', '1:70:80', '2:50:80', '1:70:0', '1:65:80', '2:50:0', '1:65:0', '1:67:80', '2:51:80', '1:67:0', '0:75:80', '1:79:80', '2:51:0', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:53:80', '0:74:0', '1:77:0', '0:72:80', '1:75:80', '2:53:0', '0:72:0', '1:75:0', '0:70:80', '1:74:80', '2:41:80', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:41:0', '0:69:0', '1:72:0', '0:70:80', '1:74:80', '2:46:80', '1:74:0', '1:58:80', '0:70:0', '1:58:0', '0:82:80', '1:62:80', '1:62:0', '1:65:80', '2:46:0', '0:82:0', '1:65:0', '0:70:80', '1:70:80', '2:50:80', '1:70:0', '1:65:80', '2:50:0', '1:65:0', '1:67:80', '2:51:80', '0:70:0', '1:67:0', '0:75:80', '1:79:80', '2:51:0', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:53:80', '0:74:0', '1:77:0', '0:72:80', '1:75:80', '2:53:0', '0:72:0', '1:75:0', '0:70:80', '1:74:80', '2:41:80', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:41:0', '0:69:0', '1:72:0', '0:70:80', '1:74:80', '2:46:80', '1:74:0', '1:77:80', '0:70:0', '1:77:0', '0:77:80', '1:75:80', '1:75:0', '1:74:80', '2:46:0', '0:77:0', '1:74:0', '0:79:80', '1:72:80', '2:51:80', '1:72:0', '1:70:80', '0:79:0', '2:51:0', '1:70:0', '0:72:80', '1:69:80', '2:53:80', '0:72:0', '1:69:0', '2:53:0', '0:74:80', '1:70:80', '2:58:80', '2:58:0', '2:53:80', '2:53:0', '2:50:80', '0:74:0', '1:70:0', '2:50:0', '0:75:80', '1:72:80', '2:46:80', '0:75:0', '1:72:0', '0:77:80', '1:74:80', '2:46:0', '0:77:0', '1:74:0', '0:72:80', '1:69:80', '2:53:80', '0:72:0', '1:69:0', '2:53:0', '0:74:80', '1:70:80', '2:58:80', '2:58:0', '2:53:80', '2:53:0', '2:50:80', '0:74:0', '1:70:0', '2:50:0', '0:75:80', '1:72:80', '2:46:80', '0:75:0', '1:72:0', '0:77:80', '1:74:80', '2:46:0', '0:77:0', '1:74:0', '0:72:80', '1:69:80', '2:53:80', '0:72:0', '0:65:80', '2:53:0', '2:51:80', '1:69:0', '0:65:0', '2:51:0', '0:77:80', '2:50:80', '2:50:0', '2:48:80', '2:48:0', '2:47:80', '0:77:0', '0:80:80', '0:80:0', '0:79:80', '1:74:80', '0:79:0', '0:77:80', '0:77:0', '0:75:80', '0:75:0', '2:47:0', '0:74:80', '1:74:0', '0:74:0', '0:75:80', '1:74:80', '2:48:80', '0:75:0', '2:48:0', '0:74:80', '2:47:80', '1:74:0', '0:74:0', '2:47:0', '0:75:80', '1:67:80', '2:48:80', '0:75:0', '2:48:0', '0:77:80', '2:50:80', '1:67:0', '0:77:0', '2:50:0', '0:79:80', '1:72:80', '2:51:80', '1:72:0', '2:51:0', '1:70:80', '2:48:80', '1:70:0', '2:48:0', '1:68:80', '2:53:80', '0:79:0', '1:68:0', '0:77:80', '1:80:80', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '0:74:0', '1:77:0', '2:53:0', '0:72:80', '1:75:80', '0:72:0', '1:75:0', '0:71:80', '1:74:80', '2:55:80', '0:71:0', '1:74:0', '2:55:0', '0:72:80', '1:75:80', '2:60:80', '2:60:0', '2:55:80', '2:55:0', '2:51:80', '0:72:0', '1:75:0', '2:51:0', '0:74:80', '1:77:80', '2:48:80', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:48:0', '0:75:0', '1:79:0', '0:71:80', '1:74:80', '2:55:80', '0:71:0', '1:74:0', '2:55:0', '0:72:80', '1:75:80', '2:60:80', '2:60:0', '2:55:80', '2:55:0', '2:51:80', '0:72:0', '1:75:0', '2:51:0', '0:74:80', '1:77:80', '2:48:80', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:48:0', '0:75:0', '1:79:0', '0:71:80', '1:74:80', '2:55:80', '0:71:0', '0:79:80', '1:74:0', '2:55:0', '0:79:0', '0:79:80', '2:59:80', '0:79:0', '0:77:80', '2:59:0', '0:77:0', '0:77:80', '2:55:80', '0:77:0', '0:75:80', '2:55:0', '0:75:0', '0:74:80', '2:60:80', '1:84:80', '0:74:0', '2:60:0', '1:84:0', '0:76:80', '1:84:80', '2:48:80', '1:84:0', '1:82:80', '2:48:0', '1:82:0', '1:82:80', '2:60:80', '1:82:0', '1:80:80', '0:76:0', '2:60:0', '1:80:0', '1:80:80', '2:53:80', '0:77:80', '1:80:0', '2:53:0', '0:77:0', '0:77:80', '1:81:80', '2:41:80', '0:77:0', '0:75:80', '2:41:0', '0:75:0', '0:75:80', '2:53:80', '0:75:0', '0:74:80', '1:81:0', '2:53:0', '0:74:0', '0:74:80', '2:58:80', '1:82:80', '0:74:0', '2:58:0', '1:82:0', '0:74:80', '1:82:80', '2:46:80', '1:82:0', '1:80:80', '2:46:0', '1:80:0', '1:80:80', '2:58:80', '1:80:0', '1:79:80', '0:74:0', '2:58:0', '1:79:0', '0:75:80', '1:79:80', '2:51:80', '1:79:0', '1:79:80', '0:75:0', '2:51:0', '1:79:0', '0:63:80', '1:80:80', '2:48:80', '1:80:0', '1:79:80', '0:63:0', '2:48:0', '1:79:0', '0:75:80', '1:77:80', '2:60:80', '1:77:0', '1:75:80', '0:75:0', '1:75:0', '0:74:80', '1:77:80', '1:77:0', '1:77:80', '0:74:0', '2:60:0', '1:77:0', '0:75:80', '1:79:80', '2:59:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '0:74:0', '0:75:80', '0:75:0', '0:74:80', '0:74:0', '0:75:80', '1:77:0', '0:75:0', '0:74:80', '1:75:80', '1:75:0', '1:74:80', '2:59:0', '0:74:0', '1:74:0', '1:75:80', '2:60:80', '0:79:80', '1:75:0', '2:60:0', '0:79:0', '0:80:80', '1:63:80', '2:48:80', '0:80:0', '0:79:80', '1:63:0', '2:48:0', '0:79:0', '0:77:80', '1:75:80', '2:60:80', '0:77:0', '0:75:80', '1:75:0', '2:60:0', '0:75:0', '0:77:80', '1:74:80', '2:60:80', '0:77:0', '0:77:80', '1:74:0', '2:60:0', '0:77:0', '0:79:80', '1:74:80', '2:59:80', '0:79:0', '0:77:80', '0:77:0', '0:75:80', '0:75:0', '0:74:80', '1:74:0', '2:59:0', '0:74:0', '0:75:80', '2:60:80', '0:75:0', '1:60:80', '2:60:0', '2:58:80', '1:60:0', '2:58:0', '1:72:80', '2:57:80', '2:57:0', '2:55:80', '2:55:0', '2:54:80', '1:72:0', '1:75:80', '1:75:0', '0:69:80', '1:74:80', '1:74:0', '1:72:80', '1:72:0', '1:70:80', '1:70:0', '2:54:0', '1:69:80', '1:69:0', '1:70:80', '2:55:80', '1:70:0', '2:55:0', '1:69:80', '2:54:80', '0:69:0', '1:69:0', '2:54:0', '0:62:80', '1:70:80', '2:55:80', '1:70:0', '2:55:0', '1:72:80', '2:57:80', '0:62:0', '1:72:0', '2:57:0', '0:67:80', '1:74:80', '2:59:80', '0:67:0', '2:59:0', '0:65:80', '2:55:80', '0:65:0', '2:55:0', '0:63:80', '2:60:80', '0:63:0', '1:74:0', '2:60:0', '0:75:80', '1:72:80', '2:48:80', '0:75:0', '1:72:0', '0:74:80', '1:70:80', '0:74:0', '1:70:0', '0:72:80', '1:69:80', '0:72:0', '1:69:0', '0:70:80', '1:67:80', '2:48:0', '0:70:0', '1:67:0', '0:69:80', '1:66:80', '2:50:80', '0:69:0', '1:66:0', '2:50:0', '0:70:80', '1:67:80', '2:55:80', '2:55:0', '2:50:80', '2:50:0', '2:46:80', '0:70:0', '1:67:0', '2:46:0', '0:72:80', '1:69:80', '2:43:80', '0:72:0', '1:69:0', '0:74:80', '1:70:80', '2:43:0', '0:74:0', '1:70:0', '0:69:80', '1:66:80', '2:50:80', '0:69:0', '1:66:0', '2:50:0', '0:70:80', '1:67:80', '2:55:80', '2:55:0', '2:50:80', '2:50:0', '2:46:80', '0:70:0', '1:67:0', '2:46:0', '0:72:80', '1:69:80', '2:43:80', '0:72:0', '1:69:0', '0:74:80', '1:70:80', '2:43:0', '0:74:0', '1:70:0', '0:69:80', '1:66:80', '2:50:80', '1:66:0', '1:74:80', '0:69:0', '2:50:0', '1:74:0', '1:74:80', '2:54:80', '1:74:0', '1:72:80', '2:54:0', '1:72:0', '1:72:80', '2:50:80', '1:72:0', '1:70:80', '2:50:0', '1:70:0', '1:70:80', '2:55:80', '0:79:80', '1:70:0', '2:55:0', '0:79:0', '0:79:80', '1:71:80', '2:43:80', '0:79:0', '0:77:80', '2:43:0', '0:77:0', '0:77:80', '2:55:80', '0:77:0', '0:75:80', '1:71:0', '2:55:0', '0:75:0', '0:75:80', '2:48:80', '1:72:80', '0:75:0', '2:48:0', '1:72:0', '0:76:80', '1:72:80', '2:60:80', '1:72:0', '1:70:80', '2:60:0', '1:70:0', '1:70:80', '2:48:80', '1:70:0', '1:69:80', '0:76:0', '2:48:0', '1:69:0', '1:69:80', '2:53:80', '0:77:80', '2:53:0', '0:77:0', '0:77:80', '2:41:80', '0:77:0', '0:75:80', '2:41:0', '0:75:0', '0:75:80', '2:53:80', '0:75:0', '1:69:0', '0:74:80', '2:53:0', '0:74:0', '0:74:80', '1:70:80', '2:46:80', '0:74:0', '0:74:80', '2:46:0', '0:74:0', '0:75:80', '2:43:80', '0:75:0', '0:74:80', '2:43:0', '0:74:0', '0:72:80', '2:55:80', '0:72:0', '1:70:0', '0:70:80', '2:55:0', '0:70:0', '0:72:80', '1:69:80', '2:55:80', '0:72:0', '0:72:80', '1:69:0', '2:55:0', '0:72:0', '0:74:80', '1:69:80', '2:54:80', '0:74:0', '0:72:80', '0:72:0', '0:70:80', '0:70:0', '0:69:80', '1:69:0', '2:54:0', '0:69:0', '0:70:80', '2:55:80', '1:74:80', '0:70:0', '2:55:0', '1:74:0', '0:70:80', '1:75:80', '2:43:80', '1:75:0', '1:74:80', '2:43:0', '1:74:0', '1:72:80', '2:55:80', '1:72:0', '1:70:80', '0:70:0', '2:55:0', '1:70:0', '0:69:80', '1:72:80', '2:55:80', '1:72:0', '1:72:80', '0:69:0', '2:55:0', '1:72:0', '0:69:80', '1:74:80', '2:54:80', '1:74:0', '1:72:80', '1:72:0', '1:70:80', '1:70:0', '1:69:80', '0:69:0', '2:54:0', '1:69:0', '0:67:80', '1:70:80', '2:55:80', '0:67:0', '1:70:0', '0:82:80', '1:74:80', '2:55:0', '0:82:0', '1:74:0', '0:82:80', '1:74:80', '2:43:80', '0:82:0', '1:74:0', '0:81:80', '1:72:80', '2:43:0', '0:81:0', '1:72:0', '0:81:80', '1:72:80', '2:48:80', '0:81:0', '1:72:0', '0:79:80', '1:70:80', '2:48:0', '0:79:0', '1:70:0', '0:79:80', '1:70:80', '2:50:80', '0:79:0', '1:70:0', '0:78:80', '1:69:80', '0:78:0', '1:69:0', '0:79:80', '1:70:80', '0:79:0', '1:70:0', '0:78:80', '1:69:80', '2:50:0', '0:78:0', '1:69:0', '0:79:80', '1:70:80', '0:79:0', '1:70:0', '0:78:80', '1:69:80', '2:48:80', '0:78:0', '1:69:0', '2:48:0', '0:79:80', '1:67:80', '2:46:80', '0:79:0', '0:74:80', '1:67:0', '2:46:0', '0:74:0', '0:75:80', '1:67:80', '2:48:80', '0:75:0', '1:67:0', '0:72:80', '1:69:80', '2:48:0', '0:72:0', '1:69:0', '0:70:80', '1:67:80', '2:50:80', '0:70:0', '1:67:0', '0:69:80', '1:66:80', '0:69:0', '1:66:0', '0:70:80', '1:67:80', '0:70:0', '1:67:0', '0:69:80', '1:66:80', '2:50:0', '0:69:0', '1:66:0', '0:70:80', '1:67:80', '0:70:0', '1:67:0', '0:69:80', '1:66:80', '2:38:80', '0:69:0', '1:66:0', '0:67:80', '1:67:80', '2:38:0', '0:67:0', '1:67:0', '0:67:80', '1:67:80', '2:43:80', '0:67:0', '0:70:80', '2:43:0', '0:70:0', '0:72:80', '2:44:80', '0:72:0', '2:44:0', '0:74:80', '2:46:80', '1:67:0', '0:74:0', '2:46:0', '0:75:80', '2:48:80', '0:75:0', '2:48:0', '0:77:80', '2:50:80', '0:77:0', '2:50:0', '0:79:80', '2:51:80', '1:63:80', '1:63:0', '1:67:80', '1:67:0', '1:70:80', '0:79:0', '2:51:0', '1:70:0', '1:75:80', '2:55:80', '1:75:0', '1:70:80', '2:55:0', '1:70:0', '1:72:80', '2:56:80', '1:72:0', '0:80:80', '1:84:80', '2:56:0', '0:80:0', '1:84:0', '0:79:80', '1:82:80', '2:58:80', '0:79:0', '1:82:0', '0:77:80', '1:80:80', '2:58:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:46:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:46:0', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:51:80', '1:79:0', '1:63:80', '0:75:0', '1:63:0', '0:63:80', '1:67:80', '1:67:0', '1:70:80', '2:51:0', '0:63:0', '1:70:0', '0:63:80', '1:75:80', '2:55:80', '1:75:0', '1:70:80', '0:63:0', '2:55:0', '1:70:0', '0:63:80', '1:72:80', '2:56:80', '0:63:0', '1:72:0', '0:80:80', '1:84:80', '2:56:0', '0:80:0', '1:84:0', '0:79:80', '1:82:80', '2:58:80', '0:79:0', '1:82:0', '0:77:80', '1:80:80', '2:58:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:46:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:46:0', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:51:80', '0:75:0', '0:70:80', '0:70:0', '0:75:80', '0:75:0', '0:79:80', '1:79:0', '2:51:0', '0:79:0', '0:82:80', '2:55:80', '0:82:0', '0:70:80', '2:55:0', '0:70:0', '0:72:80', '2:56:80', '0:72:0', '0:82:80', '2:56:0', '0:82:0', '0:80:80', '2:53:80', '0:80:0', '0:79:80', '2:53:0', '0:79:0', '0:80:80', '0:80:0', '0:72:80', '0:72:0', '0:70:80', '2:50:80', '0:70:0', '0:80:80', '2:50:0', '0:80:0', '0:79:80', '2:51:80', '0:79:0', '0:77:80', '2:51:0', '0:77:0', '0:79:80', '0:79:0', '0:70:80', '0:70:0', '0:68:80', '2:48:80', '0:68:0', '0:79:80', '2:48:0', '0:79:0', '0:77:80', '2:50:80', '0:77:0', '0:75:80', '2:50:0', '0:75:0', '0:77:80', '2:46:80', '0:77:0', '0:68:80', '2:46:0', '0:68:0', '0:67:80', '2:51:80', '1:70:80', '0:67:0', '1:70:0', '1:75:80', '1:75:0', '1:79:80', '2:51:0', '1:79:0', '1:82:80', '2:55:80', '1:82:0', '1:70:80', '2:55:0', '1:70:0', '1:72:80', '2:56:80', '1:72:0', '1:82:80', '2:56:0', '1:82:0', '1:80:80', '2:53:80', '1:80:0', '1:79:80', '2:53:0', '1:79:0', '1:80:80', '1:80:0', '1:72:80', '1:72:0', '1:70:80', '2:50:80', '1:70:0', '1:80:80', '2:50:0', '1:80:0', '1:79:80', '2:51:80', '1:79:0', '1:77:80', '2:51:0', '1:77:0', '1:79:80', '1:79:0', '1:70:80', '1:70:0', '1:68:80', '2:48:80', '1:68:0', '1:79:80', '2:48:0', '1:79:0', '1:77:80', '2:50:80', '1:77:0', '1:75:80', '2:50:0', '1:75:0', '1:77:80', '2:46:80', '1:77:0', '1:68:80', '2:46:0', '1:68:0', '1:67:80', '2:51:80', '0:70:80', '0:70:0', '0:75:80', '1:67:0', '0:75:0', '0:73:80', '1:70:80', '2:51:0', '0:73:0', '1:70:0', '0:72:80', '1:68:80', '2:55:80', '0:72:0', '1:68:0', '0:70:80', '1:67:80', '2:55:0', '0:70:0', '1:67:0', '0:72:80', '1:68:80', '2:56:80', '0:72:0', '1:68:0', '0:73:80', '1:75:80', '2:56:0', '0:73:0', '1:75:0', '0:72:80', '1:80:80', '2:53:80', '0:72:0', '1:80:0', '0:70:80', '1:79:80', '2:53:0', '0:70:0', '1:79:0', '0:68:80', '1:77:80', '0:68:0', '1:77:0', '0:67:80', '1:75:80', '0:67:0', '1:75:0', '0:65:80', '1:74:80', '2:46:80', '0:65:0', '0:70:80', '1:74:0', '0:70:0', '0:74:80', '1:70:80', '0:74:0', '0:65:80', '2:46:0', '1:70:0', '0:65:0', '0:67:80', '1:70:80', '2:51:80', '0:67:0', '0:75:80', '1:70:0', '2:51:0', '0:75:0', '0:68:80', '1:70:80', '2:50:80', '1:70:0', '1:72:80', '0:68:0', '0:67:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:50:0', '0:67:0', '0:68:80', '1:72:0', '1:70:80', '2:46:80', '1:70:0', '1:72:80', '0:68:0', '0:77:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:46:0', '0:77:0', '0:67:80', '1:72:0', '1:70:80', '2:51:80', '1:70:0', '1:72:80', '0:67:0', '0:75:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:51:0', '0:75:0', '0:65:80', '1:72:0', '1:70:80', '2:46:80', '1:70:0', '1:72:80', '0:65:0', '0:70:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '0:70:0', '0:74:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '0:74:0', '0:65:80', '1:72:0', '1:70:80', '2:46:0', '1:70:0', '1:72:80', '0:65:0', '0:67:80', '1:72:0', '1:70:80', '2:51:80', '1:70:0', '1:72:80', '0:67:0', '0:75:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:51:0', '0:75:0', '0:68:80', '1:72:0', '1:70:80', '2:50:80', '1:70:0', '1:72:80', '0:68:0', '0:67:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:50:0', '0:67:0', '0:68:80', '1:72:0', '1:70:80', '2:46:80', '1:70:0', '1:72:80', '0:68:0', '0:77:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:46:0', '0:77:0', '0:67:80', '1:72:0', '1:70:80', '2:51:80', '1:70:0', '1:72:80', '0:67:0', '0:75:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:51:0', '0:75:0', '0:65:80', '1:72:0', '1:70:80', '2:46:80', '0:65:0', '0:70:80', '1:70:0', '2:46:0', '0:70:0', '0:74:80', '1:70:80', '2:58:80', '0:74:0', '1:70:0', '0:75:80', '1:72:80', '2:58:0', '0:75:0', '1:72:0', '0:77:80', '1:74:80', '2:46:80', '0:77:0', '1:74:0', '0:79:80', '1:75:80', '2:46:0', '0:79:0', '1:75:0', '0:80:80', '1:77:80', '2:46:80', '0:80:0', '1:77:0', '0:79:80', '1:75:80', '2:46:0', '0:79:0', '1:75:0', '0:77:80', '1:74:80', '2:46:80', '0:77:0', '1:74:0', '0:79:80', '1:75:80', '2:46:0', '0:79:0', '1:75:0', '0:80:80', '1:77:80', '2:46:80', '0:80:0', '1:77:0', '0:77:80', '1:74:80', '2:46:0', '0:77:0', '1:74:0', '0:79:80', '1:73:80', '2:51:80', '1:73:0', '1:72:80', '0:79:0', '1:72:0', '0:75:80', '1:73:80', '1:73:0', '1:82:80', '2:51:0', '0:75:0', '1:82:0', '0:75:80', '1:72:80', '2:56:80', '1:72:0', '1:80:80', '0:75:0', '2:56:0', '1:80:0', '0:75:80', '1:73:80', '2:55:80', '0:75:0', '0:77:80', '1:73:0', '0:77:0', '0:75:80', '1:72:80', '0:75:0', '0:77:80', '2:55:0', '1:72:0', '0:77:0', '0:75:80', '1:73:80', '2:51:80', '0:75:0', '0:77:80', '1:73:0', '0:77:0', '0:75:80', '1:82:80', '0:75:0', '0:77:80', '2:51:0', '1:82:0', '0:77:0', '0:75:80', '1:72:80', '2:56:80', '0:75:0', '0:77:80', '1:72:0', '0:77:0', '0:75:80', '1:80:80', '0:75:0', '0:77:80', '2:56:0', '1:80:0', '0:77:0', '0:75:80', '1:70:80', '2:51:80', '0:75:0', '0:77:80', '1:70:0', '0:77:0', '0:75:80', '1:75:80', '0:75:0', '0:77:80', '1:75:0', '0:77:0', '0:75:80', '1:79:80', '0:75:0', '0:77:80', '1:79:0', '0:77:0', '0:75:80', '1:70:80', '2:51:0', '0:75:0', '0:77:80', '1:70:0', '0:77:0', '0:75:80', '1:72:80', '2:56:80', '0:75:0', '0:77:80', '1:72:0', '0:77:0', '0:75:80', '1:80:80', '0:75:0', '0:77:80', '2:56:0', '1:80:0', '0:77:0', '0:75:80', '1:73:80', '2:55:80', '0:75:0', '0:77:80', '1:73:0', '0:77:0', '0:75:80', '1:72:80', '0:75:0', '0:77:80', '2:55:0', '1:72:0', '0:77:0', '0:75:80', '1:73:80', '2:51:80', '0:75:0', '0:77:80', '1:73:0', '0:77:0', '0:75:80', '1:82:80', '0:75:0', '0:77:80', '2:51:0', '1:82:0', '0:77:0', '0:75:80', '1:72:80', '2:56:80', '0:75:0', '0:77:80', '1:72:0', '0:77:0', '0:75:80', '1:80:80', '0:75:0', '0:77:80', '2:56:0', '1:80:0', '0:77:0', '0:75:80', '1:70:80', '2:51:80', '1:70:0', '1:75:80', '2:51:0', '1:75:0', '1:79:80', '2:39:80', '0:75:0', '1:79:0', '0:77:80', '1:80:80', '2:39:0', '0:77:0', '1:80:0', '0:79:80', '1:82:80', '2:51:80', '0:79:0', '1:82:0', '0:80:80', '1:84:80', '2:51:0', '0:80:0', '1:84:0', '0:82:80', '1:85:80', '2:51:80', '0:82:0', '1:85:0', '0:80:80', '1:84:80', '2:51:0', '0:80:0', '1:84:0', '0:79:80', '1:82:80', '2:51:80', '0:79:0', '1:82:0', '0:82:80', '1:85:80', '2:51:0', '0:82:0', '1:85:0', '0:80:80', '1:84:80', '2:51:80', '0:80:0', '1:84:0', '0:79:80', '1:82:80', '2:51:0', '0:79:0', '1:82:0', '0:80:80', '1:84:80', '2:56:80', '1:84:0', '1:82:80', '0:80:0', '2:56:0', '1:82:0', '0:84:80', '1:80:80', '2:56:80', '0:84:0', '1:80:0', '0:82:80', '1:79:80', '2:56:0', '0:82:0', '1:79:0', '0:80:80', '1:77:80', '2:56:80', '0:80:0', '1:77:0', '0:79:80', '1:75:80', '2:56:0', '0:79:0', '1:75:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '1:74:0', '0:75:80', '1:75:80', '2:58:0', '0:75:0', '1:75:0', '0:74:80', '1:77:80', '2:46:80', '0:74:0', '1:77:0', '0:77:80', '1:80:80', '2:46:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:46:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:46:0', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:51:80', '0:75:0', '1:79:0', '0:77:80', '1:80:80', '2:51:0', '0:77:0', '1:80:0', '0:79:80', '1:82:80', '2:51:80', '2:51:0', '2:53:80', '2:53:0', '2:55:80', '2:55:0', '2:56:80', '2:56:0', '2:58:80', '0:79:0', '1:82:0', '0:74:80', '1:77:80', '2:58:0', '0:74:0', '1:77:0', '0:74:80', '1:77:80', '2:46:80', '0:74:0', '1:77:0', '0:77:80', '1:80:80', '2:46:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:58:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:58:0', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:51:80', '0:75:0', '1:79:0', '0:77:80', '1:80:80', '2:51:0', '0:77:0', '1:80:0', '0:79:80', '1:82:80', '2:51:80', '2:51:0', '2:53:80', '2:53:0', '2:55:80', '2:55:0', '2:56:80', '2:56:0', '2:58:80', '0:79:0', '1:82:0', '0:74:80', '1:77:80', '2:58:0', '0:74:0', '1:77:0', '0:74:80', '1:77:80', '2:46:80', '0:74:0', '1:77:0', '0:77:80', '1:80:80', '2:46:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:58:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:58:0', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:51:80', '1:79:0', '1:80:80', '2:51:0', '1:80:0', '1:80:80', '2:53:80', '1:80:0', '1:82:80', '2:53:0', '1:82:0', '1:82:80', '2:55:80', '0:75:0', '1:82:0', '1:84:80', '2:55:0', '1:84:0', '0:75:80', '1:84:80', '2:56:80', '1:84:0', '1:74:80', '0:75:0', '0:80:80', '2:56:0', '1:74:0', '0:80:0', '0:80:80', '1:74:80', '2:58:80', '1:74:0', '1:75:80', '0:80:0', '2:58:0', '1:75:0', '0:79:80', '1:75:80', '2:60:80', '1:75:0', '1:70:80', '0:79:0', '2:60:0', '1:70:0', '0:77:80', '1:72:80', '2:56:80', '1:72:0', '1:80:80', '0:77:0', '2:56:0', '1:80:0', '0:75:80', '1:79:80', '2:58:80', '0:75:0', '1:79:0', '2:58:0', '0:74:80', '1:77:80', '2:46:80', '0:74:0', '1:77:0', '2:46:0', '0:75:80', '1:75:80', '2:51:80', '0:75:0', '1:75:0', '2:51:0', '0:70:80', '1:67:80', '2:51:80', '2:51:0', '2:53:80', '2:53:0', '2:55:80', '2:55:0', '2:56:80', '2:56:0', '2:58:80', '0:70:0', '1:67:0', '0:65:80', '1:62:80', '2:58:0', '0:65:0', '1:62:0', '0:65:80', '1:62:80', '2:46:80', '0:65:0', '1:62:0', '0:68:80', '1:65:80', '2:46:0', '0:68:0', '1:65:0', '0:67:80', '1:63:80', '2:46:80', '0:67:0', '1:63:0', '0:65:80', '1:62:80', '2:46:0', '0:65:0', '1:62:0', '0:67:80', '1:63:80', '2:51:80', '0:67:0', '1:63:0', '0:68:80', '1:65:80', '2:51:0', '0:68:0', '1:65:0', '0:70:80', '1:67:80', '2:39:80', '2:39:0', '2:41:80', '2:41:0', '2:43:80', '2:43:0', '2:44:80', '2:44:0', '2:46:80', '0:70:0', '1:67:0', '0:65:80', '1:62:80', '2:46:0', '0:65:0', '1:62:0', '0:65:80', '1:62:80', '2:46:80', '0:65:0', '1:62:0', '0:68:80', '1:65:80', '2:46:0', '0:68:0', '1:65:0', '0:67:80', '1:63:80', '2:46:80', '0:67:0', '1:63:0', '0:65:80', '1:62:80', '2:46:0', '0:65:0', '1:62:0', '0:67:80', '1:63:80', '2:51:80', '0:67:0', '0:70:80', '1:63:0', '2:51:0', '0:70:0', '0:75:80', '1:70:80', '2:55:80', '0:75:0', '0:74:80', '2:55:0', '0:74:0', '0:72:80', '2:51:80', '0:72:0', '0:70:80', '2:51:0', '0:70:0', '0:72:80', '2:44:80', '0:72:0', '0:70:80', '1:70:0', '0:70:0', '0:72:80', '1:68:80', '2:44:0', '0:72:0', '0:74:80', '2:46:80', '1:68:0', '0:74:0', '2:46:0', '0:75:80', '1:67:80', '2:48:80', '0:75:0', '0:63:80', '1:67:0', '2:48:0', '0:63:0', '0:72:80', '1:68:80', '2:44:80', '0:72:0', '1:68:0', '0:68:80', '1:65:80', '2:44:0', '0:68:0', '1:65:0', '0:67:80', '1:63:80', '2:46:80', '0:67:0', '1:63:0', '0:65:80', '1:62:80', '0:65:0', '0:67:80', '1:62:0', '1:63:80', '0:67:0', '0:65:80', '1:63:0', '1:62:80', '2:46:0', '0:65:0', '0:67:80', '1:62:0', '1:63:80', '0:67:0', '0:65:80', '1:63:0', '1:62:80', '2:46:80', '0:65:0', '0:63:80', '1:62:0', '1:63:80', '2:46:0', '0:63:0', '1:63:0', '0:63:80', '1:63:80', '2:51:80', '0:63:0', '1:63:0', '2:51:0']\n"
     ]
    }
   ],
   "source": [
    "import mido\n",
    "\n",
    "#outport = mido.open_output()\n",
    "niter = 0\n",
    "noteslst = []\n",
    "for msg in mido.MidiFile('bjs1031c.mid').play():\n",
    "    #outport.send(msg)\n",
    "    #print msg.bytes()\n",
    "    #print str(msg)\n",
    "    thismsg = str(msg).split(\" \") # ['note_on', 'channel=0', 'note=60', 'velocity=61', 'time=0.00168918958333']\n",
    "    if thismsg[0] == 'note_on':\n",
    "        #print thismsg\n",
    "        #print thismsg[2][5:]\n",
    "        noteslst.append(thismsg[1][8:]+':'+thismsg[2][5:]+':'+thismsg[3][9:]) # channel, note, velocity\n",
    "    niter += 1\n",
    "    if niter >= 5000:\n",
    "        break\n",
    "print noteslst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best: ('e', 's') : 9\n",
      "best: ('es', 't') : 9\n",
      "best: ('est', '</w>') : 9\n",
      "best: ('l', 'o') : 7\n",
      "best: ('lo', 'w') : 7\n",
      "best: ('n', 'e') : 6\n",
      "best: ('ne', 'w') : 6\n",
      "best: ('new', 'est</w>') : 6\n",
      "best: ('low', '</w>') : 5\n",
      "best: ('w', 'i') : 3\n"
     ]
    }
   ],
   "source": [
    "# Byte Pair Encoding BPE - Instead of merging frequent pairs of bytes, we merge characters or character sequences.\n",
    "# Frequent character n-grams (or whole words) are eventually merged into a single symbol\n",
    "import re, collections\n",
    "\n",
    "def wordsep(word): # returns a word split by ' ' with delimiter </w> as required by BPE\n",
    "    return ' '.join(list(word)) + ' </w>'\n",
    "\n",
    "def inp_rep(words): # turns a list of (word-string, frequency) tuples into a BPE-input dict\n",
    "    res = {}\n",
    "    for w in words:\n",
    "        res[wordsep(w[0])] = w[1]\n",
    "    return res\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int) # dict subclass - initialize the symbol vocabulary with the character vocabulary\n",
    "    for word, freq in vocab.items(): # items: return the list with all dictionary keys with values\n",
    "        symbols = word.split()\n",
    "        #symbols = list(word) # this works without spaces in between\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq # iteratively count all symbol pairs\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in): # replace each occurrence of the most frequent pair (A, B) with a new symbol AB\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair)) # Escape all the characters in pattern except ASCII letters and numbers\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word) # used to replace substrings\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # represent each word as a sequence of characters, plus a special end-ofword symbol , which allows us to restore the\n",
    "    # original tokenization after translation\n",
    "    #vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,'n e w e s t </w>':6, 'w i d e s t </w>':3}\n",
    "    #vocab = {'t h i s </w>' : 5, 'i s </w>' : 6,'a </w>':6, 't e s t </w>':3}\n",
    "    #vocab = {'low </w>' : 5, 'lower </w>' : 2,'newest </w>':6, 'widest </w>':3}\n",
    "    vocab = inp_rep([('low', 5), ('lower', 2), ('newest', 6), ('widest', 3)])\n",
    "    num_merges = 10\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if len(pairs) > 1:\n",
    "            best = max(pairs, key=pairs.get) # get returns a value for the given key - find most frequent pair\n",
    "            vocab = merge_vocab(best, vocab) # replace each occurrence of the most frequent pair (A, B) with a new symbol AB\n",
    "            print('best: %s : %s' % (best, pairs[best[0], best[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('m', 1), ('i', 4), ('s', 4), ('p', 2)])\n",
      "['mississippi']\n",
      "t e s t </w>\n",
      "{'t e s t </w>': 5, 's o m e </w>': 2, 'o n e </w>': 8, 'm i s s i s s i p p i </w>': 2}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "def wordsep(word):\n",
    "    return ' '.join(list(word)) + ' </w>'\n",
    "\n",
    "def inp_rep(words):\n",
    "    res = {}\n",
    "    for w in words:\n",
    "        res[wordsep(w[0])] = w[1]\n",
    "    return res\n",
    "\n",
    "if __name__=='__main__':\n",
    "    s = 'mississippi'\n",
    "    d = collections.defaultdict(int)\n",
    "    for k in s:\n",
    "        d[k] += 1\n",
    "    print(d.items()) # return the list with all dictionary keys with values\n",
    "    print(s.split())\n",
    "    print(wordsep('test'))\n",
    "    wl = [('test', 5), ('some', 2), ('one', 8), ('mississippi', 2)]\n",
    "    print(inp_rep(wl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1415926535913457\n"
     ]
    }
   ],
   "source": [
    "# Arithmetic-geometric mean and pi\n",
    "import math\n",
    "\n",
    "def agm(a,g,n):\n",
    "    a0 = a\n",
    "    g0 = g\n",
    "    for i in range(n):\n",
    "        an = (a0+g0)/2.\n",
    "        gn = math.sqrt(a0*g0)\n",
    "        a0 = an\n",
    "        g0 = gn\n",
    "    return (an,gn)\n",
    "\n",
    "def piagm(a,g,n):\n",
    "    a0 = a\n",
    "    g0 = g\n",
    "    agsum = 0.\n",
    "    twop = 2.\n",
    "    for i in range(1,n):\n",
    "        an = (a0+g0)/2.\n",
    "        gn = math.sqrt(a0*g0)\n",
    "        twop *= 2.\n",
    "        agsum += twop * (an*an - gn*gn)\n",
    "        a0 = an\n",
    "        g0 = gn\n",
    "    return 4. * (agm(1., 1./math.sqrt(2.), 100)[0] ** 2) / (1. - agsum)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    #print(agm(1., 1./math.sqrt(2.), 100))\n",
    "    print(piagm(1., 1./math.sqrt(2.), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.21742849e-05  2.40038508e-04  2.40678251e-04  8.47253735e-05\n",
      "  -1.28055799e-05  4.23046013e-04  1.38969143e-04 -5.51629916e-05\n",
      "   2.90629974e-04 -3.72688084e-04]]\n",
      "[[ 3.01755738e-08  1.65382648e-05 -0.00000000e+00 -1.44909836e-07\n",
      "   2.46414218e-08 -9.02442797e-08 -2.06793148e-07  0.00000000e+00\n",
      "  -0.00000000e+00  1.96541329e-07]]\n",
      "[[ 0.00000000e+00  1.65382648e-05 -0.00000000e+00 -0.00000000e+00\n",
      "   0.00000000e+00 -0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  -0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# simple feed forward network with ReLU example\n",
    "import numpy as np\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # parameters\n",
    "    inp_size = 10 # input size\n",
    "    etha = 0.1 # learning rate\n",
    "\n",
    "    # input\n",
    "    x = np.zeros((1, inp_size)) # input\n",
    "    x = [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]\n",
    "\n",
    "    # model parameters\n",
    "    W1 = np.random.randn(inp_size, inp_size)*0.01 # input to hidden\n",
    "    W2 = np.random.randn(inp_size, inp_size)*0.01 # hidden to output\n",
    "    b1 = np.zeros((1, inp_size)) # inp-hidden bias\n",
    "    b2 = np.zeros((1, inp_size)) # hidden-out bias\n",
    "\n",
    "    # forward pass\n",
    "    h1 = np.dot(x, W1) + b1\n",
    "    h1 = np.maximum(h1, 0, h1) # ReLU\n",
    "    o2 = np.dot(h1, W2) + b2\n",
    "    print(o2)\n",
    "\n",
    "    # backward pass\n",
    "    y = [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]\n",
    "    h1 = np.dot(x, W1) + b1\n",
    "    dW1 = - etha * (o2 - y) * np.maximum(h1, 0, h1)\n",
    "    dW2 = dW1 * ((h1 > 0) * 1.) * x\n",
    "    print(dW1)\n",
    "    print(dW2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08017331  1.00081823  0.0066398  -0.02038512  0.02915765  0.02587995\n",
      "  -0.00555739  0.04111021 -0.04911599 -0.07523451]]\n"
     ]
    }
   ],
   "source": [
    "# simple feed forward network with ReLU example\n",
    "import numpy as np\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # parameters\n",
    "    inp_size = 10 # input size\n",
    "    etha = 0.1 # learning rate\n",
    "    nruns = 20 # n training runs\n",
    "\n",
    "    # model parameters\n",
    "    W1 = np.random.randn(inp_size, inp_size)*0.01 # input to hidden\n",
    "    W2 = np.random.randn(inp_size, inp_size)*0.01 # hidden to output\n",
    "    b1 = np.zeros((1, inp_size)) # inp-hidden bias\n",
    "    b2 = np.zeros((1, inp_size)) # hidden-out bias\n",
    "    \n",
    "    for k in range(10):\n",
    "        # input\n",
    "        x = np.zeros((1, inp_size)) # input\n",
    "        x[0][k] = 1.\n",
    "\n",
    "        for i in range(nruns):\n",
    "            # forward pass\n",
    "            h1 = np.dot(x, W1) + b1\n",
    "            h1 = np.maximum(h1, 0, h1) # ReLU\n",
    "            o2 = np.dot(h1, W2) + b2\n",
    "            #print(o2)\n",
    "\n",
    "            # backward pass\n",
    "            y = [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]\n",
    "            h1 = np.dot(x, W1) + b1\n",
    "            dW1 = - etha * (o2 - y) * np.maximum(h1, 0, h1)\n",
    "            dW2 = dW1 * ((h1 > 0) * 1.) * x\n",
    "            #print(dW1)\n",
    "            #print(dW2)\n",
    "            W1 += dW1\n",
    "            W2 += dW2\n",
    "    \n",
    "    #print(W1)\n",
    "    #print(W2)\n",
    "    # forward pass\n",
    "    x = np.zeros((1, inp_size)) # input\n",
    "    x[0][1] = 1.\n",
    "    h1 = np.dot(x, W1) + b1\n",
    "    h1 = np.maximum(h1, 0, h1) # ReLU\n",
    "    o2 = np.dot(h1, W2) + b2\n",
    "    print(o2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['the', 'red', 'fox', 'jumps', 'over', 'the', 'lazy'], ['red', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'], ['fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'and'], ['jumps', 'over', 'the', 'lazy', 'dog', 'and', 'eats'], ['over', 'the', 'lazy', 'dog', 'and', 'eats', 'the'], ['the', 'lazy', 'dog', 'and', 'eats', 'the', 'hen']], ['jumps', 'over', 'the', 'lazy', 'dog', 'and'])\n"
     ]
    }
   ],
   "source": [
    "# key-value memories\n",
    "\n",
    "def cwinkv(text, wsize): # center-window encoding\n",
    "    lt = len(text)\n",
    "    if lt < wsize:\n",
    "        return []\n",
    "    wsh = int(wsize/2)\n",
    "    keys = []\n",
    "    values = []\n",
    "    for i in range(wsh, lt-wsh):\n",
    "        onekey = []\n",
    "        for j in range(i-wsh, i+wsh+1):\n",
    "            onekey.append(text[j]) # build the window\n",
    "            if j == i:\n",
    "                values.append(text[j]) # values are the center words\n",
    "        keys.append(onekey)\n",
    "    return (keys, values)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    stxt = 'the red fox jumps over the lazy dog and eats the hen'\n",
    "    ltxt = stxt.split(' ')\n",
    "    #print(ltxt)\n",
    "    print(cwinkv(ltxt, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['the', 'red', 'fox', 'jumps', 'over', 'the', 'lazy'], ['red', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'], ['fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'and'], ['jumps', 'over', 'the', 'lazy', 'dog', 'and', 'eats'], ['over', 'the', 'lazy', 'dog', 'and', 'eats', 'the'], ['the', 'lazy', 'dog', 'and', 'eats', 'the', 'hen']], ['jumps', 'over', 'the', 'lazy', 'dog', 'and'])\n",
      "key hash: [0, 1, 2, 3, 4]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[0, 0, 0, 0, 1]\n",
      "[0.9539980920057239, 1.483997032008904, 2.0139959720120837, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161]\n",
      "['fox', 'jumps', 'over', 'what']\n"
     ]
    }
   ],
   "source": [
    "# key-value memory network\n",
    "# see also https://github.com/jojonki/key-value-memory-networks\n",
    "import numpy as np\n",
    "\n",
    "def cwinkv(text, wsize): # center-window encoding\n",
    "    lt = len(text)\n",
    "    if lt < wsize:\n",
    "        return ()\n",
    "    wsh = int(wsize/2)\n",
    "    keys = []\n",
    "    values = []\n",
    "    for i in range(wsh, lt-wsh):\n",
    "        onekey = []\n",
    "        for j in range(i-wsh, i+wsh+1):\n",
    "            onekey.append(text[j]) # build the window\n",
    "            if j == i:\n",
    "                values.append(text[j]) # values are the center words\n",
    "        keys.append(onekey)\n",
    "    return (keys, values)\n",
    "\n",
    "def keyhsh(mem, quest, n): # key hash for memory and question quest\n",
    "    memid = []\n",
    "    idc = 0\n",
    "    for qw in quest:\n",
    "        for i in range(len(mem[0])):\n",
    "            if qw in mem[0][i] and not i in memid: # question-word in memory?\n",
    "                if idc > n:\n",
    "                    return memid # return ids of memories containing question words (id = list of words of size winsize)\n",
    "                memid.append(i)\n",
    "                idc += 1\n",
    "    return memid\n",
    "\n",
    "def memf(memid): # check memory frequencies\n",
    "    # F-check (freq < 1000)\n",
    "    mem = []\n",
    "    memf = []\n",
    "    for e in memid:\n",
    "        mct = memid.count(e)\n",
    "        if mct < 1000 and not e in mem:\n",
    "            memf.append((e, mct))\n",
    "            mem.append(e)\n",
    "    return memf # return positions in memory for matching memories and f: (memid, f)\n",
    "\n",
    "def onehot(vec): # takes a (kind-of)bow-vector and turns it into a combined 1-hot (kind of...)\n",
    "    lv = len(vec)\n",
    "    res = [0] * lv\n",
    "    for e in vec:\n",
    "        if e > 0 and e < lv:\n",
    "            res[e] += 1\n",
    "    return res\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def BatchNormalization(a):\n",
    "    mean = np.mean(a)\n",
    "    stdv = np.std(a)\n",
    "    return [(x - mean) / stdv for x in a]\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    stxt = 'the red fox jumps over the lazy dog and eats the hen'\n",
    "    ltxt = stxt.split(' ')\n",
    "    \n",
    "    #print(ltxt)\n",
    "    #print(cwinkv(ltxt, 7))\n",
    "    \n",
    "    # dimensions\n",
    "    #key = Input((mem_size, mem_len,), name='Key_Input')\n",
    "    #val = Input((mem_size, mem_len,), name='Val_Input')\n",
    "    #question = Input((query_maxlen,), name='Question_Input')\n",
    "    \n",
    "    # build memory with given window size\n",
    "    memory = cwinkv(ltxt, 7) # memory has the structure ([[keys0], [keys1],...], [values])\n",
    "    print(memory)\n",
    "    w2i = dict((c, i) for i, c in enumerate(ltxt, 1))\n",
    "    i2w = dict((i, c) for i, c in enumerate(ltxt, 1))\n",
    "    #print(w2i)\n",
    "    #print(i2w)\n",
    "    \n",
    "    dimd = len(w2i) # embedding dimension\n",
    "    \n",
    "    # define question\n",
    "    question = 'fox jumps over what'\n",
    "    qx = question.split(' ')\n",
    "    \n",
    "    # todo: wrap in NN\n",
    "    \n",
    "    # embedding matrices take dimensions D to d\n",
    "    # todo: align dims: memory M, key hash N, question D then d, values same as ph which is d but should be N ?, Rj dxd\n",
    "    \n",
    "    # key hashing: key shares at least one word with the question (which memories contain words from the question?)\n",
    "    #kh = memf(keyhsh(memory, qx, 10))\n",
    "    kh = keyhsh(memory, qx, 10)\n",
    "    print('key hash: %s' % (str(kh)))\n",
    "    \n",
    "    # key addressing: candidate memory gets assigned a relevance probability\n",
    "    # ph = dot([question_encoded, key_encoded], axes=(1, 2))  # (None, mem_size) - Keras dot-product layer\n",
    "    q = [w2i[w] for w in qx if w in w2i] # embed question\n",
    "    q = q[:dimd] # limit length\n",
    "    padl = max(0, dimd - len(q)) # pad length\n",
    "    q += [0] * padl # pad question length\n",
    "    #print('q: %s' % (str(q)))\n",
    "    #print(onehot(q))\n",
    "    #q = BatchNormalization(q)\n",
    "    ph = []\n",
    "    # first loop: find best matching memory for question x (relevance of candidate memories)\n",
    "    for memid in kh:\n",
    "        k = [w2i[w] for w in memory[0][memid] if w in w2i] # embed candidate key\n",
    "        k = k[:dimd] # limit length\n",
    "        padl = max(0, dimd - len(k)) # pad length\n",
    "        k += [0] * padl # pad \n",
    "        #print(k)\n",
    "        #print(softmax(np.dot(q,k)))\n",
    "        #ph.append(softmax(np.dot(q, k)))\n",
    "        #ph.append(np.dot(onehot(q), onehot(k))) # dot of question and candidate memory key\n",
    "        ph.append(softmax(np.dot(onehot(q), onehot(k))))\n",
    "    #print(ph)\n",
    "    #ph = BatchNormalization(ph)\n",
    "    #print(ph)\n",
    "    \n",
    "    # value reading\n",
    "    # o = dot([ph, val_encoded], axes=(1, 1)) # (None, embd_size)\n",
    "    v = []\n",
    "    for k in kh:\n",
    "        v.append(w2i[memory[1][k]])  # all values for candidate keys\n",
    "    #print(np.dot(ph, v))\n",
    "    #print(v)\n",
    "    #o = np.dot(ph, v) # weighted sum of memories\n",
    "    print(ph)\n",
    "    print(onehot(v))\n",
    "    o = np.dot(ph, onehot(v)) # weighted sum of memories\n",
    "    \n",
    "    # with result o, update query with q2 = R1(q+o) and repeat from memory access (from key addressing)\n",
    "    #R = Dense(embd_size, input_shape=(embd_size,), name='R_Dense_h' + str(h+1))\n",
    "    #q = R(add([q,  o])) # (None, embd_size)\n",
    "    q += o\n",
    "    q = BatchNormalization(q)\n",
    "    print(q)\n",
    "    print(qx)\n",
    "    #print(o)\n",
    "    #print([i2w[i] for i in v if i in i2w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['the', 'red', 'fox', 'jumps', 'over', 'the', 'lazy'], ['red', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'], ['fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'and'], ['jumps', 'over', 'the', 'lazy', 'dog', 'and', 'eats'], ['over', 'the', 'lazy', 'dog', 'and', 'eats', 'the'], ['the', 'lazy', 'dog', 'and', 'eats', 'the', 'hen']], ['jumps', 'over', 'the', 'lazy', 'dog', 'and'])\n",
      "key hash: [0, 1, 2, 3, 4]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[0, 0, 0, 0, 1]\n",
      "[0.9539980920057239, 1.483997032008904, 2.0139959720120837, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161]\n",
      "['fox', 'jumps', 'over', 'what']\n"
     ]
    }
   ],
   "source": [
    "# key-value memory network\n",
    "# see also https://github.com/jojonki/key-value-memory-networks\n",
    "import numpy as np\n",
    "\n",
    "def cwinkv(text, wsize): # center-window encoding\n",
    "    lt = len(text)\n",
    "    if lt < wsize:\n",
    "        return ()\n",
    "    wsh = int(wsize/2)\n",
    "    keys = []\n",
    "    values = []\n",
    "    for i in range(wsh, lt-wsh):\n",
    "        onekey = []\n",
    "        for j in range(i-wsh, i+wsh+1):\n",
    "            onekey.append(text[j]) # build the window\n",
    "            if j == i:\n",
    "                values.append(text[j]) # values are the center words\n",
    "        keys.append(onekey)\n",
    "    return (keys, values)\n",
    "\n",
    "def keyhsh(mem, quest, n): # key hash for memory and question quest\n",
    "    memid = []\n",
    "    idc = 0\n",
    "    for qw in quest:\n",
    "        for i in range(len(mem[0])):\n",
    "            if qw in mem[0][i] and not i in memid: # question-word in memory?\n",
    "                if idc > n:\n",
    "                    return memid # return ids of memories containing question words (id = list of words of size winsize)\n",
    "                memid.append(i)\n",
    "                idc += 1\n",
    "    return memid\n",
    "\n",
    "def memf(memid): # check memory frequencies\n",
    "    # F-check (freq < 1000)\n",
    "    mem = []\n",
    "    memf = []\n",
    "    for e in memid:\n",
    "        mct = memid.count(e)\n",
    "        if mct < 1000 and not e in mem:\n",
    "            memf.append((e, mct))\n",
    "            mem.append(e)\n",
    "    return memf # return positions in memory for matching memories and f: (memid, f)\n",
    "\n",
    "def onehot(vec): # takes a (kind-of)bow-vector and turns it into a combined 1-hot (kind of...)\n",
    "    lv = len(vec)\n",
    "    res = [0] * lv\n",
    "    for e in vec:\n",
    "        if e > 0 and e < lv:\n",
    "            res[e] += 1\n",
    "    return res\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def BatchNormalization(a):\n",
    "    mean = np.mean(a)\n",
    "    stdv = np.std(a)\n",
    "    return [(x - mean) / stdv for x in a]\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    stxt = 'the red fox jumps over the lazy dog and eats the hen'\n",
    "    ltxt = stxt.split(' ')\n",
    "    \n",
    "    #print(ltxt)\n",
    "    #print(cwinkv(ltxt, 7))\n",
    "    \n",
    "    # dimensions\n",
    "    #key = Input((mem_size, mem_len,), name='Key_Input')\n",
    "    #val = Input((mem_size, mem_len,), name='Val_Input')\n",
    "    #question = Input((query_maxlen,), name='Question_Input')\n",
    "    \n",
    "    # build memory with given window size\n",
    "    memory = cwinkv(ltxt, 7) # memory has the structure ([[keys0], [keys1],...], [values])\n",
    "    print(memory)\n",
    "    w2i = dict((c, i) for i, c in enumerate(ltxt, 1))\n",
    "    i2w = dict((i, c) for i, c in enumerate(ltxt, 1))\n",
    "    #print(w2i)\n",
    "    #print(i2w)\n",
    "    \n",
    "    dimd = len(w2i) # embedding dimension\n",
    "    \n",
    "    # define question\n",
    "    question = 'fox jumps over what'\n",
    "    qx = question.split(' ')\n",
    "    \n",
    "    # todo: wrap in NN\n",
    "    \n",
    "    # embedding matrices take dimensions D to d\n",
    "    # todo: align dims: memory M, key hash N, question D then d, values same as ph which is d but should be N ?, Rj dxd\n",
    "    \n",
    "    # key hashing: key shares at least one word with the question (which memories contain words from the question?)\n",
    "    #kh = memf(keyhsh(memory, qx, 10))\n",
    "    kh = keyhsh(memory, qx, 10)\n",
    "    print('key hash: %s' % (str(kh)))\n",
    "    \n",
    "    # key addressing: candidate memory gets assigned a relevance probability\n",
    "    # ph = dot([question_encoded, key_encoded], axes=(1, 2))  # (None, mem_size) - Keras dot-product layer\n",
    "    q = [w2i[w] for w in qx if w in w2i] # embed question\n",
    "    q = q[:dimd] # limit length\n",
    "    padl = max(0, dimd - len(q)) # pad length\n",
    "    q += [0] * padl # pad question length\n",
    "    #print('q: %s' % (str(q)))\n",
    "    #print(onehot(q))\n",
    "    #q = BatchNormalization(q)\n",
    "    ph = []\n",
    "    # first loop: find best matching memory for question x (relevance of candidate memories)\n",
    "    for memid in kh:\n",
    "        k = [w2i[w] for w in memory[0][memid] if w in w2i] # embed candidate key\n",
    "        k = k[:dimd] # limit length\n",
    "        padl = max(0, dimd - len(k)) # pad length\n",
    "        k += [0] * padl # pad \n",
    "        #print(k)\n",
    "        #print(softmax(np.dot(q,k)))\n",
    "        #ph.append(softmax(np.dot(q, k)))\n",
    "        #ph.append(np.dot(onehot(q), onehot(k))) # dot of question and candidate memory key\n",
    "        ph.append(softmax(np.dot(onehot(q), onehot(k))))\n",
    "    #print(ph)\n",
    "    #ph = BatchNormalization(ph)\n",
    "    #print(ph)\n",
    "    \n",
    "    # value reading\n",
    "    # o = dot([ph, val_encoded], axes=(1, 1)) # (None, embd_size)\n",
    "    v = []\n",
    "    for k in kh:\n",
    "        v.append(w2i[memory[1][k]])  # all values for candidate keys\n",
    "    #print(np.dot(ph, v))\n",
    "    #print(v)\n",
    "    #o = np.dot(ph, v) # weighted sum of memories\n",
    "    print(ph)\n",
    "    print(onehot(v))\n",
    "    o = np.dot(ph, onehot(v)) # weighted sum of memories\n",
    "    \n",
    "    # with result o, update query with q2 = R1(q+o) and repeat from memory access (from key addressing)\n",
    "    #R = Dense(embd_size, input_shape=(embd_size,), name='R_Dense_h' + str(h+1))\n",
    "    #q = R(add([q,  o])) # (None, embd_size)\n",
    "    q += o\n",
    "    q = BatchNormalization(q)\n",
    "    print(q)\n",
    "    print(qx)\n",
    "    #print(o)\n",
    "    #print([i2w[i] for i in v if i in i2w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sukhbaatar: end-to-end memory networs\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from past.builtins import xrange\n",
    "\n",
    "class MemN2N(object):\n",
    "    def __init__(self, config, sess):\n",
    "        self.nwords = config.nwords\n",
    "        self.init_hid = config.init_hid\n",
    "        self.init_std = config.init_std\n",
    "        self.batch_size = config.batch_size\n",
    "        self.nepoch = config.nepoch\n",
    "        self.nhop = config.nhop\n",
    "        self.edim = config.edim\n",
    "        self.mem_size = config.mem_size\n",
    "        self.lindim = config.lindim\n",
    "        self.max_grad_norm = config.max_grad_norm\n",
    "\n",
    "        self.show = config.show\n",
    "        self.is_test = config.is_test\n",
    "        self.checkpoint_dir = config.checkpoint_dir\n",
    "\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            raise Exception(\" [!] Directory %s not found\" % self.checkpoint_dir)\n",
    "\n",
    "        self.input = tf.placeholder(tf.float32, [None, self.edim], name=\"input\")\n",
    "        self.time = tf.placeholder(tf.int32, [None, self.mem_size], name=\"time\")\n",
    "        self.target = tf.placeholder(tf.float32, [self.batch_size, self.nwords], name=\"target\")\n",
    "        self.context = tf.placeholder(tf.int32, [self.batch_size, self.mem_size], name=\"context\")\n",
    "\n",
    "        self.hid = []\n",
    "        self.hid.append(self.input)\n",
    "        self.share_list = []\n",
    "        self.share_list.append([])\n",
    "\n",
    "        self.lr = None\n",
    "        self.current_lr = config.init_lr\n",
    "        self.loss = None\n",
    "        self.step = None\n",
    "        self.optim = None\n",
    "\n",
    "        self.sess = sess\n",
    "        self.log_loss = []\n",
    "        self.log_perp = []\n",
    "\n",
    "    def build_memory(self):\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\")\n",
    "\n",
    "        self.A = tf.Variable(tf.random_normal([self.nwords, self.edim], stddev=self.init_std))\n",
    "        self.B = tf.Variable(tf.random_normal([self.nwords, self.edim], stddev=self.init_std))\n",
    "        self.C = tf.Variable(tf.random_normal([self.edim, self.edim], stddev=self.init_std))\n",
    "\n",
    "        # Temporal Encoding\n",
    "        self.T_A = tf.Variable(tf.random_normal([self.mem_size, self.edim], stddev=self.init_std))\n",
    "        self.T_B = tf.Variable(tf.random_normal([self.mem_size, self.edim], stddev=self.init_std))\n",
    "\n",
    "        # m_i = sum A_ij * x_ij + T_A_i\n",
    "        Ain_c = tf.nn.embedding_lookup(self.A, self.context)\n",
    "        Ain_t = tf.nn.embedding_lookup(self.T_A, self.time)\n",
    "        Ain = tf.add(Ain_c, Ain_t)\n",
    "\n",
    "        # c_i = sum B_ij * u + T_B_i\n",
    "        Bin_c = tf.nn.embedding_lookup(self.B, self.context)\n",
    "        Bin_t = tf.nn.embedding_lookup(self.T_B, self.time)\n",
    "        Bin = tf.add(Bin_c, Bin_t)\n",
    "\n",
    "        for h in xrange(self.nhop):\n",
    "            self.hid3dim = tf.reshape(self.hid[-1], [-1, 1, self.edim])\n",
    "            Aout = tf.matmul(self.hid3dim, Ain, adjoint_b=True)\n",
    "            Aout2dim = tf.reshape(Aout, [-1, self.mem_size])\n",
    "            P = tf.nn.softmax(Aout2dim)\n",
    "\n",
    "            probs3dim = tf.reshape(P, [-1, 1, self.mem_size])\n",
    "            Bout = tf.matmul(probs3dim, Bin)\n",
    "            Bout2dim = tf.reshape(Bout, [-1, self.edim])\n",
    "\n",
    "            Cout = tf.matmul(self.hid[-1], self.C)\n",
    "            Dout = tf.add(Cout, Bout2dim)\n",
    "\n",
    "            self.share_list[0].append(Cout)\n",
    "\n",
    "            if self.lindim == self.edim:\n",
    "                self.hid.append(Dout)\n",
    "            elif self.lindim == 0:\n",
    "                self.hid.append(tf.nn.relu(Dout))\n",
    "            else:\n",
    "                F = tf.slice(Dout, [0, 0], [self.batch_size, self.lindim])\n",
    "                G = tf.slice(Dout, [0, self.lindim], [self.batch_size, self.edim-self.lindim])\n",
    "                K = tf.nn.relu(G)\n",
    "                self.hid.append(tf.concat(axis=1, values=[F, K]))\n",
    "\n",
    "    def build_model(self):\n",
    "        self.build_memory()\n",
    "\n",
    "        self.W = tf.Variable(tf.random_normal([self.edim, self.nwords], stddev=self.init_std))\n",
    "        z = tf.matmul(self.hid[-1], self.W)\n",
    "\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=z, labels=self.target)\n",
    "\n",
    "        self.lr = tf.Variable(self.current_lr)\n",
    "        self.opt = tf.train.GradientDescentOptimizer(self.lr)\n",
    "\n",
    "        params = [self.A, self.B, self.C, self.T_A, self.T_B, self.W]\n",
    "        grads_and_vars = self.opt.compute_gradients(self.loss,params)\n",
    "        clipped_grads_and_vars = [(tf.clip_by_norm(gv[0], self.max_grad_norm), gv[1]) \\\n",
    "                                   for gv in grads_and_vars]\n",
    "\n",
    "        inc = self.global_step.assign_add(1)\n",
    "        with tf.control_dependencies([inc]):\n",
    "            self.optim = self.opt.apply_gradients(clipped_grads_and_vars)\n",
    "\n",
    "        tf.global_variables_initializer().run()\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def train(self, data):\n",
    "        N = int(math.ceil(len(data) / self.batch_size))\n",
    "        cost = 0\n",
    "\n",
    "        x = np.ndarray([self.batch_size, self.edim], dtype=np.float32)\n",
    "        time = np.ndarray([self.batch_size, self.mem_size], dtype=np.int32)\n",
    "        target = np.zeros([self.batch_size, self.nwords]) # one-hot-encoded\n",
    "        context = np.ndarray([self.batch_size, self.mem_size])\n",
    "\n",
    "        x.fill(self.init_hid)\n",
    "        for t in xrange(self.mem_size):\n",
    "            time[:,t].fill(t)\n",
    "\n",
    "        if self.show:\n",
    "            from utils import ProgressBar\n",
    "            bar = ProgressBar('Train', max=N)\n",
    "\n",
    "        for idx in xrange(N):\n",
    "            if self.show: bar.next()\n",
    "            target.fill(0)\n",
    "            for b in xrange(self.batch_size):\n",
    "                m = random.randrange(self.mem_size, len(data))\n",
    "                target[b][data[m]] = 1\n",
    "                context[b] = data[m - self.mem_size:m]\n",
    "\n",
    "            _, loss, self.step = self.sess.run([self.optim,\n",
    "                                                self.loss,\n",
    "                                                self.global_step],\n",
    "                                                feed_dict={\n",
    "                                                    self.input: x,\n",
    "                                                    self.time: time,\n",
    "                                                    self.target: target,\n",
    "                                                    self.context: context})\n",
    "            cost += np.sum(loss)\n",
    "\n",
    "        if self.show: bar.finish()\n",
    "        return cost/N/self.batch_size\n",
    "\n",
    "    def test(self, data, label='Test'):\n",
    "        N = int(math.ceil(len(data) / self.batch_size))\n",
    "        cost = 0\n",
    "\n",
    "        x = np.ndarray([self.batch_size, self.edim], dtype=np.float32)\n",
    "        time = np.ndarray([self.batch_size, self.mem_size], dtype=np.int32)\n",
    "        target = np.zeros([self.batch_size, self.nwords]) # one-hot-encoded\n",
    "        context = np.ndarray([self.batch_size, self.mem_size])\n",
    "\n",
    "        x.fill(self.init_hid)\n",
    "        for t in xrange(self.mem_size):\n",
    "            time[:,t].fill(t)\n",
    "\n",
    "        if self.show:\n",
    "            from utils import ProgressBar\n",
    "            bar = ProgressBar(label, max=N)\n",
    "\n",
    "        m = self.mem_size\n",
    "        for idx in xrange(N):\n",
    "            if self.show: bar.next()\n",
    "            target.fill(0)\n",
    "            for b in xrange(self.batch_size):\n",
    "                target[b][data[m]] = 1\n",
    "                context[b] = data[m - self.mem_size:m]\n",
    "                m += 1\n",
    "\n",
    "                if m >= len(data):\n",
    "                    m = self.mem_size\n",
    "\n",
    "            loss = self.sess.run([self.loss], feed_dict={self.input: x,\n",
    "                                                         self.time: time,\n",
    "                                                         self.target: target,\n",
    "                                                         self.context: context})\n",
    "            cost += np.sum(loss)\n",
    "\n",
    "        if self.show: bar.finish()\n",
    "        return cost/N/self.batch_size\n",
    "\n",
    "    def run(self, train_data, test_data):\n",
    "        if not self.is_test:\n",
    "            for idx in xrange(self.nepoch):\n",
    "                train_loss = np.sum(self.train(train_data))\n",
    "                test_loss = np.sum(self.test(test_data, label='Validation'))\n",
    "\n",
    "                # Logging\n",
    "                self.log_loss.append([train_loss, test_loss])\n",
    "                self.log_perp.append([math.exp(train_loss), math.exp(test_loss)])\n",
    "\n",
    "                state = {\n",
    "                    'perplexity': math.exp(train_loss),\n",
    "                    'epoch': idx,\n",
    "                    'learning_rate': self.current_lr,\n",
    "                    'valid_perplexity': math.exp(test_loss)\n",
    "                }\n",
    "                print(state)\n",
    "\n",
    "                # Learning rate annealing\n",
    "                if len(self.log_loss) > 1 and self.log_loss[idx][1] > self.log_loss[idx-1][1] * 0.9999:\n",
    "                    self.current_lr = self.current_lr / 1.5\n",
    "                    self.lr.assign(self.current_lr).eval()\n",
    "                if self.current_lr < 1e-5: break\n",
    "\n",
    "                if idx % 10 == 0:\n",
    "                    self.saver.save(self.sess,\n",
    "                                    os.path.join(self.checkpoint_dir, \"MemN2N.model\"),\n",
    "                                    global_step = self.step.astype(int))\n",
    "        else:\n",
    "            self.load()\n",
    "\n",
    "            valid_loss = np.sum(self.test(train_data, label='Validation'))\n",
    "            test_loss = np.sum(self.test(test_data, label='Test'))\n",
    "\n",
    "            state = {\n",
    "                'valid_perplexity': math.exp(valid_loss),\n",
    "                'test_perplexity': math.exp(test_loss)\n",
    "            }\n",
    "            print(state)\n",
    "\n",
    "    def load(self):\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        ckpt = tf.train.get_checkpoint_state(self.checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            raise Exception(\" [!] Trest mode but no checkpoint found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 36979 characters, 14 unique.\n",
      "----\n",
      " 6:61020:9:757 7393078058773,453:':,9:2,481015::38711'3',9626550549537654997: ::'8' 7 14':'931':'937,7394 47933,3266:23'3950335:  '' 0196:226:98, 9020263144 6'6 ,, 1'047520'5976 428,7238884 637943771 63418:',118 865738 66:  97073  20',5 27095:22,542158871:7,,46 8845'723175:5 04891:,',53197053635573809 17 :28215, 7000897:2707,77:10229141''14 '7 '30:1:'1:23278:9, 781 9'1,7:1168':79,42305 858:04327860947140070039454,78641 1770':99250 938328,77269'8453415:90 06473'70' 9,69,049 ,3,28:7'334168,83 :69 36:5,9:688490':72156 ''801 2:,'51'4,, :448 ,3224858:5995733 9493:9760417':48 0673190'9:785'4,:  16010'2'3 :580908521'193887,997802164881367'983'14 794945,1' 16:8785'49,95 8,,'7:79:290:6607 77970,,68,532'828'66529 777::7::55184 411226,763:94 7'51 19,3 :49,6363735'000'73,79900:861729532598771,50914026958'3 1998:943'6178'130:2'2342103'9:17 16 3'38107 '9'47 7,2' 530:'750587111222:4 319,0'8,2'8,7462,05 8166542401:'447: 08:429 ''06': '9'7146289611695283 3,97640581,028787736:'8:128414'  0,8:7,208117065:22558731  994380 2::'226560'796,,2892603811'894 9:,: 3'2846885 2083 20174 5'48 1'9217657: 4 5880, 6,176684 766,333362 '94966,,1883,2'14,,6952928'160111'2:2 10,5271516 175038:0183' 40'70'249::'682,006'6:35054636,'4,5491298 893:6221'8187513819122  585910,38,:5674,13'0036:829001,84'9 ,,54:270'34438'4,,76256'72328333 66 7::3,64:427,4 91876230769319:,192325::30940644071 '34,0435''3872471587:802937' '31302658 10682:3 :7,218320 36 94100:82168129286873':550048'6490434,03'1286543461790:061'978',6 8521 0' \n",
      "----\n",
      "iter 0, loss: 65.976431\n",
      "----\n",
      " 270:07 :3:7, 87:,:00 ' '1:64:82105:05,0,1,0, '90 7:2 ::02,30 16' 80:0',0 1 0'29 90':21'904420:6160, 6:19:'2,8013:9  '29: 1:1'2011516:3 5,,::':5'10' 65'0:609,':''701'7,::1 7650306,589878'1',,2,: :':097 '2:'',021 6 ,00, ' : ::0,1 471':0'00 1:9006:04 ,:0,340:29'6'00 '0 : '001'908' 0:1,0018 ,8:'0 01 9880:4',6'1'1 08'80,13:70,2:5,96024008 0150',1,,,0:0:,08',0' 10,:, ' '8123101  ,0013:' ', 82 6,9'3' 15::1 ,07501,4:7 7',19,85000 0' 0 ,1'14:9''',3'18',88:7605,'',':25: 0'1,, :2',:517'50 2:0:,09:701 0,,788' '5''''40'0:,57:,::,:2, ,0'40:0 ':,,0: '0'10 98','0:,:0,',':920 ,'2510:0,195 ',8 2'9:5'1' :: 8:8::80:2,6018'8076'',120 :20130''1''0:615:90:50106 ':4,7164'68':51915',4:,0::5:0880 015''5:,3:8:69020:980'',:40',0:8,''1:8 01'12',','780:015,''6:4'8:0616 ',190::2,2':0,9,,75:, :0,028:00'190 0'10','' 0,2702, 80101'84 ,4'30, '0:75:02,2 50 , 0'43,8702,0:85 '0005030156780 0'1 80,, 78',',8107:5:,108:5:'0'1 21',2,:22:0:,7:: '1  5'1'0',2:91, :0:371  ''0' 07',7'215:0', :,201'802''0,,2809:85:0,6:4':010:'151', ',3'232:8,7:7' 0:0: ',',1,40:38'0, 80':53'01' '',:2715',0614618,980:50:0:0629',0:5 0,0:04 074,'129 070'  ,20010:040::9:82   2710850 , 6','1'53:,05:95095: 09'':,03 0:'60:06:',,37:2', :4:'' :6'421,8:0:006'68,37'56',59'0 ,'18 65' 57:1' 0 02 :1: 6' 7:4:21193, 288':4'4421010':0'42182040',,::61 45:1,:8:4'' :'6:,'506:5831 35''5':8',,1,6:4'5':,:',',0:'3',02'10: 0''20,4, ' ,609601270:1:101000'562104 80 ',0:'9'0':55,927,'19' 8:'061' 5:5'60:0,800 :0:0 001 0,6'87::0'50 :',:9:0 '9105'7:16 ', ',,,',00::' '750 \n",
      "----\n",
      "iter 5, loss: 65.938111\n",
      "----\n",
      "  :0 7':: ':8,:18',0'0:,0 :00:, :8: 2''902'00' :0 '', :,:37, ', ::06'7','0 01'8:7,,'5,:,2: 5:10', '870'' '90,80,,:70'19 ' :7',,::0'' 03:1::00'':' 0:00 :00''':8  '0 :1'17,:'6::0:5 220:1,8'9182':1'9'0,5',', '2,00'80,0:'0'40 :2'38' ,::01:50,',, '64609'0:0'6::8 7 6:4,0'1',2: '1805'':81'8:0' ', '7'122 7',70::0':'',1 0 0'' 06' 0'' '6', 5,1:5:5080'58:20:'1 0:,8'8602::1'0:0:5' 0,:00 :060:1:':8091', '108:00 ,8 0::061:' ::8:6001:'902:,0',6, 0:':0'::0'00:':02'8':5'0:4':28: ',:81 '7'90 ,0':080'0',0:0,8 ,1:8'50:00400':7,7 :801:14:97705:95:6',:0':,05:0'0:,7':4:0'80'0 0':1809::,:7:73:1''2: :927:7187','1:810'04'7:8,9:3',:::,:,057::1:8:8: ,', :3:0 0,606', '9,260:1:80090 ' 1':040:0:010,:07' 2 '4:700:8:'8:8:82:::40',',:71, '0:50070 00:,902:41:,:80'0:8:0:8'1:',1:21807870:: 1:72:0'62':  0,  8:07,,0','18'0:6:: 0' '6''58, 0,3:0:7':85'1 1 '6:5497:50' :9'' 0:71,0 :, 0::809:0 :,','':2:45850',:'::1', 55'90:05'7,:10:04'0080:'091:0,0:'8:09, 4204 ',3,:0:'2''2'0'5:',:0 80:0,2'0,:,:0:0 020:0,8':1'0':0: 19,'7:0 '10:007:00'8:0,3': 09' ',302,::,' 0',09602',50150801',1'69' 7:7'01 :,',0 ,0:8:'0 '8:'  0,:0': : 0':1 ':7'0'0 0:,::0:4' :8:85 0':00''1:8080:7, 7','1 3'50' 3:,,'8'500,:'6':0::8:'0'16'2,000:0: 5',:18028',02:0',0: ''7:,8'': :80'4:800',:5':' :89:0: 0,'28'8::89:'2:08',10'0:' :'0',:0,0:6::800 00:'1 90,70'0 ::8017:8:0 ',:,:1' '7'6:1 7',:96: 0':7007''8'8'9:0 7:0,0'' '07:''0' '':0 0:,0800:2':5,6:515:70',''8' ' '9'68,2 ':'6,:7'50':0:0 ',:' 04::0:,8:0: ''16'' 00''400::00:0''::00:00:' :0 0'', ', :8:''',', 0:0: :6'1 \n",
      "----\n",
      "iter 10, loss: 65.861092\n",
      "----\n",
      " 5'5,09:,0:9:1:8'0:1 41  ',931 7:0:0:0:9:0,010:7,'9:,:0' 0,758 ' '3:5 0:8:8',:'1',765:':010:8' ,:8,:790'  8' '17 '1:5 '87  1 '304','11394:8'04'0 7: ':','6:6,,69 '5727 '7:8:07,'7'':8:',0', '3:0:0:',:00:80:8:3:0:76' 64871:8'0:,,0:8':2 0'8:8'8:0,,',::1'70:76'50:9 0, 0,6:401:'5508'8:,: 7:78 ' 097,:0:840:8:08:1:7',:8', ' ',:9'8:74:8 :8:8'8'17 8:0'0 '2:06', 1 6'151'3103202934 8: 30:2:0::00'18:6'1'4'40',032'8' 080:8' :640:,0:8'207 ' 6:80' 50::: 0 070 0:0:', ' '150293, '01 8 17:8:0'50:88:,:500:,:20'8:7:8', 700 0'7 '610'1:92:077,:41:94:, 4,:568:8 , ',90:67:,:8:6'0:10'070', ',023':5':91 410:805'55:, '682'3'8:070:,8:0:70',6' 2'61',8:631,57,'8:48 8 7:7  0 '7'8:8 ' '0':',:' 8,:, '0:78,: 010'34'5:8',9,0:8'0: '10:8:82'10:80:0:7::0 7''7',',932::8:'68:, '1'903:4 0'''8506'2:',0:5, 47 '95',0'4: 0:',8:8:'9''970 8':0:,'12:8:',:7'11 2:34', '8,'665'8 :,06 7'458:05 ':8'1:8' ', ' 5'2'8',:8:230, 907:'1 '027 84:5:', '8:'::7:80:7 ,:8':4'1:,0:,'42 '87 06:5,89:8','7060:08:79'0','13 ',51 599'25'8'1:01':,09',:9':818', 9419 '7', '6'73:':8':,:,:8' '4',:73700'2:80:0':3, '0',3:04,''52', '98:4'0:80, '81'10:''78'3,0 ' '4:',:8:8097002 1 0:5'7'6''0'17'8',584 072:,797,',2'19'406'0:0'08'6 '84908:8:'82'0:2 09':8':8'7''2'4:2: 91 ',0:12'8 ' 03', 6:':6 '11120583328'0:,:2:7:75'1'68:8:0': '2:87:0 :00:7',10',7578:520,820,, ' 02:068::8'8, 8:5:0:, '5 '8 700 7'0:0:70'8,:8:78:,:',:0:1 ,:70:0 :01 ',294'4:7:0:2078:17 0':8 '8:50, 50:5,4:57'6:7:,02 ',03300:5'70:0:0':9 0'02:,5:5:1',46 8',:57 '1 ':9:5'4'0'6:'9:80:8 7:5,0',':0081:7:1:  \n",
      "----\n",
      "iter 15, loss: 65.767026\n",
      "----\n",
      " 63:, :81''4:71 7,4'',  6'6'' '278 ','43:402:7 :',':'1:7985' '14,:3',:':40:87,2 ',:6'5009: 2:8 ',   ' 9','75 0:1:4 4,0', 3, '8:80', '3:7 0',0 '137'23'7', 5:','3'1'2:6:90,70'8:,8:',0,7:':''8:470, :, ','335,'2:0:8,:9,:7'4 7:,'8',490:0:0':0098:,:8:,0',:','3854 ':',3' 25:7,::7 0''2:6'1'1770:8:',', 35'2'1:7', 78:, ''1 '1 76'6030'78:,:'04'1 ',0:8, 6'06 9,':80,58'4,:,:, 7':41 '04:4', 78',: ', 0:6'6'1:831 0:4002 ', 0 '7 ',75 8:, '0 '09'1'32,'36:0'1 4:70 ',:79,8:2,:2'5', '7 0:5:78:0 :8:':748'8''1,:90,:7'07':8',7 0',:7',0:5'1:7'4:5:8:':9,:90,,5'1:70:6'7,8':8'0 :8',', 7 ',:0:2, 0:7' 09:1:7: ',4 '99,6,,103770 ',77 24:,0:' ' ',02371:8'0''02' 5',:9'8:7'',:6'1' 71'7:0'2'5750:'2:80, 25:8:,:7','1:7030 4' '1'3'0'4:, '4','3:8:, '2:9 :2, '70:0:7:01','3 '7':'76',2:,:5'2'8: :0, '3:80'1:7',:', 79',' '66:8', ':'8:8',:70,0 '0 '0:8:8: :':1:48: ' ',05038',8:8:8, 0'2'',:3:78'5,:8:, 00'9 4:'8',72: '0:7, ', 0:8:1 7' '1578:1 '7'14',52 5'0, 7'7:6:90 :9:8'2,174,:0 ' '1:6:7'1', 79'8'1'04, 9', ','62:69 ' '0:7 6: 6',1'1392'87:4,:8' :0:,756'5 0'7'3:6,95,:52',',137:3'0',:4:0 ',3,5' '10364:6:'0:8'0:8:,:, '974:0'19',:76 '6', ,,58:','26:8', 2,66'0330:,0'4' :0,'7'' 0:58',92, ':'020:0':8'8'15',:7'8:0:0 0'8:8  '2:8'1:707 '1:78,8'02'0 :8:8', '9:80 ', 01'554','3','3'6:2','96  ' ',:5:70:7', '5'8':0: 1:, '38 ',92',:5':8:2:0:0 :8:0:'',504:06 ,'1'4:7 2',00:7:1:'5' '1 '98' ''7' 2'4'0:0'7 '501:0:,2'0:80,01':0023:' '8:7'0',:7 '1 '19:5', '1:38:6:7',0:0'8:5:9'8'8', '6:8:',:527, 7:8'7:0',:2:'68:,0750:,4'2','8:7'71,',4' 0:5':', 8:71 \n",
      "----\n",
      "iter 20, loss: 65.670478\n",
      "----\n",
      " :0 ', ',  '15'86:7:8:09', 5  1, '0:50,'1'',:78:8', ','1:32 66:6'1:57:8'9'0'', 0:4'4' 7 010',::0'4,  '8',:05,'0'0:7'  ,'00:00'1 ',:9'1'1'1 375:84 1 8 ',:, 40 0 '77', ', ',:66:18 ', '1'03:'5:77'',50, ','1:6:'9 '8'0,40:, 6',0 ,'0',5'2:65:6,0:,8'','8'',6 53:'1:76'0 0','2', 76:7,0:',:5',0'1 0:8'460'0',:1:,'1'037',0:3 ', 0'107',:',',:2'1 633:6',,:0:8'0 ,:, ', 0'0,:68':8'1'0:0'1',:8:0',:'07'9'0' 4'  :,2:,'1','6:50'800,'20',00,'7 0 900   '0'04'0'7:',:0'00'0'4 52,0'7'0:82, 0 , '050 '2''7'5':,00 0:,,,' 0 '58020 '5: '1'2 '7:8 :,'7:0'7',9',:,000' ,'69400,08',, '07510,'9', ':,:7,'0:',: ''3,1',',0:6077:8 78',8 ',5 ', 0:8''0'8,'0'0'6'',:0:0:74,5'8,':7 0, ,',',7:79,70, ',:8  '8 :0',:8 ',',:2', :8: :  0,',:70,','' '6450,'1 '1:734:8 2 8000',480 0'', ', '44':0 0,10:,'0',0',:8:4 ',' '6:0 0:20 ', '14':64'00 8',:2,'7' ',0:18 0:',', ' 78', 0',5','1:67:7:6'0'':801 40:,0 0 0 '0'1780, '0'14 0:0:0'40',01 :,:,'59,41 '1':0, ,0', '6'1:2:0:','2',0'6'157632'6:12:10:'0', '6',:,4 '8, 0'00' 00 0 '0, ' ',:2', '0:,00' ':0, ', ',:6 50:',','2:7:8'00  ,:,':8, ':2 0 '55 :','71'0 '1:6'7,0:, :1'4 :0,' ', ' 552'',',0961:7 ', :70',', '750 5','9:''6,'0', 01 7070 ,0'0:8'':70,0108 :',:,:01 ,8'0,'2',9'10, '035  0'0','20','1:6:282:'638:57:0', 0 ', 50: 0'0:0002:','80800'1':0'8',:0,'', :970' ', 0'', '0:68:7:86 0, 0:':8,4' ''090'2''1'0:9:,'5040 :8'30,'2:''1 '24 09,:0':, '8'1':'2:,',08:0',,1', 63:8' '1:8'173:, ,'5', 10:86'79 1,':200,'8, ',',:7,':2:8:5,',0 ',:8:',0'2:0', 010:,0:0,'00 ':2:0',:0,','1:4700'1 ',08:00':0'','09'2:8 ',' \n",
      "----\n",
      "iter 25, loss: 65.548201\n",
      "----\n",
      " '6',',:7'6',:6 ':8,'1:1:7',56:8','97, '09' '108  79'960,'1 ',:'0: 06',:4' 0:05', ','57:8 8, ',7'2'6'1:8:7'', 2:8':5',4',3'570:6 ',:87'40'04 '10:1 '109'1  51:05' 08:095', 8',:8', ':7:8  ',7:6',0:8:0:0', :',3''2'000:2:7:, '1:6:40'01 0 ',8:80'0' : 6 '8:0':8' ',:,:80:0:,'6'2':7:',6 '0','1:7, 6 '0',:::84'03',',2' '76:0: '0:4 ', 0,0:6:0 ',:701:0'09'000,'0'1:8:7', 3,6:,5'04','1 53',:0:, '1'1:6:8 0 0',0:8'4910,2:0:0 8700', 7 7  5'0',0:7:05 8 10'1:0', '1:6:90:'706,', ','30:6000202 '2:0',06'0'005:5 '2'3205 '29''0,'65',0:6 01:,6:009, 020, ',''2:8:8'80','5:201'0 0', ',:0: '1 :8:5',:6 8:, '0:8:705',5 '90, ', 770'1 '6:00:,'8'3:690',:'2:,:8':70 '7, ' 0'0:8'07: ,0:80',', 9:,'2:, :97',800'4'0'40:0300,, ',:,'1:5:3,0,:,'5'002:1:0'2:6:8  ',00,:71'8:7 :8', ',:1 :9 :7 ',0:7',0,9:8:':,5'04 ', ',7:8'',:8 '0:8 0:': ':0 '7 90, 6'',', ',9'1:2:8:0:09 0070 '0:8'00 '04', 50', :8'50'2:0,0,'1:8:',:87',',:7 0,:0'0 '12'1:8 9'40:', ',17:80,'1:8:5:,7, ', '1:556 0,,'::8,6 8,'' 2 70'0'0'1:72:71','5790 7 :, ','6:20',:8', ',77:7' 0'','171:4:85', ', :5 ','10:759'0:,,964,0 :,:82 0260',:58' 3,0',',:65'09:1', '6:0:0 ',',:45:00 ,'002 :8:8',:4:,''16:1 '107:509 1 ', '1:,:72',','6:90:8'55,06'8 7',:'0',:96':8 ',0 6',:0:0  ' 080,::040 ', :', ', 3:7'0800'0:0:'1 0:0'70 '0:0', ':0, 779'8',0:7 '90,:0'0,',:07:,1 0:7'702' '9:0: ',:8'4:0::,'104'8'0:8:8:00 ,0:8'10:8'2 , :04,''15: 0:'0'1 0'8:7'0420,03':150 '2:, ',91 ',3:,'12:77'0:0'0:',01:68'1:8: 00'0:, 0'1:7 ',6'1:46,:020, 9 0 0'7:80''0, '8:5' 7:8,:0:0:7','6'9'8'0 '2 38'0:70:0':0''1 \n",
      "----\n",
      "iter 30, loss: 65.440161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " '',0:5'6:8 0','7::,'1'0:6:0:8  '1:8', ',:7 ', '2'0:7' :8',:',7',04040',5', 8', 230', '7'3', :, '5490:'2 ',0:860,00', ',0:8', 4: 58'0'00 ',',9:5,3 '0:5002,:07',70:, 2 0, ',:8'0:0, :', '1: '0,:70: '9'1:0:107001,0,'1:0:0:,', 7',''16:580', ',', 6 '5, ' '2:8:4'0',0003' 190, 0',0'2:803'1 ,0' 8 0',:8 8068'', '8:,:010, '1'1:78'0:8'4',:5'' '40 ', 2:74 '1:0,':8, :', '8:0':8'0, 8',075 ', 8:8:'7'',3',,: ', :,:'46:,0 '8:8 0', '44, ',:4 00 0'6','26:8 '0:0,',650', '0, ', '1:730',200,0',0 ', '9:8',:5 0',:20, 01:1, '1 ' 604 0:,',:9'60:',0', 7'01:8:8000:7',,',:8:8',:5,:'0'65 4 , 7, 2',070, 0:5:, '18'0:77 0', ':8 1 ', ',:70:70, ',:8'0',3 '2 0:0',:70', 40'1':2',0,0'0'5:1 ', 0'6:72:0:850'5 82 00 00:', ','1:8:780' 0,20'8'9'0',3'7:', ',  :0',00:50 0'90', '1 '5',7:8', ', '036803 4,1,8 '0,0:0:0'10',:0', ',:8:00, '0:8 0000 ':14019,0'000'0:',:'0:7 0,8',4, 4',44','008 , :48,'52, ''01 '2 :16:7 ,':6 ,0','94',',7:, ',40'0',':8:8 00'9'5 5 8 90 ', ', '2 6:8:89',:'16','9'0:0,0'0'','0'32:2 8':3:, '006:,', '9:'3:''1'177 ',0:71 '2:1',0,0:,0270:763''8',',:7','84:8 0:0'0:8'0', '8:, 3000,, 0:8:8, 0'8,:400, '100:8:', '0'1:7:51', ',31807091 ,04 ',','78'1::7:090,:0',:80,2'0:0 '1:80'3':5',:0', ':'18'1:6 '03'8'',0:8'8:8'4'1'2'0'0:00':8', '1:10:8 0'08', '1'18'6:8 0,:0:,0:  1 040',5,'2'6',:6'1 ', '1:790:080''7, 0,8 015030 ,9 ',:0404:,63'',0:,0'0:6''1 '0'78:0',0 4'01:009:03''1:0'1,  ,: 0:4 :1 ' '5:400  ',:8'18'0:7:,5'02'8': :50', '4'0' ','1::7:0,0:'6 '12 ', ',:7:,:0',0',:8'1 ':70'2'40050,:7 0:,'1'9:00060,30'0, '1 0:0':9'00 \n",
      "----\n",
      "iter 35, loss: 65.312316\n",
      "----\n",
      " 5:0'8:54:, ', 4', 8:8:7,0'1:7' '02:757409 0, 5:8'6':00: 0:8'2 08520'0,:8,0'0:10:4000,'00:7 ,:7: :, '8:'8'0:70:10',:8:7',:2:078700'0''8:8','7:1:8':7001 9 440,0',002:8:7 0',7:1 '6:8','8:70'0', :79', '6:8',:8' ',:8:8:3',:0,90', ' '7:8:8107:'1 '0'71 8:75:80,0':5'8:''6'8:7 '8',:40'3 '0:8''2',:7 4: '1:7 :, '0:73,:20 0',3:09 ''3:,',:5'0,533:'', 8',430,0:10'1:7:80','8:85',:5'8'8:1:6:0:80'1 ',:8', '59: 03050 '2'1 ,'1:9'2:8:'4:5',0:5'04:7:8',05:,'8:8', ' 8:7:6 00',0'6:2:8:2 '90','1:8:0:1  9:2:'80', 0'1 70:7::8'0'5:0:0: 0','127 7:7'60 '18'1:430', ', '40:8:0:6  ','1910,378:80'608',036,', 6 , ','777:0409',8:8', '1 72'1 04:8:300, ,'0':77:04 6:', :75:,:8', ',:0:760'0:80'8: 9','2 ', 4:8'0'8:7',', 775', 7 9 ', 9:80:0'8''9'700','1:777 :0'902 0, 5 ','6:7:'7 ' '1:7790', '1:50:, ', 704'0' '4:8'6',':5',0795'9:0',0:',:78 77 ', '2:7:7'09:70', '1:8:8'0:009',', '9:, '1:9','4',:71 '1 '335013 5 '0:8:0:0',:3',:, '0':2:7'77:7 '0:,:2:80''13:0',36'0'0'1:8 ',:5:0'6 ',0:79:8 700'1 ', 9:,:0', : 0' '5:700'0:80'0 9'', ',0::9 '1 56:8:0',57:, '01:8:6'0:77770:0:,'8:0:, '0:5'47009016,0, ' '6::, ',072 8:1:7'1 '48:7550: 0', '16:6'1:5 7 6 8''2''1:75740', '1:0:5:720:00', '0:7:8'90'0'0 :59', 73503'0', ', '1:76:', '19'9'21 778:8, '2:90:'0:0 '0:8 ', '1:771 :7:0:8:8', '072:2  6'40:'0'1 34 ,040'0', 7:,:6','5, '4 ',:79 ',0:69'0'0:24 ' 7'8:74'0'5:7 6: '0'2:7 1:7:7', '130:74,8:8:':8:0:, :7 , '591 3,0',980'10:803', ' ', 8:75,:0, 7', :0: '6: 0:0' '6'1 2:8:0', '2:5 '6','0':131:776:10:8 1 :8',:8:8:',:8'11:7:'80'0:80', ', '4 ',:2906 \n",
      "----\n",
      "iter 40, loss: 65.162055\n",
      "----\n",
      " :2:7:500',:8:8 84:, '1:8:80',0','77:7:03:0',:, ',:'0:0'551 7:658'0':827010:80740:0'06', '1:8:58'0:840, '8:86 '13 7'70:02',063:7':03, '1 :80', '6',:0',:7900',', 0:8:8 '10:770:,'1:8'6:8:870,0:8:69 ,6 2005'6',5'8'1 '17'2'',:57:8',:0'6',:0'1 01 0:'1 577:,0',704:,0:8'0:7',', 78'9:0 10:80'1 23073:,:8:9'8'0,8:8':8:0150',',::70', 0',:80'1:8 '6',8:860,9:8:':8050'0, ', 7'8:8:03:,:8005 '0:', 7:6', '1:8:,:8 0:6:190:, '1:7',:80020,98:, '2:66 ', ',:'1:70'8:80', '1 '10:1:8:7'6':61:50',:8'0'1:5, '0:925:,'1 8:8:6:8', ',2:8:7 0:070:80','1 02:01 ,:0::7:, '1'2:9'40,:2:05,9'6 490,5,'1 '18'2:5:8 '10', 0'6'240'1''8'184'', '6:8:8'0'2'0' 72 2'16',8:8:0'10: 78'06'004',:6'28:6'1:7:8 ', '17718:0:8'5'8'1 8:800:2'09',:0'0:1:8'', '1 '',9:,0'1:3',:04:7',:8:'6',', 7:9',0'2:70'0:, 300:0',4:5800,8',:8'18',:6'1:8', ', '7 0047 0:',:7', '52'1 '1':'98,0, ', 1:'1 70:8'1 '14:7 0'2:99'0'1 ', '1 '1:'1:8:0'6'18'2:8:0'8',:8:07 0', :8:,:6:6 70',: ',:78':8 '',:770030''1 :0'1', 0:'178'6:80',:8:0'5'8', '1 6 '2'1 8'1 4 ,:1', 53'6 6 ',:8:,2:'0:8 00','6:8'2'0:8900''1 4:,0:88 0:8',:0'6:382',:', '1:7'1 '1073'1:7',:6''04242'',0'0 6:8', ',:78: , '0:5 ',:7',70:8'6:'2'50''1:71:',:80',:40:, '1:7:4'1 8:862:','1:8:8:03 7', ', '1 0'1'7:5605 5', ', '1 2:9:030', '1 '9 44 0, 51 '0:8:,90'1:802',',:4580',''1 090'2'2 7'0:8:7:0',', 30'1:751:,',:8 '5',:80'1:8:8'0 '8:82:'1 ',:5:0:, 0', 80:401'1:8'0'0:6:8',9:',:70'0' 9',47',590, ',:8:5:5 5:2:0', '1:2:7:0',:8', '34,,:809'''1 '578'0'1:50'1:8:304, 2 80',:70:03', 0:8 09:6 56 0',8:3,5'2 '6'::7', ', :8 \n",
      "----\n",
      "iter 45, loss: 65.062400\n",
      "----\n",
      " ','1 ':8:750, '0'2 7:, ',:0' '1:75'60'6:,:8'09'8:1 '6',:5', 60, '5:5500'7, '0'17:6 ',:8'40'15007 '03 1:4 :5,:0', ',:8:4 0,0',:8', '1 0', 4:0660'8:7',0'1:2':79':1 ', '8:7 8'0',:8:0'0'1'1 70:77:0:0'9'0'0',:8:6'0'0:8,' ', 0:9:,'6'0'178:8', ', 9,0',:, '0',0:77'0'779:040'1 4:,0', ',0:'6:78:: '1'1:4980,3:0',:827008000,','1:70:7:0'6'1 ', '1:8:8:07 00',000:51:82:8',:5900'00:0:0', 47 0',0:8',8:', 0 ', '1:'48'0':1 703,84'1'0'78',:8:78',', 76:00'18'1 '1:01:6:8 , ', '0 018'1:8', 032:7:80, '8',67 0:0'60'20'0'1 '8:8':,:8' '2', ',:831 '2:8'0:', ', '1 '17'20, 074:80:25',0'1:2:4::0, ,'6:0'0' ',:5:8:, ', ',8 758'050 ,:90, ':5'10, 5:8'0'1 '8:8:0, ', '1:830', 8'1 79, :90, ,0:0:0'0', 0'354:8:, '13'56:, 0',', 5'1'177' ',:75, '1 ',:9',:8'0:5 70'6'1:450',:8:0:,80:2:,0:50'00'20'0',:7'1:8:803:02''',:5:6 0 ',042  0'2 '0'6 40', '1 '1 0,2 63:0:800'40:8, '1'8'1 79:8:0:0'1'1:751:8 ',04:'0'3 ','4'1:6 ',:04'0''1 7 ':8:8 ':', ', 48:50:8:'0:70:0'6:80 0'5300'8' '0', 9:8:80:0'14 0200,'15'6 ''1:70:8:8:8:02,0'8 ', '0'77:69 :', 900 :0:1:0',:60'1 '140:'48:0'1:70:0:0'6'1'1 9:0'0:8:0', 1 '0 '1:1 8:700', '1'1:8:8:8 '0:8 0'8', '0:78'0:178:, ',:750,0', '2:8:8:1 '','1 ,1:'557'0:6:6', '0'18:8:,0'1:8: '4'0',:8:8'60',:8',0', 1 1:7:82:8 0500:6, '1 '1 '1:4:1 '0', 6:6'1 0'1:7:8, ':8:, '7'1 ',:5:77', 0'1 '1 4:0:, 0'0'6:8', '1 75 '80'1:8:50'530', ',555,0:5050 2:'8:7 0:3',1 '4:5:0:, ', 0:8:90,3, 00, ', '0309:8:0','0'75 9:02'06:0:09:, ', 400,0:908:6:2:8:85'0'0',:70:8:8:6',:8:,'1 , '1 7:8:0'7 , '1:, :80' '0 51', 4'402 0', ',8:7', ', \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# end-to-end memory RNN\n",
    "import numpy as np\n",
    "\n",
    "nruns = 50\n",
    "noutput = 5\n",
    "\n",
    "# data I/O\n",
    "data = open('notes0.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 10 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxu = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Wuu = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Wuo = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bu = np.zeros((hidden_size, 1)) # hidden bias\n",
    "bo = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, us, os, ps = {}, {}, {}, {}\n",
    "    mi, pi, ci = {}, {}, {}\n",
    "    us[-1] = np.copy(uprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        us[t] = np.tanh(np.dot(Wxu, xs[t]) + np.dot(Wuu, us[t-1]) + bu) # hidden state\n",
    "        # pi=softmax(u*mi)\n",
    "        mi[t] = np.dot(Wxu, xs[t]) + bu\n",
    "        pi[t] = softmax(np.dot(us[t].T, mi[t]))\n",
    "        #ys[t] = np.dot(Wuy, us[t]) + by # unnormalized log probabilities for next chars\n",
    "        # o=pi*ci\n",
    "        ci[t] = np.dot(Wuo, us[t]) + bo\n",
    "        os[t] = pi[t] * ci[t]\n",
    "        ps[t] = softmax(os[t]) # probabilities for next chars (=softmax)\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxu, dWuu, dWuo = np.zeros_like(Wxu), np.zeros_like(Wuu), np.zeros_like(Wuo)\n",
    "    dbu, dbo = np.zeros_like(bu), np.zeros_like(bo)\n",
    "    #dmi, dpi = np.zeros_like(mi), np.zeros_like(pi)\n",
    "    dunext = np.zeros_like(us[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        do = np.copy(ps[t])\n",
    "        do[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        \n",
    "        # pi=softmax(u*mi)\n",
    "        #dmi[t] = np.dot(Wxu, xs[t]) + bu\n",
    "        #dpi[t] = softmax(np.dot(us[t].T, mi[t]))\n",
    "        \n",
    "        #dWuo += np.dot(do, us[t].T)\n",
    "        dWuo += np.dot(do, np.dot(pi[t], us[t].T))\n",
    "        dbo += np.dot(do, pi[t])\n",
    "        \n",
    "        #du = np.dot(Wuo.T, do) + dunext # backprop into u\n",
    "        #print('shape ci: %s' % (str(np.shape(ci[t]))))\n",
    "        du = np.multiply(ci[t].T, (1-pi[t])*mi[t])\n",
    "        du += Wuo.T\n",
    "        duraw = (1 - us[t] * us[t]) # tanh'=1-tanh^2\n",
    "        #dbu += np.dot(do, pi[t]*np.dot(duraw, (np.multiply(ci[t].T, (1-pi[t])*mi[t]) + Wuo)))\n",
    "        dbu += np.dot(pi[t]*np.multiply(du, duraw), do)\n",
    "        \n",
    "        dWuu += duraw*us[t-1]*np.dot(du, do) \n",
    "        du = np.dot(pi[t]*du, do)\n",
    "        #print(np.shape(duraw*pi[t]*((1-pi[t])*np.multiply(ci[t], mi[t].T) + Wuo).T*xs[t].T*do.T))\n",
    "        dWxu += duraw*pi[t]*((1-pi[t])*np.multiply(ci[t], mi[t].T) + Wuo).T*xs[t].T*do.T\n",
    "        \n",
    "        #dunext = np.dot(Wuu.T, duraw)\n",
    "    \n",
    "    for dparam in [dWxu, dWuu, dWuo, dbu, dbo]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxu, dWuu, dWuo, dbu, dbo, us[len(inputs)-1]\n",
    "\n",
    "def sample(u, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        u = np.tanh(np.dot(Wxu, x) + np.dot(Wuu, u) + bu)\n",
    "        mmi = np.dot(Wxu, x) + bu\n",
    "        ppi = softmax(np.dot(u.T, mmi))\n",
    "        #o = np.dot(Wuo, u) + bo\n",
    "        c = np.dot(Wuo, u) + bo\n",
    "        o = ppi * c\n",
    "        p = softmax(o)\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxu, mWuu, mWuo = np.zeros_like(Wxu), np.zeros_like(Wuu), np.zeros_like(Wuo)\n",
    "mbu, mbo = np.zeros_like(bu), np.zeros_like(bo) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        uprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % noutput == 0 or n == nruns-1:\n",
    "        sample_ix = sample(uprev, inputs[0], 1500)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxu, dWuu, dWuo, dbu, dbo, uprev = lossFun(inputs, targets, uprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "\n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxu, Wuu, Wuo, bu, bo], \n",
    "                                [dWxu, dWuu, dWuo, dbu, dbo], \n",
    "                                [mWxu, mWuu, mWuo, mbu, mbo]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4090909090909091, 0.13636363636363635, 0.13636363636363635, 0.06818181818181818, 0.1590909090909091, 0.09090909090909091]\n"
     ]
    }
   ],
   "source": [
    "# biased dice\n",
    "import random\n",
    "from numpy.random import choice\n",
    "\n",
    "def rolldice(b):\n",
    "    return choice([1,2,3,4,5,6], p=b)\n",
    "\n",
    "def hit():\n",
    "    hits = []\n",
    "    for d in range(1,7):\n",
    "        #dice = rolldice([1./6.,1./6.,1./6.,1./6.,1./6.,1./6.])\n",
    "        dice = rolldice([0.3,0.15,0.15,0.15,0.15,0.1])\n",
    "        if dice == d:\n",
    "            hits.append(d)\n",
    "    if len(hits) > 1 or len(hits) == 0:\n",
    "        return 0\n",
    "    return hits[0]\n",
    "\n",
    "if __name__=='__main__':\n",
    "    #print(hit())\n",
    "    ddistr = [0.] * 6\n",
    "    for i in range(100):\n",
    "        for d in range(1,7):\n",
    "            if hit() == d:\n",
    "                ddistr[d-1] += 1\n",
    "    dsum = 0.\n",
    "    for d in ddistr:\n",
    "        dsum += d\n",
    "    for d in range(len(ddistr)):\n",
    "        ddistr[d] /= dsum\n",
    "    print(ddistr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
