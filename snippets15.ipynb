{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame.midi\n",
    "import time\n",
    "\n",
    "pygame.midi.init()\n",
    "player = pygame.midi.Output(0)\n",
    "player.set_instrument(48) # https://pjb.com.au/muscript/gm.html\n",
    "player.note_on(64, 127) # note_on(note, velocity=None, channel = 0) https://en.scratch-wiki.info/wiki/MIDI_Notes\n",
    "time.sleep(2)\n",
    "player.note_off(64, 127)\n",
    "del player\n",
    "pygame.midi.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 36979 characters, 14 unique.\n",
      "----\n",
      " 4767 11  :989680535227'56'':71 64077934' 2,,6586435:0::7'17 46,:867: 769'8 350'430961,:2960' 993:,984, 9550',5,90:6,'7985489'8 7'1548963,''1763,6844  2628 0,0846,9048'845:1770:13:0'78782: '7 ,486':735806 1'0'1:24238: 3501 254:505':4 50'5,, 5125013'7305021222,1'40:'889'74,44'5 1565285:7254159'123 :401985,734416914980191963,26'252017:, 2845 :,1 '1  674895 ,:134'9886720105 6''8:140 1,'792'55,4839'9578279 924:650996:36377',, 46'466,,'40:4,' 349 1556450525,7451465'1:03 4:71890650 4947821,030351:,,2,5 \n",
      "----\n",
      "iter 0, loss: 65.976431\n",
      "----\n",
      " , '1', ', ','6 ,,72:'0:851:',   30:7'8 :70 ''1 ',0 ', 5:'4 ', :7070:01: 082:62: 6, ',0'0:1 '0:2:40::7,2:87:', ::,1 '3 '1'0 ', '''0''9,2',06 '3:',:8:60:'182', '0', ', :::8 :80:,, ::0:1', '2:71:00 '',''8,2',0:'87 :8  '0:0:,0:',::',74:80:',2 '5, :70',8, '0:0:07::00 ','::0:7'61::061:,00'9 ',',::'0:': :5,, ',:4:10:',04', :5'6 ',0', 51:80 4, 2', '4', ', 9:82:',  ',0 ''0 ',3 ',0:5, 51,'2:50:',1::',0', '0:72'80'1', :0'1:68 :080', '4', ',0:'6 :', ',', '8 '8' 0', 13:77', ::7, ', :',  ', '1:'2:,0:0',::89:8 \n",
      "----\n",
      "iter 100, loss: 65.377330\n",
      "----\n",
      " 70:0', ', '0:55:80', '0::0', '0:50:80', ', '0:65: '0:70:80'8 '0:70:, '2:70:80', '2:75:68'6 '0:8 '0:'0:0:, ', '4:'0:0', '2:62:0', '0:0', '0:74:080',1'0:80', '0 70:60', '0:6'9:080', '0:75:0', '8 '1:00'0:09:0', '0:70:5:0'8:'0:70:, '2:78:680'3:70:70',:'0:80:0', '0:65:0', '0:72:0'0 '0:60:20:81'1:70:70'2:80', ,0:0', '1:78:67:19:1',8'2:80', '1:70:0', ', '0:86',:,0', ', '0:50:82', '0:70:0', :8 ',:'0:75:0::0', '2:0', '2:70:0', '2:76:1', '0:88:0', '2 60:0', '0770:70:0', '0:50', '1:50: ', '4:78:0', '1:50:0 \n",
      "----\n",
      "iter 200, loss: 61.523816\n",
      "----\n",
      " 75:80', '1:69:80', '9:53:80', '0:54:0',0'1::2:80'3:82:0', '0:74:80', '1:78:0', 80:70:0', '0:75:80', ', ' :5,:0', '0:71:8, '8:74:80', '4:75:80', '2:61:80', '1:59:2:'' '0:54:0', '1:71:0', '0:77:80', '1:75:80', '0:62:80', '0 65:80', '0965:0', '0:85:0', '0 50', '1:75:72:77:80', '2:59:80', ', '1:77:80'0:'1:76:70:11', '0:85:8457 '0:71:8', '1:75:80', '0:59:8,:'2:72:80', '2:54:0', '2:64:7, '1:75:81',0', '2:59:,0', ', '1:75:0' :62:0', '0:59:0', ' :79:80', '0:77:0', '1:74:0', '0:73:0', '1:86:0', '0:5,:80' \n",
      "----\n",
      "iter 300, loss: 57.190356\n",
      "----\n",
      " :', '2:79:80', '2:74:80', '0:73:0', '0:42:0', '0:74:80:87'1:77:0', '1:61:0', '1:75:0'' '7:52:0'1 '2:76:0', '0:64:80', '0:73:0', '0:74:0', '1:73:80', '2:70:0', '0:5::0', '0:77:0', '1:5::0', '9:81:80', '0:72:0', '0:75:0', '0:72:81', '2:63:0', '2:75:0', '0:74'0570:0', '1:74:70:0', '0:74:80', '1:70:80', '1:74:0', '2:7::0', '2:72:80', '0:71:80', ',:51:0', '0:70:0', '1:72:80', '0:75:80', '0:74:0', '2:79:0', '1:7':0', '0:63:0', '1:72:80', '1:77:80', '1:'0:0', :0:70:1:'0:25:80', '1:52:0', '1:77:60:0', ' \n",
      "----\n",
      "iter 400, loss: 53.038867\n",
      "----\n",
      " :880', '1:72:80', '1:72:80', '0:71:80', '1:77:80', '2:72:80', '0:71:0', '0:70:80', '9:74:0', '2:80:0', '0:71:80', '0:72:0',0'0:65:80', '0:74:80', '0:70'1:,1:75:0', '1:75:0', '0:71:1 '0:61:0', '0:42:0', '0:40:80', '0:80:0', '0:43:0', '2:77:80', '0:78:07, '1:75:80', '1:79:0', '0:74:80', '1:77:0', '1:77:0', '0:74:80'1 '0:72:0', '0:70:0', '1:'1:80', '0:72:0', '1:42:0', '0:41:80', '0:08:0', '0:72:80', '2:74:0', '0:71:80', '2:72:0',:'0:74:80', '2:59:52:80', '0:75:, ', '0:74:0', '1:79:80', '1:70:0', '2 \n",
      "----\n",
      "iter 500, loss: 49.205703\n",
      "----\n",
      " 0', '0:72:0', '0:60:80', '':69:80', '2:40:80', '1:53:0', '0:71:0', '1:58:80', '0:42:80', '2:56:0', '2:71:880', '1:75:0', '2:64:80', '0:51:0', '1:40:0', '2:79:0', '0:67:80', '0:44:80', '2:56:80', '1:77:80', ', '0:49:0', '1:70:80', '1:74'1:80', '1:73:5'' ', '2:77:80', :0', '2:71:80', '1:51:80', '0'48:0', '0:75:0', '2:43:80', '0:79:80', '0:82:80', '0:09:80', '2:48:0', '2:82:80', '1:51:0', '0:79:,2'2:52:0', '1:73:880', '2:55:0', '0:73:0', '2:72:80', '1:48:80', '0:64:84', '0:72:80', '1:52:80', '2:4': \n",
      "----\n",
      "iter 600, loss: 45.820505\n",
      "----\n",
      " 0', '1:70:80', '1:73:80', '1:72:0', '1:53:80', '1:70:0', '0:79:80', '2:76:80', '1:53:0', '0:79:0', '2:75:80', '2:72:80', '1:27:80', '0:74:80', '2:58:0', '1:74:0', '1:43:80', '0:78:0', '1:86:0', '2:70:0', '2:71:80', '1:51:0', '0:76:80', '1:72:80', '0:45:0', '0:70:80', '2:72:80', '2:58:0', '2:79:80', '1:43:80', '0:74:80', '2:51:0', '1:55:80', '1:72:80', '2:54:80', '1:59:80', '1:73:80', '2:50:0', '0:72:80', '1:54:0', '2:79:80', '0:70:0', '2:53:0', '2:47:80', '1:70:0', '0:41:0', '1:53:80', '0:70:0', \n",
      "----\n",
      "iter 700, loss: 42.697909\n",
      "----\n",
      " , '1:55:0', '0:55:80', '1:55:80', '0:72:0', '2:40:0', '1:55:0', '1:55:80', '2:43:82:80:62:80', '0'51:0', '1:45:80', '1:75:80', '2:54:0', '1:45:80', '2:72:0', '0:74:80', '0:71:80', '0:79:80', '2:77:80', '2:55:50'0 '2:80:0', '2:40:80', ',:75:80', '1:53:80', '2:63:0', '1:72:80', '2:82:0', '0:78:0', '1:90:0', '0:74:0', '1:86:0', '2:54:0', '0:77:80', '2:80:80', '0:48:80', '1:77:0', '2:55:0', '2:82:80', '1:50:80', '0:72:80', '0:65:80', '1:72:0', '0:79:80', '0:65:0', '1:82:80', '0:74:0', '1:64:0', '2:7 \n",
      "----\n",
      "iter 800, loss: 39.862944\n",
      "----\n",
      "  '2:79:80', '0:72:80', '2:70:80', '0:74:80', '2:69:0', '0:66:80', '0:60:0', '1:72:80', '0:60:80', '0:74:80', '0:79:0', '2:80:0', '0:60:80', '1:76:0', '0:74:80', '0:62:0', '2:80:0', '0:70:0',:50:70:0', '0:75:0', '0:73:80', '2:50:80', '2:40:0', '0:69:80', '0:64:80'1::4:0', '2:70:80', '0:72:0', '0:67:0', '2:50:80', '2:80:80', '1:69:80', '1:68:0', '2:74:0', '0:60:80', '0:57:0', '0:75:0', '2:58:0', '2:71:0', '2:50:52:0', '1:72:0', '2:84:0', '1:62:0', '0:62:0', '2::5:0', '0:79:0', '2:55:0', '2:64:80', \n",
      "----\n",
      "iter 900, loss: 37.256241\n"
     ]
    }
   ],
   "source": [
    "# from https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "\n",
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "nruns = 1000\n",
    "noutput = 100\n",
    "\n",
    "# data I/O\n",
    "data = open('notes0.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % noutput == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 500)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pygame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-28f0dd588c7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmidi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmidi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pygame'"
     ]
    }
   ],
   "source": [
    "import pygame.midi\n",
    "import time\n",
    "import random\n",
    "\n",
    "pygame.midi.init()\n",
    "player = pygame.midi.Output(0)\n",
    "player.set_instrument(1) # https://pjb.com.au/muscript/gm.html\n",
    "notel = [55, 72, 77, 74, 50, 67, 77, 79, 55, 79, 81, 53, 58, 50, 77, 75, 75, 51, 74, 79, 72, 58, 77, 74, 75, 57, 74, 74, 72, 69, 72, 65, 72, 63, 51, 67, 67, 46, 70, 75, 72, 41, 70, 68, 48, 63, 74, 63, 70, 63, 70, 70, 75, 51, 69, 65, 67, 70, 50, 63, 75, 72, 46, 70, 65, 69, 82, 72, 74, 68, 72, 53, 72, 69, 77, 67, 75, 72, 55, 69, 65, 82, 51, 75, 77, 77, 75, 50, 79, 79, 74, 79, 79, 82, 67, 79, 63, 79, 79, 79, 75, 53, 79, 74, 74, 75, 46, 70, 77, 74, 55, 77, 79, 77, 82, 77, 48, 53, 79, 79, 51, 74, 79, 79]\n",
    "for n in notel:\n",
    "    player.note_on(n, 127) # note_on(note, velocity=None, channel = 0) https://en.scratch-wiki.info/wiki/MIDI_Notes\n",
    "    time.sleep(0.17)\n",
    "    player.note_off(n, 127)\n",
    "del player\n",
    "pygame.midi.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51, 63, 63, 67, 67, 70, 51, 70, 75, 55, 75, 70, 55, 70, 72, 56, 72, 80, 84, 56, 80, 84, 79, 82, 58, 79, 82, 77, 80, 58, 77, 80, 75, 79, 46, 75, 79, 74, 77, 46, 74, 77, 75, 79, 51, 79, 63, 75, 63, 63, 67, 67, 70, 51, 63, 70, 63, 75, 55, 75, 70, 63, 55, 70, 63, 72, 56, 63, 72, 80, 84, 56, 80, 84, 79, 82, 58, 79, 82, 77, 80, 58, 77, 80, 75, 79, 46, 75, 79, 74, 77, 46, 74, 77, 75, 79, 51, 79, 82, 75, 82, 82, 80, 80, 79, 51, 82, 79, 84, 77, 56, 77, 75, 84, 56, 75, 77, 74, 58, 77, 74, 58, 79, 75, 63, 63, 58, 58, 55, 79, 75, 55, 80, 77, 51, 80, 77, 82, 79, 51, 82, 79, 77, 74, 58, 77, 74, 58, 79, 75, 63, 63, 58, 58, 55, 79, 75, 55, 80, 77, 51, 80, 77, 82, 79, 51, 82, 79, 77, 74, 58, 77, 74, 75, 72, 58, 75, 72, 74, 70, 46, 74, 70, 72, 68, 46, 72, 68, 70, 67, 58, 70, 67, 68, 65, 58, 68, 65, 67, 63, 51, 67, 63, 63, 67, 67, 70, 63, 51, 70, 75, 70, 55, 75, 70, 70, 67, 55, 70, 67, 72, 63, 56, 72, 63, 84, 80, 56, 84, 80, 82, 79, 58, 82, 79, 80, 77, 58, 80, 77, 79, 75, 46, 79, 75, 77, 74, 46, 77, 74, 79, 75, 51, 79, 63, 75, 63, 67, 63, 67, 70, 51, 63, 70, 75, 63, 55, 75, 70, 63, 55, 70, 72, 63, 56, 72, 63, 84, 80, 56, 84, 80, 82, 79, 58, 82, 79, 80, 77, 58, 80, 77, 79, 75, 46, 79, 75, 77, 74, 46, 77, 74, 79, 75, 51, 79, 77, 75, 51, 77, 75, 79, 51, 75, 77, 79, 51, 77, 79, 75, 51, 79, 80, 75, 51, 80, 82, 77, 50, 82, 77, 77, 50, 77, 74, 82, 50, 74, 70, 82, 50, 70, 77, 50, 77, 74, 50, 74, 75, 48, 75, 74, 48, 74, 72, 75, 48, 72, 74, 75, 48, 74, 75, 72, 48, 75, 77, 72, 48, 77, 79, 74, 46, 79, 74, 74, 46, 74, 70, 79, 46, 70, 67, 79, 46, 67, 74, 46, 74, 70, 46, 70, 72, 44, 72, 68, 44, 68, 75, 72, 56, 75, 72, 72, 56, 72, 70, 75, 56, 70, 68, 56, 68, 70, 55, 70, 67, 75, 55, 67, 75, 70, 55, 75, 70, 70, 55, 70, 68, 75, 55, 68, 67, 75, 55, 67, 68, 74, 53, 68, 74, 67, 75, 53, 67, 75, 68, 77, 50, 68, 70, 77, 50, 70, 67, 75, 51, 67, 68, 75, 51, 68, 65, 74, 46, 65, 72, 74, 46, 72, 70, 50, 70, 68, 50, 68, 67, 46, 67, 65, 46, 65, 67, 51, 67, 77, 51, 77, 75, 48, 75, 74, 48, 74, 72, 51, 72, 70, 51, 70, 69, 53, 69, 79, 53, 79, 77, 57, 77, 75, 57, 75, 74, 53, 74, 72, 53, 72, 74, 58, 65, 74, 58, 65, 74, 70, 58, 70, 72, 74, 58, 72, 70, 74, 58, 74, 75, 70, 58, 75, 72, 77, 57, 77, 72, 72, 57, 72, 77, 69, 57, 69, 65, 77, 57, 65, 72, 57, 72, 69, 57, 69, 70, 55, 70, 69, 55, 69, 70, 67, 55, 67, 69, 70, 55, 69, 67, 70, 55, 70, 72, 67, 55, 72, 69, 74, 53, 74, 69, 69, 53, 69, 74, 65, 53, 65, 62, 74, 53, 62, 69, 53, 69, 65, 53, 65, 67, 51, 67, 63, 51, 63, 67, 70, 51, 70, 67, 67, 51, 67, 70, 65, 51, 65, 63, 51, 63, 65, 50, 65, 62, 70, 50, 62, 65, 70, 50, 70, 65, 65, 50, 65, 70, 63, 50, 63, 62, 70, 50, 62, 69, 63, 48, 69, 63, 70, 62, 48, 70, 62, 72, 63, 45, 63, 65, 72, 45, 65, 70, 62, 46, 62, 63, 70, 46, 63, 79, 60, 51, 79, 80, 60, 51, 80, 79, 72, 51, 79, 77, 72, 51, 77, 75, 51, 75, 74, 51, 74, 75, 79, 51, 75, 79, 77, 80, 51, 77, 80, 75, 79, 51, 75, 79, 74, 77, 51, 74, 77, 72, 75, 51, 72, 75, 70, 74, 51, 70, 74, 69, 72, 53, 72, 77, 69, 77, 77, 81, 81, 72, 53, 77, 72, 77, 74, 58, 74, 82, 77, 58, 82, 77, 75, 57, 77, 79, 75, 79, 77, 74, 77, 79, 57, 74, 79, 77, 75, 53, 77, 79, 75, 79, 77, 84, 77, 79, 53, 84, 79, 77, 74, 58, 77, 79, 74, 79, 77, 82, 77, 79, 58, 82, 79, 77, 72, 53, 77, 79, 72, 79, 77, 77, 77, 79, 77, 79, 77, 81, 77, 79, 81, 79, 77, 72, 53, 77, 79, 72, 79, 77, 74, 58, 77, 79, 74, 79, 77, 82, 77, 79, 58, 82, 79, 77, 75, 57, 77, 79, 75, 79, 77, 74, 77, 79, 57, 74, 79, 77, 75, 53, 77, 79, 75, 79, 77, 84, 77, 79, 53, 84, 79, 77, 74, 58, 77, 79, 74, 79, 77, 82, 77, 79, 58, 82, 79, 77, 72, 53, 72, 77, 77, 53, 77, 72, 81, 41, 72, 81, 70, 79, 41, 70, 79, 72, 81, 41, 72, 81, 74, 82, 41, 74, 82, 75, 84, 41, 75, 84, 74, 82, 41, 74, 82, 72, 81, 41, 72, 81, 75, 79, 41, 75, 79, 74, 77, 41, 74, 77, 72, 75, 41, 72, 75, 74, 46, 70, 74, 70, 74, 70, 74, 65, 46, 70, 65, 67, 70, 51, 67, 75, 70, 51, 75, 68, 70, 50, 70, 72, 68, 67, 72, 70, 70, 72, 50, 67, 68, 72, 70, 46, 70, 72, 68, 77, 72, 70, 70, 72, 46, 77, 67, 72, 70, 51, 70, 72, 67, 75, 72, 70, 70, 72, 51, 75, 65, 72, 70, 46, 70, 72, 65, 70, 72, 70, 70, 72, 70, 74, 72, 70, 70, 72, 74, 65, 72, 70, 46, 70, 72, 65, 67, 72, 70, 51, 70, 72, 67, 75, 72, 70, 70, 72, 51, 75, 68, 72, 70, 50, 70, 72, 68, 67, 72, 70, 70, 72, 50, 67, 68, 72, 70, 46, 70, 72, 68, 77, 72, 70, 70, 72, 46, 77, 67, 72, 70, 51, 70, 72, 67, 75, 72, 70, 70, 72, 51, 75, 65, 72, 70, 46, 65, 70, 70, 46, 70, 74, 70, 46, 74, 70, 75, 72, 46, 75, 72, 77, 74, 46, 77, 74, 79, 75, 46, 79, 75, 80, 77, 46, 80, 77, 79, 75, 46, 79, 75, 77, 74, 58, 77, 74, 80, 77, 58, 80, 77, 79, 75, 46, 79, 75, 77, 74, 46, 77, 74, 79, 75, 51, 79, 77, 75, 51, 77, 75, 79, 48, 75, 79, 74, 77, 48, 74, 77, 72, 75, 51, 72, 75, 70, 74, 51, 70, 74, 69, 72, 53, 69, 72, 70, 70, 53, 70, 70, 72, 69, 41, 72, 69, 75, 72, 41, 75, 72, 74, 70, 53, 74, 70, 72, 69, 53, 72, 69, 74, 70, 58, 74, 70, 75, 72, 58, 75, 72, 77, 74, 46, 46, 48, 48, 50, 50, 51, 51, 53, 77, 74, 72, 69, 53, 72, 69, 72, 69, 41, 72, 69, 75, 72, 41, 75, 72, 74, 70, 53, 74, 70, 72, 69, 53, 72, 69, 74, 70, 46, 74, 70, 75, 72, 46, 75, 72, 77, 74, 46, 46, 48, 48, 50, 50, 51, 51, 53, 77, 74, 72, 69, 53, 72, 69, 72, 69, 41, 72, 69, 75, 72, 41, 75, 72, 74, 70, 53, 74, 70, 72, 69, 53, 72, 69, 74, 70, 46, 74, 75, 46, 75, 75, 48, 75, 77, 48, 77, 77, 50, 77, 79, 50, 79, 79, 51, 79, 81, 70, 75, 51, 81, 75, 81, 75, 53, 81, 82, 75, 53, 82, 82, 74, 55, 82, 70, 74, 55, 70, 79, 72, 51, 79, 75, 72, 51, 75, 74, 70, 53, 74, 70, 53, 72, 69, 41, 72, 69, 41, 70, 74, 46, 70, 74, 72, 75, 46, 72, 75, 74, 77, 46, 46, 48, 48, 50, 50, 51, 51, 53, 74, 77, 69, 72, 53, 69, 72, 69, 72, 41, 69, 72, 72, 75, 41, 72, 75, 70, 74, 53, 70, 74, 69, 72, 53, 69, 72, 70, 74, 46, 70, 74, 72, 75, 46, 72, 75, 74, 77, 46, 46, 48, 48, 50, 50, 51, 51, 53, 74, 77, 69, 72, 53, 69, 72, 69, 72, 41, 69, 72, 72, 75, 41, 72, 75, 70, 74, 53, 70, 74, 69, 72, 53, 69, 72, 70, 74, 46, 70, 74, 74, 77, 46, 74, 77, 77, 82, 50, 82, 81, 50, 81, 79, 46, 79, 77, 46, 77, 79, 51, 79, 77, 77, 77, 75, 79, 51, 79, 81, 53, 75, 81, 53, 74, 82, 55, 74, 82, 55, 75, 79, 51, 75, 79, 72, 75, 51, 72, 75, 70, 74, 53, 70, 74, 69, 72, 69, 72, 70, 74, 70, 74, 69, 72, 53, 69, 72, 70, 74, 70, 74, 69, 72, 41, 69, 72, 70, 70, 41, 70, 70, 70, 70, 46, 70, 70, 46, 46, 58, 58, 62, 62, 65, 46, 65, 70, 50, 70, 65, 50, 65, 67, 51, 67, 75, 79, 51, 75, 79, 74, 77, 53, 74, 77, 72, 75, 53, 72, 75, 70, 74, 41, 70, 74, 69, 72, 41, 69, 72, 70, 74, 46, 74, 58, 70, 58, 82, 62, 62, 65, 46, 82, 65, 70, 70, 50, 70, 65, 50, 65, 67, 51, 70, 67, 75, 79, 51, 75, 79, 74, 77, 53, 74, 77, 72, 75, 53, 72, 75, 70, 74, 41, 70, 74, 69, 72, 41, 69, 72, 70, 74, 46, 74, 77, 70, 77, 77, 75, 75, 74, 46, 77, 74, 79, 72, 51, 72, 70, 79, 51, 70, 72, 69, 53, 72, 69, 53, 74, 70, 58, 58, 53, 53, 50, 74, 70, 50, 75, 72, 46, 75, 72, 77, 74, 46, 77, 74, 72, 69, 53, 72, 69, 53, 74, 70, 58, 58, 53, 53, 50, 74, 70, 50, 75, 72, 46, 75, 72, 77, 74, 46, 77, 74, 72, 69, 53, 72, 65, 53, 51, 69, 65, 51, 77, 50, 50, 48, 48, 47, 77, 80, 80, 79, 74, 79, 77, 77, 75, 75, 47, 74, 74, 74, 75, 74, 48, 75, 48, 74, 47, 74, 74, 47, 75, 67, 48, 75, 48, 77, 50, 67, 77, 50, 79, 72, 51, 72, 51, 70, 48, 70, 48, 68, 53, 79, 68, 77, 80, 77, 80, 75, 79, 75, 79, 74, 77, 74, 77, 53, 72, 75, 72, 75, 71, 74, 55, 71, 74, 55, 72, 75, 60, 60, 55, 55, 51, 72, 75, 51, 74, 77, 48, 74, 77, 75, 79, 48, 75, 79, 71, 74, 55, 71, 74, 55, 72, 75, 60, 60, 55, 55, 51, 72, 75, 51, 74, 77, 48, 74, 77, 75, 79, 48, 75, 79, 71, 74, 55, 71, 79, 74, 55, 79, 79, 59, 79, 77, 59, 77, 77, 55, 77, 75, 55, 75, 74, 60, 84, 74, 60, 84, 76, 84, 48, 84, 82, 48, 82, 82, 60, 82, 80, 76, 60, 80, 80, 53, 77, 80, 53, 77, 77, 81, 41, 77, 75, 41, 75, 75, 53, 75, 74, 81, 53, 74, 74, 58, 82, 74, 58, 82, 74, 82, 46, 82, 80, 46, 80, 80, 58, 80, 79, 74, 58, 79, 75, 79, 51, 79, 79, 75, 51, 79, 63, 80, 48, 80, 79, 63, 48, 79, 75, 77, 60, 77, 75, 75, 75, 74, 77, 77, 77, 74, 60, 77, 75, 79, 59, 75, 79, 74, 77, 74, 75, 75, 74, 74, 75, 77, 75, 74, 75, 75, 74, 59, 74, 74, 75, 60, 79, 75, 60, 79, 80, 63, 48, 80, 79, 63, 48, 79, 77, 75, 60, 77, 75, 75, 60, 75, 77, 74, 60, 77, 77, 74, 60, 77, 79, 74, 59, 79, 77, 77, 75, 75, 74, 74, 59, 74, 75, 60, 75, 60, 60, 58, 60, 58, 72, 57, 57, 55, 55, 54, 72, 75, 75, 69, 74, 74, 72, 72, 70, 70, 54, 69, 69, 70, 55, 70, 55, 69, 54, 69, 69, 54, 62, 70, 55, 70, 55, 72, 57, 62, 72, 57, 67, 74, 59, 67, 59, 65, 55, 65, 55, 63, 60, 63, 74, 60, 75, 72, 48, 75, 72, 74, 70, 74, 70, 72, 69, 72, 69, 70, 67, 48, 70, 67, 69, 66, 50, 69, 66, 50, 70, 67, 55, 55, 50, 50, 46, 70, 67, 46, 72, 69, 43, 72, 69, 74, 70, 43, 74, 70, 69, 66, 50, 69, 66, 50, 70, 67, 55, 55, 50, 50, 46, 70, 67, 46, 72, 69, 43, 72, 69, 74, 70, 43, 74, 70, 69, 66, 50, 66, 74, 69, 50, 74, 74, 54, 74, 72, 54, 72, 72, 50, 72, 70, 50, 70, 70, 55, 79, 70, 55, 79, 79, 71, 43, 79, 77, 43, 77, 77, 55, 77, 75, 71, 55, 75, 75, 48, 72, 75, 48, 72, 76, 72, 60, 72, 70, 60, 70, 70, 48, 70, 69, 76, 48, 69, 69, 53, 77, 53, 77, 77, 41, 77, 75, 41, 75, 75, 53, 75, 69, 74, 53, 74, 74, 70, 46, 74, 74, 46, 74, 75, 43, 75, 74, 43, 74, 72, 55, 72, 70, 70, 55, 70, 72, 69, 55, 72, 72, 69, 55, 72, 74, 69, 54, 74, 72, 72, 70, 70, 69, 69, 54, 69, 70, 55, 74, 70, 55, 74, 70, 75, 43, 75, 74, 43, 74, 72, 55, 72, 70, 70, 55, 70, 69, 72, 55, 72, 72, 69, 55, 72, 69, 74, 54, 74, 72, 72, 70, 70, 69, 69, 54, 69, 67, 70, 55, 67, 70, 82, 74, 55, 82, 74, 82, 74, 43, 82, 74, 81, 72, 43, 81, 72, 81, 72, 48, 81, 72, 79, 70, 48, 79, 70, 79, 70, 50, 79, 70, 78, 69, 78, 69, 79, 70, 79, 70, 78, 69, 50, 78, 69, 79, 70, 79, 70, 78, 69, 48, 78, 69, 48, 79, 67, 46, 79, 74, 67, 46, 74, 75, 67, 48, 75, 67, 72, 69, 48, 72, 69, 70, 67, 50, 70, 67, 69, 66, 69, 66, 70, 67, 70, 67, 69, 66, 50, 69, 66, 70, 67, 70, 67, 69, 66, 38, 69, 66, 67, 67, 38, 67, 67, 67, 67, 43, 67, 70, 43, 70, 72, 44, 72, 44, 74, 46, 67, 74, 46, 75, 48, 75, 48, 77, 50, 77, 50, 79, 51, 63, 63, 67, 67, 70, 79, 51, 70, 75, 55, 75, 70, 55, 70, 72, 56, 72, 80, 84, 56, 80, 84, 79, 82, 58, 79, 82, 77, 80, 58, 77, 80, 75, 79, 46, 75, 79, 74, 77, 46, 74, 77, 75, 79, 51, 79, 63, 75, 63, 63, 67, 67, 70, 51, 63, 70, 63, 75, 55, 75, 70, 63, 55, 70, 63, 72, 56, 63, 72, 80, 84, 56, 80, 84, 79, 82, 58, 79, 82, 77, 80, 58, 77, 80, 75, 79, 46, 75, 79, 74, 77, 46, 74, 77, 75, 79, 51, 75, 70, 70, 75, 75, 79, 79, 51, 79, 82, 55, 82, 70, 55, 70, 72, 56, 72, 82, 56, 82, 80, 53, 80, 79, 53, 79, 80, 80, 72, 72, 70, 50, 70, 80, 50, 80, 79, 51, 79, 77, 51, 77, 79, 79, 70, 70, 68, 48, 68, 79, 48, 79, 77, 50, 77, 75, 50, 75, 77, 46, 77, 68, 46, 68, 67, 51, 70, 67, 70, 75, 75, 79, 51, 79, 82, 55, 82, 70, 55, 70, 72, 56, 72, 82, 56, 82, 80, 53, 80, 79, 53, 79, 80, 80, 72, 72, 70, 50, 70, 80, 50, 80, 79, 51, 79, 77, 51, 77, 79, 79, 70, 70, 68, 48, 68, 79, 48, 79, 77, 50, 77, 75, 50, 75, 77, 46, 77, 68, 46, 68, 67, 51, 70, 70, 75, 67, 75, 73, 70, 51, 73, 70, 72, 68, 55, 72, 68, 70, 67, 55, 70, 67, 72, 68, 56, 72, 68, 73, 75, 56, 73, 75, 72, 80, 53, 72, 80, 70, 79, 53, 70, 79, 68, 77, 68, 77, 67, 75, 67, 75, 65, 74, 46, 65, 70, 74, 70, 74, 70, 74, 65, 46, 70, 65, 67, 70, 51, 67, 75, 70, 51, 75, 68, 70, 50, 70, 72, 68, 67, 72, 70, 70, 72, 50, 67, 68, 72, 70, 46, 70, 72, 68, 77, 72, 70, 70, 72, 46, 77, 67, 72, 70, 51, 70, 72, 67, 75, 72, 70, 70, 72, 51, 75, 65, 72, 70, 46, 70, 72, 65, 70, 72, 70, 70, 72, 70, 74, 72, 70, 70, 72, 74, 65, 72, 70, 46, 70, 72, 65, 67, 72, 70, 51, 70, 72, 67, 75, 72, 70, 70, 72, 51, 75, 68, 72, 70, 50, 70, 72, 68, 67, 72, 70, 70, 72, 50, 67, 68, 72, 70, 46, 70, 72, 68, 77, 72, 70, 70, 72, 46, 77, 67, 72, 70, 51, 70, 72, 67, 75, 72, 70, 70, 72, 51, 75, 65, 72, 70, 46, 65, 70, 70, 46, 70, 74, 70, 58, 74, 70, 75, 72, 58, 75, 72, 77, 74, 46, 77, 74, 79, 75, 46, 79, 75, 80, 77, 46, 80, 77, 79, 75, 46, 79, 75, 77, 74, 46, 77, 74, 79, 75, 46, 79, 75, 80, 77, 46, 80, 77, 77, 74, 46, 77, 74, 79, 73, 51, 73, 72, 79, 72, 75, 73, 73, 82, 51, 75, 82, 75, 72, 56, 72, 80, 75, 56, 80, 75, 73, 55, 75, 77, 73, 77, 75, 72, 75, 77, 55, 72, 77, 75, 73, 51, 75, 77, 73, 77, 75, 82, 75, 77, 51, 82, 77, 75, 72, 56, 75, 77, 72, 77, 75, 80, 75, 77, 56, 80, 77, 75, 70, 51, 75, 77, 70, 77, 75, 75, 75, 77]\n"
     ]
    }
   ],
   "source": [
    "import mido\n",
    "\n",
    "#outport = mido.open_output()\n",
    "niter = 0\n",
    "noteslst = []\n",
    "for msg in mido.MidiFile('bjs1031c.mid').play():\n",
    "    #outport.send(msg)\n",
    "    #print msg.bytes()\n",
    "    #print str(msg)\n",
    "    thismsg = str(msg).split(\" \") # ['note_on', 'channel=0', 'note=60', 'velocity=61', 'time=0.00168918958333']\n",
    "    # The velocity specifies the volume or force, with which the note is played\n",
    "    if thismsg[0] == 'note_on':\n",
    "        #print thismsg[2][5:]\n",
    "        noteslst.append(int(thismsg[2][5:]))\n",
    "    niter += 1\n",
    "    if niter >= 3000:\n",
    "        break\n",
    "print noteslst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e', 's')\n",
      "('es', 't')\n",
      "('est', '</w>')\n",
      "('l', 'o')\n",
      "('lo', 'w')\n",
      "('n', 'e')\n",
      "('ne', 'w')\n",
      "('new', 'est</w>')\n",
      "('low', '</w>')\n",
      "('w', 'i')\n"
     ]
    }
   ],
   "source": [
    "# BPE\n",
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "if __name__=='__main__':\n",
    "    vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,'n e w e s t </w>':6, 'w i d e s t </w>':3}\n",
    "    num_merges = 10\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "        print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-52fe8f31a26a>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-52fe8f31a26a>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    print 'data has %d characters, %d unique.' % (data_size, vocab_size)\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# from https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "\n",
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "nruns = 50000\n",
    "noutput = 5000\n",
    "\n",
    "# data I/O\n",
    "data = open('notes0.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % noutput == 0 or n == nruns-1:\n",
    "    sample_ix = sample(hprev, inputs[0], 1500)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame.midi\n",
    "import time\n",
    "import random\n",
    "\n",
    "pygame.midi.init()\n",
    "player = pygame.midi.Output(0)\n",
    "player.set_instrument(0) # https://pjb.com.au/muscript/gm.html\n",
    "# channel - note - velocity - time\n",
    "notel = ['0:70:0', '0:74:0', '1:74:80', '1:79:80', '1:82:0', '2:43:0', '0:74:80', '2:46:80', '1:75:0', '1:72:80', '1:74:0', '1:72:0', '1:80:80', '2:53:80', '0:75:80', '2:53:80', '1:82:80', '2:57:0', '1:69:80', '1:72:80', '1:72:0', '0:75:80', '1:69:0', '0:74:0', '0:75:80', '2:53:80', '0:74:80', '2:46:80', '0:72:80', '2:48:0', '1:69:80', '2:53:0', '0:74:80', '0:75:80', '2:53:80', '0:75:80', '2:53:80', '1:74:80', '2:55:80', '2:53:0', '0:75:80', '1:79:80', '1:79:80', '2:46:0', '2:51:80', '0:72:0', '1:69:0', '1:75:80', '0:77:0', '0:72:0', '1:74:0', '0:74:0', '1:74:80', '0:74:80', '1:74:80', '2:53:0', '0:75:0', '1:74:0', '1:72:0', '2:70:80', '1:69:80', '2:41:0', '1:70:80', '0:72:0', '1:74:0', '1:69:0', '1:75:0', '0:70:80', '1:75:0', '1:74:80', '2:50:80', '0:70:0', '0:69:80', '2:53:80', '2:51:0', '2:55:0', '0:84:80', '1:75:0', '0:79:80', '2:51:80', '0:72:0', '0:72:80', '1:70:0', '1:74:0', '0:70:80', '2:48:80', '0:72:80', '1:72:0', '0:69:0', '2:53:0', '1:74:0', '1:75:0', '1:70:80', '1:75:80', '1:79:80', '1:65:80', '1:75:0', '2:51:80', '0:75:80', '2:41:80', '1:70:0', '1:82:0', '1:74:0', '1:75:80', '2:51:80', '0:75:0', '1:69:80', '1:79:80', '1:75:0', '1:72:0', '1:74:80', '2:51:80', '0:77:80', '2:51:80', '0:77:80', '2:51:0', '1:77:80', '1:69:80', '0:75:80', '2:53:0', '0:79:0', '0:75:80', '2:41:80', '2:58:0', '0:72:80', '1:84:80', '1:75:0', '1:81:80', '2:51:0', '0:69:80', '1:72:80', '2:51:0', '0:74:80', '2:50:80', '2:53:0', '0:69:0', '2:51:0', '1:79:80', '2:53:0', '1:79:80', '1:67:0', '0:79:80']\n",
    "for n in notel:\n",
    "    ns = n.split(':')\n",
    "    #print ns\n",
    "    player.note_on(int(ns[1]), int(ns[2]), int(ns[0])) # note_on(note, velocity=None, channel = 0) https://en.scratch-wiki.info/wiki/MIDI_Notes\n",
    "    time.sleep(int(ns[2])/200)\n",
    "    player.note_off(int(ns[1]), int(ns[2]), int(ns[0]))\n",
    "del player\n",
    "pygame.midi.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2:51:80', '1:63:80', '1:63:0', '1:67:80', '1:67:0', '1:70:80', '2:51:0', '1:70:0', '1:75:80', '2:55:80', '1:75:0', '1:70:80', '2:55:0', '1:70:0', '1:72:80', '2:56:80', '1:72:0', '0:80:80', '1:84:80', '2:56:0', '0:80:0', '1:84:0', '0:79:80', '1:82:80', '2:58:80', '0:79:0', '1:82:0', '0:77:80', '1:80:80', '2:58:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:46:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:46:0', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:51:80', '1:79:0', '1:63:80', '0:75:0', '1:63:0', '0:63:80', '1:67:80', '1:67:0', '1:70:80', '2:51:0', '0:63:0', '1:70:0', '0:63:80', '1:75:80', '2:55:80', '1:75:0', '1:70:80', '0:63:0', '2:55:0', '1:70:0', '0:63:80', '1:72:80', '2:56:80', '0:63:0', '1:72:0', '0:80:80', '1:84:80', '2:56:0', '0:80:0', '1:84:0', '0:79:80', '1:82:80', '2:58:80', '0:79:0', '1:82:0', '0:77:80', '1:80:80', '2:58:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:46:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:46:0', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:51:80', '1:79:0', '1:82:80', '0:75:0', '1:82:0', '0:82:80', '1:80:80', '1:80:0', '1:79:80', '2:51:0', '0:82:0', '1:79:0', '0:84:80', '1:77:80', '2:56:80', '1:77:0', '1:75:80', '0:84:0', '2:56:0', '1:75:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '1:74:0', '2:58:0', '0:79:80', '1:75:80', '2:63:80', '2:63:0', '2:58:80', '2:58:0', '2:55:80', '0:79:0', '1:75:0', '2:55:0', '0:80:80', '1:77:80', '2:51:80', '0:80:0', '1:77:0', '0:82:80', '1:79:80', '2:51:0', '0:82:0', '1:79:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '1:74:0', '2:58:0', '0:79:80', '1:75:80', '2:63:80', '2:63:0', '2:58:80', '2:58:0', '2:55:80', '0:79:0', '1:75:0', '2:55:0', '0:80:80', '1:77:80', '2:51:80', '0:80:0', '1:77:0', '0:82:80', '1:79:80', '2:51:0', '0:82:0', '1:79:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '1:74:0', '0:75:80', '1:72:80', '2:58:0', '0:75:0', '1:72:0', '0:74:80', '1:70:80', '2:46:80', '0:74:0', '1:70:0', '0:72:80', '1:68:80', '2:46:0', '0:72:0', '1:68:0', '0:70:80', '1:67:80', '2:58:80', '0:70:0', '1:67:0', '0:68:80', '1:65:80', '2:58:0', '0:68:0', '1:65:0', '0:67:80', '1:63:80', '2:51:80', '0:67:0', '0:63:80', '0:63:0', '0:67:80', '0:67:0', '0:70:80', '1:63:0', '2:51:0', '0:70:0', '0:75:80', '1:70:80', '2:55:80', '0:75:0', '1:70:0', '0:70:80', '1:67:80', '2:55:0', '0:70:0', '1:67:0', '0:72:80', '1:63:80', '2:56:80', '0:72:0', '1:63:0', '0:84:80', '1:80:80', '2:56:0', '0:84:0', '1:80:0', '0:82:80', '1:79:80', '2:58:80', '0:82:0', '1:79:0', '0:80:80', '1:77:80', '2:58:0', '0:80:0', '1:77:0', '0:79:80', '1:75:80', '2:46:80', '0:79:0', '1:75:0', '0:77:80', '1:74:80', '2:46:0', '0:77:0', '1:74:0', '0:79:80', '1:75:80', '2:51:80', '0:79:0', '0:63:80', '1:75:0', '0:63:0', '0:67:80', '1:63:80', '0:67:0', '0:70:80', '2:51:0', '1:63:0', '0:70:0', '0:75:80', '1:63:80', '2:55:80', '0:75:0', '0:70:80', '1:63:0', '2:55:0', '0:70:0', '0:72:80', '1:63:80', '2:56:80', '0:72:0', '1:63:0', '0:84:80', '1:80:80', '2:56:0', '0:84:0', '1:80:0', '0:82:80', '1:79:80', '2:58:80', '0:82:0', '1:79:0', '0:80:80', '1:77:80', '2:58:0', '0:80:0', '1:77:0', '0:79:80', '1:75:80', '2:46:80', '0:79:0', '1:75:0', '0:77:80', '1:74:80', '2:46:0', '0:77:0', '1:74:0', '0:79:80', '1:75:80', '2:51:80', '0:79:0', '0:77:80', '1:75:0', '2:51:0', '0:77:0', '0:75:80', '1:79:80', '2:51:80', '0:75:0', '0:77:80', '1:79:0', '2:51:0', '0:77:0', '0:79:80', '1:75:80', '2:51:80', '0:79:0', '0:80:80', '1:75:0', '2:51:0', '0:80:0', '0:82:80', '1:77:80', '2:50:80', '0:82:0', '0:77:80', '1:77:0', '2:50:0', '0:77:0', '0:74:80', '1:82:80', '2:50:80', '0:74:0', '0:70:80', '1:82:0', '2:50:0', '0:70:0', '0:77:80', '2:50:80', '0:77:0', '0:74:80', '2:50:0', '0:74:0', '0:75:80', '2:48:80', '0:75:0', '0:74:80', '2:48:0', '0:74:0', '0:72:80', '1:75:80', '2:48:80', '0:72:0', '0:74:80', '1:75:0', '2:48:0', '0:74:0', '0:75:80', '1:72:80', '2:48:80', '0:75:0', '0:77:80', '1:72:0', '2:48:0', '0:77:0', '0:79:80', '1:74:80', '2:46:80', '0:79:0', '0:74:80', '1:74:0', '2:46:0', '0:74:0', '0:70:80', '1:79:80', '2:46:80', '0:70:0', '0:67:80', '1:79:0', '2:46:0', '0:67:0', '0:74:80', '2:46:80', '0:74:0', '0:70:80', '2:46:0', '0:70:0', '0:72:80', '2:44:80', '0:72:0', '0:68:80', '2:44:0', '0:68:0', '0:75:80', '1:72:80', '2:56:80', '0:75:0', '0:72:80', '1:72:0', '2:56:0', '0:72:0', '0:70:80', '1:75:80', '2:56:80', '0:70:0', '0:68:80', '2:56:0', '0:68:0', '0:70:80', '2:55:80', '0:70:0', '0:67:80', '1:75:0', '2:55:0', '0:67:0', '0:75:80', '1:70:80', '2:55:80', '0:75:0', '0:70:80', '1:70:0', '2:55:0', '0:70:0', '0:68:80', '1:75:80', '2:55:80', '0:68:0', '0:67:80', '1:75:0', '2:55:0', '0:67:0', '0:68:80', '1:74:80', '2:53:80', '0:68:0', '1:74:0', '0:67:80', '1:75:80', '2:53:0', '0:67:0', '1:75:0', '0:68:80', '1:77:80', '2:50:80', '0:68:0', '0:70:80', '1:77:0', '2:50:0', '0:70:0', '0:67:80', '1:75:80', '2:51:80', '0:67:0', '0:68:80', '1:75:0', '2:51:0', '0:68:0', '0:65:80', '1:74:80', '2:46:80', '0:65:0', '0:72:80', '1:74:0', '2:46:0', '0:72:0', '0:70:80', '2:50:80', '0:70:0', '0:68:80', '2:50:0', '0:68:0', '0:67:80', '2:46:80', '0:67:0', '0:65:80', '2:46:0', '0:65:0', '0:67:80', '2:51:80', '0:67:0', '0:77:80', '2:51:0', '0:77:0', '0:75:80', '2:48:80', '0:75:0', '0:74:80', '2:48:0', '0:74:0', '0:72:80', '2:51:80', '0:72:0', '0:70:80', '2:51:0', '0:70:0', '0:69:80', '2:53:80', '0:69:0', '0:79:80', '2:53:0', '0:79:0', '0:77:80', '2:57:80', '0:77:0', '0:75:80', '2:57:0', '0:75:0', '0:74:80', '2:53:80', '0:74:0', '0:72:80', '2:53:0', '0:72:0', '0:74:80', '2:58:80', '1:65:80', '0:74:0', '2:58:0', '1:65:0', '0:74:80', '1:70:80', '2:58:80', '1:70:0', '1:72:80', '0:74:0', '2:58:0', '1:72:0', '0:70:80', '1:74:80', '2:58:80', '1:74:0', '1:75:80', '0:70:0', '2:58:0', '1:75:0', '0:72:80', '1:77:80', '2:57:80', '1:77:0', '1:72:80', '0:72:0', '2:57:0', '1:72:0', '0:77:80', '1:69:80', '2:57:80', '1:69:0', '1:65:80', '0:77:0', '2:57:0', '1:65:0', '1:72:80', '2:57:80', '1:72:0', '1:69:80', '2:57:0', '1:69:0', '1:70:80', '2:55:80', '1:70:0', '1:69:80', '2:55:0', '1:69:0', '0:70:80', '1:67:80', '2:55:80', '1:67:0', '1:69:80', '0:70:0', '2:55:0', '1:69:0', '0:67:80', '1:70:80', '2:55:80', '1:70:0', '1:72:80', '0:67:0', '2:55:0', '1:72:0', '0:69:80', '1:74:80', '2:53:80', '1:74:0', '1:69:80', '0:69:0', '2:53:0', '1:69:0', '0:74:80', '1:65:80', '2:53:80', '1:65:0', '1:62:80', '0:74:0', '2:53:0', '1:62:0', '1:69:80', '2:53:80', '1:69:0', '1:65:80', '2:53:0', '1:65:0', '1:67:80', '2:51:80', '1:67:0', '1:63:80', '2:51:0', '1:63:0', '0:67:80', '1:70:80', '2:51:80', '1:70:0', '1:67:80', '0:67:0', '2:51:0', '1:67:0', '0:70:80', '1:65:80', '2:51:80', '1:65:0', '1:63:80', '2:51:0', '1:63:0', '1:65:80', '2:50:80', '1:65:0', '1:62:80', '0:70:0', '2:50:0', '1:62:0', '0:65:80', '1:70:80', '2:50:80', '1:70:0', '1:65:80', '0:65:0', '2:50:0', '1:65:0', '0:70:80', '1:63:80', '2:50:80', '1:63:0', '1:62:80', '0:70:0', '2:50:0', '1:62:0', '0:69:80', '1:63:80', '2:48:80', '0:69:0', '1:63:0', '0:70:80', '1:62:80', '2:48:0', '0:70:0', '1:62:0', '0:72:80', '1:63:80', '2:45:80', '1:63:0', '1:65:80', '0:72:0', '2:45:0', '1:65:0', '0:70:80', '1:62:80', '2:46:80', '1:62:0', '1:63:80', '0:70:0', '2:46:0', '1:63:0', '0:79:80', '1:60:80', '2:51:80', '0:79:0', '0:80:80', '1:60:0', '2:51:0', '0:80:0', '0:79:80', '1:72:80', '2:51:80', '0:79:0', '0:77:80', '1:72:0', '2:51:0', '0:77:0', '0:75:80', '2:51:80', '0:75:0', '0:74:80', '2:51:0', '0:74:0', '0:75:80', '1:79:80', '2:51:80', '0:75:0', '1:79:0', '0:77:80', '1:80:80', '2:51:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:51:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:51:0', '0:74:0', '1:77:0', '0:72:80', '1:75:80', '2:51:80', '0:72:0', '1:75:0', '0:70:80', '1:74:80', '2:51:0', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:53:80', '1:72:0', '1:77:80', '0:69:0', '1:77:0', '0:77:80', '1:81:80', '1:81:0', '1:72:80', '2:53:0', '0:77:0', '1:72:0', '0:77:80', '1:74:80', '2:58:80', '1:74:0', '1:82:80', '0:77:0', '2:58:0', '1:82:0', '0:77:80', '1:75:80', '2:57:80', '0:77:0', '0:79:80', '1:75:0', '0:79:0', '0:77:80', '1:74:80', '0:77:0', '0:79:80', '2:57:0', '1:74:0', '0:79:0', '0:77:80', '1:75:80', '2:53:80', '0:77:0', '0:79:80', '1:75:0', '0:79:0', '0:77:80', '1:84:80', '0:77:0', '0:79:80', '2:53:0', '1:84:0', '0:79:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '0:79:80', '1:74:0', '0:79:0', '0:77:80', '1:82:80', '0:77:0', '0:79:80', '2:58:0', '1:82:0', '0:79:0', '0:77:80', '1:72:80', '2:53:80', '0:77:0', '0:79:80', '1:72:0', '0:79:0', '0:77:80', '1:77:80', '0:77:0', '0:79:80', '1:77:0', '0:79:0', '0:77:80', '1:81:80', '0:77:0', '0:79:80', '1:81:0', '0:79:0', '0:77:80', '1:72:80', '2:53:0', '0:77:0', '0:79:80', '1:72:0', '0:79:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '0:79:80', '1:74:0', '0:79:0', '0:77:80', '1:82:80', '0:77:0', '0:79:80', '2:58:0', '1:82:0', '0:79:0', '0:77:80', '1:75:80', '2:57:80', '0:77:0', '0:79:80', '1:75:0', '0:79:0', '0:77:80', '1:74:80', '0:77:0', '0:79:80', '2:57:0', '1:74:0', '0:79:0', '0:77:80', '1:75:80', '2:53:80', '0:77:0', '0:79:80', '1:75:0', '0:79:0', '0:77:80', '1:84:80', '0:77:0', '0:79:80', '2:53:0', '1:84:0', '0:79:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '0:79:80', '1:74:0', '0:79:0', '0:77:80', '1:82:80', '0:77:0', '0:79:80', '2:58:0', '1:82:0', '0:79:0', '0:77:80', '1:72:80', '2:53:80', '1:72:0', '1:77:80', '0:77:0', '2:53:0', '1:77:0', '0:72:80', '1:81:80', '2:41:80', '0:72:0', '1:81:0', '0:70:80', '1:79:80', '2:41:0', '0:70:0', '1:79:0', '0:72:80', '1:81:80', '2:41:80', '0:72:0', '1:81:0', '0:74:80', '1:82:80', '2:41:0', '0:74:0', '1:82:0', '0:75:80', '1:84:80', '2:41:80', '0:75:0', '1:84:0', '0:74:80', '1:82:80', '2:41:0', '0:74:0', '1:82:0', '0:72:80', '1:81:80', '2:41:80', '0:72:0', '1:81:0', '0:75:80', '1:79:80', '2:41:0', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:41:80', '0:74:0', '1:77:0', '0:72:80', '1:75:80', '2:41:0', '0:72:0', '1:75:0', '1:74:80', '2:46:80', '0:70:80', '1:74:0', '0:70:0', '0:74:80', '1:70:80', '0:74:0', '0:65:80', '2:46:0', '1:70:0', '0:65:0', '0:67:80', '1:70:80', '2:51:80', '0:67:0', '0:75:80', '1:70:0', '2:51:0', '0:75:0', '0:68:80', '1:70:80', '2:50:80', '1:70:0', '1:72:80', '0:68:0', '0:67:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:50:0', '0:67:0', '0:68:80', '1:72:0', '1:70:80', '2:46:80', '1:70:0', '1:72:80', '0:68:0', '0:77:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:46:0', '0:77:0', '0:67:80', '1:72:0', '1:70:80', '2:51:80', '1:70:0', '1:72:80', '0:67:0', '0:75:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:51:0', '0:75:0', '0:65:80', '1:72:0', '1:70:80', '2:46:80', '1:70:0', '1:72:80', '0:65:0', '0:70:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '0:70:0', '0:74:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '0:74:0', '0:65:80', '1:72:0', '1:70:80', '2:46:0', '1:70:0', '1:72:80', '0:65:0', '0:67:80', '1:72:0', '1:70:80', '2:51:80', '1:70:0', '1:72:80', '0:67:0', '0:75:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:51:0', '0:75:0', '0:68:80', '1:72:0', '1:70:80', '2:50:80', '1:70:0', '1:72:80', '0:68:0', '0:67:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:50:0', '0:67:0', '0:68:80', '1:72:0', '1:70:80', '2:46:80', '1:70:0', '1:72:80', '0:68:0', '0:77:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:46:0', '0:77:0', '0:67:80', '1:72:0', '1:70:80', '2:51:80', '1:70:0', '1:72:80', '0:67:0', '0:75:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:51:0', '0:75:0', '0:65:80', '1:72:0', '1:70:80', '2:46:80', '0:65:0', '0:70:80', '1:70:0', '2:46:0', '0:70:0', '0:74:80', '1:70:80', '2:46:80', '0:74:0', '1:70:0', '0:75:80', '1:72:80', '2:46:0', '0:75:0', '1:72:0', '0:77:80', '1:74:80', '2:46:80', '0:77:0', '1:74:0', '0:79:80', '1:75:80', '2:46:0', '0:79:0', '1:75:0', '0:80:80', '1:77:80', '2:46:80', '0:80:0', '1:77:0', '0:79:80', '1:75:80', '2:46:0', '0:79:0', '1:75:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '1:74:0', '0:80:80', '1:77:80', '2:58:0', '0:80:0', '1:77:0', '0:79:80', '1:75:80', '2:46:80', '0:79:0', '1:75:0', '0:77:80', '1:74:80', '2:46:0', '0:77:0', '1:74:0', '0:79:80', '1:75:80', '2:51:80', '0:79:0', '0:77:80', '1:75:0', '2:51:0', '0:77:0', '0:75:80', '1:79:80', '2:48:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:48:0', '0:74:0', '1:77:0', '0:72:80', '1:75:80', '2:51:80', '0:72:0', '1:75:0', '0:70:80', '1:74:80', '2:51:0', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:53:80', '0:69:0', '1:72:0', '0:70:80', '1:70:80', '2:53:0', '0:70:0', '1:70:0', '0:72:80', '1:69:80', '2:41:80', '0:72:0', '1:69:0', '0:75:80', '1:72:80', '2:41:0', '0:75:0', '1:72:0', '0:74:80', '1:70:80', '2:53:80', '0:74:0', '1:70:0', '0:72:80', '1:69:80', '2:53:0', '0:72:0', '1:69:0', '0:74:80', '1:70:80', '2:58:80', '0:74:0', '1:70:0', '0:75:80', '1:72:80', '2:58:0', '0:75:0', '1:72:0', '0:77:80', '1:74:80', '2:46:80', '2:46:0', '2:48:80', '2:48:0', '2:50:80', '2:50:0', '2:51:80', '2:51:0', '2:53:80', '0:77:0', '1:74:0', '0:72:80', '1:69:80', '2:53:0', '0:72:0', '1:69:0', '0:72:80', '1:69:80', '2:41:80', '0:72:0', '1:69:0', '0:75:80', '1:72:80', '2:41:0', '0:75:0', '1:72:0', '0:74:80', '1:70:80', '2:53:80', '0:74:0', '1:70:0', '0:72:80', '1:69:80', '2:53:0', '0:72:0', '1:69:0', '0:74:80', '1:70:80', '2:46:80', '0:74:0', '1:70:0', '0:75:80', '1:72:80', '2:46:0', '0:75:0', '1:72:0', '0:77:80', '1:74:80', '2:46:80', '2:46:0', '2:48:80', '2:48:0', '2:50:80', '2:50:0', '2:51:80', '2:51:0', '2:53:80', '0:77:0', '1:74:0', '0:72:80', '1:69:80', '2:53:0', '0:72:0', '1:69:0', '0:72:80', '1:69:80', '2:41:80', '0:72:0', '1:69:0', '0:75:80', '1:72:80', '2:41:0', '0:75:0', '1:72:0', '0:74:80', '1:70:80', '2:53:80', '0:74:0', '1:70:0', '0:72:80', '1:69:80', '2:53:0', '0:72:0', '1:69:0', '0:74:80', '1:70:80', '2:46:80', '0:74:0', '0:75:80', '2:46:0', '0:75:0', '0:75:80', '2:48:80', '0:75:0', '0:77:80', '2:48:0', '0:77:0', '0:77:80', '2:50:80', '0:77:0', '0:79:80', '2:50:0', '0:79:0', '0:79:80', '2:51:80', '0:79:0', '0:81:80', '1:70:0', '1:75:80', '2:51:0', '0:81:0', '1:75:0', '0:81:80', '1:75:80', '2:53:80', '0:81:0', '0:82:80', '1:75:0', '2:53:0', '0:82:0', '0:82:80', '1:74:80', '2:55:80', '0:82:0', '0:70:80', '1:74:0', '2:55:0', '0:70:0', '0:79:80', '1:72:80', '2:51:80', '0:79:0', '0:75:80', '1:72:0', '2:51:0', '0:75:0', '0:74:80', '1:70:80', '2:53:80', '0:74:0', '1:70:0', '2:53:0', '0:72:80', '1:69:80', '2:41:80', '0:72:0', '1:69:0', '2:41:0', '0:70:80', '1:74:80', '2:46:80', '0:70:0', '1:74:0', '0:72:80', '1:75:80', '2:46:0', '0:72:0', '1:75:0', '0:74:80', '1:77:80', '2:46:80', '2:46:0', '2:48:80', '2:48:0', '2:50:80', '2:50:0', '2:51:80', '2:51:0', '2:53:80', '0:74:0', '1:77:0', '0:69:80', '1:72:80', '2:53:0', '0:69:0', '1:72:0', '0:69:80', '1:72:80', '2:41:80', '0:69:0', '1:72:0', '0:72:80', '1:75:80', '2:41:0', '0:72:0', '1:75:0', '0:70:80', '1:74:80', '2:53:80', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:53:0', '0:69:0', '1:72:0', '0:70:80', '1:74:80', '2:46:80', '0:70:0', '1:74:0', '0:72:80', '1:75:80', '2:46:0', '0:72:0', '1:75:0', '0:74:80', '1:77:80', '2:46:80', '2:46:0', '2:48:80', '2:48:0', '2:50:80', '2:50:0', '2:51:80', '2:51:0', '2:53:80', '0:74:0', '1:77:0', '0:69:80', '1:72:80', '2:53:0', '0:69:0', '1:72:0', '0:69:80', '1:72:80', '2:41:80', '0:69:0', '1:72:0', '0:72:80', '1:75:80', '2:41:0', '0:72:0', '1:75:0', '0:70:80', '1:74:80', '2:53:80', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:53:0', '0:69:0', '1:72:0', '0:70:80', '1:74:80', '2:46:80', '0:70:0', '1:74:0', '0:74:80', '1:77:80', '2:46:0', '0:74:0', '1:77:0', '0:77:80', '1:82:80', '2:50:80', '1:82:0', '1:81:80', '2:50:0', '1:81:0', '1:79:80', '2:46:80', '1:79:0', '1:77:80', '2:46:0', '1:77:0', '1:79:80', '2:51:80', '1:79:0', '1:77:80', '0:77:0', '1:77:0', '0:75:80', '1:79:80', '2:51:0', '1:79:0', '1:81:80', '2:53:80', '0:75:0', '1:81:0', '2:53:0', '0:74:80', '1:82:80', '2:55:80', '0:74:0', '1:82:0', '2:55:0', '0:75:80', '1:79:80', '2:51:80', '0:75:0', '1:79:0', '0:72:80', '1:75:80', '2:51:0', '0:72:0', '1:75:0', '0:70:80', '1:74:80', '2:53:80', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '0:69:0', '1:72:0', '0:70:80', '1:74:80', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:53:0', '0:69:0', '1:72:0', '0:70:80', '1:74:80', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:41:80', '0:69:0', '1:72:0', '0:70:80', '1:70:80', '2:41:0', '0:70:0', '1:70:0', '0:70:80', '1:70:80', '2:46:80', '0:70:0', '1:70:0', '2:46:0', '2:46:80', '1:58:80', '1:58:0', '1:62:80', '1:62:0', '1:65:80', '2:46:0', '1:65:0', '1:70:80', '2:50:80', '1:70:0', '1:65:80', '2:50:0', '1:65:0', '1:67:80', '2:51:80', '1:67:0', '0:75:80', '1:79:80', '2:51:0', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:53:80', '0:74:0', '1:77:0', '0:72:80', '1:75:80', '2:53:0', '0:72:0', '1:75:0', '0:70:80', '1:74:80', '2:41:80', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:41:0', '0:69:0', '1:72:0', '0:70:80', '1:74:80', '2:46:80', '1:74:0', '1:58:80', '0:70:0', '1:58:0', '0:82:80', '1:62:80', '1:62:0', '1:65:80', '2:46:0', '0:82:0', '1:65:0', '0:70:80', '1:70:80', '2:50:80', '1:70:0', '1:65:80', '2:50:0', '1:65:0', '1:67:80', '2:51:80', '0:70:0', '1:67:0', '0:75:80', '1:79:80', '2:51:0', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:53:80', '0:74:0', '1:77:0', '0:72:80', '1:75:80', '2:53:0', '0:72:0', '1:75:0', '0:70:80', '1:74:80', '2:41:80', '0:70:0', '1:74:0', '0:69:80', '1:72:80', '2:41:0', '0:69:0', '1:72:0', '0:70:80', '1:74:80', '2:46:80', '1:74:0', '1:77:80', '0:70:0', '1:77:0', '0:77:80', '1:75:80', '1:75:0', '1:74:80', '2:46:0', '0:77:0', '1:74:0', '0:79:80', '1:72:80', '2:51:80', '1:72:0', '1:70:80', '0:79:0', '2:51:0', '1:70:0', '0:72:80', '1:69:80', '2:53:80', '0:72:0', '1:69:0', '2:53:0', '0:74:80', '1:70:80', '2:58:80', '2:58:0', '2:53:80', '2:53:0', '2:50:80', '0:74:0', '1:70:0', '2:50:0', '0:75:80', '1:72:80', '2:46:80', '0:75:0', '1:72:0', '0:77:80', '1:74:80', '2:46:0', '0:77:0', '1:74:0', '0:72:80', '1:69:80', '2:53:80', '0:72:0', '1:69:0', '2:53:0', '0:74:80', '1:70:80', '2:58:80', '2:58:0', '2:53:80', '2:53:0', '2:50:80', '0:74:0', '1:70:0', '2:50:0', '0:75:80', '1:72:80', '2:46:80', '0:75:0', '1:72:0', '0:77:80', '1:74:80', '2:46:0', '0:77:0', '1:74:0', '0:72:80', '1:69:80', '2:53:80', '0:72:0', '0:65:80', '2:53:0', '2:51:80', '1:69:0', '0:65:0', '2:51:0', '0:77:80', '2:50:80', '2:50:0', '2:48:80', '2:48:0', '2:47:80', '0:77:0', '0:80:80', '0:80:0', '0:79:80', '1:74:80', '0:79:0', '0:77:80', '0:77:0', '0:75:80', '0:75:0', '2:47:0', '0:74:80', '1:74:0', '0:74:0', '0:75:80', '1:74:80', '2:48:80', '0:75:0', '2:48:0', '0:74:80', '2:47:80', '1:74:0', '0:74:0', '2:47:0', '0:75:80', '1:67:80', '2:48:80', '0:75:0', '2:48:0', '0:77:80', '2:50:80', '1:67:0', '0:77:0', '2:50:0', '0:79:80', '1:72:80', '2:51:80', '1:72:0', '2:51:0', '1:70:80', '2:48:80', '1:70:0', '2:48:0', '1:68:80', '2:53:80', '0:79:0', '1:68:0', '0:77:80', '1:80:80', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '0:74:0', '1:77:0', '2:53:0', '0:72:80', '1:75:80', '0:72:0', '1:75:0', '0:71:80', '1:74:80', '2:55:80', '0:71:0', '1:74:0', '2:55:0', '0:72:80', '1:75:80', '2:60:80', '2:60:0', '2:55:80', '2:55:0', '2:51:80', '0:72:0', '1:75:0', '2:51:0', '0:74:80', '1:77:80', '2:48:80', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:48:0', '0:75:0', '1:79:0', '0:71:80', '1:74:80', '2:55:80', '0:71:0', '1:74:0', '2:55:0', '0:72:80', '1:75:80', '2:60:80', '2:60:0', '2:55:80', '2:55:0', '2:51:80', '0:72:0', '1:75:0', '2:51:0', '0:74:80', '1:77:80', '2:48:80', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:48:0', '0:75:0', '1:79:0', '0:71:80', '1:74:80', '2:55:80', '0:71:0', '0:79:80', '1:74:0', '2:55:0', '0:79:0', '0:79:80', '2:59:80', '0:79:0', '0:77:80', '2:59:0', '0:77:0', '0:77:80', '2:55:80', '0:77:0', '0:75:80', '2:55:0', '0:75:0', '0:74:80', '2:60:80', '1:84:80', '0:74:0', '2:60:0', '1:84:0', '0:76:80', '1:84:80', '2:48:80', '1:84:0', '1:82:80', '2:48:0', '1:82:0', '1:82:80', '2:60:80', '1:82:0', '1:80:80', '0:76:0', '2:60:0', '1:80:0', '1:80:80', '2:53:80', '0:77:80', '1:80:0', '2:53:0', '0:77:0', '0:77:80', '1:81:80', '2:41:80', '0:77:0', '0:75:80', '2:41:0', '0:75:0', '0:75:80', '2:53:80', '0:75:0', '0:74:80', '1:81:0', '2:53:0', '0:74:0', '0:74:80', '2:58:80', '1:82:80', '0:74:0', '2:58:0', '1:82:0', '0:74:80', '1:82:80', '2:46:80', '1:82:0', '1:80:80', '2:46:0', '1:80:0', '1:80:80', '2:58:80', '1:80:0', '1:79:80', '0:74:0', '2:58:0', '1:79:0', '0:75:80', '1:79:80', '2:51:80', '1:79:0', '1:79:80', '0:75:0', '2:51:0', '1:79:0', '0:63:80', '1:80:80', '2:48:80', '1:80:0', '1:79:80', '0:63:0', '2:48:0', '1:79:0', '0:75:80', '1:77:80', '2:60:80', '1:77:0', '1:75:80', '0:75:0', '1:75:0', '0:74:80', '1:77:80', '1:77:0', '1:77:80', '0:74:0', '2:60:0', '1:77:0', '0:75:80', '1:79:80', '2:59:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '0:74:0', '0:75:80', '0:75:0', '0:74:80', '0:74:0', '0:75:80', '1:77:0', '0:75:0', '0:74:80', '1:75:80', '1:75:0', '1:74:80', '2:59:0', '0:74:0', '1:74:0', '1:75:80', '2:60:80', '0:79:80', '1:75:0', '2:60:0', '0:79:0', '0:80:80', '1:63:80', '2:48:80', '0:80:0', '0:79:80', '1:63:0', '2:48:0', '0:79:0', '0:77:80', '1:75:80', '2:60:80', '0:77:0', '0:75:80', '1:75:0', '2:60:0', '0:75:0', '0:77:80', '1:74:80', '2:60:80', '0:77:0', '0:77:80', '1:74:0', '2:60:0', '0:77:0', '0:79:80', '1:74:80', '2:59:80', '0:79:0', '0:77:80', '0:77:0', '0:75:80', '0:75:0', '0:74:80', '1:74:0', '2:59:0', '0:74:0', '0:75:80', '2:60:80', '0:75:0', '1:60:80', '2:60:0', '2:58:80', '1:60:0', '2:58:0', '1:72:80', '2:57:80', '2:57:0', '2:55:80', '2:55:0', '2:54:80', '1:72:0', '1:75:80', '1:75:0', '0:69:80', '1:74:80', '1:74:0', '1:72:80', '1:72:0', '1:70:80', '1:70:0', '2:54:0', '1:69:80', '1:69:0', '1:70:80', '2:55:80', '1:70:0', '2:55:0', '1:69:80', '2:54:80', '0:69:0', '1:69:0', '2:54:0', '0:62:80', '1:70:80', '2:55:80', '1:70:0', '2:55:0', '1:72:80', '2:57:80', '0:62:0', '1:72:0', '2:57:0', '0:67:80', '1:74:80', '2:59:80', '0:67:0', '2:59:0', '0:65:80', '2:55:80', '0:65:0', '2:55:0', '0:63:80', '2:60:80', '0:63:0', '1:74:0', '2:60:0', '0:75:80', '1:72:80', '2:48:80', '0:75:0', '1:72:0', '0:74:80', '1:70:80', '0:74:0', '1:70:0', '0:72:80', '1:69:80', '0:72:0', '1:69:0', '0:70:80', '1:67:80', '2:48:0', '0:70:0', '1:67:0', '0:69:80', '1:66:80', '2:50:80', '0:69:0', '1:66:0', '2:50:0', '0:70:80', '1:67:80', '2:55:80', '2:55:0', '2:50:80', '2:50:0', '2:46:80', '0:70:0', '1:67:0', '2:46:0', '0:72:80', '1:69:80', '2:43:80', '0:72:0', '1:69:0', '0:74:80', '1:70:80', '2:43:0', '0:74:0', '1:70:0', '0:69:80', '1:66:80', '2:50:80', '0:69:0', '1:66:0', '2:50:0', '0:70:80', '1:67:80', '2:55:80', '2:55:0', '2:50:80', '2:50:0', '2:46:80', '0:70:0', '1:67:0', '2:46:0', '0:72:80', '1:69:80', '2:43:80', '0:72:0', '1:69:0', '0:74:80', '1:70:80', '2:43:0', '0:74:0', '1:70:0', '0:69:80', '1:66:80', '2:50:80', '1:66:0', '1:74:80', '0:69:0', '2:50:0', '1:74:0', '1:74:80', '2:54:80', '1:74:0', '1:72:80', '2:54:0', '1:72:0', '1:72:80', '2:50:80', '1:72:0', '1:70:80', '2:50:0', '1:70:0', '1:70:80', '2:55:80', '0:79:80', '1:70:0', '2:55:0', '0:79:0', '0:79:80', '1:71:80', '2:43:80', '0:79:0', '0:77:80', '2:43:0', '0:77:0', '0:77:80', '2:55:80', '0:77:0', '0:75:80', '1:71:0', '2:55:0', '0:75:0', '0:75:80', '2:48:80', '1:72:80', '0:75:0', '2:48:0', '1:72:0', '0:76:80', '1:72:80', '2:60:80', '1:72:0', '1:70:80', '2:60:0', '1:70:0', '1:70:80', '2:48:80', '1:70:0', '1:69:80', '0:76:0', '2:48:0', '1:69:0', '1:69:80', '2:53:80', '0:77:80', '2:53:0', '0:77:0', '0:77:80', '2:41:80', '0:77:0', '0:75:80', '2:41:0', '0:75:0', '0:75:80', '2:53:80', '0:75:0', '1:69:0', '0:74:80', '2:53:0', '0:74:0', '0:74:80', '1:70:80', '2:46:80', '0:74:0', '0:74:80', '2:46:0', '0:74:0', '0:75:80', '2:43:80', '0:75:0', '0:74:80', '2:43:0', '0:74:0', '0:72:80', '2:55:80', '0:72:0', '1:70:0', '0:70:80', '2:55:0', '0:70:0', '0:72:80', '1:69:80', '2:55:80', '0:72:0', '0:72:80', '1:69:0', '2:55:0', '0:72:0', '0:74:80', '1:69:80', '2:54:80', '0:74:0', '0:72:80', '0:72:0', '0:70:80', '0:70:0', '0:69:80', '1:69:0', '2:54:0', '0:69:0', '0:70:80', '2:55:80', '1:74:80', '0:70:0', '2:55:0', '1:74:0', '0:70:80', '1:75:80', '2:43:80', '1:75:0', '1:74:80', '2:43:0', '1:74:0', '1:72:80', '2:55:80', '1:72:0', '1:70:80', '0:70:0', '2:55:0', '1:70:0', '0:69:80', '1:72:80', '2:55:80', '1:72:0', '1:72:80', '0:69:0', '2:55:0', '1:72:0', '0:69:80', '1:74:80', '2:54:80', '1:74:0', '1:72:80', '1:72:0', '1:70:80', '1:70:0', '1:69:80', '0:69:0', '2:54:0', '1:69:0', '0:67:80', '1:70:80', '2:55:80', '0:67:0', '1:70:0', '0:82:80', '1:74:80', '2:55:0', '0:82:0', '1:74:0', '0:82:80', '1:74:80', '2:43:80', '0:82:0', '1:74:0', '0:81:80', '1:72:80', '2:43:0', '0:81:0', '1:72:0', '0:81:80', '1:72:80', '2:48:80', '0:81:0', '1:72:0', '0:79:80', '1:70:80', '2:48:0', '0:79:0', '1:70:0', '0:79:80', '1:70:80', '2:50:80', '0:79:0', '1:70:0', '0:78:80', '1:69:80', '0:78:0', '1:69:0', '0:79:80', '1:70:80', '0:79:0', '1:70:0', '0:78:80', '1:69:80', '2:50:0', '0:78:0', '1:69:0', '0:79:80', '1:70:80', '0:79:0', '1:70:0', '0:78:80', '1:69:80', '2:48:80', '0:78:0', '1:69:0', '2:48:0', '0:79:80', '1:67:80', '2:46:80', '0:79:0', '0:74:80', '1:67:0', '2:46:0', '0:74:0', '0:75:80', '1:67:80', '2:48:80', '0:75:0', '1:67:0', '0:72:80', '1:69:80', '2:48:0', '0:72:0', '1:69:0', '0:70:80', '1:67:80', '2:50:80', '0:70:0', '1:67:0', '0:69:80', '1:66:80', '0:69:0', '1:66:0', '0:70:80', '1:67:80', '0:70:0', '1:67:0', '0:69:80', '1:66:80', '2:50:0', '0:69:0', '1:66:0', '0:70:80', '1:67:80', '0:70:0', '1:67:0', '0:69:80', '1:66:80', '2:38:80', '0:69:0', '1:66:0', '0:67:80', '1:67:80', '2:38:0', '0:67:0', '1:67:0', '0:67:80', '1:67:80', '2:43:80', '0:67:0', '0:70:80', '2:43:0', '0:70:0', '0:72:80', '2:44:80', '0:72:0', '2:44:0', '0:74:80', '2:46:80', '1:67:0', '0:74:0', '2:46:0', '0:75:80', '2:48:80', '0:75:0', '2:48:0', '0:77:80', '2:50:80', '0:77:0', '2:50:0', '0:79:80', '2:51:80', '1:63:80', '1:63:0', '1:67:80', '1:67:0', '1:70:80', '0:79:0', '2:51:0', '1:70:0', '1:75:80', '2:55:80', '1:75:0', '1:70:80', '2:55:0', '1:70:0', '1:72:80', '2:56:80', '1:72:0', '0:80:80', '1:84:80', '2:56:0', '0:80:0', '1:84:0', '0:79:80', '1:82:80', '2:58:80', '0:79:0', '1:82:0', '0:77:80', '1:80:80', '2:58:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:46:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:46:0', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:51:80', '1:79:0', '1:63:80', '0:75:0', '1:63:0', '0:63:80', '1:67:80', '1:67:0', '1:70:80', '2:51:0', '0:63:0', '1:70:0', '0:63:80', '1:75:80', '2:55:80', '1:75:0', '1:70:80', '0:63:0', '2:55:0', '1:70:0', '0:63:80', '1:72:80', '2:56:80', '0:63:0', '1:72:0', '0:80:80', '1:84:80', '2:56:0', '0:80:0', '1:84:0', '0:79:80', '1:82:80', '2:58:80', '0:79:0', '1:82:0', '0:77:80', '1:80:80', '2:58:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:46:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:46:0', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:51:80', '0:75:0', '0:70:80', '0:70:0', '0:75:80', '0:75:0', '0:79:80', '1:79:0', '2:51:0', '0:79:0', '0:82:80', '2:55:80', '0:82:0', '0:70:80', '2:55:0', '0:70:0', '0:72:80', '2:56:80', '0:72:0', '0:82:80', '2:56:0', '0:82:0', '0:80:80', '2:53:80', '0:80:0', '0:79:80', '2:53:0', '0:79:0', '0:80:80', '0:80:0', '0:72:80', '0:72:0', '0:70:80', '2:50:80', '0:70:0', '0:80:80', '2:50:0', '0:80:0', '0:79:80', '2:51:80', '0:79:0', '0:77:80', '2:51:0', '0:77:0', '0:79:80', '0:79:0', '0:70:80', '0:70:0', '0:68:80', '2:48:80', '0:68:0', '0:79:80', '2:48:0', '0:79:0', '0:77:80', '2:50:80', '0:77:0', '0:75:80', '2:50:0', '0:75:0', '0:77:80', '2:46:80', '0:77:0', '0:68:80', '2:46:0', '0:68:0', '0:67:80', '2:51:80', '1:70:80', '0:67:0', '1:70:0', '1:75:80', '1:75:0', '1:79:80', '2:51:0', '1:79:0', '1:82:80', '2:55:80', '1:82:0', '1:70:80', '2:55:0', '1:70:0', '1:72:80', '2:56:80', '1:72:0', '1:82:80', '2:56:0', '1:82:0', '1:80:80', '2:53:80', '1:80:0', '1:79:80', '2:53:0', '1:79:0', '1:80:80', '1:80:0', '1:72:80', '1:72:0', '1:70:80', '2:50:80', '1:70:0', '1:80:80', '2:50:0', '1:80:0', '1:79:80', '2:51:80', '1:79:0', '1:77:80', '2:51:0', '1:77:0', '1:79:80', '1:79:0', '1:70:80', '1:70:0', '1:68:80', '2:48:80', '1:68:0', '1:79:80', '2:48:0', '1:79:0', '1:77:80', '2:50:80', '1:77:0', '1:75:80', '2:50:0', '1:75:0', '1:77:80', '2:46:80', '1:77:0', '1:68:80', '2:46:0', '1:68:0', '1:67:80', '2:51:80', '0:70:80', '0:70:0', '0:75:80', '1:67:0', '0:75:0', '0:73:80', '1:70:80', '2:51:0', '0:73:0', '1:70:0', '0:72:80', '1:68:80', '2:55:80', '0:72:0', '1:68:0', '0:70:80', '1:67:80', '2:55:0', '0:70:0', '1:67:0', '0:72:80', '1:68:80', '2:56:80', '0:72:0', '1:68:0', '0:73:80', '1:75:80', '2:56:0', '0:73:0', '1:75:0', '0:72:80', '1:80:80', '2:53:80', '0:72:0', '1:80:0', '0:70:80', '1:79:80', '2:53:0', '0:70:0', '1:79:0', '0:68:80', '1:77:80', '0:68:0', '1:77:0', '0:67:80', '1:75:80', '0:67:0', '1:75:0', '0:65:80', '1:74:80', '2:46:80', '0:65:0', '0:70:80', '1:74:0', '0:70:0', '0:74:80', '1:70:80', '0:74:0', '0:65:80', '2:46:0', '1:70:0', '0:65:0', '0:67:80', '1:70:80', '2:51:80', '0:67:0', '0:75:80', '1:70:0', '2:51:0', '0:75:0', '0:68:80', '1:70:80', '2:50:80', '1:70:0', '1:72:80', '0:68:0', '0:67:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:50:0', '0:67:0', '0:68:80', '1:72:0', '1:70:80', '2:46:80', '1:70:0', '1:72:80', '0:68:0', '0:77:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:46:0', '0:77:0', '0:67:80', '1:72:0', '1:70:80', '2:51:80', '1:70:0', '1:72:80', '0:67:0', '0:75:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:51:0', '0:75:0', '0:65:80', '1:72:0', '1:70:80', '2:46:80', '1:70:0', '1:72:80', '0:65:0', '0:70:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '0:70:0', '0:74:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '0:74:0', '0:65:80', '1:72:0', '1:70:80', '2:46:0', '1:70:0', '1:72:80', '0:65:0', '0:67:80', '1:72:0', '1:70:80', '2:51:80', '1:70:0', '1:72:80', '0:67:0', '0:75:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:51:0', '0:75:0', '0:68:80', '1:72:0', '1:70:80', '2:50:80', '1:70:0', '1:72:80', '0:68:0', '0:67:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:50:0', '0:67:0', '0:68:80', '1:72:0', '1:70:80', '2:46:80', '1:70:0', '1:72:80', '0:68:0', '0:77:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:46:0', '0:77:0', '0:67:80', '1:72:0', '1:70:80', '2:51:80', '1:70:0', '1:72:80', '0:67:0', '0:75:80', '1:72:0', '1:70:80', '1:70:0', '1:72:80', '2:51:0', '0:75:0', '0:65:80', '1:72:0', '1:70:80', '2:46:80', '0:65:0', '0:70:80', '1:70:0', '2:46:0', '0:70:0', '0:74:80', '1:70:80', '2:58:80', '0:74:0', '1:70:0', '0:75:80', '1:72:80', '2:58:0', '0:75:0', '1:72:0', '0:77:80', '1:74:80', '2:46:80', '0:77:0', '1:74:0', '0:79:80', '1:75:80', '2:46:0', '0:79:0', '1:75:0', '0:80:80', '1:77:80', '2:46:80', '0:80:0', '1:77:0', '0:79:80', '1:75:80', '2:46:0', '0:79:0', '1:75:0', '0:77:80', '1:74:80', '2:46:80', '0:77:0', '1:74:0', '0:79:80', '1:75:80', '2:46:0', '0:79:0', '1:75:0', '0:80:80', '1:77:80', '2:46:80', '0:80:0', '1:77:0', '0:77:80', '1:74:80', '2:46:0', '0:77:0', '1:74:0', '0:79:80', '1:73:80', '2:51:80', '1:73:0', '1:72:80', '0:79:0', '1:72:0', '0:75:80', '1:73:80', '1:73:0', '1:82:80', '2:51:0', '0:75:0', '1:82:0', '0:75:80', '1:72:80', '2:56:80', '1:72:0', '1:80:80', '0:75:0', '2:56:0', '1:80:0', '0:75:80', '1:73:80', '2:55:80', '0:75:0', '0:77:80', '1:73:0', '0:77:0', '0:75:80', '1:72:80', '0:75:0', '0:77:80', '2:55:0', '1:72:0', '0:77:0', '0:75:80', '1:73:80', '2:51:80', '0:75:0', '0:77:80', '1:73:0', '0:77:0', '0:75:80', '1:82:80', '0:75:0', '0:77:80', '2:51:0', '1:82:0', '0:77:0', '0:75:80', '1:72:80', '2:56:80', '0:75:0', '0:77:80', '1:72:0', '0:77:0', '0:75:80', '1:80:80', '0:75:0', '0:77:80', '2:56:0', '1:80:0', '0:77:0', '0:75:80', '1:70:80', '2:51:80', '0:75:0', '0:77:80', '1:70:0', '0:77:0', '0:75:80', '1:75:80', '0:75:0', '0:77:80', '1:75:0', '0:77:0', '0:75:80', '1:79:80', '0:75:0', '0:77:80', '1:79:0', '0:77:0', '0:75:80', '1:70:80', '2:51:0', '0:75:0', '0:77:80', '1:70:0', '0:77:0', '0:75:80', '1:72:80', '2:56:80', '0:75:0', '0:77:80', '1:72:0', '0:77:0', '0:75:80', '1:80:80', '0:75:0', '0:77:80', '2:56:0', '1:80:0', '0:77:0', '0:75:80', '1:73:80', '2:55:80', '0:75:0', '0:77:80', '1:73:0', '0:77:0', '0:75:80', '1:72:80', '0:75:0', '0:77:80', '2:55:0', '1:72:0', '0:77:0', '0:75:80', '1:73:80', '2:51:80', '0:75:0', '0:77:80', '1:73:0', '0:77:0', '0:75:80', '1:82:80', '0:75:0', '0:77:80', '2:51:0', '1:82:0', '0:77:0', '0:75:80', '1:72:80', '2:56:80', '0:75:0', '0:77:80', '1:72:0', '0:77:0', '0:75:80', '1:80:80', '0:75:0', '0:77:80', '2:56:0', '1:80:0', '0:77:0', '0:75:80', '1:70:80', '2:51:80', '1:70:0', '1:75:80', '2:51:0', '1:75:0', '1:79:80', '2:39:80', '0:75:0', '1:79:0', '0:77:80', '1:80:80', '2:39:0', '0:77:0', '1:80:0', '0:79:80', '1:82:80', '2:51:80', '0:79:0', '1:82:0', '0:80:80', '1:84:80', '2:51:0', '0:80:0', '1:84:0', '0:82:80', '1:85:80', '2:51:80', '0:82:0', '1:85:0', '0:80:80', '1:84:80', '2:51:0', '0:80:0', '1:84:0', '0:79:80', '1:82:80', '2:51:80', '0:79:0', '1:82:0', '0:82:80', '1:85:80', '2:51:0', '0:82:0', '1:85:0', '0:80:80', '1:84:80', '2:51:80', '0:80:0', '1:84:0', '0:79:80', '1:82:80', '2:51:0', '0:79:0', '1:82:0', '0:80:80', '1:84:80', '2:56:80', '1:84:0', '1:82:80', '0:80:0', '2:56:0', '1:82:0', '0:84:80', '1:80:80', '2:56:80', '0:84:0', '1:80:0', '0:82:80', '1:79:80', '2:56:0', '0:82:0', '1:79:0', '0:80:80', '1:77:80', '2:56:80', '0:80:0', '1:77:0', '0:79:80', '1:75:80', '2:56:0', '0:79:0', '1:75:0', '0:77:80', '1:74:80', '2:58:80', '0:77:0', '1:74:0', '0:75:80', '1:75:80', '2:58:0', '0:75:0', '1:75:0', '0:74:80', '1:77:80', '2:46:80', '0:74:0', '1:77:0', '0:77:80', '1:80:80', '2:46:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:46:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:46:0', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:51:80', '0:75:0', '1:79:0', '0:77:80', '1:80:80', '2:51:0', '0:77:0', '1:80:0', '0:79:80', '1:82:80', '2:51:80', '2:51:0', '2:53:80', '2:53:0', '2:55:80', '2:55:0', '2:56:80', '2:56:0', '2:58:80', '0:79:0', '1:82:0', '0:74:80', '1:77:80', '2:58:0', '0:74:0', '1:77:0', '0:74:80', '1:77:80', '2:46:80', '0:74:0', '1:77:0', '0:77:80', '1:80:80', '2:46:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:58:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:58:0', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:51:80', '0:75:0', '1:79:0', '0:77:80', '1:80:80', '2:51:0', '0:77:0', '1:80:0', '0:79:80', '1:82:80', '2:51:80', '2:51:0', '2:53:80', '2:53:0', '2:55:80', '2:55:0', '2:56:80', '2:56:0', '2:58:80', '0:79:0', '1:82:0', '0:74:80', '1:77:80', '2:58:0', '0:74:0', '1:77:0', '0:74:80', '1:77:80', '2:46:80', '0:74:0', '1:77:0', '0:77:80', '1:80:80', '2:46:0', '0:77:0', '1:80:0', '0:75:80', '1:79:80', '2:58:80', '0:75:0', '1:79:0', '0:74:80', '1:77:80', '2:58:0', '0:74:0', '1:77:0', '0:75:80', '1:79:80', '2:51:80', '1:79:0', '1:80:80', '2:51:0', '1:80:0', '1:80:80', '2:53:80', '1:80:0', '1:82:80', '2:53:0', '1:82:0', '1:82:80', '2:55:80', '0:75:0', '1:82:0', '1:84:80', '2:55:0', '1:84:0', '0:75:80', '1:84:80', '2:56:80', '1:84:0', '1:74:80', '0:75:0', '0:80:80', '2:56:0', '1:74:0', '0:80:0', '0:80:80', '1:74:80', '2:58:80', '1:74:0', '1:75:80', '0:80:0', '2:58:0', '1:75:0', '0:79:80', '1:75:80', '2:60:80', '1:75:0', '1:70:80', '0:79:0', '2:60:0', '1:70:0', '0:77:80', '1:72:80', '2:56:80', '1:72:0', '1:80:80', '0:77:0', '2:56:0', '1:80:0', '0:75:80', '1:79:80', '2:58:80', '0:75:0', '1:79:0', '2:58:0', '0:74:80', '1:77:80', '2:46:80', '0:74:0', '1:77:0', '2:46:0', '0:75:80', '1:75:80', '2:51:80', '0:75:0', '1:75:0', '2:51:0', '0:70:80', '1:67:80', '2:51:80', '2:51:0', '2:53:80', '2:53:0', '2:55:80', '2:55:0', '2:56:80', '2:56:0', '2:58:80', '0:70:0', '1:67:0', '0:65:80', '1:62:80', '2:58:0', '0:65:0', '1:62:0', '0:65:80', '1:62:80', '2:46:80', '0:65:0', '1:62:0', '0:68:80', '1:65:80', '2:46:0', '0:68:0', '1:65:0', '0:67:80', '1:63:80', '2:46:80', '0:67:0', '1:63:0', '0:65:80', '1:62:80', '2:46:0', '0:65:0', '1:62:0', '0:67:80', '1:63:80', '2:51:80', '0:67:0', '1:63:0', '0:68:80', '1:65:80', '2:51:0', '0:68:0', '1:65:0', '0:70:80', '1:67:80', '2:39:80', '2:39:0', '2:41:80', '2:41:0', '2:43:80', '2:43:0', '2:44:80', '2:44:0', '2:46:80', '0:70:0', '1:67:0', '0:65:80', '1:62:80', '2:46:0', '0:65:0', '1:62:0', '0:65:80', '1:62:80', '2:46:80', '0:65:0', '1:62:0', '0:68:80', '1:65:80', '2:46:0', '0:68:0', '1:65:0', '0:67:80', '1:63:80', '2:46:80', '0:67:0', '1:63:0', '0:65:80', '1:62:80', '2:46:0', '0:65:0', '1:62:0', '0:67:80', '1:63:80', '2:51:80', '0:67:0', '0:70:80', '1:63:0', '2:51:0', '0:70:0', '0:75:80', '1:70:80', '2:55:80', '0:75:0', '0:74:80', '2:55:0', '0:74:0', '0:72:80', '2:51:80', '0:72:0', '0:70:80', '2:51:0', '0:70:0', '0:72:80', '2:44:80', '0:72:0', '0:70:80', '1:70:0', '0:70:0', '0:72:80', '1:68:80', '2:44:0', '0:72:0', '0:74:80', '2:46:80', '1:68:0', '0:74:0', '2:46:0', '0:75:80', '1:67:80', '2:48:80', '0:75:0', '0:63:80', '1:67:0', '2:48:0', '0:63:0', '0:72:80', '1:68:80', '2:44:80', '0:72:0', '1:68:0', '0:68:80', '1:65:80', '2:44:0', '0:68:0', '1:65:0', '0:67:80', '1:63:80', '2:46:80', '0:67:0', '1:63:0', '0:65:80', '1:62:80', '0:65:0', '0:67:80', '1:62:0', '1:63:80', '0:67:0', '0:65:80', '1:63:0', '1:62:80', '2:46:0', '0:65:0', '0:67:80', '1:62:0', '1:63:80', '0:67:0', '0:65:80', '1:63:0', '1:62:80', '2:46:80', '0:65:0', '0:63:80', '1:62:0', '1:63:80', '2:46:0', '0:63:0', '1:63:0', '0:63:80', '1:63:80', '2:51:80', '0:63:0', '1:63:0', '2:51:0']\n"
     ]
    }
   ],
   "source": [
    "import mido\n",
    "\n",
    "#outport = mido.open_output()\n",
    "niter = 0\n",
    "noteslst = []\n",
    "for msg in mido.MidiFile('bjs1031c.mid').play():\n",
    "    #outport.send(msg)\n",
    "    #print msg.bytes()\n",
    "    #print str(msg)\n",
    "    thismsg = str(msg).split(\" \") # ['note_on', 'channel=0', 'note=60', 'velocity=61', 'time=0.00168918958333']\n",
    "    if thismsg[0] == 'note_on':\n",
    "        #print thismsg\n",
    "        #print thismsg[2][5:]\n",
    "        noteslst.append(thismsg[1][8:]+':'+thismsg[2][5:]+':'+thismsg[3][9:]) # channel, note, velocity\n",
    "    niter += 1\n",
    "    if niter >= 5000:\n",
    "        break\n",
    "print noteslst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best: ('e', 's') : 9\n",
      "best: ('es', 't') : 9\n",
      "best: ('est', '</w>') : 9\n",
      "best: ('l', 'o') : 7\n",
      "best: ('lo', 'w') : 7\n",
      "best: ('n', 'e') : 6\n",
      "best: ('ne', 'w') : 6\n",
      "best: ('new', 'est</w>') : 6\n",
      "best: ('low', '</w>') : 5\n",
      "best: ('w', 'i') : 3\n"
     ]
    }
   ],
   "source": [
    "# Byte Pair Encoding BPE - Instead of merging frequent pairs of bytes, we merge characters or character sequences.\n",
    "# Frequent character n-grams (or whole words) are eventually merged into a single symbol\n",
    "import re, collections\n",
    "\n",
    "def wordsep(word): # returns a word split by ' ' with delimiter </w> as required by BPE\n",
    "    return ' '.join(list(word)) + ' </w>'\n",
    "\n",
    "def inp_rep(words): # turns a list of (word-string, frequency) tuples into a BPE-input dict\n",
    "    res = {}\n",
    "    for w in words:\n",
    "        res[wordsep(w[0])] = w[1]\n",
    "    return res\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int) # dict subclass - initialize the symbol vocabulary with the character vocabulary\n",
    "    for word, freq in vocab.items(): # items: return the list with all dictionary keys with values\n",
    "        symbols = word.split()\n",
    "        #symbols = list(word) # this works without spaces in between\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq # iteratively count all symbol pairs\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in): # replace each occurrence of the most frequent pair (‘A’, ‘B’) with a new symbol ‘AB’\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair)) # Escape all the characters in pattern except ASCII letters and numbers\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word) # used to replace substrings\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # represent each word as a sequence of characters, plus a special end-ofword symbol ‘·’, which allows us to restore the\n",
    "    # original tokenization after translation\n",
    "    #vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,'n e w e s t </w>':6, 'w i d e s t </w>':3}\n",
    "    #vocab = {'t h i s </w>' : 5, 'i s </w>' : 6,'a </w>':6, 't e s t </w>':3}\n",
    "    #vocab = {'low </w>' : 5, 'lower </w>' : 2,'newest </w>':6, 'widest </w>':3}\n",
    "    vocab = inp_rep([('low', 5), ('lower', 2), ('newest', 6), ('widest', 3)])\n",
    "    num_merges = 10\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if len(pairs) > 1:\n",
    "            best = max(pairs, key=pairs.get) # get returns a value for the given key - find most frequent pair\n",
    "            vocab = merge_vocab(best, vocab) # replace each occurrence of the most frequent pair (‘A’, ‘B’) with a new symbol ‘AB’\n",
    "            print('best: %s : %s' % (best, pairs[best[0], best[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('m', 1), ('i', 4), ('s', 4), ('p', 2)])\n",
      "['mississippi']\n",
      "t e s t </w>\n",
      "{'t e s t </w>': 5, 's o m e </w>': 2, 'o n e </w>': 8, 'm i s s i s s i p p i </w>': 2}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "def wordsep(word):\n",
    "    return ' '.join(list(word)) + ' </w>'\n",
    "\n",
    "def inp_rep(words):\n",
    "    res = {}\n",
    "    for w in words:\n",
    "        res[wordsep(w[0])] = w[1]\n",
    "    return res\n",
    "\n",
    "if __name__=='__main__':\n",
    "    s = 'mississippi'\n",
    "    d = collections.defaultdict(int)\n",
    "    for k in s:\n",
    "        d[k] += 1\n",
    "    print(d.items()) # return the list with all dictionary keys with values\n",
    "    print(s.split())\n",
    "    print(wordsep('test'))\n",
    "    wl = [('test', 5), ('some', 2), ('one', 8), ('mississippi', 2)]\n",
    "    print(inp_rep(wl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1415926535913457\n"
     ]
    }
   ],
   "source": [
    "# Arithmetic-geometric mean and pi\n",
    "import math\n",
    "\n",
    "def agm(a,g,n):\n",
    "    a0 = a\n",
    "    g0 = g\n",
    "    for i in range(n):\n",
    "        an = (a0+g0)/2.\n",
    "        gn = math.sqrt(a0*g0)\n",
    "        a0 = an\n",
    "        g0 = gn\n",
    "    return (an,gn)\n",
    "\n",
    "def piagm(a,g,n):\n",
    "    a0 = a\n",
    "    g0 = g\n",
    "    agsum = 0.\n",
    "    twop = 2.\n",
    "    for i in range(1,n):\n",
    "        an = (a0+g0)/2.\n",
    "        gn = math.sqrt(a0*g0)\n",
    "        twop *= 2.\n",
    "        agsum += twop * (an*an - gn*gn)\n",
    "        a0 = an\n",
    "        g0 = gn\n",
    "    return 4. * (agm(1., 1./math.sqrt(2.), 100)[0] ** 2) / (1. - agsum)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    #print(agm(1., 1./math.sqrt(2.), 100))\n",
    "    print(piagm(1., 1./math.sqrt(2.), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.21742849e-05  2.40038508e-04  2.40678251e-04  8.47253735e-05\n",
      "  -1.28055799e-05  4.23046013e-04  1.38969143e-04 -5.51629916e-05\n",
      "   2.90629974e-04 -3.72688084e-04]]\n",
      "[[ 3.01755738e-08  1.65382648e-05 -0.00000000e+00 -1.44909836e-07\n",
      "   2.46414218e-08 -9.02442797e-08 -2.06793148e-07  0.00000000e+00\n",
      "  -0.00000000e+00  1.96541329e-07]]\n",
      "[[ 0.00000000e+00  1.65382648e-05 -0.00000000e+00 -0.00000000e+00\n",
      "   0.00000000e+00 -0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  -0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# simple feed forward network with ReLU example\n",
    "import numpy as np\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # parameters\n",
    "    inp_size = 10 # input size\n",
    "    etha = 0.1 # learning rate\n",
    "\n",
    "    # input\n",
    "    x = np.zeros((1, inp_size)) # input\n",
    "    x = [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]\n",
    "\n",
    "    # model parameters\n",
    "    W1 = np.random.randn(inp_size, inp_size)*0.01 # input to hidden\n",
    "    W2 = np.random.randn(inp_size, inp_size)*0.01 # hidden to output\n",
    "    b1 = np.zeros((1, inp_size)) # inp-hidden bias\n",
    "    b2 = np.zeros((1, inp_size)) # hidden-out bias\n",
    "\n",
    "    # forward pass\n",
    "    h1 = np.dot(x, W1) + b1\n",
    "    h1 = np.maximum(h1, 0, h1) # ReLU\n",
    "    o2 = np.dot(h1, W2) + b2\n",
    "    print(o2)\n",
    "\n",
    "    # backward pass\n",
    "    y = [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]\n",
    "    h1 = np.dot(x, W1) + b1\n",
    "    dW1 = - etha * (o2 - y) * np.maximum(h1, 0, h1)\n",
    "    dW2 = dW1 * ((h1 > 0) * 1.) * x\n",
    "    print(dW1)\n",
    "    print(dW2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08017331  1.00081823  0.0066398  -0.02038512  0.02915765  0.02587995\n",
      "  -0.00555739  0.04111021 -0.04911599 -0.07523451]]\n"
     ]
    }
   ],
   "source": [
    "# simple feed forward network with ReLU example\n",
    "import numpy as np\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # parameters\n",
    "    inp_size = 10 # input size\n",
    "    etha = 0.1 # learning rate\n",
    "    nruns = 20 # n training runs\n",
    "\n",
    "    # model parameters\n",
    "    W1 = np.random.randn(inp_size, inp_size)*0.01 # input to hidden\n",
    "    W2 = np.random.randn(inp_size, inp_size)*0.01 # hidden to output\n",
    "    b1 = np.zeros((1, inp_size)) # inp-hidden bias\n",
    "    b2 = np.zeros((1, inp_size)) # hidden-out bias\n",
    "    \n",
    "    for k in range(10):\n",
    "        # input\n",
    "        x = np.zeros((1, inp_size)) # input\n",
    "        x[0][k] = 1.\n",
    "\n",
    "        for i in range(nruns):\n",
    "            # forward pass\n",
    "            h1 = np.dot(x, W1) + b1\n",
    "            h1 = np.maximum(h1, 0, h1) # ReLU\n",
    "            o2 = np.dot(h1, W2) + b2\n",
    "            #print(o2)\n",
    "\n",
    "            # backward pass\n",
    "            y = [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]\n",
    "            h1 = np.dot(x, W1) + b1\n",
    "            dW1 = - etha * (o2 - y) * np.maximum(h1, 0, h1)\n",
    "            dW2 = dW1 * ((h1 > 0) * 1.) * x\n",
    "            #print(dW1)\n",
    "            #print(dW2)\n",
    "            W1 += dW1\n",
    "            W2 += dW2\n",
    "    \n",
    "    #print(W1)\n",
    "    #print(W2)\n",
    "    # forward pass\n",
    "    x = np.zeros((1, inp_size)) # input\n",
    "    x[0][1] = 1.\n",
    "    h1 = np.dot(x, W1) + b1\n",
    "    h1 = np.maximum(h1, 0, h1) # ReLU\n",
    "    o2 = np.dot(h1, W2) + b2\n",
    "    print(o2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['the', 'red', 'fox', 'jumps', 'over', 'the', 'lazy'], ['red', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'], ['fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'and'], ['jumps', 'over', 'the', 'lazy', 'dog', 'and', 'eats'], ['over', 'the', 'lazy', 'dog', 'and', 'eats', 'the'], ['the', 'lazy', 'dog', 'and', 'eats', 'the', 'hen']], ['jumps', 'over', 'the', 'lazy', 'dog', 'and'])\n"
     ]
    }
   ],
   "source": [
    "# key-value memories\n",
    "\n",
    "def cwinkv(text, wsize): # center-window encoding\n",
    "    lt = len(text)\n",
    "    if lt < wsize:\n",
    "        return []\n",
    "    wsh = int(wsize/2)\n",
    "    keys = []\n",
    "    values = []\n",
    "    for i in range(wsh, lt-wsh):\n",
    "        onekey = []\n",
    "        for j in range(i-wsh, i+wsh+1):\n",
    "            onekey.append(text[j]) # build the window\n",
    "            if j == i:\n",
    "                values.append(text[j]) # values are the center words\n",
    "        keys.append(onekey)\n",
    "    return (keys, values)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    stxt = 'the red fox jumps over the lazy dog and eats the hen'\n",
    "    ltxt = stxt.split(' ')\n",
    "    #print(ltxt)\n",
    "    print(cwinkv(ltxt, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['the', 'red', 'fox', 'jumps', 'over', 'the', 'lazy'], ['red', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'], ['fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'and'], ['jumps', 'over', 'the', 'lazy', 'dog', 'and', 'eats'], ['over', 'the', 'lazy', 'dog', 'and', 'eats', 'the'], ['the', 'lazy', 'dog', 'and', 'eats', 'the', 'hen']], ['jumps', 'over', 'the', 'lazy', 'dog', 'and'])\n",
      "key hash: [0, 1, 2, 3, 4]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[0, 0, 0, 0, 1]\n",
      "[0.9539980920057239, 1.483997032008904, 2.0139959720120837, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161]\n",
      "['fox', 'jumps', 'over', 'what']\n"
     ]
    }
   ],
   "source": [
    "# key-value memory network\n",
    "# see also https://github.com/jojonki/key-value-memory-networks\n",
    "import numpy as np\n",
    "\n",
    "def cwinkv(text, wsize): # center-window encoding\n",
    "    lt = len(text)\n",
    "    if lt < wsize:\n",
    "        return ()\n",
    "    wsh = int(wsize/2)\n",
    "    keys = []\n",
    "    values = []\n",
    "    for i in range(wsh, lt-wsh):\n",
    "        onekey = []\n",
    "        for j in range(i-wsh, i+wsh+1):\n",
    "            onekey.append(text[j]) # build the window\n",
    "            if j == i:\n",
    "                values.append(text[j]) # values are the center words\n",
    "        keys.append(onekey)\n",
    "    return (keys, values)\n",
    "\n",
    "def keyhsh(mem, quest, n): # key hash for memory and question quest\n",
    "    memid = []\n",
    "    idc = 0\n",
    "    for qw in quest:\n",
    "        for i in range(len(mem[0])):\n",
    "            if qw in mem[0][i] and not i in memid: # question-word in memory?\n",
    "                if idc > n:\n",
    "                    return memid # return ids of memories containing question words (id = list of words of size winsize)\n",
    "                memid.append(i)\n",
    "                idc += 1\n",
    "    return memid\n",
    "\n",
    "def memf(memid): # check memory frequencies\n",
    "    # F-check (freq < 1000)\n",
    "    mem = []\n",
    "    memf = []\n",
    "    for e in memid:\n",
    "        mct = memid.count(e)\n",
    "        if mct < 1000 and not e in mem:\n",
    "            memf.append((e, mct))\n",
    "            mem.append(e)\n",
    "    return memf # return positions in memory for matching memories and f: (memid, f)\n",
    "\n",
    "def onehot(vec): # takes a (kind-of)bow-vector and turns it into a combined 1-hot (kind of...)\n",
    "    lv = len(vec)\n",
    "    res = [0] * lv\n",
    "    for e in vec:\n",
    "        if e > 0 and e < lv:\n",
    "            res[e] += 1\n",
    "    return res\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def BatchNormalization(a):\n",
    "    mean = np.mean(a)\n",
    "    stdv = np.std(a)\n",
    "    return [(x - mean) / stdv for x in a]\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    stxt = 'the red fox jumps over the lazy dog and eats the hen'\n",
    "    ltxt = stxt.split(' ')\n",
    "    \n",
    "    #print(ltxt)\n",
    "    #print(cwinkv(ltxt, 7))\n",
    "    \n",
    "    # dimensions\n",
    "    #key = Input((mem_size, mem_len,), name='Key_Input')\n",
    "    #val = Input((mem_size, mem_len,), name='Val_Input')\n",
    "    #question = Input((query_maxlen,), name='Question_Input')\n",
    "    \n",
    "    # build memory with given window size\n",
    "    memory = cwinkv(ltxt, 7) # memory has the structure ([[keys0], [keys1],...], [values])\n",
    "    print(memory)\n",
    "    w2i = dict((c, i) for i, c in enumerate(ltxt, 1))\n",
    "    i2w = dict((i, c) for i, c in enumerate(ltxt, 1))\n",
    "    #print(w2i)\n",
    "    #print(i2w)\n",
    "    \n",
    "    dimd = len(w2i) # embedding dimension\n",
    "    \n",
    "    # define question\n",
    "    question = 'fox jumps over what'\n",
    "    qx = question.split(' ')\n",
    "    \n",
    "    # todo: wrap in NN\n",
    "    \n",
    "    # embedding matrices take dimensions D to d\n",
    "    # todo: align dims: memory M, key hash N, question D then d, values same as ph which is d but should be N ?, Rj dxd\n",
    "    \n",
    "    # key hashing: key shares at least one word with the question (which memories contain words from the question?)\n",
    "    #kh = memf(keyhsh(memory, qx, 10))\n",
    "    kh = keyhsh(memory, qx, 10)\n",
    "    print('key hash: %s' % (str(kh)))\n",
    "    \n",
    "    # key addressing: candidate memory gets assigned a relevance probability\n",
    "    # ph = dot([question_encoded, key_encoded], axes=(1, 2))  # (None, mem_size) - Keras dot-product layer\n",
    "    q = [w2i[w] for w in qx if w in w2i] # embed question\n",
    "    q = q[:dimd] # limit length\n",
    "    padl = max(0, dimd - len(q)) # pad length\n",
    "    q += [0] * padl # pad question length\n",
    "    #print('q: %s' % (str(q)))\n",
    "    #print(onehot(q))\n",
    "    #q = BatchNormalization(q)\n",
    "    ph = []\n",
    "    # first loop: find best matching memory for question x (relevance of candidate memories)\n",
    "    for memid in kh:\n",
    "        k = [w2i[w] for w in memory[0][memid] if w in w2i] # embed candidate key\n",
    "        k = k[:dimd] # limit length\n",
    "        padl = max(0, dimd - len(k)) # pad length\n",
    "        k += [0] * padl # pad \n",
    "        #print(k)\n",
    "        #print(softmax(np.dot(q,k)))\n",
    "        #ph.append(softmax(np.dot(q, k)))\n",
    "        #ph.append(np.dot(onehot(q), onehot(k))) # dot of question and candidate memory key\n",
    "        ph.append(softmax(np.dot(onehot(q), onehot(k))))\n",
    "    #print(ph)\n",
    "    #ph = BatchNormalization(ph)\n",
    "    #print(ph)\n",
    "    \n",
    "    # value reading\n",
    "    # o = dot([ph, val_encoded], axes=(1, 1)) # (None, embd_size)\n",
    "    v = []\n",
    "    for k in kh:\n",
    "        v.append(w2i[memory[1][k]])  # all values for candidate keys\n",
    "    #print(np.dot(ph, v))\n",
    "    #print(v)\n",
    "    #o = np.dot(ph, v) # weighted sum of memories\n",
    "    print(ph)\n",
    "    print(onehot(v))\n",
    "    o = np.dot(ph, onehot(v)) # weighted sum of memories\n",
    "    \n",
    "    # with result o, update query with q2 = R1(q+o) and repeat from memory access (from key addressing)\n",
    "    #R = Dense(embd_size, input_shape=(embd_size,), name='R_Dense_h' + str(h+1))\n",
    "    #q = R(add([q,  o])) # (None, embd_size)\n",
    "    q += o\n",
    "    q = BatchNormalization(q)\n",
    "    print(q)\n",
    "    print(qx)\n",
    "    #print(o)\n",
    "    #print([i2w[i] for i in v if i in i2w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['the', 'red', 'fox', 'jumps', 'over', 'the', 'lazy'], ['red', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'], ['fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'and'], ['jumps', 'over', 'the', 'lazy', 'dog', 'and', 'eats'], ['over', 'the', 'lazy', 'dog', 'and', 'eats', 'the'], ['the', 'lazy', 'dog', 'and', 'eats', 'the', 'hen']], ['jumps', 'over', 'the', 'lazy', 'dog', 'and'])\n",
      "key hash: [0, 1, 2, 3, 4]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[0, 0, 0, 0, 1]\n",
      "[0.9539980920057239, 1.483997032008904, 2.0139959720120837, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161, -0.6359987280038161]\n",
      "['fox', 'jumps', 'over', 'what']\n"
     ]
    }
   ],
   "source": [
    "# key-value memory network\n",
    "# see also https://github.com/jojonki/key-value-memory-networks\n",
    "import numpy as np\n",
    "\n",
    "def cwinkv(text, wsize): # center-window encoding\n",
    "    lt = len(text)\n",
    "    if lt < wsize:\n",
    "        return ()\n",
    "    wsh = int(wsize/2)\n",
    "    keys = []\n",
    "    values = []\n",
    "    for i in range(wsh, lt-wsh):\n",
    "        onekey = []\n",
    "        for j in range(i-wsh, i+wsh+1):\n",
    "            onekey.append(text[j]) # build the window\n",
    "            if j == i:\n",
    "                values.append(text[j]) # values are the center words\n",
    "        keys.append(onekey)\n",
    "    return (keys, values)\n",
    "\n",
    "def keyhsh(mem, quest, n): # key hash for memory and question quest\n",
    "    memid = []\n",
    "    idc = 0\n",
    "    for qw in quest:\n",
    "        for i in range(len(mem[0])):\n",
    "            if qw in mem[0][i] and not i in memid: # question-word in memory?\n",
    "                if idc > n:\n",
    "                    return memid # return ids of memories containing question words (id = list of words of size winsize)\n",
    "                memid.append(i)\n",
    "                idc += 1\n",
    "    return memid\n",
    "\n",
    "def memf(memid): # check memory frequencies\n",
    "    # F-check (freq < 1000)\n",
    "    mem = []\n",
    "    memf = []\n",
    "    for e in memid:\n",
    "        mct = memid.count(e)\n",
    "        if mct < 1000 and not e in mem:\n",
    "            memf.append((e, mct))\n",
    "            mem.append(e)\n",
    "    return memf # return positions in memory for matching memories and f: (memid, f)\n",
    "\n",
    "def onehot(vec): # takes a (kind-of)bow-vector and turns it into a combined 1-hot (kind of...)\n",
    "    lv = len(vec)\n",
    "    res = [0] * lv\n",
    "    for e in vec:\n",
    "        if e > 0 and e < lv:\n",
    "            res[e] += 1\n",
    "    return res\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def BatchNormalization(a):\n",
    "    mean = np.mean(a)\n",
    "    stdv = np.std(a)\n",
    "    return [(x - mean) / stdv for x in a]\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    stxt = 'the red fox jumps over the lazy dog and eats the hen'\n",
    "    ltxt = stxt.split(' ')\n",
    "    \n",
    "    #print(ltxt)\n",
    "    #print(cwinkv(ltxt, 7))\n",
    "    \n",
    "    # dimensions\n",
    "    #key = Input((mem_size, mem_len,), name='Key_Input')\n",
    "    #val = Input((mem_size, mem_len,), name='Val_Input')\n",
    "    #question = Input((query_maxlen,), name='Question_Input')\n",
    "    \n",
    "    # build memory with given window size\n",
    "    memory = cwinkv(ltxt, 7) # memory has the structure ([[keys0], [keys1],...], [values])\n",
    "    print(memory)\n",
    "    w2i = dict((c, i) for i, c in enumerate(ltxt, 1))\n",
    "    i2w = dict((i, c) for i, c in enumerate(ltxt, 1))\n",
    "    #print(w2i)\n",
    "    #print(i2w)\n",
    "    \n",
    "    dimd = len(w2i) # embedding dimension\n",
    "    \n",
    "    # define question\n",
    "    question = 'fox jumps over what'\n",
    "    qx = question.split(' ')\n",
    "    \n",
    "    # todo: wrap in NN\n",
    "    \n",
    "    # embedding matrices take dimensions D to d\n",
    "    # todo: align dims: memory M, key hash N, question D then d, values same as ph which is d but should be N ?, Rj dxd\n",
    "    \n",
    "    # key hashing: key shares at least one word with the question (which memories contain words from the question?)\n",
    "    #kh = memf(keyhsh(memory, qx, 10))\n",
    "    kh = keyhsh(memory, qx, 10)\n",
    "    print('key hash: %s' % (str(kh)))\n",
    "    \n",
    "    # key addressing: candidate memory gets assigned a relevance probability\n",
    "    # ph = dot([question_encoded, key_encoded], axes=(1, 2))  # (None, mem_size) - Keras dot-product layer\n",
    "    q = [w2i[w] for w in qx if w in w2i] # embed question\n",
    "    q = q[:dimd] # limit length\n",
    "    padl = max(0, dimd - len(q)) # pad length\n",
    "    q += [0] * padl # pad question length\n",
    "    #print('q: %s' % (str(q)))\n",
    "    #print(onehot(q))\n",
    "    #q = BatchNormalization(q)\n",
    "    ph = []\n",
    "    # first loop: find best matching memory for question x (relevance of candidate memories)\n",
    "    for memid in kh:\n",
    "        k = [w2i[w] for w in memory[0][memid] if w in w2i] # embed candidate key\n",
    "        k = k[:dimd] # limit length\n",
    "        padl = max(0, dimd - len(k)) # pad length\n",
    "        k += [0] * padl # pad \n",
    "        #print(k)\n",
    "        #print(softmax(np.dot(q,k)))\n",
    "        #ph.append(softmax(np.dot(q, k)))\n",
    "        #ph.append(np.dot(onehot(q), onehot(k))) # dot of question and candidate memory key\n",
    "        ph.append(softmax(np.dot(onehot(q), onehot(k))))\n",
    "    #print(ph)\n",
    "    #ph = BatchNormalization(ph)\n",
    "    #print(ph)\n",
    "    \n",
    "    # value reading\n",
    "    # o = dot([ph, val_encoded], axes=(1, 1)) # (None, embd_size)\n",
    "    v = []\n",
    "    for k in kh:\n",
    "        v.append(w2i[memory[1][k]])  # all values for candidate keys\n",
    "    #print(np.dot(ph, v))\n",
    "    #print(v)\n",
    "    #o = np.dot(ph, v) # weighted sum of memories\n",
    "    print(ph)\n",
    "    print(onehot(v))\n",
    "    o = np.dot(ph, onehot(v)) # weighted sum of memories\n",
    "    \n",
    "    # with result o, update query with q2 = R1(q+o) and repeat from memory access (from key addressing)\n",
    "    #R = Dense(embd_size, input_shape=(embd_size,), name='R_Dense_h' + str(h+1))\n",
    "    #q = R(add([q,  o])) # (None, embd_size)\n",
    "    q += o\n",
    "    q = BatchNormalization(q)\n",
    "    print(q)\n",
    "    print(qx)\n",
    "    #print(o)\n",
    "    #print([i2w[i] for i in v if i in i2w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sukhbaatar: end-to-end memory networs\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from past.builtins import xrange\n",
    "\n",
    "class MemN2N(object):\n",
    "    def __init__(self, config, sess):\n",
    "        self.nwords = config.nwords\n",
    "        self.init_hid = config.init_hid\n",
    "        self.init_std = config.init_std\n",
    "        self.batch_size = config.batch_size\n",
    "        self.nepoch = config.nepoch\n",
    "        self.nhop = config.nhop\n",
    "        self.edim = config.edim\n",
    "        self.mem_size = config.mem_size\n",
    "        self.lindim = config.lindim\n",
    "        self.max_grad_norm = config.max_grad_norm\n",
    "\n",
    "        self.show = config.show\n",
    "        self.is_test = config.is_test\n",
    "        self.checkpoint_dir = config.checkpoint_dir\n",
    "\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            raise Exception(\" [!] Directory %s not found\" % self.checkpoint_dir)\n",
    "\n",
    "        self.input = tf.placeholder(tf.float32, [None, self.edim], name=\"input\")\n",
    "        self.time = tf.placeholder(tf.int32, [None, self.mem_size], name=\"time\")\n",
    "        self.target = tf.placeholder(tf.float32, [self.batch_size, self.nwords], name=\"target\")\n",
    "        self.context = tf.placeholder(tf.int32, [self.batch_size, self.mem_size], name=\"context\")\n",
    "\n",
    "        self.hid = []\n",
    "        self.hid.append(self.input)\n",
    "        self.share_list = []\n",
    "        self.share_list.append([])\n",
    "\n",
    "        self.lr = None\n",
    "        self.current_lr = config.init_lr\n",
    "        self.loss = None\n",
    "        self.step = None\n",
    "        self.optim = None\n",
    "\n",
    "        self.sess = sess\n",
    "        self.log_loss = []\n",
    "        self.log_perp = []\n",
    "\n",
    "    def build_memory(self):\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\")\n",
    "\n",
    "        self.A = tf.Variable(tf.random_normal([self.nwords, self.edim], stddev=self.init_std))\n",
    "        self.B = tf.Variable(tf.random_normal([self.nwords, self.edim], stddev=self.init_std))\n",
    "        self.C = tf.Variable(tf.random_normal([self.edim, self.edim], stddev=self.init_std))\n",
    "\n",
    "        # Temporal Encoding\n",
    "        self.T_A = tf.Variable(tf.random_normal([self.mem_size, self.edim], stddev=self.init_std))\n",
    "        self.T_B = tf.Variable(tf.random_normal([self.mem_size, self.edim], stddev=self.init_std))\n",
    "\n",
    "        # m_i = sum A_ij * x_ij + T_A_i\n",
    "        Ain_c = tf.nn.embedding_lookup(self.A, self.context)\n",
    "        Ain_t = tf.nn.embedding_lookup(self.T_A, self.time)\n",
    "        Ain = tf.add(Ain_c, Ain_t)\n",
    "\n",
    "        # c_i = sum B_ij * u + T_B_i\n",
    "        Bin_c = tf.nn.embedding_lookup(self.B, self.context)\n",
    "        Bin_t = tf.nn.embedding_lookup(self.T_B, self.time)\n",
    "        Bin = tf.add(Bin_c, Bin_t)\n",
    "\n",
    "        for h in xrange(self.nhop):\n",
    "            self.hid3dim = tf.reshape(self.hid[-1], [-1, 1, self.edim])\n",
    "            Aout = tf.matmul(self.hid3dim, Ain, adjoint_b=True)\n",
    "            Aout2dim = tf.reshape(Aout, [-1, self.mem_size])\n",
    "            P = tf.nn.softmax(Aout2dim)\n",
    "\n",
    "            probs3dim = tf.reshape(P, [-1, 1, self.mem_size])\n",
    "            Bout = tf.matmul(probs3dim, Bin)\n",
    "            Bout2dim = tf.reshape(Bout, [-1, self.edim])\n",
    "\n",
    "            Cout = tf.matmul(self.hid[-1], self.C)\n",
    "            Dout = tf.add(Cout, Bout2dim)\n",
    "\n",
    "            self.share_list[0].append(Cout)\n",
    "\n",
    "            if self.lindim == self.edim:\n",
    "                self.hid.append(Dout)\n",
    "            elif self.lindim == 0:\n",
    "                self.hid.append(tf.nn.relu(Dout))\n",
    "            else:\n",
    "                F = tf.slice(Dout, [0, 0], [self.batch_size, self.lindim])\n",
    "                G = tf.slice(Dout, [0, self.lindim], [self.batch_size, self.edim-self.lindim])\n",
    "                K = tf.nn.relu(G)\n",
    "                self.hid.append(tf.concat(axis=1, values=[F, K]))\n",
    "\n",
    "    def build_model(self):\n",
    "        self.build_memory()\n",
    "\n",
    "        self.W = tf.Variable(tf.random_normal([self.edim, self.nwords], stddev=self.init_std))\n",
    "        z = tf.matmul(self.hid[-1], self.W)\n",
    "\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=z, labels=self.target)\n",
    "\n",
    "        self.lr = tf.Variable(self.current_lr)\n",
    "        self.opt = tf.train.GradientDescentOptimizer(self.lr)\n",
    "\n",
    "        params = [self.A, self.B, self.C, self.T_A, self.T_B, self.W]\n",
    "        grads_and_vars = self.opt.compute_gradients(self.loss,params)\n",
    "        clipped_grads_and_vars = [(tf.clip_by_norm(gv[0], self.max_grad_norm), gv[1]) \\\n",
    "                                   for gv in grads_and_vars]\n",
    "\n",
    "        inc = self.global_step.assign_add(1)\n",
    "        with tf.control_dependencies([inc]):\n",
    "            self.optim = self.opt.apply_gradients(clipped_grads_and_vars)\n",
    "\n",
    "        tf.global_variables_initializer().run()\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def train(self, data):\n",
    "        N = int(math.ceil(len(data) / self.batch_size))\n",
    "        cost = 0\n",
    "\n",
    "        x = np.ndarray([self.batch_size, self.edim], dtype=np.float32)\n",
    "        time = np.ndarray([self.batch_size, self.mem_size], dtype=np.int32)\n",
    "        target = np.zeros([self.batch_size, self.nwords]) # one-hot-encoded\n",
    "        context = np.ndarray([self.batch_size, self.mem_size])\n",
    "\n",
    "        x.fill(self.init_hid)\n",
    "        for t in xrange(self.mem_size):\n",
    "            time[:,t].fill(t)\n",
    "\n",
    "        if self.show:\n",
    "            from utils import ProgressBar\n",
    "            bar = ProgressBar('Train', max=N)\n",
    "\n",
    "        for idx in xrange(N):\n",
    "            if self.show: bar.next()\n",
    "            target.fill(0)\n",
    "            for b in xrange(self.batch_size):\n",
    "                m = random.randrange(self.mem_size, len(data))\n",
    "                target[b][data[m]] = 1\n",
    "                context[b] = data[m - self.mem_size:m]\n",
    "\n",
    "            _, loss, self.step = self.sess.run([self.optim,\n",
    "                                                self.loss,\n",
    "                                                self.global_step],\n",
    "                                                feed_dict={\n",
    "                                                    self.input: x,\n",
    "                                                    self.time: time,\n",
    "                                                    self.target: target,\n",
    "                                                    self.context: context})\n",
    "            cost += np.sum(loss)\n",
    "\n",
    "        if self.show: bar.finish()\n",
    "        return cost/N/self.batch_size\n",
    "\n",
    "    def test(self, data, label='Test'):\n",
    "        N = int(math.ceil(len(data) / self.batch_size))\n",
    "        cost = 0\n",
    "\n",
    "        x = np.ndarray([self.batch_size, self.edim], dtype=np.float32)\n",
    "        time = np.ndarray([self.batch_size, self.mem_size], dtype=np.int32)\n",
    "        target = np.zeros([self.batch_size, self.nwords]) # one-hot-encoded\n",
    "        context = np.ndarray([self.batch_size, self.mem_size])\n",
    "\n",
    "        x.fill(self.init_hid)\n",
    "        for t in xrange(self.mem_size):\n",
    "            time[:,t].fill(t)\n",
    "\n",
    "        if self.show:\n",
    "            from utils import ProgressBar\n",
    "            bar = ProgressBar(label, max=N)\n",
    "\n",
    "        m = self.mem_size\n",
    "        for idx in xrange(N):\n",
    "            if self.show: bar.next()\n",
    "            target.fill(0)\n",
    "            for b in xrange(self.batch_size):\n",
    "                target[b][data[m]] = 1\n",
    "                context[b] = data[m - self.mem_size:m]\n",
    "                m += 1\n",
    "\n",
    "                if m >= len(data):\n",
    "                    m = self.mem_size\n",
    "\n",
    "            loss = self.sess.run([self.loss], feed_dict={self.input: x,\n",
    "                                                         self.time: time,\n",
    "                                                         self.target: target,\n",
    "                                                         self.context: context})\n",
    "            cost += np.sum(loss)\n",
    "\n",
    "        if self.show: bar.finish()\n",
    "        return cost/N/self.batch_size\n",
    "\n",
    "    def run(self, train_data, test_data):\n",
    "        if not self.is_test:\n",
    "            for idx in xrange(self.nepoch):\n",
    "                train_loss = np.sum(self.train(train_data))\n",
    "                test_loss = np.sum(self.test(test_data, label='Validation'))\n",
    "\n",
    "                # Logging\n",
    "                self.log_loss.append([train_loss, test_loss])\n",
    "                self.log_perp.append([math.exp(train_loss), math.exp(test_loss)])\n",
    "\n",
    "                state = {\n",
    "                    'perplexity': math.exp(train_loss),\n",
    "                    'epoch': idx,\n",
    "                    'learning_rate': self.current_lr,\n",
    "                    'valid_perplexity': math.exp(test_loss)\n",
    "                }\n",
    "                print(state)\n",
    "\n",
    "                # Learning rate annealing\n",
    "                if len(self.log_loss) > 1 and self.log_loss[idx][1] > self.log_loss[idx-1][1] * 0.9999:\n",
    "                    self.current_lr = self.current_lr / 1.5\n",
    "                    self.lr.assign(self.current_lr).eval()\n",
    "                if self.current_lr < 1e-5: break\n",
    "\n",
    "                if idx % 10 == 0:\n",
    "                    self.saver.save(self.sess,\n",
    "                                    os.path.join(self.checkpoint_dir, \"MemN2N.model\"),\n",
    "                                    global_step = self.step.astype(int))\n",
    "        else:\n",
    "            self.load()\n",
    "\n",
    "            valid_loss = np.sum(self.test(train_data, label='Validation'))\n",
    "            test_loss = np.sum(self.test(test_data, label='Test'))\n",
    "\n",
    "            state = {\n",
    "                'valid_perplexity': math.exp(valid_loss),\n",
    "                'test_perplexity': math.exp(test_loss)\n",
    "            }\n",
    "            print(state)\n",
    "\n",
    "    def load(self):\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        ckpt = tf.train.get_checkpoint_state(self.checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            raise Exception(\" [!] Trest mode but no checkpoint found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 4648 characters, 15 unique.\n",
      "----\n",
      " inbfbhfcffaibgajjkanhd nihemgajcclnnejfkfekijhjicfi gadallebgmffei hhafh gllcmgnecaegimdkhbefmcdglmledmfkebimamifdincfllabhmmedmcfnfib jejejeflcmmajcmdklfbihnhldhjgkdffdncaejfjgij hecigmfa ngejndfidiaimhcjaeeicjbnijnfjdnmdmimdbfndaabdjbaglalhdbhfflelcnmlfkmhdhcnndimkmhk hjfdhlga kjbfjhkkhjdlm im agcjefljegcjmdngkafabknfaccmcgandadaknbdlhnhffmlbfkflaaagfckdjghnaned gafdmfhfk mccdednjdmf knmgncmbcmddnh hjbbai kbjche dhdenjiihmmmngebejchajfnkefmgeaahfhekhnlhjekdifhnmmiihknhelagijhlcbemjhanalgcmgcnncmfkgmgkbic fcagebfaknngnhcegcncbb nanalmlnhadelaah ikgankddkkimgbffhnbb mdejmhcnnmbmchdllfn jmkebghjffinegclnhdgdd knmcbchidlfjnhgagiedihee cfikm ailhmmhknbgnlcbfnkejbgagmbenmefhhga ddclcaefjaj ldhk f hkjdcmkacdielfcfjgdfidi eagkm indn hcee mbihdl chgajekmjlbahgjkfdgdfabggffieinbilm ldfejbccflg kgmjmimhgnf himhlhjmafanffdemjbcmbiag dmhjmikajmebgcnmheigkhlkhacl  c  m gbjm knfhbh lehmaakkhiemekdhg b gfbhcngnchmgk dm ad  kb lbmkb kicnhljcacmdhglmhfabam mfnkcnceablkag nmhnaig bdjbkjkdl ailnenljiaa ekffjcn cjjcmejcniclna cfn cchhjkkjbbacelh jdhnhj n ninikhbmdcdkmifgkkckn gnnflegiankemmdmic jkminfd begdahnikfglkefc dmaamclimiahakgkdnkendjncclgiga mkaknejanehjdnccamagcgjkaccaccjkch cg nkmncm bihgcnh mekeejb m mnie dnbihkmmkkhihb cl  nhbeiendkbjcnlancenafiaegdakmhcn  k ngkgbndhidahalabcfalhelmhb kkfjmlgalihdkjdhnl bdlnab eeeejbljmdmdibmfimccn igbdhnlecjmne i jnlenhigageidf ijmelhlmia hnekkcdkkfbccmb cdfgmaclcbkfcdf clbacknclifjljeflblgigdklbcddaklcjllihif jmbnek  dnlfdjmglelnekii cnacjjnjkghf \n",
      "----\n",
      "iter 0, loss: 67.701254\n",
      "----\n",
      " nnn bb fff iaaa ffff ccccc aa kkk lll mmm kk hhh iii jjj lllllllllllllnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm \n",
      "----\n",
      "iter 1000, loss: 41.200079\n",
      "----\n",
      "  iiiii bee bbbb mmm mmm gg jjj ndnnn aaa mmm mmm lll kk ffgg kkk n eb lll ggg iiieee f n ddd eee bbbb aaa ffmmm nnnnn ddnnnn jjj  d kk nn ieee mmm fffff nnn iiee bbb eee kkkl lll jj mmm mmm nnn dd hhhg nn jkk lll f g nnnnn gg hh ee fkkk mmm ll aaa hh fjjj kkkk nnnnnnnn ggggg ljj d ddd ccjjj bb gg gg jjj bbbb faa beee ggg ll kkkk nnn hhhh iiii lll ffffff  ggg hhhh hhhh aaa bb jjj bbb i hh aaa aa n ccccc dddd ccc e eee ll iiii eee bbb iijjj cccc mmm nnn d ll iii dd jjj fffjj kjj ccc ld mmm nnnnni eee fgg hh dddd eee kkk ccc ggg cc hhhh nn nn lkk bbeee nnn iiii kkk kk kkkk bbb ll dddd hh jj bb dnnn g hhh  iiiiii nnnn iiieee kjj nn dlll fffjj fkkk n aaa nnneee nnnn dddd ld jj mm bb ggg jjj mmm lkj aa bbbjj ccc gggg ccaaa kkk nnn  bbbb kjj mmm lll ggg bbbb lll bbbb nn iiimm mmm ll jk bb aa fff cc iiijjj mmmm n fii ddnd iii nnnnnnnnn ddd jjj ffffff eee mm kkk aaa mmm kkk mmm gg bbee mmm kk ll gggg gg h eee mmm eee iii ccc ccc aaa n hh dd hh ccc ddd hhhhh hh iiii fgg hhe iii ggg bbb ic jjj ccc eee bbb aaa bb eee jjj nndd hh kkkk kkjj bbbb ll ciidd jjm ffffkk ggg nnn aaa iiiiimm dddd aaa ccc caaa cccc aaa mm nnnneee mmm f hh jjj aaa hheee kkjj hh iiiiim bbb gg kd gg hhh hh ee ff aaa bbjj nnnn dddd ji d  fffff ggg fee hh iiiii ciii nn eee kkk ccaaa bb jjj mm dddd nnnnnn llll kkk nnnnn iid aaa lll jj iee n iiee nnn dn jj jja ffffff cccaa n iiilll lll lll lll aaa kkkk fff ii iiii cchh n cccc bcc llk nnn hh bbb g jjj kkj mmm nn mmm mm fkkk bb  c dnnn hhh ddeee kkk lll hhh ggg dd hhh hggg \n",
      "----\n",
      "iter 2000, loss: 29.953731\n",
      "----\n",
      " mm ffff hhh ddd nn ddd ghh gggg ddd iii eee lk nnn ddd ccc jjj nnn hhhh aaa ii iiii aac aaa eee n aaa bbc jjj kkk eee bc kll ffff iiii ciii eh ff ihh kk lll mmm bbb lll mmm  aa ggggg cccjj bbb ee nndd eee n fff gg gg ii gggg aaa jjj eee kdd jj nn hi ggg ddddd ii aaa bbb icc eee kkkk mmmm nnnn ccc ddd hhh ee bbb hhhh ddd aaa eee mmm bb ggg aaa nhhh eee kkjj mmm bbb eee kkjj m ggg jjj kkk fll nnn aaa jjj lll m eee nnnn jjj lll mm lll m cccc ghhh jjj nnnn iiiii eee ffff jjj kkk lll mmm kjj kkk ccc aaa ccc eee nnn bii gg iiii iii hhhh ddd ghh bbb ccc hhh ddd cccc gggg iii kkjj mmmm kkkk mmmm nnnn hhh eee  aaa bbb kkj ll fff nn hh ddd iii iill mmmm bbb kkk kkk jjj lll mm nn jj bbb lll mmm ffff gbbb lll mmm lll mm ffff iicc jjj kl kkk nddd aaaa nnn bbb dd ddd aaa hhh gggee fh hh ggggg hhh aaa bbb eee bb ic eee n gggh jbb lll mmm n aa icc jjj ffff aaa jjj mmm icc eee bbb bbb nn j ccc jjj nnnn ccjj ffg ii jj icc hhh ccc gggg gggg aaa eee kkk bbb nnnn ggg iii iiii iiihh eee fffff iiii idd ggggg dd aaa ffhh ggg ggg eee bbb ccc aaa iii ff dddd eee kll mmm ee fff aaa eee kkk lll mmm li ccjj ff ihhh jjk fbbb kkk ndd jjj lll m ii gggg hhhh eee kkk bbbb lll m ddd hhh hhi iii jb cccc aaa cc ccc hhh hibb bbbb nn dddd kkj bbb bbb eee fff ddd eee bbb nnn iii dddd hhh ddn ciih nnnnn hhh eee lll mm bbb bbb bbb ddd mmm gg ihh eee bbb kkk fff cc aaa nn ddd iill mm iii eee lll m dd aaa bbb gggg jjj kkk cjj fffff iiii aaa eee niihll mmmm fff ab dddd eee fff eee kkk eee kkk bbb aaa kkk bb jjj nn ddd l \n",
      "----\n",
      "iter 3000, loss: 23.145165\n",
      "----\n",
      " kkk fff eee fff ddd bbbb ijj lll mmmm lk kkk kjj lll m eee lll m m m m fff  ddd jjj lll m nnn ccc ghh nn ddd bbb ddd ccc hhh bbb iii eee kkk kkk fff aaa ccc ii ggg eee ff ggg ddd ee nnnn hhee kkk nnn jjj ffff eee kkk kkk fff jjj nnnn jj nn iiii hhh bbb hhh hhhh eee llk m m m m m m j mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm \n",
      "----\n",
      "iter 4000, loss: 17.283938\n",
      "----\n",
      " d aaa cc hhh ddd iic hhh bbb ccc aaa kll f aaa ccc aaa bbb eee mmm bbb hhhh eee lll mmmm bbb eee ff dddd ddd iii nnn hhh bbb jjj mmm eee kkk iii hhhh kkk lll ff hhh gg iiii hhhh nnn aaa bbb aaa bbb eee kkk nnn cc jjj ii hhh iii hhh iiii eee mmm nnn ggg ggg ggg ddd dddd eee mmmm iii aaa lll f aaa dddd dddd hhh aaa bbb hhh bbb iii gg ccc aaa eee kjj mmm bbb aaa lll nn ccc jjj ffffffffffff ggg eee fff iii ccc aaa bbb eee fff hhh ccc jjj kkk lll nn dddd iiii hhh iii nnn ccc ddd eee ll lkk nnn jjj kkk nn jjj kkk aaa kkk mmmmm bbb aaa bbb hhh nn hhh aaa bbb ccc iii ddd aaa bbb ggg iii aaaa fff jjj fff ccc aaa bbb bbb hhee lll lll mmm hhh nnn ggg ddd iiii eee lll mmmm ccc jjj kkk nnn iii lll lll ll lll mmm ccc aaa lll kkk mmm nnnn iii jjj kkk nnnn ggg gg hhh iii jjj bbb jjj mmmm bbb hhh iii ggg aaa ccc c aaa mm ccc hhh fffhhh bbb nnn dddd hhh ccc hhhh ccc eee lll f ggg jjj ffff aaa bbb fffhhh ccc iii iii ggg hhh ggg ii gg eee bbb lll fdd ggg iii ac iii hhh hhh aaa bbb lll nn aaa lll f aaa iii ddd eee fff hhh nn hhh iii bbb eee mmm bbb aaa bbb jj eee lll lll mmmm ccc aaa nn ggg ccc ddd hhh gggg ccc ddd hhh hhh eee fkkkk mmm iii eee kkk mmmm bbb hhh ccc jjjj mmmm cc aaa ccc ccc gg ggg aaa lll bbbb lll mmm bbb ccc bbb ddd iii ddd jjj f eee ffffff gg iiii flll mmm lll mmm eee kk jjj kkk mmmm iii icc ccc hhhh ccc eee nnn hhh bbb iii iii gggg jjj nn eee lll f aaa nnn aaa nnnn ggg jjj fjjj kkk lll kkk kkk kkk nn ddd ccc ccc ddd ggg aaa lll mmm aaa bbb ggg jjj bbb jjj ffffffff ddd aaa bbb b \n",
      "----\n",
      "iter 5000, loss: 18.668056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " kk bbb eee fff jjj kkk lll mmm ggg iii ggg aaa lll f eee mmm jjj lll fff ccc hhh ccc ddd jjj kjj mmmmm jjj lll fff jj nnn ddd aaa bbb dggg iii ggg aaa kkkk lll mmmm ccc nddd hhh nnn eee ffff gggg iiii ggg eee mmm gggg eee fff ddd bbb eee mmm ddd iii eee mmmm ggg ddd bbb hhh bbb bbb eee fff ddd d jjj ccc nnn aaa nnn ddd bbbb kkk kk eee fff aaa iii eee fff hhh ddd ddd eee fff jj hhh iiii iii hhg bbb ccc jjj lll ffff jjj kkk lll fff aaa nnn eee m jjj bbb ccc iiii aaa iii hhh bbb hhh kkkk fff aaa nnn hhh iii ddn ggg ddd iii aaa iii ggg hhh ccc ddd nnnn aaa nnn jjj hhh bbb ccc ggg eee mmm hhh nnn hhh lll mmmm kkkk ff ggg ccc ee ccc iii ddd nnn ggg eee m jj kkk lll mm jj iiii iii jjj ffff aaa bbb eee fff eee mm eee kk lll ff hhh ccc hhh ggg ccc jj ccc jj lll ffff iii ddd ggg ddd ccc hhh ffff ggg iii jjj kkk iii aaa iiijj lll ffff hhh bbj ccc hhh bbb ccc hhh ccc ddd i aaa kkk lll mmm ccc jjj lk kkk lll ffff jjj kkk lll ffff iiii eee mmmm neee mmm eee mmmm hhh bbb iii ddd eee mm ddd nnn jjj lll mmm ggg iiii bbb dd aaa hhh nnn eee mmm ggg aaa ccc iii aaa eee mmm jjj kkk kkk lll fff aaa bbb aaa kkk ll nn aaa bbbb dd aaa ccc ccc eee mmm aaa eee ffff hhh kkkk lll m eee fff aaa ccc dd ggg eee fff ggg ccc ggg ccc eee mmmm ddd nnn jjj nn ddd jjj lll m hhh jjj fffff eee fff aaa jj bbb nnnn iii nnn nnn jjj bbb hg hhh ccc hhh bbb iii ddd ccc hgg iii eee fff gggg nnn eee ff eee kkk lll m jjj fffff ggg eee ffff hhh hh nnn ggg hhh bbb hhh kkk lll mmmm jjj fffff ggg ggg iii aaa eee fff aaa nnn ggg \n",
      "----\n",
      "iter 6000, loss: 14.643879\n",
      "----\n",
      " ggg hhh eee fff iii iii iii jjj mmm bbb ji iii aaa aaa bbb aaa lll fff ccc eee bbb aaaa lll kkkk mmm iii aaa hh iii eee lll lll fff nn hhhh hhhh eee lll fff eee bbb eee lll fff eee mmm hhh aaa eee bbb nn ccc jjj lll kkk kkk lll fff aaaa fff jjj mmm bbb ggg iijj lll lll kkk mmm ggg bbb aaa ccc nnn d ddd nnn aaa ddd iii iii eee fff jjj mmmm bbb jjj mmm nnn ddd ccc iii jjj mmm ccc eee fff ggg iii iiii jjj mmm bbb hhh gggg hhh ggg hhh ccc ccc eee bbb hhh ddd iii ggg nn iii iii bmmm ccc bbb eee lll mmm bbb iii eee ccc gggg ii hhh hhh ccc n iii aaa iii ccc ggg nnn aaa bbbb jjj fff nn aaa hhhg dccc iii dggg iii iiic ccc aaa bbb n ccc aaa hgg nnnn jjj mmm nn nn hhh ccc hhh nn iii iii iii iii ccc ggg nnn jjj lll mmm eee bbb jjj kkk mmm nn hhh iiii jjj lll kkk fff iii iii aaa nnnn jjj lll kkk mmm nnnnn nnn hhh ddd eee lll mmm bbb ggg aaa n hhh jjj mmm nnn ggg cccc ccc jjj mmm iii ggg hhh iii ddd ccc ccc eee n aaa iii ccc ccc ddd aaa nn aaaa bbb ccc ggg nnnn ddd bbb ddd ddd ccc dccc eee lll fff jjj lll kkk lll kkk lll kkk mmm cc ccc jjj kkk kkk lll fff iijj mmm nnnnn jii gg hhh aaa ggg jjj mmm hhh hhh jkkk mmm bbb ggg eee lll fff hhh iiijj mmm iii ddd aaa eee lkk bbb dd ddd hhh hh hhh hhh ccc ccc aaa ccc ccc ggg jjj lll mmm nn  eee bb cccc jjj mmm ccc ggg ccc hhh ii ggg ccc ii ggg aaa nn hhh  eee bbb jjj kkk fff ddd ggg ddd jjj mmm bbb kkk lll fff jjj mmm ccc jjj lll kkk fff hhh cc nnn aaa hhh aaa nnnnn nnn bbb ggg ccc d eee bbb ggg bbb ggg ddd ddd jjj kkk lll ff iii nnnn jjj kkk ff ggg \n",
      "----\n",
      "iter 7000, loss: 14.243024\n",
      "----\n",
      " b ii ddd ddd hhh jjj mmm ccc aaa bbbjj kkk lll lll lll mmm fff ggg ccc hhh ddd eee fff jjj kkk kkk mmm hhc eee fff aaa bbb ddd ggg aaa bbb nn ggg nnnn hhh iijj lll fff cc aaa nnn ggg iii ggg iii ccc jjj kkk lll kkk kkk mmm bbb aaa kkk bbb iiii ddd eee kkk mmm mmmm hhh aaa ffff ggg hhh aaa beee fff aaa fff ggg eee fff hhh ji jjj lll kkk kkk kkk lll lll mmm niiii eee mmm eee ff ddd ggg aaa nnn ccc aaa nn aaa ccc ddd jjj kkk lll mmm nnn jjj ll bbb jjj lll lll mmm fff jjj lll lll kkk mmm bbb ggg miii ggg n hhh ddd iii hhh aaa bbb iii nnn ddd eee fff aaa bbb hhh ccc ccc hhhgg nnnn jjj lll lll mmm nneee fff iii eee bbb ggg aaa ccc aaa bbb aaa bbb jjj lll kkk lll mmm nn ccc hhh ccc jjj kk bbb aaa nnn ggg hhh hhh eee ffff hhh ddd aaa nnn nnnn nnnn cc ggg hhh ccc hhh nnnn iiii eee bbbb eee fff ggg hhh gggg hhh ddd ccc hhh jjj mmm ccc ccc ccc iiii ggg jjj lll ffkk mmmm bbb gg eee mmm bbb ggg jjj llll mm hhh jjj mmm bbb jjj lll kkk mmm nnn jjj kkk ll bbb nnngg jjj lll mmm nn hhh ggg aaa bbb aaa bbb ddd aaa bbb ggg nnn jjj lll mmm hhh aaa fff ccc ccc iijj mmm bbb jjj mmm beee lll fff jjj kkk lll mmm iii hhh ggg jjj fff dd ggg eee fff eee fff eee fff eee bbeee kkk lll lll fff iii ccc hhh ccc aaa nn iii eee kkk nnn ddd ddd eee fff ccc jjj lll lll mmm bbb hhh ggg ggg nnnn iii iii eee bbb iii dddd ddd eee fff hhh jjj kkk lll mmm nnn aaa bbb aaa bbb ii nnn aaa fff ggg nnnn ddd eee fff iii jjj fff ddd aaa bbb nnnn jjj mmm nnnnn iii aaa bbb eee fff aaa n ddd nn cccc jjj fff aaa iijj ff aaa fff  \n",
      "----\n",
      "iter 8000, loss: 12.750819\n",
      "----\n",
      " ii aaa bbb fff cccc eee bbb iii aaa fff ggg nnn iii aaa bb ggg ccc ddd aaa fff cccc ddd ddd ddd aaa nnn eee fff ggg nn ddd jjj lll mmm fff iii ddd ccc eee fff ddd ddd ccc ddd dd ii iii ddd eee bbb n jjj lll kkk mmm ddd iii aaa kkkk lkk mmm jjj kkk kkk mmm bbjj mmm nnn iii aaa bbb hhh ggg ggg hhh ggg ddd aaa nn iii eee fff eee fff eee bbb ccc aaa bbb dd ddd aaa bb iii hhh aaa bb iii ggg hhh iii eee nnnn jjj kkk lll lll m aaa ffff ddd aaa bbb jjj kkk lll mmm hhh eee bbb ddd ddd hhh ddd ggg nnnn jjj kkk kkk mm ggg nnn dd ddd hhh eee fff nnnn aaa ccc aaa mmm nnnn iii iii hh eee bb hhh ggg ddd eee bbb hhh ccc hhh ddd ddd eee nnn cccc eee kkkk kkk lll fff ddd aaa mmm iii ggg nn aaa hhh eee nn aaa fff ggg ddd ggg ccc ggg nn aaa kkk hhh jjj kkk mmm ffff hhh hhhh eee fff iii iijj mmm nnn ggg jjj kkk lll mmm nnn ddd eee mmm nn iii ddd eee bbb hhh aaa mmm nnn iii ddd eee nnn ccc hhh aaa mmm bbb cccc ccc jjj lll fff hhhh ggg jjj lll fff nnnn dgg jjj lll mmm nnn hhh ggg eee lll fff jjj kkk lll mmm nnn jjj lll fff hhh ccc iii ddd ccc ggg jjj mmm nn ccc ddd cc aaa bb iii ccc ggg ddd eee bbb hhh eee ccc hhhh iiii jjj kk bbb jjj kkk mm hhh ddd ccc eee fff jjj kkk lll fff ggg hhh ji hhh eee kkkk kk bbb ccc eee mmm hhh ggg ggg nn ggg iii dddd iii ji ggg nnn eee nnn ddd jjj lll kkkk kkk mm hhhh ccc eee mmmm iii ggg jjj kkk mm nnn ggg hhh eee fff iii eee hhh aaa bbb nn ddd ddd jjj mmm bbjj lll fff ddd ggg ddd ccc eee bbb hhh ccc ddd iiii eee bb hggg nnn ddd jjj kkk kkk kkk kkk kkk mmm bbb nnn iii \n",
      "----\n",
      "iter 9000, loss: 11.671103\n",
      "----\n",
      " a fff fff hhhiimmm nn gg kkkkl m mmm aajmmmmm jjjj llk eem llllllllkffkkkkkffkkllbbbmbbmiijjjjjjje c kkkkkkkkkkkkkfdfkllllllldnnnnnnllfkkkfkkkkkkkfkkkfkkkkkk dkkkllkkkfkkkkkjjm lllll cc jjje fkfkkkfkffkkfkkllllnnll jjj kkkkkllllbmmmm kkkkklfffffkklnlkkfnnnnnnnllll nnnnnnnnnnllllllnllllkklknnnnnnnjjj dnnlllllkkkkkllllk nnnnnnlllllllkkkfkkjjjji nnnnn demi lklkffkffkknnnnnnnnnnnnnnhhhhhhhhhhh llllkjj llll llllllnnlll cccc iiiiiidkkffkkeemmmmmgggggggg nnnnnnnnnlkkkllll kkkkkkflll ddnnn cc lllnnnnnnnnnnnllllllllkkkfffkkkfkkkfkkkkkkfkkff jjiiim llllllkffkkkjjemmm nnnnnnnnlllllfffkfkk jiiiiiiiemma nnnllllllhhhhhhh kkk  dlllllllllllnlkknnnnnnnnnnnl ddlllllllkkkkfkkkka lllkkkkkknnnhhhhhe ll llllll kkfkfkfaa llllllllllnlh jjjjajinnnnnllkllhhhhnnnhhhh hhhhhhhhh ll ggghhhhlllllllkkkkfkjjmmmmmmm dffkkkkfkf nnnnnnnnllllllnnlkfdkkk lkkkkkkkkkkk lllkkkkkfkkkkkfkfffkkkkkffkkkll lllllll eemmmm nkkkkflllllll kkff kkkkkke llkkkkfkccc jmm ccc h lllkkkknnnlllllllkkkffkkkllllllllkkkkkffkklllll llnl jjjjiijjfkkfkfkkfkfkkkknnfkkfkkkkkl kkkkkkkkkkkk jjjjjjabbmmm lllkkkllfkkkkk kkklllllnnnnnlllkllkkfkfk dnnnnnnnnllllkkkkkkkkkkkkffkkkkffklllllllljjjjjbbbbb llllnnnnnlnnnlllllllkfkkkkkfkkkkkkkllkkkfffkll dnnnnnnnnnnnnnnnnnnnnnlllllkkkkeembbmmm llkkkknnnnnnnnnnn jjjiiijjjjjjjjii ccccccc llll ddddaa nnnnnn ccc lnnl lkkk lllllna eemmm llkkkkkkkfkll jjjjieiiimm ll lnlllll jiiimm lllkkkkknn nnnnnnnnlllkl dddfkkkkfkkklllnnnnnnneeme ghh hhhhhhh lllfkkkfllkkfnnnh jjjjjnnnlkkbbb dkkkkkknnnnnnnllllllllll llkfkkfkkk \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# end-to-end memory RNN\n",
    "import numpy as np\n",
    "\n",
    "nruns = 10000\n",
    "noutput = 1000\n",
    "\n",
    "# data I/O\n",
    "data = open('input0.txt', 'r', encoding=\"utf8\").read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxu = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Wuu = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Wuo = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bu = np.zeros((hidden_size, 1)) # hidden bias\n",
    "bo = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, us, os, ps = {}, {}, {}, {}\n",
    "    mi, pi, ci = {}, {}, {}\n",
    "    us[-1] = np.copy(uprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        us[t] = np.tanh(np.dot(Wxu, xs[t]) + np.dot(Wuu, us[t-1]) + bu) # hidden state\n",
    "        # pi=softmax(u*mi)\n",
    "        mi[t] = np.dot(Wxu, xs[t]) + bu\n",
    "        pi[t] = softmax(np.dot(us[t].T, mi[t]))\n",
    "        #ys[t] = np.dot(Wuy, us[t]) + by # unnormalized log probabilities for next chars\n",
    "        # o=pi*ci\n",
    "        ci[t] = np.dot(Wuo, us[t]) + bo\n",
    "        os[t] = pi[t] * ci[t]\n",
    "        #ps[t] = softmax(os[t]) # probabilities for next chars (=softmax)\n",
    "        ps[t] = softmax(os[t])\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxu, dWuu, dWuo = np.zeros_like(Wxu), np.zeros_like(Wuu), np.zeros_like(Wuo)\n",
    "    dbu, dbo = np.zeros_like(bu), np.zeros_like(bo)\n",
    "    #dmi, dpi = np.zeros_like(mi), np.zeros_like(pi)\n",
    "    dunext = np.zeros_like(us[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        do = np.copy(ps[t])\n",
    "        do[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        \n",
    "        # pi=softmax(u*mi)\n",
    "        #dmi[t] = np.dot(Wxu, xs[t]) + bu\n",
    "        #dpi[t] = softmax(np.dot(us[t].T, mi[t]))\n",
    "        \n",
    "        #dWuo += np.dot(do, us[t].T)\n",
    "        dWuo += np.dot(do, np.dot(pi[t], us[t].T))\n",
    "        dbo += np.dot(do, pi[t])\n",
    "        \n",
    "        #du = np.dot(Wuo.T, do) + dunext # backprop into u\n",
    "        #print('shape ci: %s' % (str(np.shape(ci[t]))))\n",
    "        du = np.multiply(ci[t].T, (1-pi[t])*mi[t])\n",
    "        du += Wuo.T\n",
    "        duraw = (1 - us[t] * us[t]) # tanh'=1-tanh^2\n",
    "        #dbu += np.dot(do, pi[t]*np.dot(duraw, (np.multiply(ci[t].T, (1-pi[t])*mi[t]) + Wuo)))\n",
    "        dbu += np.dot(pi[t]*np.multiply(du, duraw), do)\n",
    "        \n",
    "        dWuu += duraw*us[t-1]*np.dot(du, do) \n",
    "        du = np.dot(pi[t]*du, do)\n",
    "        #print(np.shape(duraw*pi[t]*((1-pi[t])*np.multiply(ci[t], mi[t].T) + Wuo).T*xs[t].T*do.T))\n",
    "        #dWxu += duraw*pi[t]*((1-pi[t])*np.multiply(ci[t], mi[t].T) + Wuo).T*xs[t].T*do.T\n",
    "        #print(np.shape(pi[t]*(ci[t].T*(1-pi[t])*(np.dot(mi[t].T, duraw) + us[t]) + Wuo.T*duraw)*xs[t].T*do.T))\n",
    "        dWxu += pi[t]*(ci[t].T*(1-pi[t])*(np.dot(mi[t].T, duraw) + us[t]) + Wuo.T*duraw)*xs[t].T*do.T\n",
    "        \n",
    "        #dunext = np.dot(Wuu.T, duraw)\n",
    "    \n",
    "    for dparam in [dWxu, dWuu, dWuo, dbu, dbo]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxu, dWuu, dWuo, dbu, dbo, us[len(inputs)-1]\n",
    "\n",
    "def sample(u, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        u = np.tanh(np.dot(Wxu, x) + np.dot(Wuu, u) + bu)\n",
    "        mmi = np.dot(Wxu, x) + bu\n",
    "        ppi = softmax(np.dot(u.T, mmi))\n",
    "        #o = np.dot(Wuo, u) + bo\n",
    "        c = np.dot(Wuo, u) + bo\n",
    "        o = ppi * c\n",
    "        p = softmax(o)\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxu, mWuu, mWuo = np.zeros_like(Wxu), np.zeros_like(Wuu), np.zeros_like(Wuo)\n",
    "mbu, mbo = np.zeros_like(bu), np.zeros_like(bo) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        uprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % noutput == 0 or n == nruns-1:\n",
    "        sample_ix = sample(uprev, inputs[0], 1500)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxu, dWuu, dWuo, dbu, dbo, uprev = lossFun(inputs, targets, uprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "\n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxu, Wuu, Wuo, bu, bo], \n",
    "                                [dWxu, dWuu, dWuo, dbu, dbo], \n",
    "                                [mWxu, mWuu, mWuo, mbu, mbo]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4090909090909091, 0.13636363636363635, 0.13636363636363635, 0.06818181818181818, 0.1590909090909091, 0.09090909090909091]\n"
     ]
    }
   ],
   "source": [
    "# biased dice\n",
    "import random\n",
    "from numpy.random import choice\n",
    "\n",
    "def rolldice(b):\n",
    "    return choice([1,2,3,4,5,6], p=b)\n",
    "\n",
    "def hit():\n",
    "    hits = []\n",
    "    for d in range(1,7):\n",
    "        #dice = rolldice([1./6.,1./6.,1./6.,1./6.,1./6.,1./6.])\n",
    "        dice = rolldice([0.3,0.15,0.15,0.15,0.15,0.1])\n",
    "        if dice == d:\n",
    "            hits.append(d)\n",
    "    if len(hits) > 1 or len(hits) == 0:\n",
    "        return 0\n",
    "    return hits[0]\n",
    "\n",
    "if __name__=='__main__':\n",
    "    #print(hit())\n",
    "    ddistr = [0.] * 6\n",
    "    for i in range(100):\n",
    "        for d in range(1,7):\n",
    "            if hit() == d:\n",
    "                ddistr[d-1] += 1\n",
    "    dsum = 0.\n",
    "    for d in ddistr:\n",
    "        dsum += d\n",
    "    for d in range(len(ddistr)):\n",
    "        ddistr[d] /= dsum\n",
    "    print(ddistr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 4648 characters, 15 unique.\n",
      "----\n",
      " kimheflcglfgc fc adefjmd eaegnad cg  mccl j  bmnebmfnikkggnilnckb lnnmggljcaefmgad hffnnkfiegbchelbggflhgmljddidlhhebhfkhcicamjanej jgkbmajhib  nmnnfmndkgafldjinjgejjcejndncgfg gbcg ilccnggjidbflgdnclljnmafkeihiedfenf gkeaifddlddcfdafibm fbblalifmbcahikadiemdgamfmaaa bgffmfijmhkgdncg afmkkmhbmnfnmebinfjjc enll gjcidmkbjcfbfgmgcdgacchbkanhdlleemebmblifnkaihicnbgki fgl enllkgmajgnncg  gjhhlg ecmcbk eacilhbj ianmllk jbkbdhlkbchl dmnikcbnkighibebjinjmam nlaejjbcbahkalhebnakdhjlfjjaidkankfbcjlj jefeg \n",
      "----\n",
      "iter 0, loss: 67.701246\n",
      "----\n",
      " bk mii mmc jfk mdd kdh lln eif kej gda eme jhl fde aha gfe cfg acl ifl lij hmi jmn ijc cee beb bha jke deg jglllnl mff hlj jhi hji jkd imk jfk jdj hlk ddd chb ghn jka bnd nia fik cmi jlm lba ama ffh ehl hhm dbd jgg hhk jde nea ggb hbn adb jea ff  hhb jel nac bja bhf mdd jad cab hdd ake ehf  cf cim ana kkg hbk cnk ljd jml iii jjd mdj eb  cia fff fna jfa bee bbc ghg kej bgk amc kfk cdg nll aeb gga ala gig mgi kmh dhk jfl mff jl  lim fec hmf jii mjl mai jdd agh fea ebd gdm lfi mkj jhh bmc jhh hhk k \n",
      "----\n",
      "iter 100, loss: 66.698113\n",
      "----\n",
      " khl lhn nnn aag eee cdd gbb gkd eib hii iih  hh kii ljljljj iih khh hmh mlm mnd efg ee  lcc mgm ehh lkj hlj jki  hi kdk hkd mmn ddd agg bgb eih kih him lmm mmi dnn ace dni eee ffa abb aee cgk hhh ljk jcl lmm emm mdn ddd nda baf gga ggg mii jmb adgeeee ghe fgc  kn lll iii iil ndh hhl jhj hih lmj lmj mdn aaf kcj lhh ljl nmm bdh fff ffa fff bmn nje aaa ffd eee beb caa ggg cni jjn dmn fff cci bbb gcl ihm gik hmh mkn ddc fff ggg hhi mij ikd nld bcj cca fca efm bba cff ffa fee geh clj iik hjj jhk djn  \n",
      "----\n",
      "iter 200, loss: 63.714556\n",
      "----\n",
      " aaa ffa ggg hhh iik jjl lik mmm nne aaa bbb ccc ncn ffg bbh khh ljj kjj jjh ikk jja nli iii jkk jjk jli jii mml nnn eee fff ggg ijj jjj iii kij jjj jlj kjl iie hii jij hii mmm ddd eee fae ffk gee ggg hhh ikk jii lil nna aaa abb ccc ndd eee ccc ddd eea ffa bbb eih gff ggg lck mdm knn nna aad eee ffg bdb ccc ddd eee ffg hgl him mmk nnn aaa bbb ccn caa ann ccc eee gbh hhh kkk kmj djl lmb ebb ccc ddb ggg hih ijj jjj iii jjj jjj iij kkl jlj hhh jll ijl iim nnm dnd cfg eee fag hgg hhk jmc ddd aaa aaa  \n",
      "----\n",
      "iter 300, loss: 59.245954\n",
      "----\n",
      "  hhh iiihiii jjc mmm jnn aca fbe fff hhk ihk lll mml mmn nnn daf fgg bhh  mj nnn aaa aee fgg hhk lll mmm ldd fff ccc ddd eee ffa fhh cci jli jmm ddd cna bbb ccc ddd bbb ccc idm ldd caa bbb ccc ddd bbb bhh iii jlm ddm knm abb bbb fcc dnc ddd bbb ggg hhh iii jjj hii imm mmn naa aaa ccg ggg bhh jjj kkk lll jdm nna bee ffh hkc mmm nnn aaa ggg hhh iii jjh kkh jjj mmm nnn aaa bbb gcc ddd eee fff fhh jii ihi jll mmm nnn aaa bbb ccc ddd eee ffc ihh kkk lii iii jmm mmm nnn aaa faf fcc ggg hhi iii jkk klm \n",
      "----\n",
      "iter 400, loss: 54.466985\n",
      "----\n",
      "  bbb ccc ddd eee fff ggg bbh hii jjj mmm nnn aaa bbb ccc mnn aaa bbb fcc eee fff ccc jdd eee fff ggg hhj hjj kkk lll mmm nnn aab eee fff ggg hhh iii jjj kll mmm nnn aaa bbb ccc ddd eee fff ccc ddd eee aaf fgg hhh iih iii jjj kkkkkll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lfj kkk mmm naa abb bbb ccc ddd eee fff ggg hhi jjj kkk lll mmk dnn nna aaa ahf hkj jjj kij mmm nnn aaf fgg hhh ihh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iij kkk lll mmm nnn aaa bbb cac afa ggg hll \n",
      "----\n",
      "iter 500, loss: 49.609164\n",
      "----\n",
      " h ccc dmm nnn aaa bbb ccc ddd eee fff ggg hhh iii lll mmm nnn aaa bbb ccc ddd eee fff ggh hhh iii jjj kmj kkk lll mmm nnn abb hhh iii ijj kk  lll mmm nne aaf ggg hhh iii jjg mmm dne aaa bbb ccc ddb eee fff ggg hhh iii dmm nnn aaa bbb ccc ddd bbb ccc ddd eee fff ggg hhh lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iil jjj kkk lll mmm nnn aaa bbb ccb eee fff ggg ghh ihh jjj kii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh hii jll mmm nnn aan bbb ccc ddd eee fff ggg hhh iii jjj kkk llj jii jj \n",
      "----\n",
      "iter 600, loss: 45.423950\n",
      "----\n",
      " k lll mmm nnn aaa bbb ddd eee fff ggg hhh iii jjj kkk lll mmm nnn naa bbb ccc ddd eee afa bbb ccc ddd eee fff ggg ijj kkk jmj kkk lll mmm nnn aab ggg iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg iii jjj kkk lll cmm nnn aaa bbb ccc ddd eee fff ggg hhh jjj kkk jjj kkk lll mmmnnnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nn  aaa bbd ccc ddd eee fff ggg iii jjj kil jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kk \n",
      "----\n",
      "iter 700, loss: 41.236552\n",
      "----\n",
      " jj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb eee fff ggg hhh jii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb cgg ggg hhh iii jjj kkk llm nnn aaa bbb ccc ddd eee fff hhh iii jjj kkk lll mmm nnn abb eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhl jii jjj kkk lll mmm jna aaa bbb ccc ddd eee fff ggg hhh iik lll lll mmm nnn aaa bbi ccc ddd eee fff g \n",
      "----\n",
      "iter 800, loss: 37.651973\n",
      "----\n",
      " gg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm kdn aaa bbb ccc dnd eee fff ggg hhh iii jjj kjj mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hih iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee  af bgg hhh iiiiijj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg iii kkk lll ldm nnn aaa bbb ccc ddd eee fff ggg hhh i \n",
      "----\n",
      "iter 900, loss: 34.138050\n"
     ]
    }
   ],
   "source": [
    "# from https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "# diagonal constraints on weight matrices\n",
    "\n",
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "nruns = 1000\n",
    "noutput = 100\n",
    "\n",
    "# data I/O\n",
    "data = open('input0.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        clip_diag(dparam, 1)\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "def clip_diag(w, cval):\n",
    "    n = min(np.shape(w))\n",
    "    w[range(n), range(n)] = np.clip(np.diagonal(w), -cval, cval)\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % noutput == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 500)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
