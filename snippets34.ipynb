{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 24, 14, 18, 7, 15, -1]\n",
      "15\n",
      "[-1.4079717394445106, 15, 7, 11, 7, -1.4191107472943223, 1]\n",
      "[9, 9, 8, 13, 11, 21, 1]\n"
     ]
    }
   ],
   "source": [
    "# D&D chromosomes (from Munroes What if...)\n",
    "import random\n",
    "\n",
    "def ddstat(): # strict ;)\n",
    "    '''roll stat dice'''\n",
    "    return random.randint(1,6) + random.randint(1,6) + random.randint(1,6)\n",
    "\n",
    "def createcset(l):\n",
    "    '''create a a character set: either a stat value or a multiplier'''\n",
    "    res = []\n",
    "    for i in range(l-1):\n",
    "        if random.random() > 0.8:\n",
    "            res.append(-random.uniform(0.5, 2.5)) # multiplier\n",
    "        else:\n",
    "            res.append(ddstat())\n",
    "    res.append(random.choice([-1, 1])) # last stat is sex (-1 female, 1 male)\n",
    "    return res\n",
    "\n",
    "def combine(cset0, cset1):\n",
    "    '''combine two character sets'''\n",
    "    if len(cset0) != len(cset1):\n",
    "        return -1\n",
    "    res = []\n",
    "    for ic in range(len(cset0)-1):\n",
    "        if cset0[ic] < 0: # multipliers are negative\n",
    "            if cset1[ic] < 0:\n",
    "                res.append(1) # multiplier on both sides: you get a 1\n",
    "            else:\n",
    "                res.append(int(-cset0[ic] * cset1[ic]))\n",
    "        else:\n",
    "            if cset1[ic] < 0:\n",
    "                res.append(int(-cset0[ic] * cset1[ic]))\n",
    "            else:\n",
    "                res.append(max(cset0[ic], cset1[ic]))\n",
    "    if cset0[len(cset0)-1] < 0:\n",
    "        if cset1[len(cset1)-1] < 0:\n",
    "            res.append(-1) # female\n",
    "        else:\n",
    "            if random.random() > 0.5:\n",
    "                res.append(1)\n",
    "            else:\n",
    "                res.append(-1)\n",
    "    else:\n",
    "        if cset1[len(cset1)-1] < 0:\n",
    "            if random.random() > 0.5:\n",
    "                res.append(1)\n",
    "            else:\n",
    "                res.append(-1)\n",
    "        else:\n",
    "            res.append(1) # male\n",
    "    return res\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    print(combine([15, -2, -1, 12, -0.5, 14, -1], [5, 12, 14, -1.5, 14, 15, -1]))\n",
    "    print(ddstat())\n",
    "    print(createcset(7))\n",
    "    print(combine(createcset(7), createcset(7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 92\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 59\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 20s 3ms/step - loss: 1.1668 - accuracy: 0.7259 - val_loss: 1.0686 - val_accuracy: 0.7150\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.8533 - accuracy: 0.7707 - val_loss: 0.8382 - val_accuracy: 0.7666\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.6931 - accuracy: 0.8041 - val_loss: 0.7161 - val_accuracy: 0.7923\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.6032 - accuracy: 0.8251 - val_loss: 0.6617 - val_accuracy: 0.8044\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.5533 - accuracy: 0.8380 - val_loss: 0.6154 - val_accuracy: 0.8193\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.5161 - accuracy: 0.8487 - val_loss: 0.5876 - val_accuracy: 0.8261\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.4871 - accuracy: 0.8564 - val_loss: 0.5609 - val_accuracy: 0.8345\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.4626 - accuracy: 0.8631 - val_loss: 0.5510 - val_accuracy: 0.8380\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.4411 - accuracy: 0.8690 - val_loss: 0.5259 - val_accuracy: 0.8449\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.4223 - accuracy: 0.8742 - val_loss: 0.5242 - val_accuracy: 0.8428\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.4047 - accuracy: 0.8790 - val_loss: 0.4980 - val_accuracy: 0.8519\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.3885 - accuracy: 0.8837 - val_loss: 0.4883 - val_accuracy: 0.8545\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.3737 - accuracy: 0.8880 - val_loss: 0.4750 - val_accuracy: 0.8588\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.3597 - accuracy: 0.8923 - val_loss: 0.4729 - val_accuracy: 0.8595\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.3469 - accuracy: 0.8958 - val_loss: 0.4616 - val_accuracy: 0.8640\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.3343 - accuracy: 0.8994 - val_loss: 0.4553 - val_accuracy: 0.8662\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.3224 - accuracy: 0.9029 - val_loss: 0.4517 - val_accuracy: 0.8679\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.3114 - accuracy: 0.9061 - val_loss: 0.4486 - val_accuracy: 0.8694\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.3006 - accuracy: 0.9096 - val_loss: 0.4448 - val_accuracy: 0.8705\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2903 - accuracy: 0.9122 - val_loss: 0.4412 - val_accuracy: 0.8717\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2808 - accuracy: 0.9151 - val_loss: 0.4432 - val_accuracy: 0.8721\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2717 - accuracy: 0.9176 - val_loss: 0.4464 - val_accuracy: 0.8710\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2632 - accuracy: 0.9202 - val_loss: 0.4385 - val_accuracy: 0.8745\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2546 - accuracy: 0.9228 - val_loss: 0.4439 - val_accuracy: 0.8727\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2464 - accuracy: 0.9251 - val_loss: 0.4382 - val_accuracy: 0.8750\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2387 - accuracy: 0.9279 - val_loss: 0.4427 - val_accuracy: 0.8737\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2315 - accuracy: 0.9297 - val_loss: 0.4459 - val_accuracy: 0.8741\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2241 - accuracy: 0.9321 - val_loss: 0.4475 - val_accuracy: 0.8748\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.2170 - accuracy: 0.9341 - val_loss: 0.4523 - val_accuracy: 0.8749\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2107 - accuracy: 0.9361 - val_loss: 0.4570 - val_accuracy: 0.8743\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2046 - accuracy: 0.9376 - val_loss: 0.4584 - val_accuracy: 0.8749\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1987 - accuracy: 0.9395 - val_loss: 0.4585 - val_accuracy: 0.8761\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1929 - accuracy: 0.9412 - val_loss: 0.4625 - val_accuracy: 0.8761\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1871 - accuracy: 0.9425 - val_loss: 0.4657 - val_accuracy: 0.8757\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1821 - accuracy: 0.9446 - val_loss: 0.4651 - val_accuracy: 0.8767\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1768 - accuracy: 0.9458 - val_loss: 0.4703 - val_accuracy: 0.8757\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1722 - accuracy: 0.9475 - val_loss: 0.4702 - val_accuracy: 0.8765\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1670 - accuracy: 0.9489 - val_loss: 0.4793 - val_accuracy: 0.8757\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1626 - accuracy: 0.9501 - val_loss: 0.4815 - val_accuracy: 0.8750\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1584 - accuracy: 0.9516 - val_loss: 0.4852 - val_accuracy: 0.8762\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1538 - accuracy: 0.9527 - val_loss: 0.4889 - val_accuracy: 0.8754\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1498 - accuracy: 0.9540 - val_loss: 0.4936 - val_accuracy: 0.8759\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1460 - accuracy: 0.9552 - val_loss: 0.4981 - val_accuracy: 0.8760\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1421 - accuracy: 0.9564 - val_loss: 0.4953 - val_accuracy: 0.8761\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1386 - accuracy: 0.9573 - val_loss: 0.5089 - val_accuracy: 0.8746\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1354 - accuracy: 0.9581 - val_loss: 0.5068 - val_accuracy: 0.8765\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1317 - accuracy: 0.9594 - val_loss: 0.5125 - val_accuracy: 0.8753\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1282 - accuracy: 0.9605 - val_loss: 0.5208 - val_accuracy: 0.8756\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1253 - accuracy: 0.9612 - val_loss: 0.5270 - val_accuracy: 0.8749\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1225 - accuracy: 0.9621 - val_loss: 0.5283 - val_accuracy: 0.8753\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1191 - accuracy: 0.9630 - val_loss: 0.5307 - val_accuracy: 0.8751\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1162 - accuracy: 0.9640 - val_loss: 0.5352 - val_accuracy: 0.8755\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1139 - accuracy: 0.9646 - val_loss: 0.5380 - val_accuracy: 0.8750\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1112 - accuracy: 0.9653 - val_loss: 0.5450 - val_accuracy: 0.8763\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1087 - accuracy: 0.9659 - val_loss: 0.5513 - val_accuracy: 0.8750\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1062 - accuracy: 0.9668 - val_loss: 0.5508 - val_accuracy: 0.8744\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.1036 - accuracy: 0.9674 - val_loss: 0.5576 - val_accuracy: 0.8752\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1014 - accuracy: 0.9683 - val_loss: 0.5587 - val_accuracy: 0.8750\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0993 - accuracy: 0.9688 - val_loss: 0.5664 - val_accuracy: 0.8752\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.0972 - accuracy: 0.9695 - val_loss: 0.5693 - val_accuracy: 0.8741\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0952 - accuracy: 0.9699 - val_loss: 0.5739 - val_accuracy: 0.8746\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0934 - accuracy: 0.9702 - val_loss: 0.5766 - val_accuracy: 0.8740\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0913 - accuracy: 0.9712 - val_loss: 0.5826 - val_accuracy: 0.8747\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0894 - accuracy: 0.9716 - val_loss: 0.5879 - val_accuracy: 0.8741\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0877 - accuracy: 0.9722 - val_loss: 0.5885 - val_accuracy: 0.8754\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0854 - accuracy: 0.9726 - val_loss: 0.5984 - val_accuracy: 0.8738\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0842 - accuracy: 0.9731 - val_loss: 0.6002 - val_accuracy: 0.8744\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0821 - accuracy: 0.9735 - val_loss: 0.6094 - val_accuracy: 0.8738\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0806 - accuracy: 0.9743 - val_loss: 0.6061 - val_accuracy: 0.8747\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0790 - accuracy: 0.9747 - val_loss: 0.6151 - val_accuracy: 0.8739\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0776 - accuracy: 0.9750 - val_loss: 0.6209 - val_accuracy: 0.8732\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0760 - accuracy: 0.9757 - val_loss: 0.6215 - val_accuracy: 0.8738\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0746 - accuracy: 0.9756 - val_loss: 0.6260 - val_accuracy: 0.8734\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0732 - accuracy: 0.9761 - val_loss: 0.6261 - val_accuracy: 0.8742\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0717 - accuracy: 0.9769 - val_loss: 0.6372 - val_accuracy: 0.8732\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0707 - accuracy: 0.9770 - val_loss: 0.6354 - val_accuracy: 0.8740\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0692 - accuracy: 0.9773 - val_loss: 0.6426 - val_accuracy: 0.8739\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0678 - accuracy: 0.9779 - val_loss: 0.6426 - val_accuracy: 0.8729\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0669 - accuracy: 0.9781 - val_loss: 0.6503 - val_accuracy: 0.8727\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0658 - accuracy: 0.9783 - val_loss: 0.6517 - val_accuracy: 0.8728\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0646 - accuracy: 0.9788 - val_loss: 0.6534 - val_accuracy: 0.8742\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0636 - accuracy: 0.9789 - val_loss: 0.6569 - val_accuracy: 0.8734\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0623 - accuracy: 0.9794 - val_loss: 0.6645 - val_accuracy: 0.8733\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0615 - accuracy: 0.9797 - val_loss: 0.6660 - val_accuracy: 0.8730\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0601 - accuracy: 0.9800 - val_loss: 0.6707 - val_accuracy: 0.8728\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0595 - accuracy: 0.9801 - val_loss: 0.6706 - val_accuracy: 0.8732\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0583 - accuracy: 0.9802 - val_loss: 0.6781 - val_accuracy: 0.8730\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0577 - accuracy: 0.9807 - val_loss: 0.6782 - val_accuracy: 0.8732\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0566 - accuracy: 0.9808 - val_loss: 0.6804 - val_accuracy: 0.8734\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0556 - accuracy: 0.9812 - val_loss: 0.6849 - val_accuracy: 0.8739\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0549 - accuracy: 0.9815 - val_loss: 0.6912 - val_accuracy: 0.8727\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0541 - accuracy: 0.9816 - val_loss: 0.6890 - val_accuracy: 0.8732\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0532 - accuracy: 0.9819 - val_loss: 0.6956 - val_accuracy: 0.8725\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0525 - accuracy: 0.9819 - val_loss: 0.7024 - val_accuracy: 0.8721\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0519 - accuracy: 0.9822 - val_loss: 0.7051 - val_accuracy: 0.8720\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0511 - accuracy: 0.9825 - val_loss: 0.7009 - val_accuracy: 0.8730\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0504 - accuracy: 0.9827 - val_loss: 0.7012 - val_accuracy: 0.8723\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.0495 - accuracy: 0.9828 - val_loss: 0.7042 - val_accuracy: 0.8725\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0492 - accuracy: 0.9830 - val_loss: 0.7116 - val_accuracy: 0.8727\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0481 - accuracy: 0.9832 - val_loss: 0.7225 - val_accuracy: 0.8722\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Va !\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Courez !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Courez !\n",
      "\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Qui ?\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Ça alors !\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Au feu !\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: À l'aide !\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Saute.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Stop !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Stop !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Stop !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Poursuis.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Poursuis.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Poursuis.\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Bonjour !\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Bonjour !\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Je comprends.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: J'essaye.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: J'ai gagné !\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: J'ai gagné !\n",
      "\n",
      "-\n",
      "Input sentence: I won.\n",
      "Decoded sentence: J’ai gagné.\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Oh non !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attaque !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attaque !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Merci !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Merci !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Merci !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Merci !\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Lève-toi.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Vas-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Vas-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Vas-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Compris !\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Compris !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Pigé ?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Pigé ?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Pigé ?\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Montez.\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Montez.\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Serre-moi dans tes bras !\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Serre-moi dans tes bras !\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je suis tombé.\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je suis tombé.\n",
      "\n",
      "-\n",
      "Input sentence: I know.\n",
      "Decoded sentence: Je sais.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je suis parti.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je suis parti.\n",
      "\n",
      "-\n",
      "Input sentence: I lied.\n",
      "Decoded sentence: J'ai menti.\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: J'ai perdu.\n",
      "\n",
      "-\n",
      "Input sentence: I paid.\n",
      "Decoded sentence: J’ai payé.\n",
      "\n",
      "-\n",
      "Input sentence: I'm 19.\n",
      "Decoded sentence: J'ai la maiclent.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je vais bien.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je vais bien.\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: Écoutez !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est hors de question !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est hors de question !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est hors de question !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est hors de question !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est hors de question !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est hors de question !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est hors de question !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est hors de question !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est hors de question !\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Vraiment ?\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Vraiment ?\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Vraiment ?\n",
      "\n",
      "-\n",
      "Input sentence: Thanks.\n",
      "Decoded sentence: Merci !\n",
      "\n",
      "-\n",
      "Input sentence: We try.\n",
      "Decoded sentence: Nous avons essayé.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous l'avons emporté.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous l'avons emporté.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous l'avons emporté.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous l'avons emporté.\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: Demande à Tom.\n",
      "\n",
      "-\n",
      "Input sentence: Awesome!\n",
      "Decoded sentence: Fantastique !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez calmes !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez calmes !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez calmes !\n",
      "\n",
      "-\n",
      "Input sentence: Be cool.\n",
      "Decoded sentence: Sois détendu !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois équitable !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois équitable !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois équitable !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois équitable !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois équitable !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sois équitable !\n",
      "\n",
      "-\n",
      "Input sentence: Be kind.\n",
      "Decoded sentence: Sois gentil.\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Dégage !\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Appelle-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Appelle-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Appelez-nous !\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Appelez-nous !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Entre !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "'''\n",
    "#Sequence to sequence example in Keras (character-level).\n",
    "This script demonstrates how to implement a basic character-level\n",
    "sequence-to-sequence model. We apply it to translating\n",
    "short English sentences into short French sentences,\n",
    "character-by-character. Note that it is fairly unusual to\n",
    "do character-level machine translation, as word-level\n",
    "models are more common in this domain.\n",
    "**Summary of the algorithm**\n",
    "- We start with input sequences from a domain (e.g. English sentences)\n",
    "    and corresponding target sequences from another domain\n",
    "    (e.g. French sentences).\n",
    "- An encoder LSTM turns input sequences to 2 state vectors\n",
    "    (we keep the last LSTM state and discard the outputs).\n",
    "- A decoder LSTM is trained to turn the target sequences into\n",
    "    the same sequence but offset by one timestep in the future,\n",
    "    a training process called \"teacher forcing\" in this context.\n",
    "    It uses as initial state the state vectors from the encoder.\n",
    "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
    "    given `targets[...t]`, conditioned on the input sequence.\n",
    "- In inference mode, when we want to decode unknown input sequences, we:\n",
    "    - Encode the input sequence into state vectors\n",
    "    - Start with a target sequence of size 1\n",
    "        (just the start-of-sequence character)\n",
    "    - Feed the state vectors and 1-char target sequence\n",
    "        to the decoder to produce predictions for the next character\n",
    "    - Sample the next character using these predictions\n",
    "        (we simply use argmax).\n",
    "    - Append the sampled character to the target sequence\n",
    "    - Repeat until we generate the end-of-sequence character or we\n",
    "        hit the character limit.\n",
    "**Data download**\n",
    "[English to French sentence pairs.\n",
    "](http://www.manythings.org/anki/fra-eng.zip)\n",
    "[Lots of neat sentence pairs datasets.\n",
    "](http://www.manythings.org/anki/)\n",
    "**References**\n",
    "- [Sequence to Sequence Learning with Neural Networks\n",
    "   ](https://arxiv.org/abs/1409.3215)\n",
    "- [Learning Phrase Representations using\n",
    "    RNN Encoder-Decoder for Statistical Machine Translation\n",
    "    ](https://arxiv.org/abs/1406.1078)\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = r'fra.txt'\n",
    "os.chdir(r'D:\\BernieData\\DeepL')\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')\n",
    "\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 92\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 59\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 1.2189 - accuracy: 0.7267 - val_loss: 0.9825 - val_accuracy: 0.7214\n",
      "(None, None, 92)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot iterate over a tensor with unknown first dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-46bcb89e8826>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;31m#decoder_outputs, state_h = decoder_gru(decoder_inputs, initial_state=decoder_states_inputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m \u001b[0mdecoder_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_gru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoder_states_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[0mdecoder_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[0mdecoder_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    546\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m       raise TypeError(\n\u001b[1;32m--> 548\u001b[1;33m           \"Cannot iterate over a tensor with unknown first dimension.\")\n\u001b[0m\u001b[0;32m    549\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m       \u001b[1;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot iterate over a tensor with unknown first dimension."
     ]
    }
   ],
   "source": [
    "# https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "'''\n",
    "#Sequence to sequence example in Keras (character-level).\n",
    "This script demonstrates how to implement a basic character-level\n",
    "sequence-to-sequence model. We apply it to translating\n",
    "short English sentences into short French sentences,\n",
    "character-by-character. Note that it is fairly unusual to\n",
    "do character-level machine translation, as word-level\n",
    "models are more common in this domain.\n",
    "**Summary of the algorithm**\n",
    "- We start with input sequences from a domain (e.g. English sentences)\n",
    "    and corresponding target sequences from another domain\n",
    "    (e.g. French sentences).\n",
    "- An encoder LSTM turns input sequences to 2 state vectors\n",
    "    (we keep the last LSTM state and discard the outputs).\n",
    "- A decoder LSTM is trained to turn the target sequences into\n",
    "    the same sequence but offset by one timestep in the future,\n",
    "    a training process called \"teacher forcing\" in this context.\n",
    "    It uses as initial state the state vectors from the encoder.\n",
    "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
    "    given `targets[...t]`, conditioned on the input sequence.\n",
    "- In inference mode, when we want to decode unknown input sequences, we:\n",
    "    - Encode the input sequence into state vectors\n",
    "    - Start with a target sequence of size 1\n",
    "        (just the start-of-sequence character)\n",
    "    - Feed the state vectors and 1-char target sequence\n",
    "        to the decoder to produce predictions for the next character\n",
    "    - Sample the next character using these predictions\n",
    "        (we simply use argmax).\n",
    "    - Append the sampled character to the target sequence\n",
    "    - Repeat until we generate the end-of-sequence character or we\n",
    "        hit the character limit.\n",
    "**Data download**\n",
    "[English to French sentence pairs.\n",
    "](http://www.manythings.org/anki/fra-eng.zip)\n",
    "[Lots of neat sentence pairs datasets.\n",
    "](http://www.manythings.org/anki/)\n",
    "**References**\n",
    "- [Sequence to Sequence Learning with Neural Networks\n",
    "   ](https://arxiv.org/abs/1409.3215)\n",
    "- [Learning Phrase Representations using\n",
    "    RNN Encoder-Decoder for Statistical Machine Translation\n",
    "    ](https://arxiv.org/abs/1406.1078)\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, GRU, Dense\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 1  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = r'fra.txt'\n",
    "os.chdir(r'D:\\BernieData\\DeepL')\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "'''Turn the sentences into 3 Numpy arrays, encoder_input_data, decoder_input_data, decoder_target_data:\n",
    "encoder_input_data is a 3D array of shape (num_pairs, max_english_sentence_length, num_english_characters)\n",
    "    containing a one-hot vectorization of the English sentences.\n",
    "decoder_input_data is a 3D array of shape (num_pairs, max_french_sentence_length, num_french_characters)\n",
    "    containg a one-hot vectorization of the French sentences.\n",
    "decoder_target_data is the same as decoder_input_data but offset by one timestep.\n",
    "decoder_target_data[:, t, :] will be the same as decoder_input_data[:, t + 1, :].'''\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)): # over input corpus\n",
    "    for t, char in enumerate(input_text): # over characters in input text\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1. # char to char token\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = GRU(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h = encoder(encoder_inputs)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_gru = GRU(latent_dim, return_sequences=True)\n",
    "decoder_outputs = decoder_gru(decoder_inputs, initial_state=state_h)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')\n",
    "\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, state_h)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "#decoder_states_inputs = [decoder_state_input_h]\n",
    "decoder_states_inputs = Input(shape=(latent_dim,))\n",
    "#decoder_outputs, state_h = decoder_gru(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "print(np.shape(decoder_inputs))\n",
    "decoder_outputs, state_h = decoder_gru(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to something readable\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences (to simplify, here we assume a batch of size 1)\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length or find stop character\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1)\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab:  15\n",
      "(15, 15, 15)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer lstm_2 was called with an input that isn't a symbolic tensor. Received type: <class 'numpy.ndarray'>. Full input: [array([[ 2,  3,  4,  5,  6,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n       [ 1,  8,  9, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n       [ 1, 11, 12, 13, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]. All inputs to the layer should be tensors.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m                 \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mis_keras_tensor\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    696\u001b[0m         raise ValueError('Unexpectedly found an instance of type `' +\n\u001b[1;32m--> 697\u001b[1;33m                          \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m                          'Expected a symbolic tensor instance.')\n",
      "\u001b[1;31mValueError\u001b[0m: Unexpectedly found an instance of type `<class 'numpy.ndarray'>`. Expected a symbolic tensor instance.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-636726c3744b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;31m#embd_lyr = embedding_layer_creator(len(my_vocab[0]), 100, 15, embd_mtrx)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseq2seq_model_builder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_input_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_input_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-636726c3744b>\u001b[0m in \u001b[0;36mseq2seq_model_builder\u001b[1;34m(encoder_data, decoder_data, MAX_LEN, VOCAB_SIZE, HIDDEN_DIM)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;31m# encoder_embedding = embed_layer(encoder_inputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[0mencoder_LSTM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHIDDEN_DIM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m     \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_LSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m     \u001b[0mdecoder_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int32'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;31m#decoder_embedding = embed_layer(decoder_inputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m         \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    444\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    314\u001b[0m                                  \u001b[1;34m'Received type: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m                                  \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. Full input: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                                  \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. All inputs to the layer '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m                                  'should be tensors.')\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Layer lstm_2 was called with an input that isn't a symbolic tensor. Received type: <class 'numpy.ndarray'>. Full input: [array([[ 2,  3,  4,  5,  6,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n       [ 1,  8,  9, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n       [ 1, 11, 12, 13, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]. All inputs to the layer should be tensors."
     ]
    }
   ],
   "source": [
    "# from Akira Takezawa and adapted quite heavily\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "MAX_LEN = 20 # to unify the length of the input sentences\n",
    "VOCAB_SIZE = 20 # to decide the dimension of sentence’s one-hot vector\n",
    "EMBEDDING_DIM = 20 # to decide the dimension of Word2Vec\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def tagger(decoder_input_sentence):\n",
    "    bos = \"<BOS> \" # begin of sequence\n",
    "    eos = \" <EOS>\" # end of sequence\n",
    "    final_target = [bos + text + eos for text in decoder_input_sentence]\n",
    "    return final_target\n",
    "\n",
    "def vocab_creator(text_lists, VOCAB_SIZE):\n",
    "    '''create vocabulary and return wrd-id and id-wrd mappings'''\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "    tokenizer.fit_on_texts(text_lists)\n",
    "    dictionary = tokenizer.word_index\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    for k, v in dictionary.items():\n",
    "        if v < VOCAB_SIZE:\n",
    "            word2idx[k] = v\n",
    "            idx2word[v] = k\n",
    "        if v >= VOCAB_SIZE-1:\n",
    "            continue\n",
    "    return word2idx, idx2word\n",
    "\n",
    "def txt2seq(txt, vocab):\n",
    "    '''tokenize text and return list of tokens'''\n",
    "    res = []\n",
    "    for sent in txt: # txt is a list of sentences\n",
    "        res.append([vocab[0][wrd] for wrd in sent.split()])\n",
    "    return res\n",
    "\n",
    "def text2seq(encoder_text, decoder_text, vocab):\n",
    "    #tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "    #encoder_sequences = tokenizer.texts_to_sequences(encoder_text)\n",
    "    #decoder_sequences = tokenizer.texts_to_sequences(decoder_text)\n",
    "    encoder_sequences = txt2seq(encoder_text, vocab)\n",
    "    decoder_sequences = txt2seq(decoder_text, vocab)\n",
    "    return encoder_sequences, decoder_sequences\n",
    "\n",
    "def padding(encoder_sequences, decoder_sequences, MAX_LEN):\n",
    "    encoder_input_data = pad_sequences(encoder_sequences, maxlen=MAX_LEN, dtype='int32', padding='post', truncating='post')\n",
    "    decoder_input_data = pad_sequences(decoder_sequences, maxlen=MAX_LEN, dtype='int32', padding='post', truncating='post')\n",
    "    return encoder_input_data, decoder_input_data\n",
    "\n",
    "# GLOVE_DIR = path for glove.6B.100d.txt\n",
    "def glove_100d_dictionary(GLOVE_DIR):\n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join(GLOVE_DIR, r'glove.6B.100d.txt'), encoding=\"utf-8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index\n",
    "\n",
    "# this time: embedding_dimension = 100d\n",
    "def embedding_matrix_creator(word_index, embeddings_index, embedding_dimension):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dimension))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "# MAX_LEN = 20\n",
    "# num_samples = len(encoder_sequences)\n",
    "# VOCAB_SIZE = 15000\n",
    "\n",
    "def decoder_output_creator(decoder_input_data, num_samples, MAX_LEN, VOCAB_SIZE):\n",
    "    '''reshape to (num_samples, MAX_LEN, VOCAB_SIZE)'''\n",
    "    print('vocab: ', VOCAB_SIZE)\n",
    "    decoder_output_data = np.zeros((num_samples, MAX_LEN, VOCAB_SIZE), dtype=\"float32\") # empty output array\n",
    "    for i, seqs in enumerate(decoder_input_data): # iterate over samples - i till num_samples\n",
    "        for j, seq in enumerate(seqs): # iterate over tokens in current sequence - j till MAX_LEN\n",
    "            if j > 0:\n",
    "                decoder_output_data[i][j][seq] = 1.\n",
    "    print(decoder_output_data.shape)\n",
    "    return decoder_output_data\n",
    "\n",
    "def data_splitter(encoder_input_data, decoder_input_data, test_size1=0.2, test_size2=0.3):\n",
    "    en_train, en_test, de_train, de_test = train_test_split(encoder_input_data, decoder_input_data, test_size=test_size1)\n",
    "    en_train, en_val, de_train, de_val = train_test_split(en_train, de_train, test_size=test_size2)\n",
    "    return en_train, en_val, en_test, de_train, de_val, de_test\n",
    "\n",
    "def embedding_layer_creator(VOCAB_SIZE, EMBEDDING_DIM, MAX_LEN, embedding_matrix):\n",
    "    embedding_layer = Embedding(input_dim = VOCAB_SIZE, \n",
    "                              output_dim = EMBEDDING_DIM,\n",
    "                              input_length = MAX_LEN,\n",
    "                              weights = [embedding_matrix],\n",
    "                              trainable = False)\n",
    "    return embedding_layer\n",
    "\n",
    "def seq2seq_model_builder(encoder_data, decoder_data, MAX_LEN, VOCAB_SIZE, HIDDEN_DIM=300):\n",
    "    \n",
    "    '''\n",
    "encoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n",
    "encoder_embedding = embed_layer(encoder_inputs)\n",
    "encoder_LSTM = LSTM(HIDDEN_DIM, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
    "decoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n",
    "decoder_embedding = embed_layer(decoder_inputs)\n",
    "decoder_LSTM = LSTM(HIDDEN_DIM, return_state=True, return_sequences=True)\n",
    "decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=[state_h, state_c])\n",
    "# dense_layer = Dense(VOCAB_SIZE, activation='softmax')\n",
    "outputs = TimeDistributed(Dense(VOCAB_SIZE, activation='softmax'))(decoder_outputs)\n",
    "model = Model([encoder_inputs, decoder_inputs], outputs)'''\n",
    "    encoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n",
    "    # model.add(tf.keras.layers.Embedding(1000, 64, input_length=10))\n",
    "    #word_embedding=Embedding(input_dim=vocab_size,output_dim=8,input_length=maxlen)(word_input)\n",
    "    # https://www.kaggle.com/rajmehra03/a-detailed-explanation-of-keras-embedding-layer\n",
    "    encoder_embedding = Embedding(input_dim=VOCAB_SIZE, output_dim=64, input_length=MAX_LEN)(encoder_inputs)\n",
    "    # encoder_embedding = embed_layer(encoder_inputs)\n",
    "    encoder_LSTM = LSTM(HIDDEN_DIM, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_LSTM(encoder_data)\n",
    "    decoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n",
    "    #decoder_embedding = embed_layer(decoder_inputs)\n",
    "    decoder_embedding = Embedding(input_dim=VOCAB_SIZE, output_dim=64, input_length=MAX_LEN)(decoder_inputs)\n",
    "    decoder_LSTM = LSTM(HIDDEN_DIM, return_state=True, return_sequences=True)\n",
    "    decoder_outputs, _, _ = decoder_LSTM(decoder_data, initial_state=[state_h, state_c])\n",
    "    # dense_layer = Dense(VOCAB_SIZE, activation='softmax')\n",
    "    outputs = TimeDistributed(Dense(VOCAB_SIZE, activation='softmax'))(decoder_outputs)\n",
    "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "    return model\n",
    "\n",
    "'''print(clean_text('this is a test sentence'))\n",
    "print(tagger(['with the teeth of your zipper', 'so they tell me', 'so which dakota you from']))\n",
    "print(vocab_creator(['with the teeth of your zipper', 'so they tell me', 'so which dakota you from'], 30))\n",
    "enc_txt = ['with the teeth of your zipper', 'so they tell me', 'so which dakota you from']\n",
    "dec_txt = ['with the teeth of your zipper', 'so they tell me', 'so which dakota you from']\n",
    "my_vocab = vocab_creator(['with the teeth of your zipper', 'so they tell me', 'so which dakota you from'], 30)\n",
    "enc_seq, dec_seq = text2seq(enc_txt, dec_txt, my_vocab)\n",
    "print(enc_seq)\n",
    "enc_seq, dec_seq = padding(enc_seq, dec_seq, 15)\n",
    "print('enc/dec seq: ', enc_seq, dec_seq)\n",
    "#embd_idx = glove_100d_dictionary(r'D:\\BernieData\\DeepL\\data')\n",
    "#embd_mtrx = embedding_matrix_creator(my_vocab[0], embd_idx, 100)\n",
    "#print(embedding_layer_creator(len(my_vocab[0]), 100, 15, embd_mtrx))\n",
    "print('dec_seq:', dec_seq, np.shape(dec_seq))\n",
    "print(decoder_output_creator(dec_seq, 3, 15, len(my_vocab[0])+1))\n",
    "print(data_splitter(enc_seq, dec_seq))'''\n",
    "\n",
    "encoder_text = ['with the teeth of your zipper', 'so they tell me', 'so which dakota you from']\n",
    "decoder_text = ['with the teeth of your zipper', 'so they tell me', 'so which dakota you from']\n",
    "my_vocab = vocab_creator(text_lists=encoder_text+decoder_text, VOCAB_SIZE=14999)\n",
    "VOCAB_SIZE = len(my_vocab[0])\n",
    "num_samples = 15\n",
    "MAX_LEN = 15\n",
    "encoder_sequences, decoder_sequences = text2seq(encoder_text, decoder_text, my_vocab)\n",
    "encoder_input_data, decoder_input_data = padding(encoder_sequences, decoder_sequences, MAX_LEN) \n",
    "#decoder_inputs = tagger(decoder_input_data)\n",
    "decoder_output_data = decoder_output_creator(decoder_input_data, num_samples, MAX_LEN, VOCAB_SIZE+1)\n",
    "en_train, en_val, en_test, de_train, de_val, de_test = data_splitter(encoder_input_data, decoder_input_data)\n",
    "\n",
    "#embd_idx = glove_100d_dictionary(r'D:\\BernieData\\DeepL\\data')\n",
    "#embd_mtrx = embedding_matrix_creator(my_vocab[0], embd_idx, 100)\n",
    "#embd_lyr = embedding_layer_creator(len(my_vocab[0]), 100, 15, embd_mtrx)\n",
    "\n",
    "model = seq2seq_model_builder(encoder_input_data, decoder_input_data, MAX_LEN, VOCAB_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your number (1-3):3\n",
      "human total:  3\n",
      "minimaxd:  3 [4, 5, 6]  options:  [1, 1, 1]\n",
      "succ:  [4, 5, 6]  choose  0\n",
      "computer total:  4\n",
      "Enter your number (1-3):3\n",
      "human total:  7\n",
      "minimaxd:  7 [8, 9, 10]  options:  [1, 1, 1]\n",
      "succ:  [8, 9, 10]  choose  0\n",
      "computer total:  8\n",
      "Enter your number (1-3):3\n",
      "human total:  11\n",
      "minimaxd:  11 [12, 13, 14]  options:  [1, 1, 1]\n",
      "succ:  [12, 13, 14]  choose  0\n",
      "computer total:  12\n",
      "Enter your number (1-3):3\n",
      "human total:  15\n",
      "minimaxd:  15 [16, 17, 18]  options:  [1, 1, 1]\n",
      "succ:  [16, 17, 18]  choose  0\n",
      "computer total:  16\n",
      "Enter your number (1-3):3\n",
      "human total:  19\n",
      "minimaxd:  19 [20, 21]  options:  [1, 1]\n",
      "succ:  [20, 21]  choose  0\n",
      "computer total:  20\n",
      "Enter your number (1-3):3\n",
      "human total:  23\n",
      "you lose!\n"
     ]
    }
   ],
   "source": [
    "# 21 game\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class game21:\n",
    "    def __init__(self):\n",
    "        self.prt = 0\n",
    "    \n",
    "    def hummv(self):\n",
    "        mvv = -1\n",
    "        while mvv > 3 or mvv < 1:\n",
    "            mvv = int(input('Enter your number (1-3):'))\n",
    "        self.prt += mvv\n",
    "        print('human total: ', self.prt)\n",
    "        if self.prt == 21:\n",
    "            print('you win!')\n",
    "            return 0\n",
    "        if self.prt > 21:\n",
    "            print('you lose!')\n",
    "            return 1\n",
    "        return 2\n",
    "    \n",
    "    def cmpmv(self):\n",
    "        #print(self.minimaxd(self.prt))\n",
    "        chsmv = self.minimaxd(self.prt)\n",
    "        print('succ: ', self.successors(self.prt), ' choose ', chsmv)\n",
    "        self.prt = self.successors(self.prt)[chsmv]\n",
    "        print('computer total: ', self.prt)\n",
    "        if self.prt == 21:\n",
    "            print('computer wins!')\n",
    "            return 1\n",
    "        if self.prt > 21:\n",
    "            print('computer loses!')\n",
    "            return 0\n",
    "        return 2\n",
    "    \n",
    "    def playgm(self):\n",
    "        while self.prt < 21:\n",
    "            if self.hummv() < 2:\n",
    "                return\n",
    "            self.cmpmv()\n",
    "        return\n",
    "    \n",
    "    def util(self, state):\n",
    "        # todo: improve, add forward-looking?\n",
    "        #print('util: ', state)\n",
    "        if state >= 21: # terminal state\n",
    "            return 1\n",
    "        if state > 14 and state < 18:\n",
    "            return -5\n",
    "        return -1\n",
    "    \n",
    "    def successors(self, state): # for 21 the state is just the running total\n",
    "        if state >= 21:\n",
    "            return []\n",
    "        if state >= 20:\n",
    "            return [state+1] # 1 possible move\n",
    "        if state >= 19:\n",
    "            return [state+1, state+2] # 2 possible moves\n",
    "        return [state+1, state+2, state+3] # 3 possible moves\n",
    "    \n",
    "    def minimaxd(self, state):\n",
    "        opts = [self.maxv(sx) for sx in self.successors(state)]\n",
    "        print('minimaxd: ', state, self.successors(state), ' options: ', opts)\n",
    "        return np.argmax(opts) # return index af max value of successor states\n",
    "    \n",
    "    def maxv(self, state):\n",
    "        #print('max: ', state, self.successors(state))\n",
    "        if state >= 21: # terminal state\n",
    "            return self.util(state)\n",
    "        v = -100\n",
    "        for s in self.successors(state):\n",
    "            v = max(v, self.minv(s))\n",
    "        return v\n",
    "    \n",
    "    def minv(self, state):\n",
    "        #print('min: ', state, self.successors(state))\n",
    "        if state >= 21: # terminal state\n",
    "            return self.util(state)\n",
    "        v = 100\n",
    "        for s in self.successors(state):\n",
    "            v = min(v, self.maxv(s))\n",
    "        return v\n",
    "    \n",
    "    '''function minimax(node, depth, maximizingPlayer) is\n",
    "    if depth = 0 or node is a terminal node then\n",
    "        return the heuristic value of node\n",
    "    if maximizingPlayer then\n",
    "        value := −∞\n",
    "        for each child of node do\n",
    "            value := max(value, minimax(child, depth − 1, FALSE))\n",
    "        return value\n",
    "    else: # minimizing player\n",
    "        value := +∞\n",
    "        for each child of node do\n",
    "            value := min(value, minimax(child, depth − 1, TRUE))\n",
    "        return value'''\n",
    "    \n",
    "def main():\n",
    "    myg = game21()\n",
    "    myg.playgm()\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The encoding for document 1  is :  [38, 32, 39, 39, 14, 4]\n",
      "The encoding for document 2  is :  [30, 33, 39, 14, 4, 26, 39, 39, 39]\n",
      "The encoding for document 3  is :  [46, 14, 32, 28, 28, 4, 9, 43, 33, 39, 4, 28]\n",
      "The maximum number of words in any document is :  12\n",
      "No of padded documents:  3\n",
      "The padded encoding for document 1  is :  [38 32 39 39 14  4  0  0  0  0  0  0]\n",
      "The padded encoding for document 2  is :  [30 33 39 14  4 26 39 39 39  0  0  0]\n",
      "The padded encoding for document 3  is :  [46 14 32 28 28  4  9 43 33 39  4 28]\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Tensor(\"embedding_2/embedding_lookup/Identity_1:0\", shape=(None, 12, 8), dtype=float32)\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 12, 8)             400       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 96)                0         \n",
      "=================================================================\n",
      "Total params: 400\n",
      "Trainable params: 400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Shape of embeddings :  (3, 96)\n",
      "[[-0.02940606  0.03775641 -0.02129102  0.03247986 -0.04418912  0.03414216\n",
      "   0.01620764 -0.04587703 -0.03285513  0.01239655  0.0412879   0.03013508\n",
      "   0.01079828 -0.03655319 -0.03117274  0.03592993 -0.03029039 -0.03529142\n",
      "   0.04273793  0.01146447  0.01766877 -0.01363594  0.03433086  0.047592\n",
      "  -0.03029039 -0.03529142  0.04273793  0.01146447  0.01766877 -0.01363594\n",
      "   0.03433086  0.047592    0.01963654  0.02051986 -0.03246886  0.01974801\n",
      "   0.04948987 -0.01755797 -0.04440173 -0.02888116 -0.03558642 -0.04937457\n",
      "  -0.00914391 -0.02373471  0.00326438  0.01022416  0.04239    -0.03781295\n",
      "   0.00329641 -0.04572392  0.0065503   0.02232695 -0.02385912 -0.02687277\n",
      "  -0.04370102  0.01858504  0.00329641 -0.04572392  0.0065503   0.02232695\n",
      "  -0.02385912 -0.02687277 -0.04370102  0.01858504  0.00329641 -0.04572392\n",
      "   0.0065503   0.02232695 -0.02385912 -0.02687277 -0.04370102  0.01858504\n",
      "   0.00329641 -0.04572392  0.0065503   0.02232695 -0.02385912 -0.02687277\n",
      "  -0.04370102  0.01858504  0.00329641 -0.04572392  0.0065503   0.02232695\n",
      "  -0.02385912 -0.02687277 -0.04370102  0.01858504  0.00329641 -0.04572392\n",
      "   0.0065503   0.02232695 -0.02385912 -0.02687277 -0.04370102  0.01858504]\n",
      " [ 0.00580492 -0.017819    0.02932156  0.04237683  0.04979769 -0.02545656\n",
      "  -0.00973173  0.0150224  -0.00205542 -0.04720693 -0.01995349 -0.02324807\n",
      "  -0.02691364 -0.02507487 -0.00135463  0.0218794  -0.03029039 -0.03529142\n",
      "   0.04273793  0.01146447  0.01766877 -0.01363594  0.03433086  0.047592\n",
      "   0.01963654  0.02051986 -0.03246886  0.01974801  0.04948987 -0.01755797\n",
      "  -0.04440173 -0.02888116 -0.03558642 -0.04937457 -0.00914391 -0.02373471\n",
      "   0.00326438  0.01022416  0.04239    -0.03781295  0.02582625  0.04187789\n",
      "  -0.04487376 -0.04955878  0.0134261  -0.02702123 -0.02389525  0.00551695\n",
      "  -0.03029039 -0.03529142  0.04273793  0.01146447  0.01766877 -0.01363594\n",
      "   0.03433086  0.047592   -0.03029039 -0.03529142  0.04273793  0.01146447\n",
      "   0.01766877 -0.01363594  0.03433086  0.047592   -0.03029039 -0.03529142\n",
      "   0.04273793  0.01146447  0.01766877 -0.01363594  0.03433086  0.047592\n",
      "   0.00329641 -0.04572392  0.0065503   0.02232695 -0.02385912 -0.02687277\n",
      "  -0.04370102  0.01858504  0.00329641 -0.04572392  0.0065503   0.02232695\n",
      "  -0.02385912 -0.02687277 -0.04370102  0.01858504  0.00329641 -0.04572392\n",
      "   0.0065503   0.02232695 -0.02385912 -0.02687277 -0.04370102  0.01858504]\n",
      " [ 0.03681698  0.03307177  0.02305329 -0.04492823  0.02106047  0.02266976\n",
      "  -0.04250349 -0.00875502  0.01963654  0.02051986 -0.03246886  0.01974801\n",
      "   0.04948987 -0.01755797 -0.04440173 -0.02888116 -0.03285513  0.01239655\n",
      "   0.0412879   0.03013508  0.01079828 -0.03655319 -0.03117274  0.03592993\n",
      "   0.02902381 -0.03712679 -0.01160096 -0.02790076 -0.04646533  0.01276599\n",
      "   0.02075806  0.02431918  0.02902381 -0.03712679 -0.01160096 -0.02790076\n",
      "  -0.04646533  0.01276599  0.02075806  0.02431918 -0.03558642 -0.04937457\n",
      "  -0.00914391 -0.02373471  0.00326438  0.01022416  0.04239    -0.03781295\n",
      "  -0.0343943  -0.04926351  0.00598221 -0.01112943  0.01959164  0.00377821\n",
      "   0.02484058  0.0159026  -0.01705428  0.02762533 -0.02777326  0.04994107\n",
      "  -0.00249156 -0.0190267  -0.04333202 -0.04845415 -0.00205542 -0.04720693\n",
      "  -0.01995349 -0.02324807 -0.02691364 -0.02507487 -0.00135463  0.0218794\n",
      "  -0.03029039 -0.03529142  0.04273793  0.01146447  0.01766877 -0.01363594\n",
      "   0.03433086  0.047592   -0.03558642 -0.04937457 -0.00914391 -0.02373471\n",
      "   0.00326438  0.01022416  0.04239    -0.03781295  0.02902381 -0.03712679\n",
      "  -0.01160096 -0.02790076 -0.04646533  0.01276599  0.02075806  0.02431918]]\n",
      "Shape of embeddings :  (3, 12, 8)\n",
      "[[[-0.02940606  0.03775641 -0.02129102  0.03247986 -0.04418912\n",
      "    0.03414216  0.01620764 -0.04587703]\n",
      "  [-0.03285513  0.01239655  0.0412879   0.03013508  0.01079828\n",
      "   -0.03655319 -0.03117274  0.03592993]\n",
      "  [-0.03029039 -0.03529142  0.04273793  0.01146447  0.01766877\n",
      "   -0.01363594  0.03433086  0.047592  ]\n",
      "  [-0.03029039 -0.03529142  0.04273793  0.01146447  0.01766877\n",
      "   -0.01363594  0.03433086  0.047592  ]\n",
      "  [ 0.01963654  0.02051986 -0.03246886  0.01974801  0.04948987\n",
      "   -0.01755797 -0.04440173 -0.02888116]\n",
      "  [-0.03558642 -0.04937457 -0.00914391 -0.02373471  0.00326438\n",
      "    0.01022416  0.04239    -0.03781295]\n",
      "  [ 0.00329641 -0.04572392  0.0065503   0.02232695 -0.02385912\n",
      "   -0.02687277 -0.04370102  0.01858504]\n",
      "  [ 0.00329641 -0.04572392  0.0065503   0.02232695 -0.02385912\n",
      "   -0.02687277 -0.04370102  0.01858504]\n",
      "  [ 0.00329641 -0.04572392  0.0065503   0.02232695 -0.02385912\n",
      "   -0.02687277 -0.04370102  0.01858504]\n",
      "  [ 0.00329641 -0.04572392  0.0065503   0.02232695 -0.02385912\n",
      "   -0.02687277 -0.04370102  0.01858504]\n",
      "  [ 0.00329641 -0.04572392  0.0065503   0.02232695 -0.02385912\n",
      "   -0.02687277 -0.04370102  0.01858504]\n",
      "  [ 0.00329641 -0.04572392  0.0065503   0.02232695 -0.02385912\n",
      "   -0.02687277 -0.04370102  0.01858504]]\n",
      "\n",
      " [[ 0.00580492 -0.017819    0.02932156  0.04237683  0.04979769\n",
      "   -0.02545656 -0.00973173  0.0150224 ]\n",
      "  [-0.00205542 -0.04720693 -0.01995349 -0.02324807 -0.02691364\n",
      "   -0.02507487 -0.00135463  0.0218794 ]\n",
      "  [-0.03029039 -0.03529142  0.04273793  0.01146447  0.01766877\n",
      "   -0.01363594  0.03433086  0.047592  ]\n",
      "  [ 0.01963654  0.02051986 -0.03246886  0.01974801  0.04948987\n",
      "   -0.01755797 -0.04440173 -0.02888116]\n",
      "  [-0.03558642 -0.04937457 -0.00914391 -0.02373471  0.00326438\n",
      "    0.01022416  0.04239    -0.03781295]\n",
      "  [ 0.02582625  0.04187789 -0.04487376 -0.04955878  0.0134261\n",
      "   -0.02702123 -0.02389525  0.00551695]\n",
      "  [-0.03029039 -0.03529142  0.04273793  0.01146447  0.01766877\n",
      "   -0.01363594  0.03433086  0.047592  ]\n",
      "  [-0.03029039 -0.03529142  0.04273793  0.01146447  0.01766877\n",
      "   -0.01363594  0.03433086  0.047592  ]\n",
      "  [-0.03029039 -0.03529142  0.04273793  0.01146447  0.01766877\n",
      "   -0.01363594  0.03433086  0.047592  ]\n",
      "  [ 0.00329641 -0.04572392  0.0065503   0.02232695 -0.02385912\n",
      "   -0.02687277 -0.04370102  0.01858504]\n",
      "  [ 0.00329641 -0.04572392  0.0065503   0.02232695 -0.02385912\n",
      "   -0.02687277 -0.04370102  0.01858504]\n",
      "  [ 0.00329641 -0.04572392  0.0065503   0.02232695 -0.02385912\n",
      "   -0.02687277 -0.04370102  0.01858504]]\n",
      "\n",
      " [[ 0.03681698  0.03307177  0.02305329 -0.04492823  0.02106047\n",
      "    0.02266976 -0.04250349 -0.00875502]\n",
      "  [ 0.01963654  0.02051986 -0.03246886  0.01974801  0.04948987\n",
      "   -0.01755797 -0.04440173 -0.02888116]\n",
      "  [-0.03285513  0.01239655  0.0412879   0.03013508  0.01079828\n",
      "   -0.03655319 -0.03117274  0.03592993]\n",
      "  [ 0.02902381 -0.03712679 -0.01160096 -0.02790076 -0.04646533\n",
      "    0.01276599  0.02075806  0.02431918]\n",
      "  [ 0.02902381 -0.03712679 -0.01160096 -0.02790076 -0.04646533\n",
      "    0.01276599  0.02075806  0.02431918]\n",
      "  [-0.03558642 -0.04937457 -0.00914391 -0.02373471  0.00326438\n",
      "    0.01022416  0.04239    -0.03781295]\n",
      "  [-0.0343943  -0.04926351  0.00598221 -0.01112943  0.01959164\n",
      "    0.00377821  0.02484058  0.0159026 ]\n",
      "  [-0.01705428  0.02762533 -0.02777326  0.04994107 -0.00249156\n",
      "   -0.0190267  -0.04333202 -0.04845415]\n",
      "  [-0.00205542 -0.04720693 -0.01995349 -0.02324807 -0.02691364\n",
      "   -0.02507487 -0.00135463  0.0218794 ]\n",
      "  [-0.03029039 -0.03529142  0.04273793  0.01146447  0.01766877\n",
      "   -0.01363594  0.03433086  0.047592  ]\n",
      "  [-0.03558642 -0.04937457 -0.00914391 -0.02373471  0.00326438\n",
      "    0.01022416  0.04239    -0.03781295]\n",
      "  [ 0.02902381 -0.03712679 -0.01160096 -0.02790076 -0.04646533\n",
      "    0.01276599  0.02075806  0.02431918]]]\n",
      "The encoding for  1 th word in 1 th document is : \n",
      "\n",
      " [-0.02940606  0.03775641 -0.02129102  0.03247986 -0.04418912  0.03414216\n",
      "  0.01620764 -0.04587703]\n",
      "The encoding for  2 th word in 1 th document is : \n",
      "\n",
      " [-0.03285513  0.01239655  0.0412879   0.03013508  0.01079828 -0.03655319\n",
      " -0.03117274  0.03592993]\n",
      "The encoding for  3 th word in 1 th document is : \n",
      "\n",
      " [-0.03029039 -0.03529142  0.04273793  0.01146447  0.01766877 -0.01363594\n",
      "  0.03433086  0.047592  ]\n",
      "The encoding for  4 th word in 1 th document is : \n",
      "\n",
      " [-0.03029039 -0.03529142  0.04273793  0.01146447  0.01766877 -0.01363594\n",
      "  0.03433086  0.047592  ]\n",
      "The encoding for  5 th word in 1 th document is : \n",
      "\n",
      " [ 0.01963654  0.02051986 -0.03246886  0.01974801  0.04948987 -0.01755797\n",
      " -0.04440173 -0.02888116]\n",
      "The encoding for  6 th word in 1 th document is : \n",
      "\n",
      " [-0.03558642 -0.04937457 -0.00914391 -0.02373471  0.00326438  0.01022416\n",
      "  0.04239    -0.03781295]\n",
      "The encoding for  7 th word in 1 th document is : \n",
      "\n",
      " [ 0.00329641 -0.04572392  0.0065503   0.02232695 -0.02385912 -0.02687277\n",
      " -0.04370102  0.01858504]\n",
      "The encoding for  8 th word in 1 th document is : \n",
      "\n",
      " [ 0.00329641 -0.04572392  0.0065503   0.02232695 -0.02385912 -0.02687277\n",
      " -0.04370102  0.01858504]\n",
      "The encoding for  9 th word in 1 th document is : \n",
      "\n",
      " [ 0.00329641 -0.04572392  0.0065503   0.02232695 -0.02385912 -0.02687277\n",
      " -0.04370102  0.01858504]\n",
      "The encoding for  10 th word in 1 th document is : \n",
      "\n",
      " [ 0.00329641 -0.04572392  0.0065503   0.02232695 -0.02385912 -0.02687277\n",
      " -0.04370102  0.01858504]\n",
      "The encoding for  11 th word in 1 th document is : \n",
      "\n",
      " [ 0.00329641 -0.04572392  0.0065503   0.02232695 -0.02385912 -0.02687277\n",
      " -0.04370102  0.01858504]\n",
      "The encoding for  12 th word in 1 th document is : \n",
      "\n",
      " [ 0.00329641 -0.04572392  0.0065503   0.02232695 -0.02385912 -0.02687277\n",
      " -0.04370102  0.01858504]\n",
      "The encoding for  1 th word in 2 th document is : \n",
      "\n",
      " [ 0.00580492 -0.017819    0.02932156  0.04237683  0.04979769 -0.02545656\n",
      " -0.00973173  0.0150224 ]\n",
      "The encoding for  2 th word in 2 th document is : \n",
      "\n",
      " [-0.00205542 -0.04720693 -0.01995349 -0.02324807 -0.02691364 -0.02507487\n",
      " -0.00135463  0.0218794 ]\n",
      "The encoding for  3 th word in 2 th document is : \n",
      "\n",
      " [-0.03029039 -0.03529142  0.04273793  0.01146447  0.01766877 -0.01363594\n",
      "  0.03433086  0.047592  ]\n",
      "The encoding for  4 th word in 2 th document is : \n",
      "\n",
      " [ 0.01963654  0.02051986 -0.03246886  0.01974801  0.04948987 -0.01755797\n",
      " -0.04440173 -0.02888116]\n",
      "The encoding for  5 th word in 2 th document is : \n",
      "\n",
      " [-0.03558642 -0.04937457 -0.00914391 -0.02373471  0.00326438  0.01022416\n",
      "  0.04239    -0.03781295]\n",
      "The encoding for  6 th word in 2 th document is : \n",
      "\n",
      " [ 0.02582625  0.04187789 -0.04487376 -0.04955878  0.0134261  -0.02702123\n",
      " -0.02389525  0.00551695]\n",
      "The encoding for  7 th word in 2 th document is : \n",
      "\n",
      " [-0.03029039 -0.03529142  0.04273793  0.01146447  0.01766877 -0.01363594\n",
      "  0.03433086  0.047592  ]\n",
      "The encoding for  8 th word in 2 th document is : \n",
      "\n",
      " [-0.03029039 -0.03529142  0.04273793  0.01146447  0.01766877 -0.01363594\n",
      "  0.03433086  0.047592  ]\n",
      "The encoding for  9 th word in 2 th document is : \n",
      "\n",
      " [-0.03029039 -0.03529142  0.04273793  0.01146447  0.01766877 -0.01363594\n",
      "  0.03433086  0.047592  ]\n",
      "The encoding for  10 th word in 2 th document is : \n",
      "\n",
      " [ 0.00329641 -0.04572392  0.0065503   0.02232695 -0.02385912 -0.02687277\n",
      " -0.04370102  0.01858504]\n",
      "The encoding for  11 th word in 2 th document is : \n",
      "\n",
      " [ 0.00329641 -0.04572392  0.0065503   0.02232695 -0.02385912 -0.02687277\n",
      " -0.04370102  0.01858504]\n",
      "The encoding for  12 th word in 2 th document is : \n",
      "\n",
      " [ 0.00329641 -0.04572392  0.0065503   0.02232695 -0.02385912 -0.02687277\n",
      " -0.04370102  0.01858504]\n",
      "The encoding for  1 th word in 3 th document is : \n",
      "\n",
      " [ 0.03681698  0.03307177  0.02305329 -0.04492823  0.02106047  0.02266976\n",
      " -0.04250349 -0.00875502]\n",
      "The encoding for  2 th word in 3 th document is : \n",
      "\n",
      " [ 0.01963654  0.02051986 -0.03246886  0.01974801  0.04948987 -0.01755797\n",
      " -0.04440173 -0.02888116]\n",
      "The encoding for  3 th word in 3 th document is : \n",
      "\n",
      " [-0.03285513  0.01239655  0.0412879   0.03013508  0.01079828 -0.03655319\n",
      " -0.03117274  0.03592993]\n",
      "The encoding for  4 th word in 3 th document is : \n",
      "\n",
      " [ 0.02902381 -0.03712679 -0.01160096 -0.02790076 -0.04646533  0.01276599\n",
      "  0.02075806  0.02431918]\n",
      "The encoding for  5 th word in 3 th document is : \n",
      "\n",
      " [ 0.02902381 -0.03712679 -0.01160096 -0.02790076 -0.04646533  0.01276599\n",
      "  0.02075806  0.02431918]\n",
      "The encoding for  6 th word in 3 th document is : \n",
      "\n",
      " [-0.03558642 -0.04937457 -0.00914391 -0.02373471  0.00326438  0.01022416\n",
      "  0.04239    -0.03781295]\n",
      "The encoding for  7 th word in 3 th document is : \n",
      "\n",
      " [-0.0343943  -0.04926351  0.00598221 -0.01112943  0.01959164  0.00377821\n",
      "  0.02484058  0.0159026 ]\n",
      "The encoding for  8 th word in 3 th document is : \n",
      "\n",
      " [-0.01705428  0.02762533 -0.02777326  0.04994107 -0.00249156 -0.0190267\n",
      " -0.04333202 -0.04845415]\n",
      "The encoding for  9 th word in 3 th document is : \n",
      "\n",
      " [-0.00205542 -0.04720693 -0.01995349 -0.02324807 -0.02691364 -0.02507487\n",
      " -0.00135463  0.0218794 ]\n",
      "The encoding for  10 th word in 3 th document is : \n",
      "\n",
      " [-0.03029039 -0.03529142  0.04273793  0.01146447  0.01766877 -0.01363594\n",
      "  0.03433086  0.047592  ]\n",
      "The encoding for  11 th word in 3 th document is : \n",
      "\n",
      " [-0.03558642 -0.04937457 -0.00914391 -0.02373471  0.00326438  0.01022416\n",
      "  0.04239    -0.03781295]\n",
      "The encoding for  12 th word in 3 th document is : \n",
      "\n",
      " [ 0.02902381 -0.03712679 -0.01160096 -0.02790076 -0.04646533  0.01276599\n",
      "  0.02075806  0.02431918]\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/rajmehra03/a-detailed-explanation-of-keras-embedding-layer\n",
    "\n",
    "# Ignore  the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "%matplotlib inline  \n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "\n",
    "#stop-words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# tokenizing\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot,Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Flatten ,Embedding,Input\n",
    "from keras.models import Model\n",
    "\n",
    "sample_text_1=\"bitty bought a bit of butter\"\n",
    "sample_text_2=\"but the bit of butter was a bit bitter\"\n",
    "sample_text_3=\"so she bought some better butter to make the bitter butter better\"\n",
    "\n",
    "corp=[sample_text_1,sample_text_2,sample_text_3]\n",
    "no_docs=len(corp)\n",
    "\n",
    "vocab_size=50 \n",
    "encod_corp=[]\n",
    "for i,doc in enumerate(corp):\n",
    "    encod_corp.append(one_hot(doc,50))\n",
    "    print(\"The encoding for document\",i+1,\" is : \",one_hot(doc,50))\n",
    "    \n",
    "# length of maximum document. will be nedded whenever create embeddings for the words\n",
    "maxlen=-1\n",
    "for doc in corp:\n",
    "    tokens=nltk.word_tokenize(doc)\n",
    "    if(maxlen<len(tokens)):\n",
    "        maxlen=len(tokens)\n",
    "print(\"The maximum number of words in any document is : \",maxlen)\n",
    "\n",
    "# now to create embeddings all of our docs need to be of same length. hence we can pad the docs with zeros.\n",
    "pad_corp=pad_sequences(encod_corp,maxlen=maxlen,padding='post',value=0.0)\n",
    "print(\"No of padded documents: \",len(pad_corp))\n",
    "\n",
    "for i,doc in enumerate(pad_corp):\n",
    "     print(\"The padded encoding for document\",i+1,\" is : \",doc)\n",
    "        \n",
    "# specifying the input shape\n",
    "input=Input(shape=(no_docs,maxlen),dtype='float64')\n",
    "\n",
    "'''\n",
    "shape of input. \n",
    "each document has 12 element or words which is the value of our maxlen variable.\n",
    "\n",
    "'''\n",
    "word_input=Input(shape=(maxlen,),dtype='float64')  \n",
    "\n",
    "# creating the embedding\n",
    "word_embedding=Embedding(input_dim=vocab_size,output_dim=8,input_length=maxlen)(word_input)\n",
    "\n",
    "word_vec=Flatten()(word_embedding) # flatten\n",
    "embed_model =Model([word_input],word_vec) # combining all into a Keras model\n",
    "\n",
    "embed_model.compile(optimizer=keras.optimizers.Adam(lr=1e-3),loss='binary_crossentropy',metrics=['acc']) \n",
    "# compiling the model. parameters can be tuned as always.\n",
    "\n",
    "print(type(word_embedding))\n",
    "print(word_embedding)\n",
    "\n",
    "print(embed_model.summary()) # summary of the model\n",
    "\n",
    "embeddings=embed_model.predict(pad_corp) # finally getting the embeddings.\n",
    "\n",
    "print(\"Shape of embeddings : \",embeddings.shape)\n",
    "print(embeddings)\n",
    "\n",
    "embeddings=embeddings.reshape(-1,maxlen,8)\n",
    "print(\"Shape of embeddings : \",embeddings.shape) \n",
    "print(embeddings)\n",
    "\n",
    "for i,doc in enumerate(embeddings):\n",
    "    for j,word in enumerate(doc):\n",
    "        print(\"The encoding for \",j+1,\"th word\",\"in\",i+1,\"th document is : \\n\\n\",word)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The encoding for document 1  is :  [23, 39, 14, 42, 11, 7]\n",
      "The encoding for document 2  is :  [8, 20, 42, 11, 7, 45, 14, 42, 4]\n",
      "The encoding for document 3  is :  [44, 49, 39, 34, 11, 7, 5, 32, 20, 4, 7, 11]\n",
      "The maximum number of words in any document is :  12\n",
      "No of padded documents:  3\n",
      "The padded encoding for document 1  is :  [23 39 14 42 11  7  0  0  0  0  0  0]\n",
      "The padded encoding for document 2  is :  [ 8 20 42 11  7 45 14 42  4  0  0  0]\n",
      "The padded encoding for document 3  is :  [44 49 39 34 11  7  5 32 20  4  7 11]\n",
      "vocab_size:  50\n",
      "maxlen:  12\n",
      "encoder_inputs:  (None, 12)\n",
      "encoder_embedding:  (None, 12, 8)\n",
      "encoder_outputs:  (None, 3)\n",
      "state_h:  (None, 3)\n",
      "state_c:  (None, 3)\n",
      "decoder_inputs:  (None, 12)\n",
      "decoder_embedding:  (None, 12, 8)\n",
      "dec_out:  (None, 12, 3)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 12)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 12)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 12, 8)        400         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 12, 8)        400         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 3), (None, 3 144         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 12, 3), (Non 144         embedding_2[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 12, 50)       200         lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,288\n",
      "Trainable params: 1,288\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-190f512d3c1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_inputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"seq2seq_t0.png\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_layer_names, rankdir, expand_nested, dpi)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \"\"\"\n\u001b[0;32m    239\u001b[0m     dot = model_to_dot(model, show_shapes, show_layer_names, rankdir,\n\u001b[1;32m--> 240\u001b[1;33m                        expand_nested, dpi)\n\u001b[0m\u001b[0;32m    241\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names, rankdir, expand_nested, dpi, subgraph)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msubgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dashed'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         raise ImportError(\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[1;34m'Failed to import `pydot`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[1;34m'Please install `pydot`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[1;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "# attempt to straighten out the issues with seq2seq\n",
    "# Keras API: https://keras.io/guides/functional_api/\n",
    "\n",
    "# Ignore  the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import re\n",
    "\n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "%matplotlib inline  \n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "\n",
    "#stop-words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# tokenizing\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot,Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "sample_text_1=\"bitty bought a bit of butter\"\n",
    "sample_text_2=\"but the bit of butter was a bit bitter\"\n",
    "sample_text_3=\"so she bought some better butter to make the bitter butter better\"\n",
    "\n",
    "corp = [sample_text_1, sample_text_2, sample_text_3] # corpus\n",
    "no_docs = len(corp) # number of documents in corpus\n",
    "\n",
    "vocab_size = 50 # size of vocabulary\n",
    "\n",
    "encod_corp = []\n",
    "for i, doc in enumerate(corp): # docs in corpus\n",
    "    encod_corp.append(one_hot(doc,50))\n",
    "    print(\"The encoding for document\", i+1, \" is : \", one_hot(doc,50))\n",
    "    \n",
    "# length of maximum document. will be nedded whenever create embeddings for the words\n",
    "maxlen = -1\n",
    "for doc in corp: # docs in corpus\n",
    "    tokens = nltk.word_tokenize(doc) # tokenize document\n",
    "    if(maxlen < len(tokens)):\n",
    "        maxlen = len(tokens)\n",
    "print(\"The maximum number of words in any document is : \", maxlen)\n",
    "\n",
    "# now to create embeddings all of our docs need to be of same length. hence we can pad the docs with zeros.\n",
    "pad_corp = pad_sequences(encod_corp, maxlen = maxlen, padding = 'post', value = 0.0)\n",
    "print(\"No of padded documents: \", len(pad_corp))\n",
    "\n",
    "for i, doc in enumerate(pad_corp):\n",
    "     print(\"The padded encoding for document\", i+1, \" is : \", doc)\n",
    "\n",
    "        \n",
    "print('vocab_size: ', vocab_size)\n",
    "print('maxlen: ', maxlen)\n",
    "\n",
    "# clear old model\n",
    "keras.backend.clear_session()\n",
    "        \n",
    "encoder_inputs = Input(shape=(maxlen, ), dtype='int32',)\n",
    "print('encoder_inputs: ', np.shape(encoder_inputs)) # (None, 12)\n",
    "#encoder_embedding = embed_layer(encoder_inputs)\n",
    "encoder_embedding = Embedding(input_dim=vocab_size, output_dim=8, input_length=maxlen)(encoder_inputs)\n",
    "print('encoder_embedding: ', np.shape(encoder_embedding)) # (None, 12, 8)\n",
    "HIDDEN_DIM = 3\n",
    "encoder_LSTM = LSTM(HIDDEN_DIM, return_state=True)\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
    "print('encoder_outputs: ', np.shape(encoder_outputs)) # (None, 3)\n",
    "print('state_h: ', np.shape(state_h)) # (None, 3)\n",
    "print('state_c: ', np.shape(state_c)) # (None, 3)\n",
    "decoder_inputs = Input(shape=(maxlen, ), dtype='int32',)\n",
    "print('decoder_inputs: ', np.shape(decoder_inputs)) # (None, 12)\n",
    "#decoder_embedding = embed_layer(decoder_inputs)\n",
    "decoder_embedding = Embedding(input_dim=vocab_size, output_dim=8, input_length=maxlen)(decoder_inputs)\n",
    "print('decoder_embedding: ', np.shape(decoder_embedding)) # (None, 12, 8)\n",
    "decoder_LSTM = LSTM(HIDDEN_DIM, return_state=True, return_sequences=True)\n",
    "\n",
    "decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=[state_h, state_c]) # no need to return hidden states\n",
    "print('dec_out: ', np.shape(decoder_outputs)) # (None, 12, 3)\n",
    "\n",
    "# dense_layer = Dense(VOCAB_SIZE, activation='softmax')\n",
    "outputs = TimeDistributed(Dense(vocab_size, activation='softmax'))(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "model.summary()\n",
    "#keras.utils.plot_model(model, \"seq2seq_t0.png\", show_shapes=True)\n",
    "\n",
    "print(outputs.shape)\n",
    "\n",
    "print('outputs: ', np.shape(outputs)) # (None, 12, 50)\n",
    "\n",
    "\n",
    "data_path = 'D:\\BernieData\\DeepL\\data'\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "#checkpointer = ModelCheckpoint(filepath=data_path + '/model-{epoch:02d}.hdf5', verbose=1)\n",
    "\n",
    "'''model.fit_generator(train_data_generator.generate(), len(train_data)//(batch_size*num_steps), num_epochs,\n",
    "                        validation_data=valid_data_generator.generate(),\n",
    "                        validation_steps=len(valid_data) // (batch_size*num_steps), callbacks=[checkpointer])'''\n",
    "\n",
    "# Actually training the model: (x=input data, y=target data)\n",
    "# see https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
    "model.fit([pad_corp, pad_corp], outputs, epochs=1)\n",
    "\n",
    "'''model = load_model(data_path + \"\\model-40.hdf5\")\n",
    "dummy_iters = 40\n",
    "example_training_generator = KerasBatchGenerator(train_data, num_steps, 1, vocabulary,\n",
    "                                                     skip_step=1)\n",
    "print(\"Training data:\")\n",
    "for i in range(dummy_iters):\n",
    "    dummy = next(example_training_generator.generate())\n",
    "num_predict = 10\n",
    "true_print_out = \"Actual words: \"\n",
    "pred_print_out = \"Predicted words: \"\n",
    "for i in range(num_predict):\n",
    "    data = next(example_training_generator.generate())\n",
    "    prediction = model.predict(data[0])\n",
    "    predict_word = np.argmax(prediction[:, num_steps-1, :])\n",
    "    true_print_out += reversed_dictionary[train_data[num_steps + dummy_iters + i]] + \" \"\n",
    "    pred_print_out += reversed_dictionary[predict_word] + \" \"\n",
    "print(true_print_out)\n",
    "print(pred_print_out)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 0.36481335759162903\n",
      "Step: 100 Loss: 0.12735670657441167\n",
      "Step: 200 Loss: 0.10050674894852425\n",
      "Step: 300 Loss: 0.09027601045825552\n",
      "Step: 400 Loss: 0.08520745511104044\n",
      "Step: 500 Loss: 0.0819876949765725\n",
      "Step: 600 Loss: 0.07947824447851212\n",
      "Step: 700 Loss: 0.07809196312123971\n",
      "Step: 800 Loss: 0.07683713382549501\n",
      "Step: 900 Loss: 0.07587328220363065\n",
      "Step: 1000 Loss: 0.07491447665653267\n",
      "Train for 1875 steps\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0714\n",
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "# https://keras.io/getting_started/intro_to_keras_for_researchers/\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=32, intermediate_dim=64, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=tf.nn.relu)\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z\n",
    "    \n",
    "class Decoder(layers.Layer):\n",
    "    \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dim, intermediate_dim=64, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=tf.nn.relu)\n",
    "        self.dense_output = layers.Dense(original_dim, activation=tf.nn.sigmoid)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)\n",
    "    \n",
    "class VariationalAutoEncoder(layers.Layer):\n",
    "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dim, intermediate_dim=64, latent_dim=32, **kwargs):\n",
    "        super(VariationalAutoEncoder, self).__init__(**kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # Add KL divergence regularization loss.\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
    "        )\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed\n",
    "    \n",
    "# Our model.\n",
    "vae = VariationalAutoEncoder(original_dim=784, intermediate_dim=64, latent_dim=32)\n",
    "\n",
    "# Loss and optimizer.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def training_step(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        reconstructed = vae(x)  # Compute input reconstruction.\n",
    "        # Compute loss.\n",
    "        loss = loss_fn(x, reconstructed)\n",
    "        loss += sum(vae.losses)  # Add KLD term.\n",
    "    # Update the weights of the VAE.\n",
    "    grads = tape.gradient(loss, vae.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "\n",
    "losses = []  # Keep track of the losses over time.\n",
    "for step, x in enumerate(dataset):\n",
    "    loss = training_step(x)\n",
    "    # Logging.\n",
    "    losses.append(float(loss))\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", sum(losses) / len(losses))\n",
    "\n",
    "    # Stop after 1000 steps.\n",
    "    # Training the model to convergence is left\n",
    "    # as an exercise to the reader.\n",
    "    if step >= 1000:\n",
    "        break\n",
    "        \n",
    "original_dim = 784\n",
    "intermediate_dim = 64\n",
    "latent_dim = 32\n",
    "\n",
    "# Define encoder model.\n",
    "original_inputs = tf.keras.Input(shape=(original_dim,), name=\"encoder_input\")\n",
    "x = layers.Dense(intermediate_dim, activation=\"relu\")(original_inputs)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()((z_mean, z_log_var))\n",
    "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name=\"encoder\")\n",
    "\n",
    "# Define decoder model.\n",
    "latent_inputs = tf.keras.Input(shape=(latent_dim,), name=\"z_sampling\")\n",
    "x = layers.Dense(intermediate_dim, activation=\"relu\")(latent_inputs)\n",
    "outputs = layers.Dense(original_dim, activation=\"sigmoid\")(x)\n",
    "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name=\"decoder\")\n",
    "\n",
    "# Define VAE model.\n",
    "outputs = decoder(z)\n",
    "vae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name=\"vae\")\n",
    "\n",
    "# Add KL divergence regularization loss.\n",
    "kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "vae.add_loss(kl_loss)\n",
    "\n",
    "# Loss and optimizer.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    ")\n",
    "dataset = dataset.map(lambda x: (x, x))  # Use x_train as both inputs & targets\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "# Configure the model for training.\n",
    "vae.compile(optimizer, loss=loss_fn)\n",
    "\n",
    "# Actually training the model.\n",
    "vae.fit(dataset, epochs=1)\n",
    "\n",
    "# predict\n",
    "predictions = vae.predict(dataset)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CenterCrop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4ac44c699c86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextVectorization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNormalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCenterCrop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRescaling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'CenterCrop'"
     ]
    }
   ],
   "source": [
    "# https://keras.io/getting_started/intro_to_keras_for_engineers/\n",
    "\n",
    "# setup\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import CenterCrop\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "dataset = keras.preprocessing.text_dataset_from_directory(\n",
    "  'path/to/main_directory', batch_size=64)\n",
    "\n",
    "# For demonstration, iterate over the batches yielded by the dataset.\n",
    "for data, labels in dataset:\n",
    "    print(data.shape)  # (64,)\n",
    "    print(data.dtype)  # string\n",
    "    print(labels.shape)  # (64,)\n",
    "    print(labels.dtype)  # int32\n",
    "\n",
    "# Example training data, of dtype `string`.\n",
    "training_data = np.array([[\"This is the 1st sample.\"], [\"And here's the 2nd sample.\"]])\n",
    "\n",
    "# Create a TextVectorization layer instance. It can be configured to either\n",
    "# return integer token indices, or a dense token representation (e.g. multi-hot\n",
    "# or TF-IDF). The text standardization and text splitting algorithms are fully\n",
    "# configurable.\n",
    "vectorizer = TextVectorization(output_mode=\"binary\", ngrams=2)\n",
    "\n",
    "# Calling `adapt` on an array or dataset makes the layer generate a vocabulary\n",
    "# index for the data, which can then be reused when seeing new data.\n",
    "vectorizer.adapt(training_data)\n",
    "\n",
    "# After calling adapt, the layer is able to encode any n-gram it has seen before\n",
    "# in the `adapt()` data. Unknown n-grams are encoded via an \"out-of-vocabulary\"\n",
    "# token.\n",
    "integer_data = vectorizer(training_data)\n",
    "print(integer_data)\n",
    "\n",
    "# Example image data, with values in the [0, 255] range\n",
    "training_data = np.random.randint(0, 256, size=(64, 200, 200, 3)).astype(\"float32\")\n",
    "\n",
    "normalizer = Normalization(axis=-1)\n",
    "normalizer.adapt(training_data)\n",
    "\n",
    "normalized_data = normalizer(training_data)\n",
    "print(\"var: %.4f\" % np.var(normalized_data))\n",
    "print(\"mean: %.4f\" % np.mean(normalized_data))\n",
    "\n",
    "# Example image data, with values in the [0, 255] range\n",
    "training_data = np.random.randint(0, 256, size=(64, 200, 200, 3)).astype(\"float32\")\n",
    "\n",
    "cropper = CenterCrop(height=150, width=150)\n",
    "scaler = Rescaling(scale=1.0 / 255)\n",
    "\n",
    "output_data = scaler(cropper(training_data))\n",
    "print(\"shape:\", output_data.shape)\n",
    "print(\"min:\", np.min(output_data))\n",
    "print(\"max:\", np.max(output_data))\n",
    "\n",
    "dense = keras.layers.Dense(units=16)\n",
    "\n",
    "# Let's say we expect our inputs to be RGB images of arbitrary size\n",
    "inputs = keras.Input(shape=(None, None, 3))\n",
    "\n",
    "# Center-crop images to 150x150\n",
    "x = CenterCrop(height=150, width=150)(inputs)\n",
    "# Rescale images to [0, 1]\n",
    "x = Rescaling(scale=1.0 / 255)(x)\n",
    "\n",
    "# Apply some convolution and pooling layers\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=(3, 3))(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=(3, 3))(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "\n",
    "# Apply global average pooling to get flat feature vectors\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add a dense classifier on top\n",
    "num_classes = 10\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "data = np.random.randint(0, 256, size=(64, 200, 200, 3)).astype(\"float32\")\n",
    "processed_data = model(data)\n",
    "print(processed_data.shape)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.CategoricalCrossentropy())\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit(numpy_array_of_samples, numpy_array_of_labels,\n",
    "          batch_size=32, epochs=10)\n",
    "\n",
    "model.fit(dataset_of_samples_and_labels, epochs=10)\n",
    "\n",
    "# Get the data as Numpy arrays\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Build a simple model\n",
    "inputs = keras.Input(shape=(28, 28))\n",
    "x = layers.experimental.preprocessing.Rescaling(1.0 / 255)(inputs)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "\n",
    "# Train the model for 1 epoch from Numpy data\n",
    "batch_size = 64\n",
    "print(\"Fit on NumPy data\")\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=1)\n",
    "\n",
    "# Train the model for 1 epoch using a dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "print(\"Fit on Dataset\")\n",
    "history = model.fit(dataset, epochs=1)\n",
    "\n",
    "print(history.history)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    ")\n",
    "history = model.fit(dataset, epochs=1)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "history = model.fit(dataset, epochs=1, validation_data=val_dataset)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='path/to/my/model_{epoch}',\n",
    "        save_freq='epoch')\n",
    "]\n",
    "model.fit(dataset, epochs=2, callbacks=callbacks)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "]\n",
    "model.fit(dataset, epochs=2, callbacks=callbacks)\n",
    "\n",
    "# writes to: tensorboard --logdir=./logs\n",
    "\n",
    "loss, acc = model.evaluate(val_dataset)  # returns loss and metrics\n",
    "print(\"loss: %.2f\" % loss)\n",
    "print(\"acc: %.2f\" % acc)\n",
    "\n",
    "predictions = model.predict(val_dataset)\n",
    "print(predictions.shape)\n",
    "\n",
    "# end-to-end example: https://keras.io/examples/nlp/text_classification_from_scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
