{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 576 characters, 7 unique.\n",
      "----\n",
      "aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb cccccc dddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeeeeaaaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeeeeaaaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbb ccccc dddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbbdd eeeee fffff aaaaa bbbbb\n",
      "----\n",
      "iter 0, loss: 0.273529, smooth loss: 9.720095\n",
      "----\n",
      "bb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeee fffff aaaaa bbbbb cccccc dddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc dddddd eeee affff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff a\n",
      "----\n",
      "iter 1000, loss: 0.029051, smooth loss: 3.660354\n",
      "----\n",
      "d eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb bccc dddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc edd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee ffffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc \n",
      "----\n",
      "iter 2000, loss: 0.017795, smooth loss: 1.361205\n",
      "----\n",
      "aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeee fffff aaaaa fbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee\n",
      "----\n",
      "iter 3000, loss: 0.008190, smooth loss: 0.512257\n",
      "----\n",
      "ccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa b\n",
      "----\n",
      "iter 4000, loss: 0.016402, smooth loss: 0.198052\n",
      "----\n",
      "d eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbb ccccc dddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeedffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc\n",
      "----\n",
      "iter 5000, loss: 0.007805, smooth loss: 0.080976\n",
      "----\n",
      "aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc dddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb cccc ddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee f\n",
      "----\n",
      "iter 6000, loss: 0.005282, smooth loss: 0.036991\n",
      "----\n",
      "ccc ddddd eeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb cccc ddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbb ccccc dddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa dbbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eceee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbb\n",
      "----\n",
      "iter 7000, loss: 0.011728, smooth loss: 0.020055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "e fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ecccc dddddd eeeee fffff aaaaa bbbbb ccccc dddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc dddd\n",
      "----\n",
      "iter 8000, loss: 0.008485, smooth loss: 0.013232\n",
      "----\n",
      "aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc dddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeee\n",
      "----\n",
      "iter 9000, loss: 0.003720, smooth loss: 0.010097\n",
      "----\n",
      "ccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bbbbb ccccc ddddd eeeee fffff aaaaa bb\n",
      "----\n",
      "iter 10000, loss: 0.008963, smooth loss: 0.008624\n"
     ]
    }
   ],
   "source": [
    "# GRU with Minimal Gated Units\n",
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "# Seed random\n",
    "np.random.seed(0)\n",
    "\n",
    "# Read data and setup maps for integer encoding and decoding.\n",
    "data = open('input.txt', 'r').read()\n",
    "chars = sorted(list(set(data))) # Sort makes model predictable (if seeded).\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) } # dict from enumerate: char and index\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) } # dict from enumerate: index and char\n",
    "\n",
    "# Activation functions\n",
    "# NOTE: Derivatives are calculated using outcomes of their primitives (which are already calculated during forward prop).\n",
    "def sigmoid(input, deriv=False):\n",
    "    if deriv:\n",
    "        return input*(1-input) # sig' = sig(1-sig) - only if claculated in forward prop already!\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def tanh(input, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - input ** 2 # tanh'=1-tanh**2\n",
    "    else:\n",
    "        return np.tanh(input)\n",
    "\n",
    "# Derivative is directly calculated in backprop (in combination with cross-entropy loss function).\n",
    "def softmax(input):\n",
    "    # Subtraction of max value improves numerical stability.\n",
    "    e_input = np.exp(input - np.max(input))\n",
    "    return e_input / e_input.sum()\n",
    "\n",
    "# Hyper parameters\n",
    "N, h_size, o_size = vocab_size, vocab_size, vocab_size # Hidden size is set to vocab_size, assuming that level of abstractness is approximately proportional to vocab_size (but can be set to any other value).\n",
    "seq_length = 5 # Longer sequence lengths allow for lengthier latent dependencies to be trained.\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# Model parameter initialization\n",
    "if os.path.isfile(\"gru_weights.data\"):\n",
    "    initfile = open(\"gru_weights.data\", \"rb\")\n",
    "    init_data = np.load(initfile)\n",
    "    Wy = init_data['Wy']\n",
    "    Wh = init_data['Wh']\n",
    "    Wr = init_data['Wr']\n",
    "    Wz = init_data['Wz']\n",
    "    Uh = init_data['Uh']\n",
    "    Ur = init_data['Ur']\n",
    "    Uz = init_data['Uz']\n",
    "    by = init_data['by']\n",
    "    bh = init_data['bh']\n",
    "    br = init_data['br']\n",
    "    bz = init_data['bz']\n",
    "    initfile.close()\n",
    "else:\n",
    "    Wz = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "    Uz = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "    bz = np.zeros((h_size, 1))\n",
    "\n",
    "    Wr = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "    Ur = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "    br = np.zeros((h_size, 1))\n",
    "\n",
    "    Wh = np.random.rand(h_size, N) * 0.1 - 0.05\n",
    "    Uh = np.random.rand(h_size, h_size) * 0.1 - 0.05\n",
    "    bh = np.zeros((h_size, 1))\n",
    "\n",
    "    Wy = np.random.rand(o_size, h_size) * 0.1 - 0.05\n",
    "    by = np.zeros((o_size, 1))\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    # Initialize variables\n",
    "    x, z, r, h_hat, h, y, p = {}, {}, {}, {}, {-1: hprev}, {}, {} # Dictionaries contain variables for each timestep.\n",
    "    sequence_loss = 0\n",
    "\n",
    "    # Forward prop\n",
    "    for t in range(len(inputs)):\n",
    "        # Set up one-hot encoded input\n",
    "        x[t] = np.zeros((vocab_size, 1)) # for each time step t a one-hot over vocabulary size - init with all 0\n",
    "        x[t][inputs[t]] = 1 # set t'th input to one (current word in one-hot)\n",
    "        \n",
    "        # Calculate update and reset gates\n",
    "        z[t] = sigmoid(np.dot(Wz, x[t]) + np.dot(Uz, h[t-1]) + bz) # update gate\n",
    "        r[t] = sigmoid(np.dot(Wr, x[t]) + np.dot(Ur, h[t-1]) + br) # reset gate\n",
    "        \n",
    "        # Calculate hidden units\n",
    "        h_hat[t] = tanh(np.dot(Wh, x[t]) + np.dot(Uh, np.multiply(r[t], h[t-1])) + bh)\n",
    "        h[t] = np.multiply(z[t], h[t-1]) + np.multiply((1 - z[t]), h_hat[t]) # sometimes denoted s[t]\n",
    "        \n",
    "        # Regular output unit\n",
    "        y[t] = np.dot(Wy, h[t]) + by\n",
    "        \n",
    "        # Probability distribution\n",
    "        p[t] = softmax(y[t])\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        # targets[t]'s entry of p[t]: since the output is 0 or 1 only one entry contributes (and yj*log(pj) becomes just log(pj))\n",
    "        #loss = -np.sum(np.log(p[t][targets[t]])) # dict p: or time t (=one-hot position), whats the corresponsing target value?\n",
    "        loss = -np.log(p[t][targets[t]])\n",
    "        sequence_loss += loss\n",
    "\n",
    "    # Parameter gradient initialization\n",
    "    dWy, dWh, dWr, dWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n",
    "    dUh, dUr, dUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n",
    "    dby, dbh, dbr, dbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n",
    "    dhnext = np.zeros_like(h[0])\n",
    "    \n",
    "    # Backward prop\n",
    "    for t in reversed(range(len(inputs))): # index t counting down\n",
    "        # âˆ‚loss/âˆ‚y\n",
    "        dy = np.copy(p[t]) # copy output\n",
    "        dy[targets[t]] -= 1 # the current target (truth) is 1 for the current t (an 0 for all other t's)\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wy and âˆ‚loss/âˆ‚by\n",
    "        dWy += np.dot(dy, h[t].T) # weight updates: Wy -> Wy - etha * d loss / dWy -> dWy += etha * d loss / dWy\n",
    "        dby += dy # weight for bias is just 1\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        dh = np.dot(Wy.T, dy) + dhnext\n",
    "        dh_hat = np.multiply(dh, (1 - z[t]))\n",
    "        dh_hat_l = dh_hat * tanh(h_hat[t], deriv=True) # = Wy*dy*(1-zt) * (1-tanh**2(h_hat))\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wh, âˆ‚loss/âˆ‚Uh and âˆ‚loss/âˆ‚bh\n",
    "        dWh += np.dot(dh_hat_l, x[t].T) # Wy*dy*(1-zt) * (1-tanh**2(h_hat)) * xt\n",
    "        dUh += np.dot(dh_hat_l, np.multiply(r[t], h[t-1]).T) # Wy*dy*(1-zt) * (1-tanh**2(h_hat)) * (rt*ht-1)\n",
    "        dbh += dh_hat_l # weight for bias is just 1\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        drhp = np.dot(Uh.T, dh_hat_l) # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat))\n",
    "        dr = np.multiply(drhp, h[t-1]) # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat)) * ht-1\n",
    "        dr_l = dr * sigmoid(r[t], deriv=True) # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat))*ht-1*sig(rt)*(1-sig(rt))\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wr, âˆ‚loss/âˆ‚Ur and âˆ‚loss/âˆ‚br\n",
    "        dWr += np.dot(dr_l, x[t].T) # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat))*sig(rt)*(1-sig(rt))*xt\n",
    "        dUr += np.dot(dr_l, h[t-1].T) # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat))*sig(rt)*(1-sig(rt))*ht-1\n",
    "        dbr += dr_l # # Uh * Wy*dy*(1-zt) * (1-tanh**2(h_hat))*sig(rt)*(1-sig(rt))\n",
    "        \n",
    "        # Intermediary derivatives\n",
    "        dz = np.multiply(dh, h[t-1] - h_hat[t]) # dh * (ht-1 - h_hatt)\n",
    "        dz_l = dz * sigmoid(z[t], deriv=True) # dz * sig(zt)*(1-sig(zt))\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚Wz, âˆ‚loss/âˆ‚Uz and âˆ‚loss/âˆ‚bz\n",
    "        dWz += np.dot(dz_l, x[t].T) # dz * sig(zt)*(1-sig(zt))*xt\n",
    "        dUz += np.dot(dz_l, h[t-1].T) # # dz * sig(zt)*(1-sig(zt))*ht-1\n",
    "        dbz += dz_l # # dz * sig(zt)*(1-sig(zt))\n",
    "        \n",
    "        # All influences of previous layer to loss\n",
    "        dh_fz_inner = np.dot(Uz.T, dz_l) # Uz * dz * sig(zt)*(1-sig(zt))\n",
    "        dh_fz = np.multiply(dh, z[t]) # dh * zt\n",
    "        dh_fhh = np.multiply(drhp, r[t]) # Uh * Wy*dy*(1-zt) * (1-tanh**2(Wy*dy*(1-zt))) * rt\n",
    "        dh_fr = np.dot(Ur.T, dr_l) # Ur * Uh*Wy*dy*(1-zt)*ht-1*sig(rt)*(1-sig(rt))\n",
    "        \n",
    "        # âˆ‚loss/âˆ‚hð‘¡â‚‹â‚\n",
    "        dhnext = dh_fz_inner + dh_fz + dh_fhh + dh_fr\n",
    "\n",
    "    return sequence_loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, h[len(inputs) - 1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    # Initialize first word of sample ('seed') as one-hot encoded vector.\n",
    "    x = np.zeros((vocab_size, 1)) # np.zeros(shape) -> (vocab_size, 1) = (rows, cols)\n",
    "    x[seed_ix] = 1\n",
    "    ixes = [seed_ix]\n",
    "    \n",
    "    for t in range(n):\n",
    "        # Calculate update and reset gates\n",
    "        z = sigmoid(np.dot(Wz, x) + np.dot(Uz, h) + bz)\n",
    "        r = sigmoid(np.dot(Wr, x) + np.dot(Ur, h) + br)\n",
    "        \n",
    "        # Calculate hidden units\n",
    "        h_hat = tanh(np.dot(Wh, x) + np.dot(Uh, np.multiply(r, h)) + bh)\n",
    "        h = np.multiply(z, h) + np.multiply((1 - z), h_hat)\n",
    "        \n",
    "        # Regular output unit\n",
    "        y = np.dot(Wy, h) + by\n",
    "        \n",
    "        # Probability distribution\n",
    "        p = softmax(y)\n",
    "\n",
    "        # Choose next char according to the distribution\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel()) # ravel returns flattened array, P are the probabilities for choice\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    \n",
    "    return ixes\n",
    "\n",
    "# Initialize sampling parameters and memory gradients (for adagrad)\n",
    "n, p = 0, 0\n",
    "mdWy, mdWh, mdWr, mdWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n",
    "mdUh, mdUr, mdUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n",
    "mdby, mdbh, mdbr, mdbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length\n",
    "\n",
    "print_interval = 1000\n",
    "n_runs = 10000\n",
    "\n",
    "while True:\n",
    "    # Reset memory if appropriate\n",
    "    if p + seq_length + 1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((h_size, 1))\n",
    "        p = 0 # current position\n",
    "    \n",
    "    # Get input and target sequence - each an index list of characters\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]] # current position to sequence length\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]] # current position+1 to sequence length+1 (next state)\n",
    "\n",
    "    # Occasionally sample from model and print result\n",
    "    if n % print_interval == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 1000)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n%s\\n----' % (txt, ))\n",
    "\n",
    "    # Get gradients for current model based on input and target sequences\n",
    "    loss, dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    # Occasionally print loss information\n",
    "    if n % print_interval == 0:\n",
    "        print('iter %d, loss: %f, smooth loss: %f' % (n, loss, smooth_loss))\n",
    "\n",
    "    # Update model with adagrad (stochastic) gradient descent\n",
    "    for param, dparam, mem in zip([Wy,  Wh,  Wr,  Wz,  Uh,  Ur,  Uz,  by,  bh,  br,  bz], # zip combines iterables\n",
    "                                  [dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz],\n",
    "                                  [mdWy,mdWh,mdWr,mdWz,mdUh,mdUr,mdUz,mdby,mdbh,mdbr,mdbz]):\n",
    "        np.clip(dparam, -5, 5, out=dparam) # limit values of array - here the changes (dparam has the dWy,...)\n",
    "        mem += dparam * dparam # store the squared change (gradient)\n",
    "        # now actually change the model parameters\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # Small added term for numerical stability\n",
    "\n",
    "    # Prepare for next iteration\n",
    "    p += seq_length\n",
    "    n += 1\n",
    "    \n",
    "    # check exit\n",
    "    if n>n_runs:\n",
    "        break\n",
    "        \n",
    "# save weights\n",
    "outfile = open(\"gru_weights.data\", \"wb\")\n",
    "outdata = dict()\n",
    "outdata['Wy'] = Wy\n",
    "outdata['Wh'] = Wh\n",
    "outdata['Wr'] = Wr\n",
    "outdata['Wz'] = Wz\n",
    "outdata['Uh'] = Uh\n",
    "outdata['Ur'] = Ur\n",
    "outdata['Uz'] = Uz\n",
    "outdata['by'] = by\n",
    "outdata['bh'] = bh\n",
    "outdata['br'] = br\n",
    "outdata['bz'] = bz\n",
    "np.savez(outfile, **outdata)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
