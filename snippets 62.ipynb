{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c96eef5e-5e80-404c-844a-7e71228a176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba176aec-b335-4276-aaae-41fcb604d120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def gcd(u, v):\n",
    "    if v != 0:\n",
    "        (u, v) = (v, u % v)\n",
    "    return abs(u)\n",
    "\n",
    "gcd(144, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "969101c4-bd6b-4194-b149-8afb96e0ad32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demo NN\n",
    "import numpy as np\n",
    "\n",
    "x0 = np.array([[1], [0]])\n",
    "w0 = np.array([[0, 1]])\n",
    "\n",
    "y = x0 * w0\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e0715f8-658a-49fb-92e4-7fa109fdc688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAIQUlEQVR4nO3acYjfdR3H8ffd3I+53FyFrrbd2lgrl/thSPOflBu7nOwkqNiQnCdjIYIhHXoQujgqUNjkhOxvG/0nLMpgKDOorvDkcnk4KNlw4RoupPHD1rVsd7/79UfxgvC/uu23/D0e8P3jx/f7+/D+/PXk8/t9+zqdTqcAoKr6uz0AAFcPUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQDimm4P8D85d67bE8DltWZNtyegxzgpABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIhCj9i+e3eNjo8v2nr7RkfrS/v3L9p6wNVBFAAIUegB+0ZHa/KVV+p7zz5bfWvXVt/atfXW2bP1+1OnanhkpK7bvLlW33JLjTz8cJ1vtfK9Hx09Ws2hobp206b66M031xfuuaf+dvFifXtion545Ej99NixrPfLqaku7hBYLH2dTqfT7SH+a+fOdXuC/wt/uXChdt13X2296ab67thYVVW12+367M6d9cC999b9e/bU3997r775xBM1Pz9fPz9ypP70zju1/rbb6tCBA/XlXbvqr7Oz9evp6bp/z56qqvrao4/WhdnZOvz001VV9ZFVq6rRaHRtjx9Ya9Z0ewJ6zDXdHoDL7/qVK6vRaNTyZcvqYzfeWFVV4089Vbc2m/XkY4/luR9MTNTAtm116vTpmr14sebn5+srw8P1iXXrqqqquWVLnr122bL6x6VLWQ/4YBCFHvXbEyfqF1NTdd3mze+7d/rMmdo5OFhDt99ezaGhumtwsHYODtbuu++uD69adeWHBa4YUehRC51OffHOO+vg44+/797HV6+uJUuW1M+ee66mjh+vlyYn6/uHD9eBgwdr+ujR2rh+fRcmBq4EfzT3iMbSpdVeWMjnW7durd+dPFkbBgbqkxs3/sf1oeXLq6qqr6+vPr9tW31nbKxmjh2rxtKl9ZMXX/zXeo1GtdvtruwFuHxEoUdsGBio6ZmZeuvs2TrfatXX9+2r1rvv1lcfeqh+MzNTfzhzpl6anKz9jzxS7Xa7pl97rZ585pk6/vrr9ce3364fv/BC/bnVqi3//rlpw7p1deKNN+rkm2/W+Var5ubmurxDYDGIQo8Ye/DBWtLfX5/Zvr1uaDbr0txcvfz889VeWKi79u6trTt21DfGx+v6FSuqv7+/Vq5YUb+anq7hkZH61B131LcOHaqJ8fHatWNHVVU9sHdvfXrTpvrc8HDd0GzWy6++2uUdAovBK6lwNfNKKleYkwIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABB9nU6n0+0hALg6OCkAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBA/BP0hfbEeediDAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "x=y=0.1\n",
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(111, aspect='equal')\n",
    "patch= ax1.add_patch(patches.Rectangle((x, y), 0.5, 0.5, alpha=0.1,facecolor='red',label='Label'))\n",
    "\n",
    "centerx = x + 0.5/2 - 0.05\n",
    "centery = y + 0.5/2\n",
    "\n",
    "plt.axis('off')\n",
    "plt.text(centerx, centery,'test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8fa9c2-93c6-4d6f-9acc-abcc239798bc",
   "metadata": {},
   "source": [
    "## Simple NN Demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60e07701-e539-4768-9175-7db3d10fa856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight --- w1 ---- h1\n",
    "#        \\ /-----w2-/  \\ w5 - output:\n",
    "#        / \\-----w3-\\  / w6 - gender\n",
    "# height --- w4 ---- h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "181e862f-c065-4f80-93ad-5c497650b03d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.147\n",
      "Epoch 20 loss: 0.047\n",
      "Epoch 40 loss: 0.028\n",
      "Epoch 60 loss: 0.020\n",
      "Epoch 80 loss: 0.015\n",
      "Epoch 100 loss: 0.012\n",
      "Epoch 120 loss: 0.010\n",
      "Epoch 140 loss: 0.008\n",
      "Epoch 160 loss: 0.007\n",
      "Epoch 180 loss: 0.006\n",
      "execution time: 13.095617294311523 ms\n"
     ]
    }
   ],
   "source": [
    "# https://victorzhou.com/blog/intro-to-neural-networks/\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def sigmoid(x):\n",
    "  # Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "  # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))\n",
    "  fx = sigmoid(x)\n",
    "  return fx * (1 - fx)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "  # y_true and y_pred are numpy arrays of the same length.\n",
    "  return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "  '''\n",
    "  A neural network with:\n",
    "    - 2 inputs\n",
    "    - a hidden layer with 2 neurons (h1, h2)\n",
    "    - an output layer with 1 neuron (o1)\n",
    "\n",
    "  *** DISCLAIMER ***:\n",
    "  The code below is intended to be simple and educational, NOT optimal.\n",
    "  Real neural net code looks nothing like this. DO NOT use this code.\n",
    "  Instead, read/run it to understand how this specific network works.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    # Weights\n",
    "    self.w1 = np.random.normal()\n",
    "    self.w2 = np.random.normal()\n",
    "    self.w3 = np.random.normal()\n",
    "    self.w4 = np.random.normal()\n",
    "    self.w5 = np.random.normal()\n",
    "    self.w6 = np.random.normal()\n",
    "\n",
    "    # Biases\n",
    "    self.b1 = np.random.normal()\n",
    "    self.b2 = np.random.normal()\n",
    "    self.b3 = np.random.normal()\n",
    "\n",
    "  def feedforward(self, x):\n",
    "    # x is a numpy array with 2 elements.\n",
    "    h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "    h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "    o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "    return o1\n",
    "\n",
    "  def train(self, data, all_y_trues):\n",
    "    '''\n",
    "    - data is a (n x 2) numpy array, n = # of samples in the dataset.\n",
    "    - all_y_trues is a numpy array with n elements.\n",
    "      Elements in all_y_trues correspond to those in data.\n",
    "    '''\n",
    "    learn_rate = 0.15\n",
    "    epochs = 200 # number of times to loop through the entire dataset\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for x, y_true in zip(data, all_y_trues):\n",
    "        # --- Do a feedforward (we'll need these values later)\n",
    "        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "        h1 = sigmoid(sum_h1)\n",
    "\n",
    "        sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "        h2 = sigmoid(sum_h2)\n",
    "\n",
    "        sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "        o1 = sigmoid(sum_o1)\n",
    "        y_pred = o1\n",
    "\n",
    "        # --- Calculate partial derivatives.\n",
    "        # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
    "        d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "\n",
    "        # Neuron o1\n",
    "        d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
    "\n",
    "        d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
    "\n",
    "        # Neuron h1\n",
    "        d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
    "        d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
    "        d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "\n",
    "        # Neuron h2\n",
    "        d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
    "        d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
    "        d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
    "\n",
    "        # --- Update weights and biases\n",
    "        # Neuron h1\n",
    "        self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
    "        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
    "        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
    "\n",
    "        # Neuron h2\n",
    "        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
    "        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
    "        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
    "\n",
    "        # Neuron o1\n",
    "        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
    "        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
    "        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
    "\n",
    "      # --- Calculate total loss at the end of each epoch\n",
    "      if epoch % 20 == 0:\n",
    "        y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "        loss = mse_loss(all_y_trues, y_preds)\n",
    "        print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
    "\n",
    "# training dataset\n",
    "data = np.array([\n",
    "  [-2, -1],  # Alice\n",
    "  [25, 6],   # Bob\n",
    "  [17, 4],   # Charlie\n",
    "  [-15, -6], # Diana\n",
    "])\n",
    "\n",
    "all_y_trues = np.array([\n",
    "  1, # Alice\n",
    "  0, # Bob\n",
    "  0, # Charlie\n",
    "  1, # Diana\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "# Train our neural network!\n",
    "network = OurNeuralNetwork()\n",
    "network.train(data, all_y_trues)\n",
    "end = time.time()\n",
    "print(f\"execution time: {(end-start) * 10**3} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d05a6ad8-05cd-4625-a5d6-28eecf3684e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 0.0056283013651685886\n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "# test data\n",
    "testdata = np.array([\n",
    "  [-3, -1],  # Rachel\n",
    "  [21, 5],   # John\n",
    "])\n",
    "\n",
    "test_truth = np.array([\n",
    "  1, # Rachel\n",
    "  0, # John\n",
    "])\n",
    "\n",
    "test_preds = np.apply_along_axis(network.feedforward, 1, testdata)\n",
    "loss = mse_loss(test_truth, test_preds)\n",
    "print(f\"validation loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "96c31a97-ad99-4b93-968f-5d3cb258abb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emily: 0.936\n",
      "Frank: 0.075\n"
     ]
    }
   ],
   "source": [
    "# Make some predictions\n",
    "emily = np.array([-7, -3]) # 128 pounds, 63 inches\n",
    "frank = np.array([20, 2])  # 155 pounds, 68 inches\n",
    "print(\"Emily: %.3f\" % network.feedforward(emily)) # 0.951 - F\n",
    "print(\"Frank: %.3f\" % network.feedforward(frank)) # 0.039 - M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f003e843-e16d-4645-844e-6e8eb4ecf241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to prevent overfitting: https://programming-review.com/machine-learning/overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "57a16311-5b94-4475-a2ed-0c56713d840f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.213\n",
      "Epoch 20 loss: 0.045\n",
      "Epoch 40 loss: 0.032\n",
      "Epoch 60 loss: 0.024\n",
      "Epoch 80 loss: 0.019\n",
      "Epoch 100 loss: 0.016\n",
      "Epoch 120 loss: 0.014\n",
      "Epoch 140 loss: 0.012\n",
      "Epoch 160 loss: 0.010\n",
      "Epoch 180 loss: 0.009\n",
      "execution time: 36.1020565032959 ms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def sigmoid(x):\n",
    "  # Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "  # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))\n",
    "  fx = sigmoid(x)\n",
    "  return fx * (1 - fx)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "  # y_true and y_pred are numpy arrays of the same length.\n",
    "  return ((y_true - y_pred[:,0,0]) ** 2).mean()\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "  '''\n",
    "  A neural network with:\n",
    "    - 2 inputs\n",
    "    - a hidden layer with 2 neurons (h1, h2)\n",
    "    - an output layer with 1 neuron (o1)\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    # Weights\n",
    "    self.w0 = np.random.randn(2, 2)\n",
    "    self.w1 = np.random.randn(2, 1)\n",
    "\n",
    "    # Biases\n",
    "    self.b0 = np.random.randn(1, 2)\n",
    "    self.b1 = np.random.randn(1, 1)\n",
    "\n",
    "  def feedforward(self, x):\n",
    "      # x is a numpy array with 2 elements#\n",
    "      h0 = sigmoid(np.dot(x, self.w0) + self.b0)\n",
    "      o0 = sigmoid(np.dot(h0, self.w1) + self.b1)\n",
    "      return o0\n",
    "\n",
    "  def train(self, data, all_y_trues):\n",
    "      '''\n",
    "      - data is a (n x 2) numpy array, n = # of samples in the dataset.\n",
    "      - all_y_trues is a numpy array with n elements.\n",
    "      Elements in all_y_trues correspond to those in data.\n",
    "      '''\n",
    "      learn_rate = 0.15\n",
    "      epochs = 200 # number of times to loop through the entire dataset\n",
    "      \n",
    "      for epoch in range(epochs):\n",
    "          for x, y_true in zip(data, all_y_trues):\n",
    "              # --- Do a feedforward (we'll need these values later)\n",
    "              y_pred = self.feedforward(x)\n",
    "\n",
    "              # backpropagation\n",
    "              d_L_d_ypred = -2 * (y_true - y_pred)  # (1,2)\n",
    "              h0pre = np.dot(x, self.w0) + self.b0\n",
    "              h0 = sigmoid(h0pre)\n",
    "              dL_b1 = d_L_d_ypred * deriv_sigmoid(np.dot(h0, self.w1) + self.b1)\n",
    "              dL_w1 = dL_b1 * h0.T\n",
    "              dL_b0 = dL_b1 * np.dot(deriv_sigmoid(h0pre), self.w1)  # (1,2)\n",
    "              dL_w0 = dL_b0 * x\n",
    "              \n",
    "              # --- Update weights and biases\n",
    "              self.w1 -= learn_rate * dL_w1\n",
    "              self.b1 -= learn_rate * dL_b1\n",
    "              self.w0 -= learn_rate * dL_w0\n",
    "              self.b0 -= learn_rate * dL_b0\n",
    "          # --- Calculate total loss at the end of each epoch\n",
    "          if epoch % 20 == 0:\n",
    "              y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "              loss = mse_loss(all_y_trues, y_preds)\n",
    "              print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
    "\n",
    "# training dataset\n",
    "data = np.array([\n",
    "  [-2, -1],  # Alice\n",
    "  [25, 6],   # Bob\n",
    "  [17, 4],   # Charlie\n",
    "  [-15, -6], # Diana\n",
    "])\n",
    "\n",
    "all_y_trues = np.array([\n",
    "  1, # Alice\n",
    "  0, # Bob\n",
    "  0, # Charlie\n",
    "  1, # Diana\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "# Train our neural network!\n",
    "network = OurNeuralNetwork()\n",
    "network.train(data, all_y_trues)\n",
    "end = time.time()\n",
    "print(f\"execution time: {(end-start) * 10**3} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "234419db-5ce5-497a-baa8-b5c69c73ecfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emily: [[0.94764341]]\n",
      "Frank: [[0.08808194]]\n"
     ]
    }
   ],
   "source": [
    "# Make some predictions\n",
    "emily = np.array([-7, -3])\n",
    "frank = np.array([20, 2]) \n",
    "print(f\"Emily: {network.feedforward(emily)}\")\n",
    "print(f\"Frank: {network.feedforward(frank)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "44f93e61-2869-4a6a-90c6-9f2b585cf64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the model-agnostic default `max_length` (=1500) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n"
     ]
    }
   ],
   "source": [
    "# https://www.pragnakalp.com/generate-music-using-metas-musicgen-on-colab/\n",
    "# https://til.simonwillison.net/machinelearning/musicgen\n",
    "# https://huggingface.co/spaces/facebook/MusicGen\n",
    "\n",
    "from transformers import pipeline\n",
    "import scipy\n",
    "\n",
    "synthesiser = pipeline(\"text-to-audio\", \"facebook/musicgen-small\")\n",
    "\n",
    "music = synthesiser(\"dark lo-fi music with an increasing drumbeat\", forward_params={\"do_sample\": True})\n",
    "\n",
    "scipy.io.wavfile.write(\"musicgen_out.wav\", rate=music[\"sampling_rate\"], data=music[\"audio\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a02065-a78b-4839-9d0f-598778203dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
    "import scipy\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")\n",
    "\n",
    "def save(prompt, filename, num_tokens=1503):\n",
    "    inputs = processor(\n",
    "        text=[prompt],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    audio_values = model.generate(**inputs, max_new_tokens=num_tokens)\n",
    "    sampling_rate = model.config.audio_encoder.sampling_rate\n",
    "    scipy.io.wavfile.write(filename, rate=sampling_rate, data=audio_values[0, 0].numpy())\n",
    "\n",
    "save(\"trumpet mariachi frenetic excitement\", \"trumpet_mariachi.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e53f11a5-b8fd-405f-8342-99c70df20cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "975.0"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(5724.69*4 + 26880 - 9300)/74297.99\n",
    "#74297.99*0.5/14\n",
    "#37000/0.4\n",
    "39*40/40*25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e357e0f6-9f35-4207-b4e9-e8ef95a8f5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3] [0]\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/BAAI/bge-m3\n",
    "# test sentence similarity\n",
    "lst = [0, 1, 2, 3]\n",
    "\n",
    "print(lst[1:], lst[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "262a9790-8f3d-4fe5-9807-076b24cb87da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['1000100', '1000100'], ['1000'])\n",
      "100010010001001000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def euclidrhythm(elementsleft, elementsright, lsep=0):\n",
    "    lr = len(elementsright)\n",
    "    ll = len(elementsleft)\n",
    "    if lr < 2 and elementsleft[0] == elementsleft[-1]:\n",
    "        return elementsleft, elementsright\n",
    "    if lr > 1:\n",
    "        elementsleft = [el + elementsright.pop() if idx < lr else el for idx, el in enumerate(elementsleft)]\n",
    "        return euclidrhythm(elementsleft, elementsright, ll-lr+1)\n",
    "    elementsright = [el for el in elementsleft[lsep:]]\n",
    "    elementsleft = [el for el in elementsleft[:lsep]]\n",
    "    return euclidrhythm(elementsleft, elementsright, lr)\n",
    "\n",
    "def E(k, n):\n",
    "    s = [[1] if i < k else [0] for i in range(n)]\n",
    "    d = n - k\n",
    "    n = max(k, d)\n",
    "    k = min(k, d)\n",
    "    z = d\n",
    "    while z > 0 or k > 1:\n",
    "        for i in range(k):\n",
    "            s[i].extend(s[len(s) - 1 - i])\n",
    "        s = s[:-k]\n",
    "        z = z - k\n",
    "        d = n - k\n",
    "        n = max(k, d)\n",
    "        k = min(k, d)\n",
    "    return [item for sublist in s for item in sublist]\n",
    "\n",
    "el = ['1'] * 5\n",
    "er = ['0'] * 13\n",
    "print(euclidrhythm(el, er))\n",
    "print(''.join(map(str, E(5, 18))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25e42e2a-cbc7-44c4-a1fb-ba375596eebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter text to be summarized: Baseline disease severity of patients with Ulcerative Colitis influences rapid symptom relief under filgotinib treatment: post hoc analysis of the phase 2b/3 SELECTION study  Please enter text to be summarized: Background: Filgotinib (FIL) is an oral, once-daily, Janus kinase 1 preferential inhibitor approved in Europe and Japan for the treatment of ulcerative colitis (UC). A recent analysis of SELECTION trial data (NCT02914522) showed rapid and sustained improvements in UC symptoms with FIL 200 mg (FIL200) treatment in patients with moderate-to-severe UC.1 Here we assess symptomatic remission rates over time with FIL200 induction treatment according to baseline UC disease severity (partial Mayo Clinic Score [pMCS]). Methods: SELECTION was a phase 2b/3 randomized, double-blind, placebo-controlled study. Patients aged 18–75 years were randomized (2:2:1) to receive FIL200, FIL 100 mg or placebo once daily for 11 weeks in induction study A (biologic-naive patients) or induction study B (biologicexperienced patients). In this post hoc analysis, proportions of patients with symptomatic remission (Mayo rectal bleeding sub-score of 0 and Mayo stool frequency sub-score of ≤ 1), from days 2 to 15 and weeks 2 to 10 of the induction study, were analysed at each timepoint by baseline pMCS (pMCS ≥7 and pMCS <7 [cut-off previously used for severe and moderate disease, respectively]2). Symptomatic remission rates were compared between the pMCS ≥7 and pMCS <7 groups within the FIL200 and placebo arms using a Cochran–Mantel–Haenszel test adjusted by study randomization stratification factors. Nominal p values <0.05 were considered statistically significant. Results: At day 2, symptomatic remission rates with FIL200 treatment were significantly higher in patients with baseline pMCS <7 than in those with baseline pMCS ≥7 (8.4% vs 1.1%, p=0.009 [induction study A]; 8.8% vs 0.7%, p=0.004 [induction study B]) (Figure A and B). From days 2 to 15, symptomatic remission rates increased in both groups and, except for day 7 for induction study A and day 9 for induction study B, continued to be significantly higher in those with baseline pMCS <7. From week 2, symptomatic remission rates with FIL200 treatment generally continued to increase in both pMCS ≥7 and pMCS <7 groups (Figure C and D). By week 10, symptomatic remission rates with FIL200 treatment were no longer significantly different between those with baseline pMCS <7 and those with baseline pMCS ≥7 (54.8% vs 43.3%, p=0.124 [induction study A]; 39.5% vs 26.4%, p=0.099 [induction study B]). Conclusion: Symptomatic response to FIL200 occurs more rapidly in patients with lower UC disease severity than in those with higher UC disease severity. However, converging response rates over 10 weeks of treatment leads to symptomatic remission regardless of baseline UC disease severity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Filgotinib (FIL) is an oral, once-daily, Janus kinase 1 preferential inhibitor approved in Europe and Japan for the treatment of ulcerative colitis (UC) A recent analysis of SELECTION trial data (NCT02914522) showed rapid and sustained improvements in UC symptoms with FIL 200 mg (FIL200) treatment in patients with moderate-to-severe UC. In this post hoc analysis, proportions of patients with symptomatic remission (Mayo rectal bleeding sub-score of 0 and Mayo stool frequency sub- score of ≤ 1) were analysed at each timepoint by baseline pMCS. Symptomatic response to FIL200 occurs more rapidly in. patients with lower UC disease severity than in those with higher UC. disease severity.'}]\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/facebook/bart-large-cnn?text=Baseline+disease+severity+of+patients+with+Ulcerative+Colitis+influences+rapid+symptom+relief+under%0D%0Afilgotinib+treatment%3A+post+hoc+analysis+of+the+phase+2b%2F3+SELECTION+study%0D%0A%0D%0APlease+enter+text+to+be+summarized%3A+Background%3A+Filgotinib+%28FIL%29+is+an+oral%2C+once-daily%2C+Janus+kinase+1+preferential+inhibitor+approved+in+Europe+and+Japan+for+the+treatment+of+ulcerative+colitis+%28UC%29.+A+recent+analysis+of+SELECTION+trial+data+%28NCT02914522%29+showed+rapid+and+sustained+improvements+in+UC+symptoms+with+FIL+200+mg+%28FIL200%29+treatment+in+patients+with+moderate-to-severe+UC.1+Here+we+assess+symptomatic+remission+rates+over+time+with+FIL200+induction+treatment+according+to+baseline+UC+disease+severity+%28partial+Mayo+Clinic+Score+%5BpMCS%5D%29.+Methods%3A+SELECTION+was+a+phase+2b%2F3+randomized%2C+double-blind%2C+placebo-controlled+study.+Patients+aged+18%E2%80%9375+years+were+randomized+%282%3A2%3A1%29+to+receive+FIL200%2C+FIL+100+mg+or+placebo+once+daily+for+11+weeks+in+induction+study+A+%28biologic-naive+patients%29+or+induction+study+B+%28biologicexperienced+patients%29.+In+this+post+hoc+analysis%2C+proportions+of+patients+with+symptomatic+remission+%28Mayo+rectal+bleeding+sub-score+of+0+and+Mayo+stool+frequency+sub-score+of+%E2%89%A4+1%29%2C+from+days+2+to+15+and+weeks+2+to+10+of+the+induction+study%2C+were+analysed+at+each+timepoint+by+baseline+pMCS+%28pMCS+%E2%89%A57+and+pMCS+%3C7+%5Bcut-off+previously+used+for+severe+and+moderate+disease%2C+respectively%5D2%29.+Symptomatic+remission+rates+were+compared+between+the+pMCS+%E2%89%A57+and+pMCS+%3C7+groups+within+the+FIL200+and+placebo+arms+using+a+Cochran%E2%80%93Mantel%E2%80%93Haenszel+test+adjusted+by+study+randomization+stratification+factors.+Nominal+p+values+%3C0.05+were+considered+statistically+significant.+Results%3A+At+day+2%2C+symptomatic+remission+rates+with+FIL200+treatment+were+significantly+higher+in+patients+with+baseline+pMCS+%3C7+than+in+those+with+baseline+pMCS+%E2%89%A57+%288.4%25+vs+1.1%25%2C+p%3D0.009+%5Binduction+study+A%5D%3B+8.8%25+vs+0.7%25%2C+p%3D0.004+%5Binduction+study+B%5D%29+%28Figure+A+and+B%29.+From+days+2+to+15%2C+symptomatic+remission+rates+increased+in+both+groups+and%2C+except+for+day+7+for+induction+study+A+and+day+9+for+induction+study+B%2C+continued+to+be+significantly+higher+in+those+with+baseline+pMCS+%3C7.+From+week+2%2C+symptomatic+remission+rates+with+FIL200+treatment+generally+continued+to+increase+in+both+pMCS+%E2%89%A57+and+pMCS+%3C7+groups+%28Figure+C+and+D%29.+By+week+10%2C+symptomatic+remission+rates+with+FIL200+treatment+were+no+longer+significantly+different+between+those+with+baseline+pMCS+%3C7+and+those+with+baseline+pMCS+%E2%89%A57+%2854.8%25+vs+43.3%25%2C+p%3D0.124+%5Binduction+study+A%5D%3B+39.5%25+vs+26.4%25%2C+p%3D0.099+%5Binduction+study+B%5D%29.+Conclusion%3A+Symptomatic+response+to+FIL200+occurs+more+rapidly+in+patients+with+lower+UC+disease+severity+than+in+those+with+higher+UC+disease+severity.+However%2C+converging+response+rates+over+10+weeks+of+treatment+leads+to+symptomatic+remission+regardless+of+baseline+UC+disease+severity\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "#ARTICLE = \"\"\" xxxx\n",
    "#\"\"\"\n",
    "ARTICLE = input(\"Please enter text to be summarized:\")\n",
    "print(summarizer(ARTICLE, max_length=200, min_length=130, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9116b1-db7f-40e5-b304-67e2f28f80c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
